{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Benefits of JanusGraph JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions. General JanusGraph Benefits Support for very large graphs. JanusGraph graphs scale with the number of machines in the cluster. Support for very many concurrent transactions and operational graph processing. JanusGraph\u2019s transactional capacity scales with the number of machines in the cluster and answers complex traversal queries on huge graphs in milliseconds. Support for global graph analytics and batch graph processing through the Hadoop framework. Support for geo, numeric range, and full text search for vertices and edges on very large graphs. Native support for the popular property graph data model exposed by Apache TinkerPop . Native support for the graph traversal language Gremlin . Numerous graph-level configurations provide knobs for tuning performance. Vertex-centric indices provide vertex-level querying to alleviate issues with the infamous super node problem . Provides an optimized disk representation to allow for efficient use of storage and speed of access. Open source under the liberal Apache 2 license . Benefits of JanusGraph with Apache Cassandra Continuously available with no single point of failure. No read/write bottlenecks to the graph as there is no master/slave architecture. Elastic scalability allows for the introduction and removal of machines. Caching layer ensures that continuously accessed data is available in memory. Increase the size of the cache by adding more machines to the cluster. Integration with Apache Hadoop . Open source under the liberal Apache 2 license. Benefits of JanusGraph with HBase Tight integration with the Apache Hadoop ecosystem. Native support for strong consistency . Linear scalability with the addition of more machines. Strictly consistent reads and writes. Convenient base classes for backing Hadoop MapReduce jobs with HBase tables. Support for exporting metrics via JMX . Open source under the liberal Apache 2 license. JanusGraph and the CAP Theorem Despite your best efforts, your system will experience enough faults that it will have to make a choice between reducing yield (i.e., stop answering requests) and reducing harvest (i.e., giving answers based on incomplete data). This decision should be based on business requirements. \u2014 Coda Hale When using a database, the CAP theorem should be thoroughly considered (C=Consistency, A=Availability, P=Partition tolerance). JanusGraph is distributed with 3 supporting backends: Apache Cassandra , Apache HBase , and Oracle Berkeley DB Java Edition . Note that BerkeleyDB JE is a non-distributed database and is typically only used with JanusGraph for testing and exploration purposes. HBase gives preference to consistency at the expense of yield, i.e. the probability of completing a request. Cassandra gives preference to availability at the expense of harvest, i.e. the completeness of the answer to the query (data available/complete data).","title":"Introduction"},{"location":"#the-benefits-of-janusgraph","text":"JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions.","title":"The Benefits of JanusGraph"},{"location":"#general-janusgraph-benefits","text":"Support for very large graphs. JanusGraph graphs scale with the number of machines in the cluster. Support for very many concurrent transactions and operational graph processing. JanusGraph\u2019s transactional capacity scales with the number of machines in the cluster and answers complex traversal queries on huge graphs in milliseconds. Support for global graph analytics and batch graph processing through the Hadoop framework. Support for geo, numeric range, and full text search for vertices and edges on very large graphs. Native support for the popular property graph data model exposed by Apache TinkerPop . Native support for the graph traversal language Gremlin . Numerous graph-level configurations provide knobs for tuning performance. Vertex-centric indices provide vertex-level querying to alleviate issues with the infamous super node problem . Provides an optimized disk representation to allow for efficient use of storage and speed of access. Open source under the liberal Apache 2 license .","title":"General JanusGraph Benefits"},{"location":"#benefits-of-janusgraph-with-apache-cassandra","text":"Continuously available with no single point of failure. No read/write bottlenecks to the graph as there is no master/slave architecture. Elastic scalability allows for the introduction and removal of machines. Caching layer ensures that continuously accessed data is available in memory. Increase the size of the cache by adding more machines to the cluster. Integration with Apache Hadoop . Open source under the liberal Apache 2 license.","title":"Benefits of JanusGraph with Apache Cassandra"},{"location":"#benefits-of-janusgraph-with-hbase","text":"Tight integration with the Apache Hadoop ecosystem. Native support for strong consistency . Linear scalability with the addition of more machines. Strictly consistent reads and writes. Convenient base classes for backing Hadoop MapReduce jobs with HBase tables. Support for exporting metrics via JMX . Open source under the liberal Apache 2 license.","title":"Benefits of JanusGraph with HBase"},{"location":"#janusgraph-and-the-cap-theorem","text":"Despite your best efforts, your system will experience enough faults that it will have to make a choice between reducing yield (i.e., stop answering requests) and reducing harvest (i.e., giving answers based on incomplete data). This decision should be based on business requirements. \u2014 Coda Hale When using a database, the CAP theorem should be thoroughly considered (C=Consistency, A=Availability, P=Partition tolerance). JanusGraph is distributed with 3 supporting backends: Apache Cassandra , Apache HBase , and Oracle Berkeley DB Java Edition . Note that BerkeleyDB JE is a non-distributed database and is typically only used with JanusGraph for testing and exploration purposes. HBase gives preference to consistency at the expense of yield, i.e. the probability of completing a request. Cassandra gives preference to availability at the expense of harvest, i.e. the completeness of the answer to the query (data available/complete data).","title":"JanusGraph and the CAP Theorem"},{"location":"changelog/","text":"Changelog Version Compatibility The JanusGraph project is growing along with the rest of the graph and big data ecosystem and utilized storage and indexing backends. Below are version compatibilities between the various versions of components. For dependent backend systems, different minor versions are typically supported as well. It is strongly encouraged to verify version compatibility prior to deploying JanusGraph. Although JanusGraph may be compatible with older and no longer supported versions of its dependencies, users are warned that there are possible risks and security exposures with running software that is no longer supported or updated. Please check with the software providers to understand their supported versions. Users are strongly encouraged to use the latest versions of the software. Version Compatibility Matrix Currently supported All currently supported versions of JanusGraph are listed below. Info You are currently viewing the documentation page of JanusGraph version 1.0.0-rc2. To ensure that the information below is up to date, please double check that this is not an archived version of the documentation. JanusGraph Storage Version Cassandra HBase Bigtable ScyllaDB Elasticsearch Solr TinkerPop Spark Scala 0.6.z 2 3.0.z, 3.11.z 1.6.z, 2.2.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z N/A 6.y, 7.y 7.y, 8.y 3.5.z 3.0.z 2.12.z 1.0.z 2 3.11.z, 4.0.z 2.5.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z 5.y 6.y, 7.y, 8.y 8.y 3.6.z 3.2.z 2.12.z Info Even so ScyllaDB is marked as N/A prior version 1.0.0 it was actually supported using cql storage option. The only difference is that from version 1.0.0 JanusGraph officially supports ScyllaDB using scylla and cql storage options and have extended test coverage for ScyllaDB. End-of-Life The versions of JanusGraph listed below are outdated and will no longer receive bugfixes. JanusGraph Storage Version Cassandra HBase Bigtable ScyllaDB Elasticsearch Solr TinkerPop Spark Scala 0.1.z 1 1.2.z, 2.0.z, 2.1.z 0.98.z, 1.0.z, 1.1.z, 1.2.z 0.9.z, 1.0.0-preZ, 1.0.0 N/A 1.5.z 5.2.z 3.2.z 1.6.z 2.10.z 0.2.z 1 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 0.98.z, 1.0.z, 1.1.z, 1.2.z, 1.3.z 0.9.z, 1.0.0-preZ, 1.0.0 N/A 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.2.z 1.6.z 2.10.z 0.3.z 2 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.0.z, 1.1.z, 1.2.z, 1.3.z, 1.4.z 1.0.0, 1.1.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 N/A 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.3.z 2.2.z 2.11.z 0.4.z 2 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.2.z, 1.3.z, 1.4.z, 2.1.z N/A N/A 5.y, 6.y 7.y 3.4.z 2.2.z 2.11.z 0.5.z 2 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.2.z, 1.3.z, 1.4.z, 2.1.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z N/A 6.y, 7.y 7.y 3.4.z 2.2.z 2.11.z Release Notes Version 1.0.0 (Release Date: ???) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 1.0.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:1.0.0\" Tested Compatibility: Apache Cassandra 3.11.10, 4.0.6 Apache HBase 2.5.0 Oracle BerkeleyJE 7.5.11 ScyllaDB 5.1.4 Elasticsearch 6.0.1, 6.6.0, 7.17.8, 8.6.0 Apache Lucene 8.11.1 Apache Solr 8.11.1 Apache TinkerPop 3.6.2 Java 8, 11 Note Google Bigtable was removed from this list because there is no automatic testing in place specifically for that backend. Since the adapter for Bigtable is however just using the HBase adapter, it is also covered by the tests for HBase. We invite anyone who is interested in the Bigtable storage adapter to help with this by contributing so that the tests for HBase are also automatically executed for Bigtable. More information can be found in this GitHub issue: janusgraph/janusgraph#415 . Installed versions in the Pre-Packaged Distribution: Cassandra 4.0.6 Elasticsearch 7.14.0 Changes For more information on features and bug fixes in 1.0.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/21?closed=1 Assets JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch Upgrade Instructions New index management The index management has received an overhaul which enables proper index removal. The schema action REMOVE_INDEX is no longer available and has been replaced by DISCARD_INDEX . totals for direct index queries now applies provided offset and limit Direct index queries which search count for totals ( vertexTotals , edgeTotals , propertyTotals and direct execution of IndexProvider.totals ) now apply provided limit and offset . Previously provided limit and offset were ignored. For example, previously the following query would return 500 if there were 500 indexed elements: gremlin > graph . indexQuery ( \"textIndex\" , \"v.\\\"text\\\":fooBar\" ). limit ( 10 ). vertexTotals () Now the above query will return 10 elements because we limited the result to 10 elements only. Same applies to offset . Previously the following query would return 500 if there were 500 indexed elements: gremlin > graph . indexQuery ( \"textIndex\" , \"v.\\\"text\\\":fooBar\" ). offset ( 10 ). vertexTotals () Now the above query will return 490 as a result because we skip count of the first 10 elements. offset provided with limit will apply offset first and limit last. For example, the above query will return 10 elements now if there were 500 indexed elements: gremlin > graph . indexQuery ( \"textIndex\" , \"v.\\\"text\\\":fooBar\" ). limit ( 10 ). offset ( 10 ). vertexTotals () The new logic is applied similarly to Direct Index Queries vertexTotals() , edgeTotals() , propertyTotals() as well as internal JanusGraph method IndexProvider.totals . Add support for Java 11 JanusGraph now officially supports Java 11 in addition to Java 8. We encourage everyone to update to Java 11. Note The pre-packaged distribution now requires Java 11. Upgrade of log4j to version 2 This change requires a new log4j configuration. You can find an example configuration in conf/log4j2-server.xml . As a result of the changed configuration format, we clean up all configurations. This could lead to unexpected new log lines. Please open an issue, if you see any unwanted log line. Note Log4j is only used for standalone server deployments and JanusGraph testing. Removal of cassandra-all dependency JanusGraph had a dependency on cassandra-all only for some Hadoop-related classes. We moved these few classes into a new module cassandra-hadoop-util to reduce the dependencies of JanusGraph. If you are running JanusGraph with an embeded Cassandra, you have to exclude the cassandra-hadoop-util from janusgraph-cql . Drop support for HBase 1 We are dropping support for HBase 1. Drop support for Solr 7 We are dropping support for Solr 7. Drop support for Gryo MessageSerializer Support for Gryo MessageSerializer has been dropped in TinkerPop 3.6.0 and we therefore also no longer support it in JanusGraph. GraphBinary is now used as the default MessageSerializer.hb Batch Processing enabled by default Batch processing allows JanusGraph to fetch a batch of vertices from the storage backend together instead of requesting each vertex individually which leads to a high number of backend queries. This was however disabled by default in JanusGraph because these batches could become much larger than what was needed for the traversal and therefore have a negative performance impact for some traversals. That is why an improved batch processing mode was added in JanusGraph 0.6.0 that limits the size of these batches retrieved from the storage backend, called Limited Batch Processing . This mode therefore solves the problem of having potentially unlimited batch sizes. That is why we now enable this mode by default as most users should benefit from this limited batch processing. If you want to continue using JanusGraph without batch processing, then you have to manually disable it by setting query.batch to false . Breaking change for Geoshape GraphBinary serialization Support for the GraphBinary serialization format was added in JanusGraph 0.6.0. This also included support to serialize Geoshapes via GraphBinary. The implementation of the Geoshape serializer was unfortunately closely tied to the Java library Spatial4j that we are using to implement Geoshapes in Java. This made it very complicated to add support for GraphBinary in other languages than Java. To make it easier to support GraphBinary in non-Java environments like .NET, we have completely reimplemented the GraphBinary serialization of Geoshapes in this version. This is a breaking change for users who have already adopted GraphBinary and who are using Geoshapes. It is necessary to update JanusGraph Server and all (Java) clients that use GraphBinary at the same time since JanusGraph Server with an older version will not be able to read a Geoshape created by a client that is already on version 1.0.0 and vice versa. Users who do not use GraphBinary yet or who are not using Geoshapes are not affected by this change. ConfiguredGraphFactory now creates separate indices per graph in Elasticsearch If the ConfiguredGraphFactory is used together with Elasticsearch as the index backend, then the same Elasticsearch index is used for all graphs (if the same index names were used across different graphs). Now it is possible to let JanusGraph create the index names dynamically by using the graph.graphname if no index.[X].index-name is provided in the template configuration. This is exactly like it was already the case for the CQL keyspace name for example. Users who don't want to use this feature can simply continue providing the index name via index.[X].index-name in the template configuration. Remove support for old serialization format of JanusGraph predicates We are dropping support for old serialization format of JanusGraph predicates. The old predicates serialization format is only used by client older than 0.6. The change only affects GraphSON. Mixed index aggregation optimization A new optimization has been added to compute aggregations (min, max, sum and avg) using mixed index engine (if the aggregation function follows an indexed query). If the index backend is Elasticsearch, a double value is used to hold the result. As a result, aggregations on long numbers greater than 2^53 are approximate. In this case, if the accurate result is essential, the optimization can be disabled by removing the strategy JanusGraphMixedIndexAggStrategy : g.traversal().withoutStrategies(JanusGraphMixedIndexAggStrategy.class) . Add support for ElasticSearch 8 JanusGraph now supports ElasticSearch 8. Notice, Mapping.PREFIX_TREE mapping is no longer available for Geoshape mappings using new ElasticSearch 8 indices. Mapping.PREFIX_TREE is still supported in ElasticSearch 6, ElasticSearch 7, Solr, Lucene. For ElasticSearch the new Geoshape mapping was added Mapping.BKD . It's recommended to use Mapping.BKD mapping due to better performance characteristics over Mapping.PREFIX_TREE . The downside of Mapping.BKD is that it doesn't support Circle shapes. Thus, JanusGraph provides BKD Circle processors to convert Circle into other shapes for indexing but use Circle at the storage level. More information about Circle processors available under configuration namespace index.[X].bkd-circle-processor . ElasticSearch 8 doesn't allow creating new indexes with Mapping.PREFIX_TREE mapping, but the existing indices using Mapping.PREFIX_TREE will work in ElasticSearch 8 after migration. See ElasticSearch 8 migration guide . Add support for ScyllaDB driver A new module janusgraph-scylla provides ability to run JanusGraph with ScyllaDB Driver which is an optimized fork version of DataStax Java Driver for ScyllaDB storage backend. For ScyllaDB storage backend you can use either janusgraph-scylla or janusgraph-cql (which is a general CQL storage driver implementation). That said, it's recommended to use janusgraph-scylla for ScyllaDB due to the provided internal optimizations (more about ScyllaDB driver optimizations can be found here ). Notice that janusgraph-cql and janusgraph-scylla are mutually exclusive. Use only one module at a time and never provide both dependencies in the same classpath. See ScyllaDB Storage Backend documentation for more information about how to make scylla storage.backend options available. Disable CQL ExecutorService by default Previously CQL ExecutorService was enabled by default mostly for historical reasons. CQL ExecutorService managed by JanusGraph adds additional overhead without bringing any usefulness other than limiting amount of parallel queries which can (and should) be controlled via the underlying CQL driver. In case previous behaviour is desired then storage.cql.executor-service.enabled configuration option should be set to true , but it's recommended to tune CQL queries parallelism using CQL driver configuration options (like storage.cql.max-requests-per-connection , storage.cql.local-max-connections-per-host ) and / or storage.parallel-backend-ops.* configuration options. Removal of deprecated classes/methods/functionalities Methods JanusGraphIndexQuery.vertices replaced by JanusGraphIndexQuery.vertexStream JanusGraphIndexQuery.edges replaced by JanusGraphIndexQuery.edgeStream JanusGraphIndexQuery.properties replaced by JanusGraphIndexQuery.propertyStream IndexQueryBuilder.vertices replaced by IndexQueryBuilder.vertexStream IndexQueryBuilder.edges replaced by IndexQueryBuilder.edgeStream IndexQueryBuilder.properties replaced by IndexQueryBuilder.propertyStream IndexTransaction.query replaced by IndexTransaction.queryStream Classes/Interfaces EdgeLabelDefinition class PropertyKeyDefinition class RelationTypeDefinition class SchemaContainer class SchemaElementDefinition class SchemaProvider interface VertexLabelDefinition class JanusGraphId class AllEdgesIterable class AllEdgesIterator class ConcurrentLRUCache class PriorityQueue class RemovableRelationIterable class RemovableRelationIterator class ImmutableConfiguration class Version 0.6.3 (Release Date: February 18, 2023) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.3\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.11.0 Apache TinkerPop 3.5.5 Java 1.8 Note Google Bigtable was removed from this list because there is no automatic testing in place specifically for that backend. Since the adapter for Bigtable is however just using the HBase adapter, it is also covered by the tests for HBase. We invite anyone who is interested in the Bigtable storage adapter to help with this by contributing so that the tests for HBase are also automatically executed for Bigtable. More information can be found in this GitHub issue: janusgraph/janusgraph#415 . Changes For more information on features and bug fixes in 0.6.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/24?closed=1 Assets JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch Version 0.6.2 (Release Date: May 31, 2022) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.2\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.9.0 Apache TinkerPop 3.5.3 Java 1.8 Changes For more information on features and bug fixes in 0.6.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/23?closed=1 Assets JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch Version 0.6.1 (Release Date: January 18, 2022) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.1\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.9.0 Apache TinkerPop 3.5.1 Java 1.8 Changes For more information on features and bug fixes in 0.6.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/22?closed=1 Assets JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch Upgrade Instructions GraphManager changed to JanusGraphManager A GraphManager is used to instantiate graph instances. JanusGraph Server has used the DefaultGraphManager from TinkerPop for this by default if no other GraphManager was specified in the JanusGraph Server YAML config file. The behavior of this DefaultGraphManager was changed in TinkerPop 3.5.0 which is included in JanusGraph 0.6.0 in how it parses config values, making it impossible to provide comma separated values, e.g., to specify multiple hostnames for the storage backend. The JanusGraphManager does not have this limitation which is why it is now configured as the GraphManager in the JanusGraph Server config files: [ ... ] channelizer : org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer graphManager : org.janusgraph.graphdb.management.JanusGraphManager graphs : { graph : conf/janusgraph-berkeleyje-es.properties } [ ... ] If you however want to continue using the DefaultGraphManager , then you can simply remove the setting again or change it to the TinkerPop GraphManager that has been the default before: org.apache.tinkerpop.gremlin.server.util.DefaultGraphManager . Version 0.6.0 (Release Date: September 3, 2021) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.0\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.9.0 Apache TinkerPop 3.5.3 Java 1.8 Changes For more information on features and bug fixes in 0.6.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/17?closed=1 Assets JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch Upgrade Instructions Experimental support for Amazon Keyspaces Amazon Keyspaces is a serverless managed Apache Cassandra-compatible database service provided by Amazon. See Deploying on Amazon Keyspaces for more details. Breaking change for Configuration objects Prior to JanusGraph 0.6.0, Configuration objects were from the Apache commons-configuration library. To comply with the TinkerPop change , JanusGraph now uses the commons-configuration2 library. A typical usage of configuration object is to create configuration using ConfigurationGraphFactory . Now you would need to use the new configuration2 library. Please refer to the commons-configuration 2.0 migration guide for details. Note that this very likely does not affect gremlin console usage, since the new library is auto-imported, and the basic APIs remain the same. For java code usage, you need to import configuration2 library rather than the old configuration library. Breaking change for gremlin server configs scriptEvaluationTimeout is renamed to evaluationTimeout . You can refer to conf/gremlin-server/gremlin-server.yaml for example. Breaking change for gremlin EventStrategy usage If you are using EventStrategy , please note that now you need to register it every time you start a new transaction. An example is available at ThreadLocalTxLeakTest::eventListenersCanBeReusedAcrossTx See more background of this breaking change in this pull request . Disable smart-limit by default and change HARD_MAX_LIMIT Prior to 0.6.0, smart-limit is enabled by default. It tries to guess a small limit for each graph centric query (e.g. g.V().has(\"prop\", \"value\") ) internally, and if more results are required by user, it queries backend again with a larger limit, and repeats until either results are exhausted or user stops the query. However, this is not the same as paging mechanism. All interim results will be fetched again in next round, making the whole query costly. Even worse, if your data backend does not return results in a consistent order, then some entries might be missing in the final results. Until JanusGraph can fully utilize the paging capacity provided by backends (e.g. Elasticsearch scroll), this option is recommended to be turned off. The exception is when you have a large number of results but you only need a few of them, then enabling smart-limit can reduce latency and memory usage. An example would be: Iterator<Vertex> iter = graph.traversal().V().has(\"prop\", \"value\"); while (iter.hasNext()) { Vertex v = iter.next(); if (canStop()) break; } Prior to 0.6.0, even if smart-limit is disabled, JanusGraph adds a HARD_MAX_LIMIT that is equivalent to 100,000 to avoid fetching too many results at a time. This limit is now configurable, and by default, it's Integer.MAX_VALUE which can be interpreted as no limit. Add experimental support for Java 11 We started to work on support for Java 11. We would like to get feedback, if everything is working as expected after upgrading to Java 11. Removal of LoggingSchemaMaker The schema.default=logging option is not valid anymore. Use schema.default=default and schema.logging=true options together to make application behaviour unaltered, if you are using LoggingSchemaMaker . Replacing the server startup script is replaced The gremlin-server.sh is placed by janusgraph-server.sh . The janusgraph-server.sh brings some new functionality such as easy configuration of Java options using the jvm.options file. The jvm.options file contains some default configurations for JVM based on Cassandra's JVM configurations, Elasticsearch and the old gremlin-server.sh. Serialization of JanusGraph predicates has changed The serialization of JanusGraph predicates has changed in this version for both GraphSON and Gryo. The newest version of the JanusGraph Driver requires a JanusGraph Server version of 0.6.0 and above. The server includes a fallback for clients with an older driver to make the upgrade to version 0.6.0 easier. This means that the server can be upgraded first without having to update all clients at the same time. The fallback will however be removed in a future version of JanusGraph so clients should also be upgraded. GraphBinary is now supported GraphBinary is a new binary serialization format from TinkerPop that supersedes Gryo and it will eventually also replace GraphSON. GraphBinary is language independent and has a low serialization overhead which results in an improved performance. If you want to use GraphBinary, you have to add following to the gremlin-server.yaml after the keyword serializers . This will add the support on the server site. Note The java driver is the only driver that currently supports GraphBinary, see Connecting to JanusGraph using Java . Note Version 1.0.0 adds a breaking change to GraphBinary for Geoshape serialization, see the 1.0.0 changelog for more information . New index selection algorithm In version 0.6.0, the index selection algorithm has changed. If the number of possible indexes for a query is small enough, the new algorithm will perform an exhaustive search to minimize the number of indexes which need to be queried. The default limit is set to 10. In order to maintain the old selection algorithm regardless of the available indexes, set the key query.index-select-threshold to 0 . For more information, see Configuration Reference Removal of Cassandra Thrift support Thrift will be completely removed in Cassandra 4. All deprecated Cassandra Thrift backends were removed in JanusGraph 0.6.0. We already added support for CQL in JanusGraph 0.2.0 and we have been encouraging users to switch from Thrift to CQL since version 0.2.1. This means that the following backends were removed: cassandrathrift , cassandra , astyanax , and embeddedcassandra . Users who still use one of these Thrift backends should migrate to CQL. Our migration guide explains the necessary steps for this. The option to run Cassandra embedded in the same JVM as JanusGraph is however no longer supported with CQL. Note The source code for the Thrift backends will be moved into a dedicated repository . While we do not support them any more, users can still use them if they for some reason cannot migrate to CQL. Drop support for Cassandra 2 With the release of Cassandra 4, the support of Cassandra 2 will be dropped. Therefore, you should upgrade to Cassandra 3 or higher. Note Cassandra 3 and higher doesn't support compact storage. If you have activated or never changed the value of storage.cql.storage-compact=true , during the upgrade process you have to ensure your data is correctly migrated. Introduction of a JanusGraph Server startup class as a replacement for Gremlin Server startup The gremlin-server.sh and the janusgraph.sh are configured to use the new JanusGraph startup class. This new class introduces a default set of TinkerPop Serializers if no serializers are configured in the gremlin-server.yaml . Furthermore, JanusGraph will log the version of JanusGraph and TinkerPop after a shiny new JanusGraph header. Note If you have a custom script to startup JanusGraph, you propably would like to replace the Gremlin Server class with JanusGraph Server class: org.apache.tinkerpop.gremlin.server.GremlinServer => org.janusgraph.graphdb.server.JanusGraphServer Drop support for Ganglia metrics We are dropping Ganglia as we are using dropwizard for metrics. Dropwizard did drop Ganglia in the newest major version. DataStax cassandra driver upgrade from 3.9.0 to 4.13.0 All DataStax cassandra driver metrics are now disabled by default. To enable DataStax driver metrics you need to provide a list of Session level metrics and / or Node level metrics you want to enable. To provide a list of enabled metrics, you can use the next configuration options: storage.cql.metrics.session-enabled and storage.cql.metrics.node-enabled . Notice, DataStax metrics are enabled only when basic metrics are enabled (i.e. metrics.enabled = true ). See configuration references storage.cql.metrics for additional DataStax metrics configuration. An example configuration which enables some CQL Session level and Node level metrics reporting by JMX: metrics.enabled = true metrics.jmx.enabled = true metrics.jmx.domain = com.datastax.oss.driver metrics.jmx.agentid = agent storage.cql.metrics.session-enabled = bytes-sent,bytes-received,connected-nodes,cql-requests,throttling.delay storage.cql.metrics.node-enabled = pool.open-connections,pool.available-streams,bytes-sent,cql-messages See advanced.metrics.session.enabled and advanced.metrics.node.enabled sections in DataStax Metrics Configuration for a complete list of available Session level and Node level metrics. Due to driver upgrade the next cql configuration options have been removed: local-core-connections-per-host remote-core-connections-per-host local-max-requests-per-connection remote-max-requests-per-connection cluster-name storage.connection-timeout is now used to control initial connection timeout to CQL storage and not request timeouts. Please, use storage.cql.request-timeout to configure request timeouts instead. New cql configuration options should be used for upgrade: max-requests-per-connection session-name storage.cql.local-datacenter is mandatory now and defaults to datacenter1 . See more new cql configuration options in configuration references under storage.cql section. Automatic configurations of dynamic graph binding If the JanusGraphManager is configured, dynamic graph binding will be setup automatically, see Dynamic Graphs . Note Breaking changes in the config of the gremlin-server.yaml . Following, classes are removed and have to be replaced by tinkerpop equivalent: removed class replacement class org.janusgraph.channelizers.JanusGraphWebSocketChannelizer org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer org.janusgraph.channelizers.JanusGraphHttpChannelizer org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer org.janusgraph.channelizers.JanusGraphNioChannelizer org.apache.tinkerpop.gremlin.server.channel.NioChannelizer org.janusgraph.channelizers.JanusGraphWsAndHttpChannelizer org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer Breaking change Lucene and Solr fuzzy predicates The text predicates text.textFuzzy and text.textContainsFuzzy have been updated in both the Lucene and Solr indexing backends to align with JanusGraph and Elastic. These predicates now inspect the query length to determine the Levenshtein distance, where previously they used the backend's default max distance of 2: 0 for strings of one or two characters (exact match) 1 for strings of three, four or five characters 2 for strings of more than five characters Change Matrix: text query previous result new result ah ah true true ah ai true false hop hop true true hop hap true true hop hoop true true hop hooop true false surprises surprises true true surprises surprizes true true surprises surpprises true true surprises surpprisess false false Version 0.5.3 (Release Date: December 24, 2020) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.3\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.2 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 Changes For more information on features and bug fixes in 0.5.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/20?closed=1 Assets JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch Version 0.5.2 (Release Date: May 3, 2020) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.2\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.2 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 For more information on features and bug fixes in 0.5.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/19?closed=1 Upgrade Instructions ElasticSearch index store names cache now enabled for any amount of indexes per store In JanusGraph version 0.5.0 and 0.5.1 all ElasticSearch index store names are cached for efficient index store name retrieval and the cache is disabled if there are more than 50000 indexes available per index store. From JanusGraph version 0.5.2 index store names cache isn't limited to 50000 but instead can be disabled by using a new added parameter enable_index_names_cache . It is still recommended to disable index store names cache if more than 50000 indexes are used per index store. Version 0.5.1 (Release Date: March 25, 2020) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.1\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.1 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 For more information on features and bug fixes in 0.5.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/18?closed=1 Upgrade Instructions Two Distributed package is splitted into two version The default version of the distribution package does no longer contain the janusgraph.sh . This includes a packaged version of cassandra and elasticsearch. If you want to have janusgraph.sh , you have to download distribution with the suffix -full . Gremlin Server distributed with the release uses inmemory storage backend and no search backend by default Gremlin Server is by default configured for the inmemory storage backend and no search backend when started with bin/gremlin-server.sh . You can provide configuration for another storage backend and/or search backend by providing a path to the appropriate configuration as a second parameter ( ./bin/gremlin-server.sh ./conf/gremlin-server/[...].yaml ). Version 0.5.0 (Release Date: March 10, 2020) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.0\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.1 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 For more information on features and bug fixes in 0.5.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/13?closed=1 Upgrade Instructions Distributed package is renamed The distribution has no longer the suffix -hadoop2 . Reorder dependency of Hadoop Hadoop is now a dependency of supported backends. Therefore, MapReduceIndexJobs is now split up into different classes: Old Function New Function MapReduceIndexJobs.cassandraRepair CassandraMapReduceIndexJobsUtils.repair MapReduceIndexJobs.cassandraRemove CassandraMapReduceIndexJobsUtils.remove MapReduceIndexJobs.cqlRepair CqlMapReduceIndexJobsUtils.repair MapReduceIndexJobs.cqlRemove CqlMapReduceIndexJobsUtils.remove MapReduceIndexJobs.hbaseRepair HBaseMapReduceIndexJobsUtils.repair MapReduceIndexJobs.hbaseRemove HBaseMapReduceIndexJobsUtils.remove Note Now, you can easily support for any backend. Warning Cassandra3InputFormat is replaced by CqlInputFormat ElasticSearch: Upgrade from 6.6.0 to 7.6.1 and drop support for 5.x version The ElasticSearch version has been changed to 7.6.1 which removes support for max-retry-timeout option. That is why this option no longer available in JanusGraph. Users should be aware that by default JanusGraph setups maximum open scroll contexts to maximum value of 2147483647 with the parameter setup-max-open-scroll-contexts for ElasticSearch 7.y. This option can be disabled and updated manually in ElasticSearch but you should be aware that ElasticSearch starting from version 7 has a default limit of 500 opened contexts which most likely be reached by the normal usage of JanusGraph with ElasticSearch. By default deprecated mappings are disabled in ElasticSearch version 7. If you are upgrading your ElasticSearch index backend to version 7 from lower versions, it is recommended to reindex your JanusGraph indices to not use mappings. If you are unable to reindex your indices you may setup parameter use-mapping-for-es7 to true which will tell JanusGraph to use mapping types for ElasticSearch version 7. Due to the drop of support for 5.x version, deprecated multi-type indices are no more supported. Parameter use-deprecated-multitype-index is no more supported by JanusGraph. BerkeleyDB BerkeleyDB storage configured with SHARED_CACHE for better memory usage. Default logging location has changed If you are using janusgraph.sh to start your instance, the default logging has been changed from log to logs Version 0.4.1 (Release Date: January 14, 2020) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.4.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.4.1\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 5.6.14, 6.0.1, 6.6.0 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.4 Java 1.8 For more information on features and bug fixes in 0.4.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/15?closed=1 Upgrade Instructions TinkerPop: Upgrade from 3.4.1 to 3.4.4 Adding multiple values in the same query to a new vertex property without explicitly defined type (i.e. using Automatic Schema Maker to create a property type) requires explicit usage of VertexProperty.Cardinality for each call (only for the first query which defines a property) if the VertexProperty.Cardinality is different than VertexProperty.Cardinality.single . Version 0.4.0 (Release Date: July 1, 2019) Legacy documentation: https://old-docs.janusgraph.org/0.4.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.4.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.4.0\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 5.6.14, 6.0.1, 6.6.0 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.1 Java 1.8 For more information on features and bug fixes in 0.4.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/8?closed=1 Upgrade Instructions HBase: Upgrade from 1.2 to 2.1 The version of HBase that is included in the distribution of JanusGraph was upgraded from 1.2.6 to 2.1.5. HBase 2.x client is not fully backward compatible with HBase 1.x server. Users who operate their own HBase version 1.x cluster may need to upgrade their cluster to version 2.x. Optionally users may build their own distribution of JanusGraph which includes HBase 1.x from source with the maven flags -Dhbase.profile -Phbase1. Cassandra: Upgrade from 2.1 to 2.2 The version of Cassandra that is included in the distribution of JanusGraph was upgraded from 2.1.20 to 2.2.13. Refer to the upgrade documentation of Cassandra for detailed instructions to perform this upgrade. Users who operate their own Cassandra cluster instead of using Cassandra distributed together with JanusGraph are not affected by this upgrade. This also does not change the different versions of Cassandra that are supported by JanusGraph (see < > for a detailed list of the supported versions). BerkeleyDB : Upgrade from 7.4 to 7.5 The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk (see the BerkeleyDB changelog for reference ). This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with JanusGraph can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release. Solr: Compatible Lucene version changed from 5.0.0 to 7.0.0 in distributed config The JanusGraph distribution contains a solrconfig.xml file that can be used to configure Solr. The value luceneMatchVersion in this config that tells Solr to behave according to that Lucene version was changed from 5.0.0 to 7.0.0 as that is the default version currently used by JanusGraph. Users should generally set this value to the version of their Solr installation. If the config distributed by JanusGraph is used for an existing Solr installation that used a lower version before (like 5.0.0 from a previous versions of this file), it is highly recommended that a re-indexing is performed. Version 0.3.3 (Release Date: January 11, 2020) Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.3\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/14?closed=1 Version 0.3.2 (Release Date: June 16, 2019) Legacy documentation: https://old-docs.janusgraph.org/0.3.2/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.2\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/10?closed=1 Version 0.3.1 (Release Date: October 2, 2018) Legacy documentation: https://old-docs.janusgraph.org/0.3.1/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.1\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/7?closed=1 Version 0.3.0 (Release Date: July 31, 2018) Legacy documentation: https://old-docs.janusgraph.org/0.3.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.0\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/4?closed=1 Upgrade Instructions Important You should back-up your data prior to attempting an upgrade! Also please note that once an upgrade has been completed you will no longer be able to connect to your graph with client versions prior to 0.3.0. JanusGraph 0.3.0 implements Schema Constraints which made it necessary to also introduce the concept of a schema version. There is a check to prevent client connections that either expect a different schema version or have no concept of a schema version. To perform an upgrade, the configuration option graph.allow-upgrade=true must be set on each graph you wish to upgrade. The graph must be opened with a 0.3.0 or greater version of JanusGraph since older versions have no concept of graph.storage-version and will not allow for it to be set. Example excerpt from janusgraph.properties file # JanusGraph configuration sample: Cassandra over a socket # # This file connects to a Cassandra daemon running on localhost via # Thrift. Cassandra must already be started before starting JanusGraph # with this file. # This option should be removed as soon as the upgrade is complete. Otherwise if this file # is used in the future to connect to a different graph it could cause an unintended upgrade. graph.allow-upgrade = true gremlin.graph = org.janusgraph.core.JanusGraphFactory # The primary persistence provider used by JanusGraph. This is required. # It should be set one of JanusGraph's built-in shorthand names for its # standard storage backends (shorthands: berkeleyje, cassandrathrift, # cassandra, astyanax, embeddedcassandra, cql, hbase, inmemory) or to the # full package and classname of a custom/third-party StoreManager # implementation. # # Default: (no default value) # Data Type: String # Mutability: LOCAL storage.backend = cassandrathrift # The hostname or comma-separated list of hostnames of storage backend # servers. This is only applicable to some storage backends, such as # cassandra and hbase. # # Default: 127.0.0.1 # Data Type: class java.lang.String[] # Mutability: LOCAL storage.hostname = 127.0.0.1 If graph.allow-upgrade is set to true on a graph graph.storage-version and graph.janusgraph-version will automatically be upgraded to match the version level of the server, or local client, that is opening the graph. You can verify the upgrade was successful by opening the management API and validating the values of graph.storage-version and graph.janusgraph-version . Once the storage version has been set you should remove graph.allow-upgrade=true from your properties file and reopen your graph to ensure that the upgrade was successful. Version 0.2.3 (Release Date: May 21, 2019) Legacy documentation: https://old-docs.janusgraph.org/0.2.3/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.3\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.9 Java 1.8 For more information on features and bug fixes in 0.2.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/9?closed=1 Version 0.2.2 (Release Date: October 9, 2018) Legacy documentation: https://old-docs.janusgraph.org/0.2.2/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.2\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.9 Java 1.8 For more information on features and bug fixes in 0.2.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/6?closed=1 Version 0.2.1 (Release Date: July 9, 2018) Legacy documentation: https://old-docs.janusgraph.org/0.2.1/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.1\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.9 Java 1.8 For more information on features and bug fixes in 0.2.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/5?closed=1 Upgrade Instructions HBase TTL In JanusGraph 0.2.0, time-to-live (TTL) support was added for HBase storage backend. In order to utilize the TTL capability on HBase, the graph timestamps need to be MILLI. If the graph.timestamps property is not explicitly set to MILLI, the default is MICRO in JanusGraph 0.2.0, which does not work for HBase TTL. Since the graph.timestamps property is FIXED, a new graph needs to be created to make any change of the graph.timestamps property effective. Version 0.2.0 (Release Date: October 11, 2017) Legacy documentation: https://old-docs.janusgraph.org/0.2.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.0\" Tested Compatibility: Apache Cassandra 2.1.18, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0-pre3 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.2, 6.0.0-rc1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.6 Java 1.8 For more information on features and bug fixes in 0.2.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/2?closed=1 Upgrade Instructions Elasticsearch JanusGraph 0.1.z is compatible with Elasticsearch 1.5.z. There were several configuration options available, including transport client, node client, and legacy configuration track. JanusGraph 0.2.0 is compatible with Elasticsearch versions from 1.y through 6.y, however it offers only a single configuration option using the REST client. Transport client The TRANSPORT_CLIENT interface has been replaced with REST_CLIENT . When migrating an existing graph to JanusGraph 0.2.0, the interface property must be set when connecting to the graph: index.search.backend = elasticsearch index.search.elasticsearch.interface = REST_CLIENT index.search.hostname = 127.0.0.1 After connecting to the graph, the property update can be made permanent by making the change with JanusGraphManagement : mgmt = graph . openManagement () mgmt . set ( \"index.search.elasticsearch.interface\" , \"REST_CLIENT\" ) mgmt . commit () Node client A node client with JanusGraph can be configured in a few ways. If the node client was configured as a client-only or non-data node, follow the steps from the transport client section to connect to the existing cluster using the REST_CLIENT instead. If the node client was a data node (local-mode), then convert it into a standalone Elasticsearch node, running in a separate JVM from your application process. This can be done by using the node\u2019s configuration from the JanusGraph configuration to start a standalone Elasticsearch 1.5.z node. For example, we start with these JanusGraph 0.1.z properties: index.search.backend=elasticsearch index.search.elasticsearch.interface=NODE index.search.conf-file=es-client.yml index.search.elasticsearch.ext.node.name=alice where the configuration file es-client.yml has properties: node . data : true path . data : / var / lib / elasticsearch / data path . work : / var / lib / elasticsearch / work path . logs : / var / log / elasticsearch The properties found in the configuration file es-client.yml and the index.search.elasticsearch.ext.* properties can be inserted into $ES_HOME/config/elasticsearch.yml so that a standalone Elasticsearch 1.5.z node can be started with the same properties. Keep in mind that if any path locations have relative paths, those values may need to be updated appropriately. Once the standalone Elasticsearch node is started, follow the directions in the transport client section to complete the migration to the REST_CLIENT interface. Note that the index.search.conf-file and index.search.elasticsearch.ext.* properties are not used by the REST_CLIENT interface, so they can be removed from the configuration properties. Legacy configuration The legacy configuration track was not recommended in JanusGraph 0.1.z and is no longer supported in JanusGraph 0.2.0. Users should refer to the previous sections and migrate to the REST_CLIENT . Version 0.1.1 (Release Date: May 11, 2017) Documentation: https://old-docs.janusgraph.org/0.1.1/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.1.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.1.1\" Tested Compatibility: Apache Cassandra 2.1.9 Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4 Google Bigtable 0.9.5.1 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.5.1 Apache Lucene 4.10.4 Apache Solr 5.2.1 Apache TinkerPop 3.2.3 Java 1.8 For more information on features and bug fixes in 0.1.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/3?closed=1 Version 0.1.0 (Release Date: April 11, 2017) Documentation: https://old-docs.janusgraph.org/0.1.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.1.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.1.0\" Tested Compatibility: Apache Cassandra 2.1.9 Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4 Google Bigtable 0.9.5.1 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.5.1 Apache Lucene 4.10.4 Apache Solr 5.2.1 Apache TinkerPop 3.2.3 Java 1.8 Features added since version Titan 1.0.0: TinkerPop 3.2.3 compatibility Includes update to Spark 1.6.1 Query optimizations: JanusGraphStep folds in HasId and HasContainers can be folded in even mid-traversal Support Google Cloud Bigtable as a backend over the HBase interface Compatibility with newer versions of backend and index stores HBase 1.2 BerkeleyJE 7.3.7 Includes a number of bug fixes and optimizations For more information on features and bug fixes in 0.1.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/1?closed=1 Upgrade Instructions JanusGraph is based on the latest commit to the titan11 branch of Titan repo . JanusGraph has made the following changes to Titan, so you will need to adjust your code and configuration accordingly: module names: titan-* are now janusgraph-* package names: com.thinkaurelius.titan are now org.janusgraph class names: Titan* are now JanusGraph* except in cases where this would duplicate a word, e.g., TitanGraph is simply JanusGraph rather than JanusGraphGraph For more information on how to configure JanusGraph to read data which had previously been written by Titan refer to Migration from titan .","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#version-compatibility","text":"The JanusGraph project is growing along with the rest of the graph and big data ecosystem and utilized storage and indexing backends. Below are version compatibilities between the various versions of components. For dependent backend systems, different minor versions are typically supported as well. It is strongly encouraged to verify version compatibility prior to deploying JanusGraph. Although JanusGraph may be compatible with older and no longer supported versions of its dependencies, users are warned that there are possible risks and security exposures with running software that is no longer supported or updated. Please check with the software providers to understand their supported versions. Users are strongly encouraged to use the latest versions of the software.","title":"Version Compatibility"},{"location":"changelog/#version-compatibility-matrix","text":"","title":"Version Compatibility Matrix"},{"location":"changelog/#currently-supported","text":"All currently supported versions of JanusGraph are listed below. Info You are currently viewing the documentation page of JanusGraph version 1.0.0-rc2. To ensure that the information below is up to date, please double check that this is not an archived version of the documentation. JanusGraph Storage Version Cassandra HBase Bigtable ScyllaDB Elasticsearch Solr TinkerPop Spark Scala 0.6.z 2 3.0.z, 3.11.z 1.6.z, 2.2.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z N/A 6.y, 7.y 7.y, 8.y 3.5.z 3.0.z 2.12.z 1.0.z 2 3.11.z, 4.0.z 2.5.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z 5.y 6.y, 7.y, 8.y 8.y 3.6.z 3.2.z 2.12.z Info Even so ScyllaDB is marked as N/A prior version 1.0.0 it was actually supported using cql storage option. The only difference is that from version 1.0.0 JanusGraph officially supports ScyllaDB using scylla and cql storage options and have extended test coverage for ScyllaDB.","title":"Currently supported"},{"location":"changelog/#end-of-life","text":"The versions of JanusGraph listed below are outdated and will no longer receive bugfixes. JanusGraph Storage Version Cassandra HBase Bigtable ScyllaDB Elasticsearch Solr TinkerPop Spark Scala 0.1.z 1 1.2.z, 2.0.z, 2.1.z 0.98.z, 1.0.z, 1.1.z, 1.2.z 0.9.z, 1.0.0-preZ, 1.0.0 N/A 1.5.z 5.2.z 3.2.z 1.6.z 2.10.z 0.2.z 1 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 0.98.z, 1.0.z, 1.1.z, 1.2.z, 1.3.z 0.9.z, 1.0.0-preZ, 1.0.0 N/A 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.2.z 1.6.z 2.10.z 0.3.z 2 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.0.z, 1.1.z, 1.2.z, 1.3.z, 1.4.z 1.0.0, 1.1.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 N/A 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.3.z 2.2.z 2.11.z 0.4.z 2 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.2.z, 1.3.z, 1.4.z, 2.1.z N/A N/A 5.y, 6.y 7.y 3.4.z 2.2.z 2.11.z 0.5.z 2 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.2.z, 1.3.z, 1.4.z, 2.1.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z N/A 6.y, 7.y 7.y 3.4.z 2.2.z 2.11.z","title":"End-of-Life"},{"location":"changelog/#release-notes","text":"","title":"Release Notes"},{"location":"changelog/#version-100-release-date","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 1.0.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:1.0.0\" Tested Compatibility: Apache Cassandra 3.11.10, 4.0.6 Apache HBase 2.5.0 Oracle BerkeleyJE 7.5.11 ScyllaDB 5.1.4 Elasticsearch 6.0.1, 6.6.0, 7.17.8, 8.6.0 Apache Lucene 8.11.1 Apache Solr 8.11.1 Apache TinkerPop 3.6.2 Java 8, 11 Note Google Bigtable was removed from this list because there is no automatic testing in place specifically for that backend. Since the adapter for Bigtable is however just using the HBase adapter, it is also covered by the tests for HBase. We invite anyone who is interested in the Bigtable storage adapter to help with this by contributing so that the tests for HBase are also automatically executed for Bigtable. More information can be found in this GitHub issue: janusgraph/janusgraph#415 . Installed versions in the Pre-Packaged Distribution: Cassandra 4.0.6 Elasticsearch 7.14.0","title":"Version 1.0.0 (Release Date: ???)"},{"location":"changelog/#changes","text":"For more information on features and bug fixes in 1.0.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/21?closed=1","title":"Changes"},{"location":"changelog/#assets","text":"JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch","title":"Assets"},{"location":"changelog/#upgrade-instructions","text":"","title":"Upgrade Instructions"},{"location":"changelog/#new-index-management","text":"The index management has received an overhaul which enables proper index removal. The schema action REMOVE_INDEX is no longer available and has been replaced by DISCARD_INDEX .","title":"New index management"},{"location":"changelog/#totals-for-direct-index-queries-now-applies-provided-offset-and-limit","text":"Direct index queries which search count for totals ( vertexTotals , edgeTotals , propertyTotals and direct execution of IndexProvider.totals ) now apply provided limit and offset . Previously provided limit and offset were ignored. For example, previously the following query would return 500 if there were 500 indexed elements: gremlin > graph . indexQuery ( \"textIndex\" , \"v.\\\"text\\\":fooBar\" ). limit ( 10 ). vertexTotals () Now the above query will return 10 elements because we limited the result to 10 elements only. Same applies to offset . Previously the following query would return 500 if there were 500 indexed elements: gremlin > graph . indexQuery ( \"textIndex\" , \"v.\\\"text\\\":fooBar\" ). offset ( 10 ). vertexTotals () Now the above query will return 490 as a result because we skip count of the first 10 elements. offset provided with limit will apply offset first and limit last. For example, the above query will return 10 elements now if there were 500 indexed elements: gremlin > graph . indexQuery ( \"textIndex\" , \"v.\\\"text\\\":fooBar\" ). limit ( 10 ). offset ( 10 ). vertexTotals () The new logic is applied similarly to Direct Index Queries vertexTotals() , edgeTotals() , propertyTotals() as well as internal JanusGraph method IndexProvider.totals .","title":"totals for direct index queries now applies provided offset and limit"},{"location":"changelog/#add-support-for-java-11","text":"JanusGraph now officially supports Java 11 in addition to Java 8. We encourage everyone to update to Java 11. Note The pre-packaged distribution now requires Java 11.","title":"Add support for Java 11"},{"location":"changelog/#upgrade-of-log4j-to-version-2","text":"This change requires a new log4j configuration. You can find an example configuration in conf/log4j2-server.xml . As a result of the changed configuration format, we clean up all configurations. This could lead to unexpected new log lines. Please open an issue, if you see any unwanted log line. Note Log4j is only used for standalone server deployments and JanusGraph testing.","title":"Upgrade of log4j to version 2"},{"location":"changelog/#removal-of-cassandra-all-dependency","text":"JanusGraph had a dependency on cassandra-all only for some Hadoop-related classes. We moved these few classes into a new module cassandra-hadoop-util to reduce the dependencies of JanusGraph. If you are running JanusGraph with an embeded Cassandra, you have to exclude the cassandra-hadoop-util from janusgraph-cql .","title":"Removal of cassandra-all dependency"},{"location":"changelog/#drop-support-for-hbase-1","text":"We are dropping support for HBase 1.","title":"Drop support for HBase 1"},{"location":"changelog/#drop-support-for-solr-7","text":"We are dropping support for Solr 7.","title":"Drop support for Solr 7"},{"location":"changelog/#drop-support-for-gryo-messageserializer","text":"Support for Gryo MessageSerializer has been dropped in TinkerPop 3.6.0 and we therefore also no longer support it in JanusGraph. GraphBinary is now used as the default MessageSerializer.hb","title":"Drop support for Gryo MessageSerializer"},{"location":"changelog/#batch-processing-enabled-by-default","text":"Batch processing allows JanusGraph to fetch a batch of vertices from the storage backend together instead of requesting each vertex individually which leads to a high number of backend queries. This was however disabled by default in JanusGraph because these batches could become much larger than what was needed for the traversal and therefore have a negative performance impact for some traversals. That is why an improved batch processing mode was added in JanusGraph 0.6.0 that limits the size of these batches retrieved from the storage backend, called Limited Batch Processing . This mode therefore solves the problem of having potentially unlimited batch sizes. That is why we now enable this mode by default as most users should benefit from this limited batch processing. If you want to continue using JanusGraph without batch processing, then you have to manually disable it by setting query.batch to false .","title":"Batch Processing enabled by default"},{"location":"changelog/#breaking-change-for-geoshape-graphbinary-serialization","text":"Support for the GraphBinary serialization format was added in JanusGraph 0.6.0. This also included support to serialize Geoshapes via GraphBinary. The implementation of the Geoshape serializer was unfortunately closely tied to the Java library Spatial4j that we are using to implement Geoshapes in Java. This made it very complicated to add support for GraphBinary in other languages than Java. To make it easier to support GraphBinary in non-Java environments like .NET, we have completely reimplemented the GraphBinary serialization of Geoshapes in this version. This is a breaking change for users who have already adopted GraphBinary and who are using Geoshapes. It is necessary to update JanusGraph Server and all (Java) clients that use GraphBinary at the same time since JanusGraph Server with an older version will not be able to read a Geoshape created by a client that is already on version 1.0.0 and vice versa. Users who do not use GraphBinary yet or who are not using Geoshapes are not affected by this change.","title":"Breaking change for Geoshape GraphBinary serialization"},{"location":"changelog/#configuredgraphfactory-now-creates-separate-indices-per-graph-in-elasticsearch","text":"If the ConfiguredGraphFactory is used together with Elasticsearch as the index backend, then the same Elasticsearch index is used for all graphs (if the same index names were used across different graphs). Now it is possible to let JanusGraph create the index names dynamically by using the graph.graphname if no index.[X].index-name is provided in the template configuration. This is exactly like it was already the case for the CQL keyspace name for example. Users who don't want to use this feature can simply continue providing the index name via index.[X].index-name in the template configuration.","title":"ConfiguredGraphFactory now creates separate indices per graph in Elasticsearch"},{"location":"changelog/#remove-support-for-old-serialization-format-of-janusgraph-predicates","text":"We are dropping support for old serialization format of JanusGraph predicates. The old predicates serialization format is only used by client older than 0.6. The change only affects GraphSON.","title":"Remove support for old serialization format of JanusGraph predicates"},{"location":"changelog/#mixed-index-aggregation-optimization","text":"A new optimization has been added to compute aggregations (min, max, sum and avg) using mixed index engine (if the aggregation function follows an indexed query). If the index backend is Elasticsearch, a double value is used to hold the result. As a result, aggregations on long numbers greater than 2^53 are approximate. In this case, if the accurate result is essential, the optimization can be disabled by removing the strategy JanusGraphMixedIndexAggStrategy : g.traversal().withoutStrategies(JanusGraphMixedIndexAggStrategy.class) .","title":"Mixed index aggregation optimization"},{"location":"changelog/#add-support-for-elasticsearch-8","text":"JanusGraph now supports ElasticSearch 8. Notice, Mapping.PREFIX_TREE mapping is no longer available for Geoshape mappings using new ElasticSearch 8 indices. Mapping.PREFIX_TREE is still supported in ElasticSearch 6, ElasticSearch 7, Solr, Lucene. For ElasticSearch the new Geoshape mapping was added Mapping.BKD . It's recommended to use Mapping.BKD mapping due to better performance characteristics over Mapping.PREFIX_TREE . The downside of Mapping.BKD is that it doesn't support Circle shapes. Thus, JanusGraph provides BKD Circle processors to convert Circle into other shapes for indexing but use Circle at the storage level. More information about Circle processors available under configuration namespace index.[X].bkd-circle-processor . ElasticSearch 8 doesn't allow creating new indexes with Mapping.PREFIX_TREE mapping, but the existing indices using Mapping.PREFIX_TREE will work in ElasticSearch 8 after migration. See ElasticSearch 8 migration guide .","title":"Add support for ElasticSearch 8"},{"location":"changelog/#add-support-for-scylladb-driver","text":"A new module janusgraph-scylla provides ability to run JanusGraph with ScyllaDB Driver which is an optimized fork version of DataStax Java Driver for ScyllaDB storage backend. For ScyllaDB storage backend you can use either janusgraph-scylla or janusgraph-cql (which is a general CQL storage driver implementation). That said, it's recommended to use janusgraph-scylla for ScyllaDB due to the provided internal optimizations (more about ScyllaDB driver optimizations can be found here ). Notice that janusgraph-cql and janusgraph-scylla are mutually exclusive. Use only one module at a time and never provide both dependencies in the same classpath. See ScyllaDB Storage Backend documentation for more information about how to make scylla storage.backend options available.","title":"Add support for ScyllaDB driver"},{"location":"changelog/#disable-cql-executorservice-by-default","text":"Previously CQL ExecutorService was enabled by default mostly for historical reasons. CQL ExecutorService managed by JanusGraph adds additional overhead without bringing any usefulness other than limiting amount of parallel queries which can (and should) be controlled via the underlying CQL driver. In case previous behaviour is desired then storage.cql.executor-service.enabled configuration option should be set to true , but it's recommended to tune CQL queries parallelism using CQL driver configuration options (like storage.cql.max-requests-per-connection , storage.cql.local-max-connections-per-host ) and / or storage.parallel-backend-ops.* configuration options.","title":"Disable CQL ExecutorService by default"},{"location":"changelog/#removal-of-deprecated-classesmethodsfunctionalities","text":"","title":"Removal of deprecated classes/methods/functionalities"},{"location":"changelog/#methods","text":"JanusGraphIndexQuery.vertices replaced by JanusGraphIndexQuery.vertexStream JanusGraphIndexQuery.edges replaced by JanusGraphIndexQuery.edgeStream JanusGraphIndexQuery.properties replaced by JanusGraphIndexQuery.propertyStream IndexQueryBuilder.vertices replaced by IndexQueryBuilder.vertexStream IndexQueryBuilder.edges replaced by IndexQueryBuilder.edgeStream IndexQueryBuilder.properties replaced by IndexQueryBuilder.propertyStream IndexTransaction.query replaced by IndexTransaction.queryStream","title":"Methods"},{"location":"changelog/#classesinterfaces","text":"EdgeLabelDefinition class PropertyKeyDefinition class RelationTypeDefinition class SchemaContainer class SchemaElementDefinition class SchemaProvider interface VertexLabelDefinition class JanusGraphId class AllEdgesIterable class AllEdgesIterator class ConcurrentLRUCache class PriorityQueue class RemovableRelationIterable class RemovableRelationIterator class ImmutableConfiguration class","title":"Classes/Interfaces"},{"location":"changelog/#version-063-release-date-february-18-2023","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.3\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.11.0 Apache TinkerPop 3.5.5 Java 1.8 Note Google Bigtable was removed from this list because there is no automatic testing in place specifically for that backend. Since the adapter for Bigtable is however just using the HBase adapter, it is also covered by the tests for HBase. We invite anyone who is interested in the Bigtable storage adapter to help with this by contributing so that the tests for HBase are also automatically executed for Bigtable. More information can be found in this GitHub issue: janusgraph/janusgraph#415 .","title":"Version 0.6.3 (Release Date: February 18, 2023)"},{"location":"changelog/#changes_1","text":"For more information on features and bug fixes in 0.6.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/24?closed=1","title":"Changes"},{"location":"changelog/#assets_1","text":"JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch","title":"Assets"},{"location":"changelog/#version-062-release-date-may-31-2022","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.2\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.9.0 Apache TinkerPop 3.5.3 Java 1.8","title":"Version 0.6.2 (Release Date: May 31, 2022)"},{"location":"changelog/#changes_2","text":"For more information on features and bug fixes in 0.6.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/23?closed=1","title":"Changes"},{"location":"changelog/#assets_2","text":"JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch","title":"Assets"},{"location":"changelog/#version-061-release-date-january-18-2022","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.1\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.9.0 Apache TinkerPop 3.5.1 Java 1.8","title":"Version 0.6.1 (Release Date: January 18, 2022)"},{"location":"changelog/#changes_3","text":"For more information on features and bug fixes in 0.6.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/22?closed=1","title":"Changes"},{"location":"changelog/#assets_3","text":"JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch","title":"Assets"},{"location":"changelog/#upgrade-instructions_1","text":"","title":"Upgrade Instructions"},{"location":"changelog/#graphmanager-changed-to-janusgraphmanager","text":"A GraphManager is used to instantiate graph instances. JanusGraph Server has used the DefaultGraphManager from TinkerPop for this by default if no other GraphManager was specified in the JanusGraph Server YAML config file. The behavior of this DefaultGraphManager was changed in TinkerPop 3.5.0 which is included in JanusGraph 0.6.0 in how it parses config values, making it impossible to provide comma separated values, e.g., to specify multiple hostnames for the storage backend. The JanusGraphManager does not have this limitation which is why it is now configured as the GraphManager in the JanusGraph Server config files: [ ... ] channelizer : org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer graphManager : org.janusgraph.graphdb.management.JanusGraphManager graphs : { graph : conf/janusgraph-berkeleyje-es.properties } [ ... ] If you however want to continue using the DefaultGraphManager , then you can simply remove the setting again or change it to the TinkerPop GraphManager that has been the default before: org.apache.tinkerpop.gremlin.server.util.DefaultGraphManager .","title":"GraphManager changed to JanusGraphManager"},{"location":"changelog/#version-060-release-date-september-3-2021","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.0\" Tested Compatibility: Apache Cassandra 3.0.14, 3.11.10 Apache HBase 1.6.0, 2.2.7 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.14.0 Apache Lucene 8.9.0 Apache Solr 7.7.2, 8.9.0 Apache TinkerPop 3.5.3 Java 1.8","title":"Version 0.6.0 (Release Date: September 3, 2021)"},{"location":"changelog/#changes_4","text":"For more information on features and bug fixes in 0.6.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/17?closed=1","title":"Changes"},{"location":"changelog/#assets_4","text":"JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch","title":"Assets"},{"location":"changelog/#upgrade-instructions_2","text":"","title":"Upgrade Instructions"},{"location":"changelog/#experimental-support-for-amazon-keyspaces","text":"Amazon Keyspaces is a serverless managed Apache Cassandra-compatible database service provided by Amazon. See Deploying on Amazon Keyspaces for more details.","title":"Experimental support for Amazon Keyspaces"},{"location":"changelog/#breaking-change-for-configuration-objects","text":"Prior to JanusGraph 0.6.0, Configuration objects were from the Apache commons-configuration library. To comply with the TinkerPop change , JanusGraph now uses the commons-configuration2 library. A typical usage of configuration object is to create configuration using ConfigurationGraphFactory . Now you would need to use the new configuration2 library. Please refer to the commons-configuration 2.0 migration guide for details. Note that this very likely does not affect gremlin console usage, since the new library is auto-imported, and the basic APIs remain the same. For java code usage, you need to import configuration2 library rather than the old configuration library.","title":"Breaking change for Configuration objects"},{"location":"changelog/#breaking-change-for-gremlin-server-configs","text":"scriptEvaluationTimeout is renamed to evaluationTimeout . You can refer to conf/gremlin-server/gremlin-server.yaml for example.","title":"Breaking change for gremlin server configs"},{"location":"changelog/#breaking-change-for-gremlin-eventstrategy-usage","text":"If you are using EventStrategy , please note that now you need to register it every time you start a new transaction. An example is available at ThreadLocalTxLeakTest::eventListenersCanBeReusedAcrossTx See more background of this breaking change in this pull request .","title":"Breaking change for gremlin EventStrategy usage"},{"location":"changelog/#disable-smart-limit-by-default-and-change-hard_max_limit","text":"Prior to 0.6.0, smart-limit is enabled by default. It tries to guess a small limit for each graph centric query (e.g. g.V().has(\"prop\", \"value\") ) internally, and if more results are required by user, it queries backend again with a larger limit, and repeats until either results are exhausted or user stops the query. However, this is not the same as paging mechanism. All interim results will be fetched again in next round, making the whole query costly. Even worse, if your data backend does not return results in a consistent order, then some entries might be missing in the final results. Until JanusGraph can fully utilize the paging capacity provided by backends (e.g. Elasticsearch scroll), this option is recommended to be turned off. The exception is when you have a large number of results but you only need a few of them, then enabling smart-limit can reduce latency and memory usage. An example would be: Iterator<Vertex> iter = graph.traversal().V().has(\"prop\", \"value\"); while (iter.hasNext()) { Vertex v = iter.next(); if (canStop()) break; } Prior to 0.6.0, even if smart-limit is disabled, JanusGraph adds a HARD_MAX_LIMIT that is equivalent to 100,000 to avoid fetching too many results at a time. This limit is now configurable, and by default, it's Integer.MAX_VALUE which can be interpreted as no limit.","title":"Disable smart-limit by default and change HARD_MAX_LIMIT"},{"location":"changelog/#add-experimental-support-for-java-11","text":"We started to work on support for Java 11. We would like to get feedback, if everything is working as expected after upgrading to Java 11.","title":"Add experimental support for Java 11"},{"location":"changelog/#removal-of-loggingschemamaker","text":"The schema.default=logging option is not valid anymore. Use schema.default=default and schema.logging=true options together to make application behaviour unaltered, if you are using LoggingSchemaMaker .","title":"Removal of LoggingSchemaMaker"},{"location":"changelog/#replacing-the-server-startup-script-is-replaced","text":"The gremlin-server.sh is placed by janusgraph-server.sh . The janusgraph-server.sh brings some new functionality such as easy configuration of Java options using the jvm.options file. The jvm.options file contains some default configurations for JVM based on Cassandra's JVM configurations, Elasticsearch and the old gremlin-server.sh.","title":"Replacing the server startup script is replaced"},{"location":"changelog/#serialization-of-janusgraph-predicates-has-changed","text":"The serialization of JanusGraph predicates has changed in this version for both GraphSON and Gryo. The newest version of the JanusGraph Driver requires a JanusGraph Server version of 0.6.0 and above. The server includes a fallback for clients with an older driver to make the upgrade to version 0.6.0 easier. This means that the server can be upgraded first without having to update all clients at the same time. The fallback will however be removed in a future version of JanusGraph so clients should also be upgraded.","title":"Serialization of JanusGraph predicates has changed"},{"location":"changelog/#graphbinary-is-now-supported","text":"GraphBinary is a new binary serialization format from TinkerPop that supersedes Gryo and it will eventually also replace GraphSON. GraphBinary is language independent and has a low serialization overhead which results in an improved performance. If you want to use GraphBinary, you have to add following to the gremlin-server.yaml after the keyword serializers . This will add the support on the server site. Note The java driver is the only driver that currently supports GraphBinary, see Connecting to JanusGraph using Java . Note Version 1.0.0 adds a breaking change to GraphBinary for Geoshape serialization, see the 1.0.0 changelog for more information .","title":"GraphBinary is now supported"},{"location":"changelog/#new-index-selection-algorithm","text":"In version 0.6.0, the index selection algorithm has changed. If the number of possible indexes for a query is small enough, the new algorithm will perform an exhaustive search to minimize the number of indexes which need to be queried. The default limit is set to 10. In order to maintain the old selection algorithm regardless of the available indexes, set the key query.index-select-threshold to 0 . For more information, see Configuration Reference","title":"New index selection algorithm"},{"location":"changelog/#removal-of-cassandra-thrift-support","text":"Thrift will be completely removed in Cassandra 4. All deprecated Cassandra Thrift backends were removed in JanusGraph 0.6.0. We already added support for CQL in JanusGraph 0.2.0 and we have been encouraging users to switch from Thrift to CQL since version 0.2.1. This means that the following backends were removed: cassandrathrift , cassandra , astyanax , and embeddedcassandra . Users who still use one of these Thrift backends should migrate to CQL. Our migration guide explains the necessary steps for this. The option to run Cassandra embedded in the same JVM as JanusGraph is however no longer supported with CQL. Note The source code for the Thrift backends will be moved into a dedicated repository . While we do not support them any more, users can still use them if they for some reason cannot migrate to CQL.","title":"Removal of Cassandra Thrift support"},{"location":"changelog/#drop-support-for-cassandra-2","text":"With the release of Cassandra 4, the support of Cassandra 2 will be dropped. Therefore, you should upgrade to Cassandra 3 or higher. Note Cassandra 3 and higher doesn't support compact storage. If you have activated or never changed the value of storage.cql.storage-compact=true , during the upgrade process you have to ensure your data is correctly migrated.","title":"Drop support for Cassandra 2"},{"location":"changelog/#introduction-of-a-janusgraph-server-startup-class-as-a-replacement-for-gremlin-server-startup","text":"The gremlin-server.sh and the janusgraph.sh are configured to use the new JanusGraph startup class. This new class introduces a default set of TinkerPop Serializers if no serializers are configured in the gremlin-server.yaml . Furthermore, JanusGraph will log the version of JanusGraph and TinkerPop after a shiny new JanusGraph header. Note If you have a custom script to startup JanusGraph, you propably would like to replace the Gremlin Server class with JanusGraph Server class: org.apache.tinkerpop.gremlin.server.GremlinServer => org.janusgraph.graphdb.server.JanusGraphServer","title":"Introduction of a JanusGraph Server startup class as a replacement for Gremlin Server startup"},{"location":"changelog/#drop-support-for-ganglia-metrics","text":"We are dropping Ganglia as we are using dropwizard for metrics. Dropwizard did drop Ganglia in the newest major version.","title":"Drop support for Ganglia metrics"},{"location":"changelog/#datastax-cassandra-driver-upgrade-from-390-to-4130","text":"All DataStax cassandra driver metrics are now disabled by default. To enable DataStax driver metrics you need to provide a list of Session level metrics and / or Node level metrics you want to enable. To provide a list of enabled metrics, you can use the next configuration options: storage.cql.metrics.session-enabled and storage.cql.metrics.node-enabled . Notice, DataStax metrics are enabled only when basic metrics are enabled (i.e. metrics.enabled = true ). See configuration references storage.cql.metrics for additional DataStax metrics configuration. An example configuration which enables some CQL Session level and Node level metrics reporting by JMX: metrics.enabled = true metrics.jmx.enabled = true metrics.jmx.domain = com.datastax.oss.driver metrics.jmx.agentid = agent storage.cql.metrics.session-enabled = bytes-sent,bytes-received,connected-nodes,cql-requests,throttling.delay storage.cql.metrics.node-enabled = pool.open-connections,pool.available-streams,bytes-sent,cql-messages See advanced.metrics.session.enabled and advanced.metrics.node.enabled sections in DataStax Metrics Configuration for a complete list of available Session level and Node level metrics. Due to driver upgrade the next cql configuration options have been removed: local-core-connections-per-host remote-core-connections-per-host local-max-requests-per-connection remote-max-requests-per-connection cluster-name storage.connection-timeout is now used to control initial connection timeout to CQL storage and not request timeouts. Please, use storage.cql.request-timeout to configure request timeouts instead. New cql configuration options should be used for upgrade: max-requests-per-connection session-name storage.cql.local-datacenter is mandatory now and defaults to datacenter1 . See more new cql configuration options in configuration references under storage.cql section.","title":"DataStax cassandra driver upgrade from 3.9.0 to 4.13.0"},{"location":"changelog/#automatic-configurations-of-dynamic-graph-binding","text":"If the JanusGraphManager is configured, dynamic graph binding will be setup automatically, see Dynamic Graphs . Note Breaking changes in the config of the gremlin-server.yaml . Following, classes are removed and have to be replaced by tinkerpop equivalent: removed class replacement class org.janusgraph.channelizers.JanusGraphWebSocketChannelizer org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer org.janusgraph.channelizers.JanusGraphHttpChannelizer org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer org.janusgraph.channelizers.JanusGraphNioChannelizer org.apache.tinkerpop.gremlin.server.channel.NioChannelizer org.janusgraph.channelizers.JanusGraphWsAndHttpChannelizer org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer","title":"Automatic configurations of dynamic graph binding"},{"location":"changelog/#breaking-change-lucene-and-solr-fuzzy-predicates","text":"The text predicates text.textFuzzy and text.textContainsFuzzy have been updated in both the Lucene and Solr indexing backends to align with JanusGraph and Elastic. These predicates now inspect the query length to determine the Levenshtein distance, where previously they used the backend's default max distance of 2: 0 for strings of one or two characters (exact match) 1 for strings of three, four or five characters 2 for strings of more than five characters Change Matrix: text query previous result new result ah ah true true ah ai true false hop hop true true hop hap true true hop hoop true true hop hooop true false surprises surprises true true surprises surprizes true true surprises surpprises true true surprises surpprisess false false","title":"Breaking change Lucene and Solr fuzzy predicates"},{"location":"changelog/#version-053-release-date-december-24-2020","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.3\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.2 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8","title":"Version 0.5.3 (Release Date: December 24, 2020)"},{"location":"changelog/#changes_5","text":"For more information on features and bug fixes in 0.5.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/20?closed=1","title":"Changes"},{"location":"changelog/#assets_5","text":"JavaDoc GitHub Release JanusGraph zip JanusGraph zip with embedded Cassandra and ElasticSearch","title":"Assets"},{"location":"changelog/#version-052-release-date-may-3-2020","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.2\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.2 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 For more information on features and bug fixes in 0.5.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/19?closed=1","title":"Version 0.5.2 (Release Date: May 3, 2020)"},{"location":"changelog/#upgrade-instructions_3","text":"","title":"Upgrade Instructions"},{"location":"changelog/#elasticsearch-index-store-names-cache-now-enabled-for-any-amount-of-indexes-per-store","text":"In JanusGraph version 0.5.0 and 0.5.1 all ElasticSearch index store names are cached for efficient index store name retrieval and the cache is disabled if there are more than 50000 indexes available per index store. From JanusGraph version 0.5.2 index store names cache isn't limited to 50000 but instead can be disabled by using a new added parameter enable_index_names_cache . It is still recommended to disable index store names cache if more than 50000 indexes are used per index store.","title":"ElasticSearch index store names cache now enabled for any amount of indexes per store"},{"location":"changelog/#version-051-release-date-march-25-2020","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.1\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.1 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 For more information on features and bug fixes in 0.5.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/18?closed=1","title":"Version 0.5.1 (Release Date: March 25, 2020)"},{"location":"changelog/#upgrade-instructions_4","text":"","title":"Upgrade Instructions"},{"location":"changelog/#two-distributed-package-is-splitted-into-two-version","text":"The default version of the distribution package does no longer contain the janusgraph.sh . This includes a packaged version of cassandra and elasticsearch. If you want to have janusgraph.sh , you have to download distribution with the suffix -full .","title":"Two Distributed package is splitted into two version"},{"location":"changelog/#gremlin-server-distributed-with-the-release-uses-inmemory-storage-backend-and-no-search-backend-by-default","text":"Gremlin Server is by default configured for the inmemory storage backend and no search backend when started with bin/gremlin-server.sh . You can provide configuration for another storage backend and/or search backend by providing a path to the appropriate configuration as a second parameter ( ./bin/gremlin-server.sh ./conf/gremlin-server/[...].yaml ).","title":"Gremlin Server distributed with the release uses inmemory storage backend and no search backend by default"},{"location":"changelog/#version-050-release-date-march-10-2020","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.5.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.5.0\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 6.0.1, 6.6.0, 7.6.1 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.6 Java 1.8 For more information on features and bug fixes in 0.5.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/13?closed=1","title":"Version 0.5.0 (Release Date: March 10, 2020)"},{"location":"changelog/#upgrade-instructions_5","text":"","title":"Upgrade Instructions"},{"location":"changelog/#distributed-package-is-renamed","text":"The distribution has no longer the suffix -hadoop2 .","title":"Distributed package is renamed"},{"location":"changelog/#reorder-dependency-of-hadoop","text":"Hadoop is now a dependency of supported backends. Therefore, MapReduceIndexJobs is now split up into different classes: Old Function New Function MapReduceIndexJobs.cassandraRepair CassandraMapReduceIndexJobsUtils.repair MapReduceIndexJobs.cassandraRemove CassandraMapReduceIndexJobsUtils.remove MapReduceIndexJobs.cqlRepair CqlMapReduceIndexJobsUtils.repair MapReduceIndexJobs.cqlRemove CqlMapReduceIndexJobsUtils.remove MapReduceIndexJobs.hbaseRepair HBaseMapReduceIndexJobsUtils.repair MapReduceIndexJobs.hbaseRemove HBaseMapReduceIndexJobsUtils.remove Note Now, you can easily support for any backend. Warning Cassandra3InputFormat is replaced by CqlInputFormat","title":"Reorder dependency of Hadoop"},{"location":"changelog/#elasticsearch-upgrade-from-660-to-761-and-drop-support-for-5x-version","text":"The ElasticSearch version has been changed to 7.6.1 which removes support for max-retry-timeout option. That is why this option no longer available in JanusGraph. Users should be aware that by default JanusGraph setups maximum open scroll contexts to maximum value of 2147483647 with the parameter setup-max-open-scroll-contexts for ElasticSearch 7.y. This option can be disabled and updated manually in ElasticSearch but you should be aware that ElasticSearch starting from version 7 has a default limit of 500 opened contexts which most likely be reached by the normal usage of JanusGraph with ElasticSearch. By default deprecated mappings are disabled in ElasticSearch version 7. If you are upgrading your ElasticSearch index backend to version 7 from lower versions, it is recommended to reindex your JanusGraph indices to not use mappings. If you are unable to reindex your indices you may setup parameter use-mapping-for-es7 to true which will tell JanusGraph to use mapping types for ElasticSearch version 7. Due to the drop of support for 5.x version, deprecated multi-type indices are no more supported. Parameter use-deprecated-multitype-index is no more supported by JanusGraph.","title":"ElasticSearch: Upgrade from 6.6.0 to 7.6.1 and drop support for 5.x version"},{"location":"changelog/#berkeleydb","text":"BerkeleyDB storage configured with SHARED_CACHE for better memory usage.","title":"BerkeleyDB"},{"location":"changelog/#default-logging-location-has-changed","text":"If you are using janusgraph.sh to start your instance, the default logging has been changed from log to logs","title":"Default logging location has changed"},{"location":"changelog/#version-041-release-date-january-14-2020","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.4.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.4.1\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 5.6.14, 6.0.1, 6.6.0 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.4 Java 1.8 For more information on features and bug fixes in 0.4.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/15?closed=1","title":"Version 0.4.1 (Release Date: January 14, 2020)"},{"location":"changelog/#upgrade-instructions_6","text":"","title":"Upgrade Instructions"},{"location":"changelog/#tinkerpop-upgrade-from-341-to-344","text":"Adding multiple values in the same query to a new vertex property without explicitly defined type (i.e. using Automatic Schema Maker to create a property type) requires explicit usage of VertexProperty.Cardinality for each call (only for the first query which defines a property) if the VertexProperty.Cardinality is different than VertexProperty.Cardinality.single .","title":"TinkerPop: Upgrade from 3.4.1 to 3.4.4"},{"location":"changelog/#version-040-release-date-july-1-2019","text":"Legacy documentation: https://old-docs.janusgraph.org/0.4.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.4.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.4.0\" Tested Compatibility: Apache Cassandra 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5 Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0 Oracle BerkeleyJE 7.5.11 Elasticsearch 5.6.14, 6.0.1, 6.6.0 Apache Lucene 7.0.0 Apache Solr 7.0.0 Apache TinkerPop 3.4.1 Java 1.8 For more information on features and bug fixes in 0.4.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/8?closed=1","title":"Version 0.4.0 (Release Date: July 1, 2019)"},{"location":"changelog/#upgrade-instructions_7","text":"","title":"Upgrade Instructions"},{"location":"changelog/#hbase-upgrade-from-12-to-21","text":"The version of HBase that is included in the distribution of JanusGraph was upgraded from 1.2.6 to 2.1.5. HBase 2.x client is not fully backward compatible with HBase 1.x server. Users who operate their own HBase version 1.x cluster may need to upgrade their cluster to version 2.x. Optionally users may build their own distribution of JanusGraph which includes HBase 1.x from source with the maven flags -Dhbase.profile -Phbase1.","title":"HBase: Upgrade from 1.2 to 2.1"},{"location":"changelog/#cassandra-upgrade-from-21-to-22","text":"The version of Cassandra that is included in the distribution of JanusGraph was upgraded from 2.1.20 to 2.2.13. Refer to the upgrade documentation of Cassandra for detailed instructions to perform this upgrade. Users who operate their own Cassandra cluster instead of using Cassandra distributed together with JanusGraph are not affected by this upgrade. This also does not change the different versions of Cassandra that are supported by JanusGraph (see < > for a detailed list of the supported versions).","title":"Cassandra: Upgrade from 2.1 to 2.2"},{"location":"changelog/#berkeleydb-upgrade-from-74-to-75","text":"The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk (see the BerkeleyDB changelog for reference ). This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with JanusGraph can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release.","title":"BerkeleyDB : Upgrade from 7.4 to 7.5"},{"location":"changelog/#solr-compatible-lucene-version-changed-from-500-to-700-in-distributed-config","text":"The JanusGraph distribution contains a solrconfig.xml file that can be used to configure Solr. The value luceneMatchVersion in this config that tells Solr to behave according to that Lucene version was changed from 5.0.0 to 7.0.0 as that is the default version currently used by JanusGraph. Users should generally set this value to the version of their Solr installation. If the config distributed by JanusGraph is used for an existing Solr installation that used a lower version before (like 5.0.0 from a previous versions of this file), it is highly recommended that a re-indexing is performed.","title":"Solr: Compatible Lucene version changed from 5.0.0 to 7.0.0 in distributed config"},{"location":"changelog/#version-033-release-date-january-11-2020","text":"Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.3\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/14?closed=1","title":"Version 0.3.3 (Release Date: January 11, 2020)"},{"location":"changelog/#version-032-release-date-june-16-2019","text":"Legacy documentation: https://old-docs.janusgraph.org/0.3.2/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.2\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/10?closed=1","title":"Version 0.3.2 (Release Date: June 16, 2019)"},{"location":"changelog/#version-031-release-date-october-2-2018","text":"Legacy documentation: https://old-docs.janusgraph.org/0.3.1/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.1\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/7?closed=1","title":"Version 0.3.1 (Release Date: October 2, 2018)"},{"location":"changelog/#version-030-release-date-july-31-2018","text":"Legacy documentation: https://old-docs.janusgraph.org/0.3.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.3.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.3.0\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 1.2.6, 1.3.1, 1.4.4 Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 Oracle BerkeleyJE 7.4.5 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.3.3 Java 1.8 For more information on features and bug fixes in 0.3.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/4?closed=1","title":"Version 0.3.0 (Release Date: July 31, 2018)"},{"location":"changelog/#upgrade-instructions_8","text":"Important You should back-up your data prior to attempting an upgrade! Also please note that once an upgrade has been completed you will no longer be able to connect to your graph with client versions prior to 0.3.0. JanusGraph 0.3.0 implements Schema Constraints which made it necessary to also introduce the concept of a schema version. There is a check to prevent client connections that either expect a different schema version or have no concept of a schema version. To perform an upgrade, the configuration option graph.allow-upgrade=true must be set on each graph you wish to upgrade. The graph must be opened with a 0.3.0 or greater version of JanusGraph since older versions have no concept of graph.storage-version and will not allow for it to be set. Example excerpt from janusgraph.properties file # JanusGraph configuration sample: Cassandra over a socket # # This file connects to a Cassandra daemon running on localhost via # Thrift. Cassandra must already be started before starting JanusGraph # with this file. # This option should be removed as soon as the upgrade is complete. Otherwise if this file # is used in the future to connect to a different graph it could cause an unintended upgrade. graph.allow-upgrade = true gremlin.graph = org.janusgraph.core.JanusGraphFactory # The primary persistence provider used by JanusGraph. This is required. # It should be set one of JanusGraph's built-in shorthand names for its # standard storage backends (shorthands: berkeleyje, cassandrathrift, # cassandra, astyanax, embeddedcassandra, cql, hbase, inmemory) or to the # full package and classname of a custom/third-party StoreManager # implementation. # # Default: (no default value) # Data Type: String # Mutability: LOCAL storage.backend = cassandrathrift # The hostname or comma-separated list of hostnames of storage backend # servers. This is only applicable to some storage backends, such as # cassandra and hbase. # # Default: 127.0.0.1 # Data Type: class java.lang.String[] # Mutability: LOCAL storage.hostname = 127.0.0.1 If graph.allow-upgrade is set to true on a graph graph.storage-version and graph.janusgraph-version will automatically be upgraded to match the version level of the server, or local client, that is opening the graph. You can verify the upgrade was successful by opening the management API and validating the values of graph.storage-version and graph.janusgraph-version . Once the storage version has been set you should remove graph.allow-upgrade=true from your properties file and reopen your graph to ensure that the upgrade was successful.","title":"Upgrade Instructions"},{"location":"changelog/#version-023-release-date-may-21-2019","text":"Legacy documentation: https://old-docs.janusgraph.org/0.2.3/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.3 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.3\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.9 Java 1.8 For more information on features and bug fixes in 0.2.3, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/9?closed=1","title":"Version 0.2.3 (Release Date: May 21, 2019)"},{"location":"changelog/#version-022-release-date-october-9-2018","text":"Legacy documentation: https://old-docs.janusgraph.org/0.2.2/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.2\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.9 Java 1.8 For more information on features and bug fixes in 0.2.2, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/6?closed=1","title":"Version 0.2.2 (Release Date: October 9, 2018)"},{"location":"changelog/#version-021-release-date-july-9-2018","text":"Legacy documentation: https://old-docs.janusgraph.org/0.2.1/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.1\" Tested Compatibility: Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.9 Java 1.8 For more information on features and bug fixes in 0.2.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/5?closed=1","title":"Version 0.2.1 (Release Date: July 9, 2018)"},{"location":"changelog/#upgrade-instructions_9","text":"","title":"Upgrade Instructions"},{"location":"changelog/#hbase-ttl","text":"In JanusGraph 0.2.0, time-to-live (TTL) support was added for HBase storage backend. In order to utilize the TTL capability on HBase, the graph timestamps need to be MILLI. If the graph.timestamps property is not explicitly set to MILLI, the default is MICRO in JanusGraph 0.2.0, which does not work for HBase TTL. Since the graph.timestamps property is FIXED, a new graph needs to be created to make any change of the graph.timestamps property effective.","title":"HBase TTL"},{"location":"changelog/#version-020-release-date-october-11-2017","text":"Legacy documentation: https://old-docs.janusgraph.org/0.2.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.2.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.2.0\" Tested Compatibility: Apache Cassandra 2.1.18, 2.2.10, 3.0.14, 3.11.0 Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1 Google Bigtable 1.0.0-pre3 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.7.6, 2.4.6, 5.6.2, 6.0.0-rc1 Apache Lucene 7.0.0 Apache Solr 5.5.4, 6.6.1, 7.0.0 Apache TinkerPop 3.2.6 Java 1.8 For more information on features and bug fixes in 0.2.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/2?closed=1","title":"Version 0.2.0 (Release Date: October 11, 2017)"},{"location":"changelog/#upgrade-instructions_10","text":"","title":"Upgrade Instructions"},{"location":"changelog/#elasticsearch","text":"JanusGraph 0.1.z is compatible with Elasticsearch 1.5.z. There were several configuration options available, including transport client, node client, and legacy configuration track. JanusGraph 0.2.0 is compatible with Elasticsearch versions from 1.y through 6.y, however it offers only a single configuration option using the REST client.","title":"Elasticsearch"},{"location":"changelog/#transport-client","text":"The TRANSPORT_CLIENT interface has been replaced with REST_CLIENT . When migrating an existing graph to JanusGraph 0.2.0, the interface property must be set when connecting to the graph: index.search.backend = elasticsearch index.search.elasticsearch.interface = REST_CLIENT index.search.hostname = 127.0.0.1 After connecting to the graph, the property update can be made permanent by making the change with JanusGraphManagement : mgmt = graph . openManagement () mgmt . set ( \"index.search.elasticsearch.interface\" , \"REST_CLIENT\" ) mgmt . commit ()","title":"Transport client"},{"location":"changelog/#node-client","text":"A node client with JanusGraph can be configured in a few ways. If the node client was configured as a client-only or non-data node, follow the steps from the transport client section to connect to the existing cluster using the REST_CLIENT instead. If the node client was a data node (local-mode), then convert it into a standalone Elasticsearch node, running in a separate JVM from your application process. This can be done by using the node\u2019s configuration from the JanusGraph configuration to start a standalone Elasticsearch 1.5.z node. For example, we start with these JanusGraph 0.1.z properties: index.search.backend=elasticsearch index.search.elasticsearch.interface=NODE index.search.conf-file=es-client.yml index.search.elasticsearch.ext.node.name=alice where the configuration file es-client.yml has properties: node . data : true path . data : / var / lib / elasticsearch / data path . work : / var / lib / elasticsearch / work path . logs : / var / log / elasticsearch The properties found in the configuration file es-client.yml and the index.search.elasticsearch.ext.* properties can be inserted into $ES_HOME/config/elasticsearch.yml so that a standalone Elasticsearch 1.5.z node can be started with the same properties. Keep in mind that if any path locations have relative paths, those values may need to be updated appropriately. Once the standalone Elasticsearch node is started, follow the directions in the transport client section to complete the migration to the REST_CLIENT interface. Note that the index.search.conf-file and index.search.elasticsearch.ext.* properties are not used by the REST_CLIENT interface, so they can be removed from the configuration properties.","title":"Node client"},{"location":"changelog/#legacy-configuration","text":"The legacy configuration track was not recommended in JanusGraph 0.1.z and is no longer supported in JanusGraph 0.2.0. Users should refer to the previous sections and migrate to the REST_CLIENT .","title":"Legacy configuration"},{"location":"changelog/#version-011-release-date-may-11-2017","text":"Documentation: https://old-docs.janusgraph.org/0.1.1/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.1.1 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.1.1\" Tested Compatibility: Apache Cassandra 2.1.9 Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4 Google Bigtable 0.9.5.1 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.5.1 Apache Lucene 4.10.4 Apache Solr 5.2.1 Apache TinkerPop 3.2.3 Java 1.8 For more information on features and bug fixes in 0.1.1, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/3?closed=1","title":"Version 0.1.1 (Release Date: May 11, 2017)"},{"location":"changelog/#version-010-release-date-april-11-2017","text":"Documentation: https://old-docs.janusgraph.org/0.1.0/index.html Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.1.0 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.1.0\" Tested Compatibility: Apache Cassandra 2.1.9 Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4 Google Bigtable 0.9.5.1 Oracle BerkeleyJE 7.3.7 Elasticsearch 1.5.1 Apache Lucene 4.10.4 Apache Solr 5.2.1 Apache TinkerPop 3.2.3 Java 1.8 Features added since version Titan 1.0.0: TinkerPop 3.2.3 compatibility Includes update to Spark 1.6.1 Query optimizations: JanusGraphStep folds in HasId and HasContainers can be folded in even mid-traversal Support Google Cloud Bigtable as a backend over the HBase interface Compatibility with newer versions of backend and index stores HBase 1.2 BerkeleyJE 7.3.7 Includes a number of bug fixes and optimizations For more information on features and bug fixes in 0.1.0, see the GitHub milestone: https://github.com/JanusGraph/janusgraph/milestone/1?closed=1","title":"Version 0.1.0 (Release Date: April 11, 2017)"},{"location":"changelog/#upgrade-instructions_11","text":"JanusGraph is based on the latest commit to the titan11 branch of Titan repo . JanusGraph has made the following changes to Titan, so you will need to adjust your code and configuration accordingly: module names: titan-* are now janusgraph-* package names: com.thinkaurelius.titan are now org.janusgraph class names: Titan* are now JanusGraph* except in cases where this would duplicate a word, e.g., TitanGraph is simply JanusGraph rather than JanusGraphGraph For more information on how to configure JanusGraph to read data which had previously been written by Titan refer to Migration from titan .","title":"Upgrade Instructions"},{"location":"common-questions/","text":"Common Questions Accidental type creation By default, JanusGraph will automatically create property keys and edge labels when a new type is encountered. It is strongly encouraged that users explicitly schemata as documented in Schema and Data Modeling before loading any data and disable automatic type creation by setting the option schema.default = none . Automatic type creation can cause problems in multi-threaded or highly concurrent environments. Since JanusGraph needs to ensure that types are unique, multiple attempts at creating the same type will lead to locking or other exceptions. It is generally recommended to create all needed types up front or in one batch when new property keys and edge labels are needed. Custom Class Datatype JanusGraph supports arbitrary objects as attribute values on properties. To use a custom class as data type in JanusGraph, either register a custom serializer or ensure that the class has a no-argument constructor and implements the equals method because JanusGraph will verify that it can successfully de-/serialize objects of that class. Please see Datatype and Attribute Serializer Configuration for more information. Transactional Scope for Edges Edges should not be accessed outside the scope in which they were originally created or retrieved. Locking Exceptions When defining unique types with locking enabled (i.e. requesting that JanusGraph ensures uniqueness) it is likely to encounter locking exceptions of the type PermanentLockingException under concurrent modifications to the graph. Such exceptions are to be expected, since JanusGraph cannot know how to recover from a transactional state where an earlier read value has been modified by another transaction since this may invalidate the state of the transaction. In most cases it is sufficient to simply re-run the transaction. If locking exceptions are very frequent, try to analyze and remove the source of congestion. Ghost Vertices When the same vertex is concurrently removed in one transaction and modified in another, both transactions will successfully commit on eventually consistent storage backends and the vertex will still exist with only the modified properties or edges. This is referred to as a ghost vertex. It is possible to guard against ghost vertices on eventually consistent backends using key uniqueness but this is prohibitively expensive in most cases. A more scalable approach is to allow ghost vertices temporarily and clearing them out in regular time intervals. Another option is to detect them at read-time using the option checkInternalVertexExistence() documented in Transaction Configuration . Debug-level Logging Slows Execution When the log level is set to DEBUG JanusGraph produces a lot of logging output which is useful to understand how particular queries get compiled, optimized, and executed. However, the output is so large that it will impact the query performance noticeably. Hence, use INFO severity or higher for production systems or benchmarking. JanusGraph OutOfMemoryException or excessive Garbage Collection If you experience memory issues or excessive garbage collection while running JanusGraph it is likely that the caches are configured incorrectly. If the caches are too large, the heap may fill up with cache entries. Try reducing the size of the transaction level cache before tuning the database level cache, in particular if you have many concurrent transactions. See JanusGraph Cache for more information. Elasticsearch OutOfMemoryException When numerous clients are connecting to Elasticsearch, it is likely that an OutOfMemoryException occurs. This is not due to a memory issue, but to the OS not allowing more threads to be spawned by the user (the user running Elasticsearch). To circumvent this issue, increase the number of allowed processes to the user running Elasticsearch. For example, increase the ulimit -u from the default 1024 to 10024. Dropping a Database To drop a database using the Gremlin Console you can call JanusGraphFactory.drop(graph) . The graph you want to drop needs to be defined prior to running the drop method. With ConfiguredGraphFactory graph = ConfiguredGraphFactory . open ( 'example' ) ConfiguredGraphFactory . drop ( 'example' ); With JanusGraphFactory graph = JanusGraphFactory . open ( 'path/to/configuration.properties' ) JanusGraphFactory . drop ( graph ); Note that on JanusGraph versions prior to 0.3.0 if multiple Gremlin Server instances are connecting to the graph that has been dropped it is recommended to close the graph on all active nodes by running either JanusGraphFactory.close(graph) or ConfiguredGraphFactory.close(\"example\") depending on which graph manager is in use. Closing and reopening the graph on all active nodes will prevent cached(stale) references to the graph that has been dropped. ConfiguredGraphFactory graphs that are dropped may need to have their configurations recreated using the graph configuration singleton or template configuration .","title":"Common Questions"},{"location":"common-questions/#common-questions","text":"","title":"Common Questions"},{"location":"common-questions/#accidental-type-creation","text":"By default, JanusGraph will automatically create property keys and edge labels when a new type is encountered. It is strongly encouraged that users explicitly schemata as documented in Schema and Data Modeling before loading any data and disable automatic type creation by setting the option schema.default = none . Automatic type creation can cause problems in multi-threaded or highly concurrent environments. Since JanusGraph needs to ensure that types are unique, multiple attempts at creating the same type will lead to locking or other exceptions. It is generally recommended to create all needed types up front or in one batch when new property keys and edge labels are needed.","title":"Accidental type creation"},{"location":"common-questions/#custom-class-datatype","text":"JanusGraph supports arbitrary objects as attribute values on properties. To use a custom class as data type in JanusGraph, either register a custom serializer or ensure that the class has a no-argument constructor and implements the equals method because JanusGraph will verify that it can successfully de-/serialize objects of that class. Please see Datatype and Attribute Serializer Configuration for more information.","title":"Custom Class Datatype"},{"location":"common-questions/#transactional-scope-for-edges","text":"Edges should not be accessed outside the scope in which they were originally created or retrieved.","title":"Transactional Scope for Edges"},{"location":"common-questions/#locking-exceptions","text":"When defining unique types with locking enabled (i.e. requesting that JanusGraph ensures uniqueness) it is likely to encounter locking exceptions of the type PermanentLockingException under concurrent modifications to the graph. Such exceptions are to be expected, since JanusGraph cannot know how to recover from a transactional state where an earlier read value has been modified by another transaction since this may invalidate the state of the transaction. In most cases it is sufficient to simply re-run the transaction. If locking exceptions are very frequent, try to analyze and remove the source of congestion.","title":"Locking Exceptions"},{"location":"common-questions/#ghost-vertices","text":"When the same vertex is concurrently removed in one transaction and modified in another, both transactions will successfully commit on eventually consistent storage backends and the vertex will still exist with only the modified properties or edges. This is referred to as a ghost vertex. It is possible to guard against ghost vertices on eventually consistent backends using key uniqueness but this is prohibitively expensive in most cases. A more scalable approach is to allow ghost vertices temporarily and clearing them out in regular time intervals. Another option is to detect them at read-time using the option checkInternalVertexExistence() documented in Transaction Configuration .","title":"Ghost Vertices"},{"location":"common-questions/#debug-level-logging-slows-execution","text":"When the log level is set to DEBUG JanusGraph produces a lot of logging output which is useful to understand how particular queries get compiled, optimized, and executed. However, the output is so large that it will impact the query performance noticeably. Hence, use INFO severity or higher for production systems or benchmarking.","title":"Debug-level Logging Slows Execution"},{"location":"common-questions/#janusgraph-outofmemoryexception-or-excessive-garbage-collection","text":"If you experience memory issues or excessive garbage collection while running JanusGraph it is likely that the caches are configured incorrectly. If the caches are too large, the heap may fill up with cache entries. Try reducing the size of the transaction level cache before tuning the database level cache, in particular if you have many concurrent transactions. See JanusGraph Cache for more information.","title":"JanusGraph OutOfMemoryException or excessive Garbage Collection"},{"location":"common-questions/#elasticsearch-outofmemoryexception","text":"When numerous clients are connecting to Elasticsearch, it is likely that an OutOfMemoryException occurs. This is not due to a memory issue, but to the OS not allowing more threads to be spawned by the user (the user running Elasticsearch). To circumvent this issue, increase the number of allowed processes to the user running Elasticsearch. For example, increase the ulimit -u from the default 1024 to 10024.","title":"Elasticsearch OutOfMemoryException"},{"location":"common-questions/#dropping-a-database","text":"To drop a database using the Gremlin Console you can call JanusGraphFactory.drop(graph) . The graph you want to drop needs to be defined prior to running the drop method. With ConfiguredGraphFactory graph = ConfiguredGraphFactory . open ( 'example' ) ConfiguredGraphFactory . drop ( 'example' ); With JanusGraphFactory graph = JanusGraphFactory . open ( 'path/to/configuration.properties' ) JanusGraphFactory . drop ( graph ); Note that on JanusGraph versions prior to 0.3.0 if multiple Gremlin Server instances are connecting to the graph that has been dropped it is recommended to close the graph on all active nodes by running either JanusGraphFactory.close(graph) or ConfiguredGraphFactory.close(\"example\") depending on which graph manager is in use. Closing and reopening the graph on all active nodes will prevent cached(stale) references to the graph that has been dropped. ConfiguredGraphFactory graphs that are dropped may need to have their configurations recreated using the graph configuration singleton or template configuration .","title":"Dropping a Database"},{"location":"development/","text":"The following sections describe the JanusGraph development process. Development Decisions Many development decisions will be made during the day-to-day work of JanusGraph contributors without any extra process overhead. However, for larger bodies of work and significant updates and additions, the following process shall be followed. Significant work may include, but is not limited to: A major new feature or subproject, e.g., a new storage adapter Incrementing the version of a core dependency such as a Apache TinkerPop Addition or deprecation of a public API Internal shared data structure or API change Addition of a new major dependency For these sorts of changes, contributors will: Create one or more issues in the GitHub issue tracker Start a DISCUSS thread on the janusgraph-dev list where the proposed change may be discussed by committers and other community members When the proposer feels it appropriate, a VOTE shall be called Two +1 votes are required for the change to be accepted Branching For features that involve only one developer, developers will work in their own JanusGraph forks, managing their own branches, and submitting pull requests when ready. If multiple developers wish to collaborate on a feature, they may request that a feature branch be created in JanusGraph repository. If the developers are not committers, a JanusGraph committer will be assigned as the shepherd for that branch. The shepherd will be responsible for merging pull requests to that branch. Branch Naming Conventions All branch names hosted in the JanusGraph repository shall be prepended with Issue_#_ . Pull Requests Users wishing to contribute to JanusGraph should fork the JanusGraph repository and then submit pull requests using the GitHub pull request process . Every pull request must be associated with an existing issue. Be sure to include the relevant issue number in the title of your pull request, complete the pull request template checklist, and provide a description of what test suites were run to validate the pull request. Pull requests commit messages should clearly describe what work was accomplished. Intermediate work-in-progress commits should be squashed prior to pull request submission. Review-Then-Commit (RTC) Pull requests must be reviewed before being merged following a review-then-commit (RTC) process. The JanusGraph project uses the GitHub review process to review pull requests. Non-committers are welcomed and encouraged to review pull requests. Their review will be non-binding, but can be taken into consideration and are still very valuable community input. The following rules apply to the voting process. Approval flows 2 committer approvals are required to merge a pull request If a committer submits the pull request, it has their implicit approval so it requires 1 additional committer approval 1 committer approval followed a one week review period for objections at which point a lazy consensus is assumed One or more -1 votes within a change request will veto the pull request until the noted issue(s) are addressed and the -1 vote is withdrawn Change requests will not be considered valid without an explanation Commit-Then-Review (CTR) In instances where the committer deems the full RTC process unnecessary, a commit-then-review (CTR) process may be employed. The purpose of invoking CTR is to reduce the burden on the committers and minimize the turnaround time for merging trivial changes. Changes of this sort may include: Documentation typo or small documentation addition Addition of a new test case Backporting approved changes into other release branches Any commit that is made following CTR shall include the fact that it is a CTR in the commit comments. Community members who wish to review CTRs may subscribe to the JanusGraph commits list so that they will see CTRs as they come through. If another committer responds with a -1, the commit should be rolled back and the formal RTC process should be followed. Enable Automatic Backporting Before merging the pull request, it should be decided whether the contribution should be backported to other release branches. This should be the case for most non-breaking changes. If the change applies to something that is not present at all on other still supported release branches, like updating a dependency that was only introduced on master or code that was added / heavily modified on master , then backporting cannot work or would require too much effort. Otherwise, the appropriate labels for backporting (for example backport/v0.6 for the branch v0.6 ) should be added before the pull requests gets merged as that allows the backporting action to automatically handle the backporting. Merging of Pull Requests A pull request is ready to be merged when it has been approved (see Review-Then-Commit (RTC) ). It can either be merged manually with git commands or through the GitHub UI. For pull requests that should be backported (see Enable Automatic Backporting ), the backporting action should create a pull request to backport the changes to other release branches after the pull request has been merged. The committer who merges the pull request should ensure that this automatic backporting succeeds. It may fail in case of merge conflicts. In that case the backporting needs to be performed manually . Manual Backporting The Backport CLI tool can be used to manually backport a pull request. This can be necessary if the automatic backporting of a pull request fails. After installing the CLI tool, you also need to configure it with a GitHub access token. This is explained in the README.md of the Backport CLI tool. You can then use the tool to backport a pull request #xyz: backport --pr xyz This will checkout the repository, apply the change from the pull request to the appropriate release branch (like v0.6 ), and then ask you to resolve any merge conflicts. After the merge conflicts are resolved, it will create a pull request that backports the changes. Note that such a backporting pull request can usually be merged via CTR after all automatic checks have been executed. If non-trivial changes were necessary to resolve merge conflicts, then waiting for a review before merging the pull request may still make sense. It is of course also possible to use cherry-picking to apply the commits manually to a different release branch instead of using this CLI tool. Release Policy Any JanusGraph committer may propose a release. To propose a release, simple start a new RELEASE thread on janusgraph-dev proposing the new release and requesting feedback on what should be included in the release. After consensus is reached the release manager will perform the following tasks: Create a release branch so that work may continue on master Prepare the release artifacts Call a vote to approve the release on janusgraph-dev Committers will be given 72 hours to review and vote on the release artifacts Three +1 votes are required for a release to be approved One or more -1 votes with explanation will veto the release until the noted issues are addressed and the -1 votes are withdrawn Building JanusGraph To build JanusGraph you need git and Maven . Clone the JanusGraph repository from GitHub to a local directory. In that directory, execute mvn clean install . This will build JanusGraph and run the internal test suite. The internal test suite has no external dependencies. Note, that running all test cases requires a significant amount of time. To skip the tests when building JanusGraph, execute mvn clean install -DskipTests For comprehensive test coverage, execute mvn clean test -P comprehensive . This will run additional test covering communication to external storage backends, performance tests and concurrency tests. The comprehensive test suite uses HBase as external database and requires that HBase is installed. Note, that running the comprehensive test suite requires a significant amount of of time (> 1 hour). FAQs Maven build causes dozens of \"[WARNING] We have a duplicate\u2026\" errors Make sure to use the maven-assembly-plugin when building or depending on JanusGraph.","title":"Development"},{"location":"development/#development-decisions","text":"Many development decisions will be made during the day-to-day work of JanusGraph contributors without any extra process overhead. However, for larger bodies of work and significant updates and additions, the following process shall be followed. Significant work may include, but is not limited to: A major new feature or subproject, e.g., a new storage adapter Incrementing the version of a core dependency such as a Apache TinkerPop Addition or deprecation of a public API Internal shared data structure or API change Addition of a new major dependency For these sorts of changes, contributors will: Create one or more issues in the GitHub issue tracker Start a DISCUSS thread on the janusgraph-dev list where the proposed change may be discussed by committers and other community members When the proposer feels it appropriate, a VOTE shall be called Two +1 votes are required for the change to be accepted","title":"Development Decisions"},{"location":"development/#branching","text":"For features that involve only one developer, developers will work in their own JanusGraph forks, managing their own branches, and submitting pull requests when ready. If multiple developers wish to collaborate on a feature, they may request that a feature branch be created in JanusGraph repository. If the developers are not committers, a JanusGraph committer will be assigned as the shepherd for that branch. The shepherd will be responsible for merging pull requests to that branch.","title":"Branching"},{"location":"development/#branch-naming-conventions","text":"All branch names hosted in the JanusGraph repository shall be prepended with Issue_#_ .","title":"Branch Naming Conventions"},{"location":"development/#pull-requests","text":"Users wishing to contribute to JanusGraph should fork the JanusGraph repository and then submit pull requests using the GitHub pull request process . Every pull request must be associated with an existing issue. Be sure to include the relevant issue number in the title of your pull request, complete the pull request template checklist, and provide a description of what test suites were run to validate the pull request. Pull requests commit messages should clearly describe what work was accomplished. Intermediate work-in-progress commits should be squashed prior to pull request submission.","title":"Pull Requests"},{"location":"development/#review-then-commit-rtc","text":"Pull requests must be reviewed before being merged following a review-then-commit (RTC) process. The JanusGraph project uses the GitHub review process to review pull requests. Non-committers are welcomed and encouraged to review pull requests. Their review will be non-binding, but can be taken into consideration and are still very valuable community input. The following rules apply to the voting process. Approval flows 2 committer approvals are required to merge a pull request If a committer submits the pull request, it has their implicit approval so it requires 1 additional committer approval 1 committer approval followed a one week review period for objections at which point a lazy consensus is assumed One or more -1 votes within a change request will veto the pull request until the noted issue(s) are addressed and the -1 vote is withdrawn Change requests will not be considered valid without an explanation","title":"Review-Then-Commit (RTC)"},{"location":"development/#commit-then-review-ctr","text":"In instances where the committer deems the full RTC process unnecessary, a commit-then-review (CTR) process may be employed. The purpose of invoking CTR is to reduce the burden on the committers and minimize the turnaround time for merging trivial changes. Changes of this sort may include: Documentation typo or small documentation addition Addition of a new test case Backporting approved changes into other release branches Any commit that is made following CTR shall include the fact that it is a CTR in the commit comments. Community members who wish to review CTRs may subscribe to the JanusGraph commits list so that they will see CTRs as they come through. If another committer responds with a -1, the commit should be rolled back and the formal RTC process should be followed.","title":"Commit-Then-Review (CTR)"},{"location":"development/#enable-automatic-backporting","text":"Before merging the pull request, it should be decided whether the contribution should be backported to other release branches. This should be the case for most non-breaking changes. If the change applies to something that is not present at all on other still supported release branches, like updating a dependency that was only introduced on master or code that was added / heavily modified on master , then backporting cannot work or would require too much effort. Otherwise, the appropriate labels for backporting (for example backport/v0.6 for the branch v0.6 ) should be added before the pull requests gets merged as that allows the backporting action to automatically handle the backporting.","title":"Enable Automatic Backporting"},{"location":"development/#merging-of-pull-requests","text":"A pull request is ready to be merged when it has been approved (see Review-Then-Commit (RTC) ). It can either be merged manually with git commands or through the GitHub UI. For pull requests that should be backported (see Enable Automatic Backporting ), the backporting action should create a pull request to backport the changes to other release branches after the pull request has been merged. The committer who merges the pull request should ensure that this automatic backporting succeeds. It may fail in case of merge conflicts. In that case the backporting needs to be performed manually .","title":"Merging of Pull Requests"},{"location":"development/#manual-backporting","text":"The Backport CLI tool can be used to manually backport a pull request. This can be necessary if the automatic backporting of a pull request fails. After installing the CLI tool, you also need to configure it with a GitHub access token. This is explained in the README.md of the Backport CLI tool. You can then use the tool to backport a pull request #xyz: backport --pr xyz This will checkout the repository, apply the change from the pull request to the appropriate release branch (like v0.6 ), and then ask you to resolve any merge conflicts. After the merge conflicts are resolved, it will create a pull request that backports the changes. Note that such a backporting pull request can usually be merged via CTR after all automatic checks have been executed. If non-trivial changes were necessary to resolve merge conflicts, then waiting for a review before merging the pull request may still make sense. It is of course also possible to use cherry-picking to apply the commits manually to a different release branch instead of using this CLI tool.","title":"Manual Backporting"},{"location":"development/#release-policy","text":"Any JanusGraph committer may propose a release. To propose a release, simple start a new RELEASE thread on janusgraph-dev proposing the new release and requesting feedback on what should be included in the release. After consensus is reached the release manager will perform the following tasks: Create a release branch so that work may continue on master Prepare the release artifacts Call a vote to approve the release on janusgraph-dev Committers will be given 72 hours to review and vote on the release artifacts Three +1 votes are required for a release to be approved One or more -1 votes with explanation will veto the release until the noted issues are addressed and the -1 votes are withdrawn","title":"Release Policy"},{"location":"development/#building-janusgraph","text":"To build JanusGraph you need git and Maven . Clone the JanusGraph repository from GitHub to a local directory. In that directory, execute mvn clean install . This will build JanusGraph and run the internal test suite. The internal test suite has no external dependencies. Note, that running all test cases requires a significant amount of time. To skip the tests when building JanusGraph, execute mvn clean install -DskipTests For comprehensive test coverage, execute mvn clean test -P comprehensive . This will run additional test covering communication to external storage backends, performance tests and concurrency tests. The comprehensive test suite uses HBase as external database and requires that HBase is installed. Note, that running the comprehensive test suite requires a significant amount of of time (> 1 hour).","title":"Building JanusGraph"},{"location":"development/#faqs","text":"Maven build causes dozens of \"[WARNING] We have a duplicate\u2026\" errors Make sure to use the maven-assembly-plugin when building or depending on JanusGraph.","title":"FAQs"},{"location":"advanced-topics/commit-releases/","text":"Snapshot releases In addition to official JanusGraph releases, JanusGraph publishes releases for each commit. The commit releases allow users to use latest JanusGraph features without relying on official JanusGraph releases. Official JanusGraph releases are better tested and usually come with finalized changes which signals that the used features are most likely to be stable for long term. Commit releases are not manually verified but instead verified by main CI tests as well as scheduled full CI tests (once per week). New features in commit releases are not guaranteed to be compatible between commits. Thus, you may expect more breaking changes between commit releases and official releases. Maven repository artifacts Both official and commit release are deployed to Sonatype OSS and are available in Sonatype Maven Central Repository under the same group id org.janusgraph , but with different artifact id format. Official JanusGraph releases have the next format for artifact id: MAJOR.MINOR.PATCH . Dependencies example: Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.2\" Artifact id for commit releases have the next format: FOLLOWING_VERSION-DATE-TIME.COMMIT . FOLLOWING_VERSION is the upcoming official version to be used after release is finalized (i.e. 0.6.3 if the current latest release is 0.6.2 ). It has MAJOR.MINOR.PATCH format. DATE - date of the commit release in yyyyMMdd format. TIME - time of the commit release in HHmmss format. COMMIT - short commit hash of the commit used in the release. Dependencies example: Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.3-20230104-164606.a49366e </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.3-20230104-164606.a49366e\" JanusGraph distribution builds In addition to distribution builds provided for each official JanusGraph release, snapshot distribution builds are provided for all commits. Info GitHub allows to download distribution builds only for authenticated GitHub users. To access the distribution build bundle for any commit, please open the commit you are interested in and select its CI Release details. For example: When CI Release page is opened for a specific commit, open summary and download the attached to Artifacts section file named distribution-builds . This will download distribution-builds.zip archive containing all distribution builds. Warning Old snapshot distribution builds are expiring in GitHub due to timing and memory limits. It's not guaranteed that the snapshot distribution build downloaded yesterday is available today. We encourage to use either official release distribution builds or newer snapshot distribution builds. If you see expired distribution builds or don't see any distribution builds for a specific commit, it means that it isn't available to be downloaded anymore. Thus, the distribution builds from newer commits should be used.","title":"Snapshot Releases"},{"location":"advanced-topics/commit-releases/#snapshot-releases","text":"In addition to official JanusGraph releases, JanusGraph publishes releases for each commit. The commit releases allow users to use latest JanusGraph features without relying on official JanusGraph releases. Official JanusGraph releases are better tested and usually come with finalized changes which signals that the used features are most likely to be stable for long term. Commit releases are not manually verified but instead verified by main CI tests as well as scheduled full CI tests (once per week). New features in commit releases are not guaranteed to be compatible between commits. Thus, you may expect more breaking changes between commit releases and official releases.","title":"Snapshot releases"},{"location":"advanced-topics/commit-releases/#maven-repository-artifacts","text":"Both official and commit release are deployed to Sonatype OSS and are available in Sonatype Maven Central Repository under the same group id org.janusgraph , but with different artifact id format. Official JanusGraph releases have the next format for artifact id: MAJOR.MINOR.PATCH . Dependencies example: Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.2 </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.2\" Artifact id for commit releases have the next format: FOLLOWING_VERSION-DATE-TIME.COMMIT . FOLLOWING_VERSION is the upcoming official version to be used after release is finalized (i.e. 0.6.3 if the current latest release is 0.6.2 ). It has MAJOR.MINOR.PATCH format. DATE - date of the commit release in yyyyMMdd format. TIME - time of the commit release in HHmmss format. COMMIT - short commit hash of the commit used in the release. Dependencies example: Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-core </artifactId> <version> 0.6.3-20230104-164606.a49366e </version> </dependency> Gradle compile \"org.janusgraph:janusgraph-core:0.6.3-20230104-164606.a49366e\"","title":"Maven repository artifacts"},{"location":"advanced-topics/commit-releases/#janusgraph-distribution-builds","text":"In addition to distribution builds provided for each official JanusGraph release, snapshot distribution builds are provided for all commits. Info GitHub allows to download distribution builds only for authenticated GitHub users. To access the distribution build bundle for any commit, please open the commit you are interested in and select its CI Release details. For example: When CI Release page is opened for a specific commit, open summary and download the attached to Artifacts section file named distribution-builds . This will download distribution-builds.zip archive containing all distribution builds. Warning Old snapshot distribution builds are expiring in GitHub due to timing and memory limits. It's not guaranteed that the snapshot distribution build downloaded yesterday is available today. We encourage to use either official release distribution builds or newer snapshot distribution builds. If you see expired distribution builds or don't see any distribution builds for a specific commit, it means that it isn't available to be downloaded anymore. Thus, the distribution builds from newer commits should be used.","title":"JanusGraph distribution builds"},{"location":"advanced-topics/data-model/","text":"JanusGraph Data Model JanusGraph stores graphs in adjacency list format which means that a graph is stored as a collection of vertices with their adjacency list. The adjacency list of a vertex contains all of the vertex\u2019s incident edges (and properties). By storing a graph in adjacency list format JanusGraph ensures that all of a vertex\u2019s incident edges and properties are stored compactly in the storage backend which speeds up traversals. The downside is that each edge has to be stored twice - once for each end vertex of the edge. In addition, JanusGraph maintains the adjacency list of each vertex in sort order with the order being defined by the sort key and sort order the edge labels. The sort order enables efficient retrievals of subsets of the adjacency list using vertex centric indices . JanusGraph stores the adjacency list representation of a graph in any storage backend that supports the Bigtable data model. Bigtable Data Model Under the Bigtable data model each table is a collection of rows. Each row is uniquely identified by a key. Each row is comprised of an arbitrary (large, but limited) number of cells. A cell is composed of a column and value. A cell is uniquely identified by a column within a given row. Rows in the Bigtable model are called \"wide rows\" because they support a large number of cells and the columns of those cells don\u2019t have to be defined up front as is required in relational databases. JanusGraph has an additional requirement for the Bigtable data model: The cells must be sorted by their columns and a subset of the cells specified by a column range must be efficiently retrievable (e.g. by using index structures, skip lists, or binary search). In addition, a particular Bigtable implementation may keep the rows sorted in the order of their key. JanusGraph can exploit such key-order to effectively partition the graph which provides better loading and traversal performance for very large graphs. However, this is not a requirement. JanusGraph Data Layout JanusGraph stores each adjacency list as a row in the underlying storage backend. The (64 bit) vertex id (which JanusGraph uniquely assigns to every vertex) is the key which points to the row containing the vertex\u2019s adjacency list. Each edge and property is stored as an individual cell in the row which allows for efficient insertions and deletions. The maximum number of cells allowed per row in a particular storage backend is therefore also the maximum degree of a vertex that JanusGraph can support against this backend. If the storage backend supports key-order, the adjacency lists will be ordered by vertex id, and JanusGraph can assign vertex ids such that the graph is effectively partitioned. Ids are assigned such that vertices which are frequently co-accessed have ids with small absolute difference. Individual Edge Layout Each edge and property is stored as one cell in the rows of its adjacent vertices. They are serialized such that the byte order of the column respects the sort key of the edge label. Variable id encoding schemes and compressed object serialization are used to keep the storage footprint of each edge/cell as small as possible. Consider the storage layout of an individual edge as visualized in the top row of the graphic above. The dark blue boxes represent numbers that are encoded with a variable length encoding scheme to reduce the number of bytes they consume. Red boxes represent one or multiple property values (i.e. objects) that are serialized with compressed meta data referenced in the associated property key. Grey boxes represent uncompressed property values (i.e. serialized objects). The serialized representation of an edge starts with the edge label\u2019s unique id (as assigned by JanusGraph). This is typically a small number and compressed well with variable id encoding. The last bit of this id is offset to store whether this is an incoming or outgoing edge. Next, the property value comprising the sort key are stored. The sort key is defined with the edge label and hence the sort key objects meta data can be referenced to the edge label. After that, the id of the adjacent vertex is stored. JanusGraph does not store the actual vertex id but the difference to the id of the vertex that owns this adjacency list. It is likely that the difference is a smaller number than the absolute id and hence compresses better. The vertex id is followed by the id of this edge. Each edge is assigned a unique id by JanusGraph. This concludes the column value of the edge\u2019s cell. The value of the edge\u2019s cell contains the compressed serialization of the signature properties of the edge (as defined by the label\u2019s signature key) and any other properties that have been added to the edge in uncompressed serialization. The serialized representation of a property is simpler and only contains the property\u2019s key id in the column. The property id and the property value are stored in the value. If the property key is defined as list() , however, the property id is stored in the column as well.","title":"JanusGraph Data model"},{"location":"advanced-topics/data-model/#janusgraph-data-model","text":"JanusGraph stores graphs in adjacency list format which means that a graph is stored as a collection of vertices with their adjacency list. The adjacency list of a vertex contains all of the vertex\u2019s incident edges (and properties). By storing a graph in adjacency list format JanusGraph ensures that all of a vertex\u2019s incident edges and properties are stored compactly in the storage backend which speeds up traversals. The downside is that each edge has to be stored twice - once for each end vertex of the edge. In addition, JanusGraph maintains the adjacency list of each vertex in sort order with the order being defined by the sort key and sort order the edge labels. The sort order enables efficient retrievals of subsets of the adjacency list using vertex centric indices . JanusGraph stores the adjacency list representation of a graph in any storage backend that supports the Bigtable data model.","title":"JanusGraph Data Model"},{"location":"advanced-topics/data-model/#bigtable-data-model","text":"Under the Bigtable data model each table is a collection of rows. Each row is uniquely identified by a key. Each row is comprised of an arbitrary (large, but limited) number of cells. A cell is composed of a column and value. A cell is uniquely identified by a column within a given row. Rows in the Bigtable model are called \"wide rows\" because they support a large number of cells and the columns of those cells don\u2019t have to be defined up front as is required in relational databases. JanusGraph has an additional requirement for the Bigtable data model: The cells must be sorted by their columns and a subset of the cells specified by a column range must be efficiently retrievable (e.g. by using index structures, skip lists, or binary search). In addition, a particular Bigtable implementation may keep the rows sorted in the order of their key. JanusGraph can exploit such key-order to effectively partition the graph which provides better loading and traversal performance for very large graphs. However, this is not a requirement.","title":"Bigtable Data Model"},{"location":"advanced-topics/data-model/#janusgraph-data-layout","text":"JanusGraph stores each adjacency list as a row in the underlying storage backend. The (64 bit) vertex id (which JanusGraph uniquely assigns to every vertex) is the key which points to the row containing the vertex\u2019s adjacency list. Each edge and property is stored as an individual cell in the row which allows for efficient insertions and deletions. The maximum number of cells allowed per row in a particular storage backend is therefore also the maximum degree of a vertex that JanusGraph can support against this backend. If the storage backend supports key-order, the adjacency lists will be ordered by vertex id, and JanusGraph can assign vertex ids such that the graph is effectively partitioned. Ids are assigned such that vertices which are frequently co-accessed have ids with small absolute difference.","title":"JanusGraph Data Layout"},{"location":"advanced-topics/data-model/#individual-edge-layout","text":"Each edge and property is stored as one cell in the rows of its adjacent vertices. They are serialized such that the byte order of the column respects the sort key of the edge label. Variable id encoding schemes and compressed object serialization are used to keep the storage footprint of each edge/cell as small as possible. Consider the storage layout of an individual edge as visualized in the top row of the graphic above. The dark blue boxes represent numbers that are encoded with a variable length encoding scheme to reduce the number of bytes they consume. Red boxes represent one or multiple property values (i.e. objects) that are serialized with compressed meta data referenced in the associated property key. Grey boxes represent uncompressed property values (i.e. serialized objects). The serialized representation of an edge starts with the edge label\u2019s unique id (as assigned by JanusGraph). This is typically a small number and compressed well with variable id encoding. The last bit of this id is offset to store whether this is an incoming or outgoing edge. Next, the property value comprising the sort key are stored. The sort key is defined with the edge label and hence the sort key objects meta data can be referenced to the edge label. After that, the id of the adjacent vertex is stored. JanusGraph does not store the actual vertex id but the difference to the id of the vertex that owns this adjacency list. It is likely that the difference is a smaller number than the absolute id and hence compresses better. The vertex id is followed by the id of this edge. Each edge is assigned a unique id by JanusGraph. This concludes the column value of the edge\u2019s cell. The value of the edge\u2019s cell contains the compressed serialization of the signature properties of the edge (as defined by the label\u2019s signature key) and any other properties that have been added to the edge in uncompressed serialization. The serialized representation of a property is simpler and only contains the property\u2019s key id in the column. The property id and the property value are stored in the value. If the property key is defined as list() , however, the property id is stored in the column as well.","title":"Individual Edge Layout"},{"location":"advanced-topics/eventual-consistency/","text":"Eventually-Consistent Storage Backends When running JanusGraph against an eventually consistent storage backend special JanusGraph features must be used to ensure data consistency and special considerations must be made regarding data degradation. This page summarizes some of the aspects to consider when running JanusGraph on top of an eventually consistent storage backend like Apache Cassandra or Apache HBase. Data Consistency On eventually consistent storage backends, JanusGraph must obtain locks in order to ensure consistency because the underlying storage backend does not provide transactional isolation. In the interest of efficiency, JanusGraph does not use locking by default. Hence, the user has to decide for each schema element that defines a consistency constraint whether or not to use locking. Use JanusGraphManagement.setConsistency(element, ConsistencyModifier.LOCK) to explicitly enable locking on a schema element as shown in the following examples. mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'consistentName' ). dataType ( String . class ). make () index = mgmt . buildIndex ( 'byConsistentName' , Vertex . class ). addKey ( name ). unique (). buildCompositeIndex () mgmt . setConsistency ( name , ConsistencyModifier . LOCK ) // Ensures only one name per vertex mgmt . setConsistency ( index , ConsistencyModifier . LOCK ) // Ensures name uniqueness in the graph mgmt . commit () When updating an element that is guarded by a uniqueness constraint, JanusGraph uses the following protocol at the end of a transaction when calling tx.commit() : Acquire a lock on all elements that have a consistency constraint Re-read those elements from the storage backend and verify that they match the state of the element in the current transaction prior to modification. If not, the element was concurrently modified and a PermanentLocking exception is thrown. Persist the state of the transaction against the storage backend. Release all locks. This is a brief description of the locking protocol which leaves out optimizations (e.g. local conflict detection) and detection of failure scenarios (e.g. expired locks). The actual lock application mechanism is abstracted such that JanusGraph can use multiple implementations of a locking provider. Currently, only one locking provider is included in the JanusGraph distribution: A locking implementation based on key-consistent read and write operations that is agnostic to the underlying storage backend as long as it supports key-consistent operations (which includes Cassandra and HBase). This is the default implementation and uses timestamp based lock applications to determine which transaction holds the lock. It requires that clocks are synchronized across all machines in the cluster. Warning The locking implementation is not robust against all failure scenarios. For instance, when a Cassandra cluster drops below quorum, consistency is no longer ensured. Hence, it is suggested to use locking-based consistency constraints sparingly with eventually consistent storage backends. For use cases that require strict and or frequent consistency constraint enforcement, it is suggested to use a storage backend that provides transactional isolation. Data Consistency without Locks Because of the additional steps required to acquire a lock when committing a modifying transaction, locking is a fairly expensive way to ensure consistency and can lead to deadlock when very many concurrent transactions try to modify the same elements in the graph. Hence, locking should be used in situations where consistency is more important than write latency and the number of conflicting transactions is small. In other situations, it may be better to allow conflicting transactions to proceed and to resolve inconsistencies at read time. This is a design pattern commonly employed in large scale data systems and most effective when the actual likelihood of conflict is small. Hence, write transactions don\u2019t incur additional overhead and any (unlikely) conflict that does occur is detected and resolved at read time and later cleaned up. JanusGraph makes it easy to use this strategy through the following features. Forking Edges Because edge are stored as single records in the underlying storage backend, concurrently modifying a single edge would lead to conflict. Instead of locking, an edge label can be configured to use ConsistencyModifier.FORK . The following example creates a new edge label related and defines its consistency to FORK. mgmt = graph . openManagement () related = mgmt . makeEdgeLabel ( 'related' ). make () mgmt . setConsistency ( related , ConsistencyModifier . FORK ) mgmt . commit () When modifying an edge whose label is configured to FORK the edge is deleted and the modified edge is added as a new one. Hence, if two concurrent transactions modify the same edge, two modified copies of the edge will exist upon commit which can be resolved during querying traversals if needed. Note Edge forking only applies to MULTI edges. Edge labels with a multiplicity constraint cannot use this strategy since a constraint is built into the edge label definition that requires an explicit lock or use the conflict resolution mechanism of the underlying storage backend. Multi-Properties Modifying single valued properties on vertices concurrently can result in a conflict. Similarly to edges, one can allow an arbitrary number of properties on a vertex for a particular property key defined with cardinality LIST and FORK on modification. Hence, instead of conflict one reads multiple properties. Since JanusGraph allows properties on properties, provenance information like author can be added to the properties to facilitate resolution at read time. See multi-properties to learn how to define those. Data Inconsistency Temporary Inconsistency On eventually consistent storage backends, writes may not be immediately visible to the entire cluster causing temporary inconsistencies in the graph. This is an inherent property of eventual consistency, in the sense, that accepted updates must be propagated to other instances in the cluster and no guarantees are made with respect to read atomicity in the interest of performance. From JanusGraph\u2019s perspective, eventual consistency might cause the following temporary graph inconsistencies in addition the general inconsistency that some parts of a transaction are visible while others aren\u2019t yet. Stale Index entries Index entries might point to nonexistent vertices or edges. Similarly, a vertex or edge appears in the graph but is not yet indexed and hence ignored by global graph queries. In some situations due to server failures permanent index inconsistency can happen. See how to deal with permanent stale index entries here . Half-Edges Only one direction of an edge gets persisted or deleted which might lead to the edge not being or incorrectly being retrieved. Note In order to avoid that write failures result in permanent inconsistencies in the graph it is recommended to use storage backends that support batch write atomicity and to ensure that write atomicity is enabled. To get the benefit of write atomicity, the number modifications made in a single transaction must be smaller than the configured buffer-size option documented in Configuration Reference . The buffer size defines the maximum number of modifications that JanusGraph will persist in a single batch. If a transaction has more modifications, the persistence will be split into multiple batches which are persisted individually which is useful for batch loading but invalidates write atomicity. Ghost Vertices A permanent inconsistency that can arise when operating JanusGraph on eventually consistent storage backend is the phenomena of ghost vertices . If a vertex gets deleted while it is concurrently being modified, the vertex might re-appear as a ghost . The following strategies can be used to mitigate this issue: Existence checks Configure transactions to (double) check for the existence of vertices prior to returning them. Please see Transaction Configuration for more information and note that this can significantly decrease performance. Note, that this does not fix the inconsistencies but hides some of them from the user. Regular Clean-ups Run regular batch-jobs to repair inconsistencies in the graph using JanusGraph with TinkerPop\u2019s Hadoop-Gremlin . This is the only strategy that can address all inconsistencies and effectively repair them. Soft Deletes Instead of deleting vertices, they are marked as deleted which keeps them in the graph for future analysis but hides them from user-facing transactions.","title":"Eventually-Consistent Storage Backends"},{"location":"advanced-topics/eventual-consistency/#eventually-consistent-storage-backends","text":"When running JanusGraph against an eventually consistent storage backend special JanusGraph features must be used to ensure data consistency and special considerations must be made regarding data degradation. This page summarizes some of the aspects to consider when running JanusGraph on top of an eventually consistent storage backend like Apache Cassandra or Apache HBase.","title":"Eventually-Consistent Storage Backends"},{"location":"advanced-topics/eventual-consistency/#data-consistency","text":"On eventually consistent storage backends, JanusGraph must obtain locks in order to ensure consistency because the underlying storage backend does not provide transactional isolation. In the interest of efficiency, JanusGraph does not use locking by default. Hence, the user has to decide for each schema element that defines a consistency constraint whether or not to use locking. Use JanusGraphManagement.setConsistency(element, ConsistencyModifier.LOCK) to explicitly enable locking on a schema element as shown in the following examples. mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'consistentName' ). dataType ( String . class ). make () index = mgmt . buildIndex ( 'byConsistentName' , Vertex . class ). addKey ( name ). unique (). buildCompositeIndex () mgmt . setConsistency ( name , ConsistencyModifier . LOCK ) // Ensures only one name per vertex mgmt . setConsistency ( index , ConsistencyModifier . LOCK ) // Ensures name uniqueness in the graph mgmt . commit () When updating an element that is guarded by a uniqueness constraint, JanusGraph uses the following protocol at the end of a transaction when calling tx.commit() : Acquire a lock on all elements that have a consistency constraint Re-read those elements from the storage backend and verify that they match the state of the element in the current transaction prior to modification. If not, the element was concurrently modified and a PermanentLocking exception is thrown. Persist the state of the transaction against the storage backend. Release all locks. This is a brief description of the locking protocol which leaves out optimizations (e.g. local conflict detection) and detection of failure scenarios (e.g. expired locks). The actual lock application mechanism is abstracted such that JanusGraph can use multiple implementations of a locking provider. Currently, only one locking provider is included in the JanusGraph distribution: A locking implementation based on key-consistent read and write operations that is agnostic to the underlying storage backend as long as it supports key-consistent operations (which includes Cassandra and HBase). This is the default implementation and uses timestamp based lock applications to determine which transaction holds the lock. It requires that clocks are synchronized across all machines in the cluster. Warning The locking implementation is not robust against all failure scenarios. For instance, when a Cassandra cluster drops below quorum, consistency is no longer ensured. Hence, it is suggested to use locking-based consistency constraints sparingly with eventually consistent storage backends. For use cases that require strict and or frequent consistency constraint enforcement, it is suggested to use a storage backend that provides transactional isolation.","title":"Data Consistency"},{"location":"advanced-topics/eventual-consistency/#data-consistency-without-locks","text":"Because of the additional steps required to acquire a lock when committing a modifying transaction, locking is a fairly expensive way to ensure consistency and can lead to deadlock when very many concurrent transactions try to modify the same elements in the graph. Hence, locking should be used in situations where consistency is more important than write latency and the number of conflicting transactions is small. In other situations, it may be better to allow conflicting transactions to proceed and to resolve inconsistencies at read time. This is a design pattern commonly employed in large scale data systems and most effective when the actual likelihood of conflict is small. Hence, write transactions don\u2019t incur additional overhead and any (unlikely) conflict that does occur is detected and resolved at read time and later cleaned up. JanusGraph makes it easy to use this strategy through the following features.","title":"Data Consistency without Locks"},{"location":"advanced-topics/eventual-consistency/#forking-edges","text":"Because edge are stored as single records in the underlying storage backend, concurrently modifying a single edge would lead to conflict. Instead of locking, an edge label can be configured to use ConsistencyModifier.FORK . The following example creates a new edge label related and defines its consistency to FORK. mgmt = graph . openManagement () related = mgmt . makeEdgeLabel ( 'related' ). make () mgmt . setConsistency ( related , ConsistencyModifier . FORK ) mgmt . commit () When modifying an edge whose label is configured to FORK the edge is deleted and the modified edge is added as a new one. Hence, if two concurrent transactions modify the same edge, two modified copies of the edge will exist upon commit which can be resolved during querying traversals if needed. Note Edge forking only applies to MULTI edges. Edge labels with a multiplicity constraint cannot use this strategy since a constraint is built into the edge label definition that requires an explicit lock or use the conflict resolution mechanism of the underlying storage backend.","title":"Forking Edges"},{"location":"advanced-topics/eventual-consistency/#multi-properties","text":"Modifying single valued properties on vertices concurrently can result in a conflict. Similarly to edges, one can allow an arbitrary number of properties on a vertex for a particular property key defined with cardinality LIST and FORK on modification. Hence, instead of conflict one reads multiple properties. Since JanusGraph allows properties on properties, provenance information like author can be added to the properties to facilitate resolution at read time. See multi-properties to learn how to define those.","title":"Multi-Properties"},{"location":"advanced-topics/eventual-consistency/#data-inconsistency","text":"","title":"Data Inconsistency"},{"location":"advanced-topics/eventual-consistency/#temporary-inconsistency","text":"On eventually consistent storage backends, writes may not be immediately visible to the entire cluster causing temporary inconsistencies in the graph. This is an inherent property of eventual consistency, in the sense, that accepted updates must be propagated to other instances in the cluster and no guarantees are made with respect to read atomicity in the interest of performance. From JanusGraph\u2019s perspective, eventual consistency might cause the following temporary graph inconsistencies in addition the general inconsistency that some parts of a transaction are visible while others aren\u2019t yet. Stale Index entries Index entries might point to nonexistent vertices or edges. Similarly, a vertex or edge appears in the graph but is not yet indexed and hence ignored by global graph queries. In some situations due to server failures permanent index inconsistency can happen. See how to deal with permanent stale index entries here . Half-Edges Only one direction of an edge gets persisted or deleted which might lead to the edge not being or incorrectly being retrieved. Note In order to avoid that write failures result in permanent inconsistencies in the graph it is recommended to use storage backends that support batch write atomicity and to ensure that write atomicity is enabled. To get the benefit of write atomicity, the number modifications made in a single transaction must be smaller than the configured buffer-size option documented in Configuration Reference . The buffer size defines the maximum number of modifications that JanusGraph will persist in a single batch. If a transaction has more modifications, the persistence will be split into multiple batches which are persisted individually which is useful for batch loading but invalidates write atomicity.","title":"Temporary Inconsistency"},{"location":"advanced-topics/eventual-consistency/#ghost-vertices","text":"A permanent inconsistency that can arise when operating JanusGraph on eventually consistent storage backend is the phenomena of ghost vertices . If a vertex gets deleted while it is concurrently being modified, the vertex might re-appear as a ghost . The following strategies can be used to mitigate this issue: Existence checks Configure transactions to (double) check for the existence of vertices prior to returning them. Please see Transaction Configuration for more information and note that this can significantly decrease performance. Note, that this does not fix the inconsistencies but hides some of them from the user. Regular Clean-ups Run regular batch-jobs to repair inconsistencies in the graph using JanusGraph with TinkerPop\u2019s Hadoop-Gremlin . This is the only strategy that can address all inconsistencies and effectively repair them. Soft Deletes Instead of deleting vertices, they are marked as deleted which keeps them in the graph for future analysis but hides them from user-facing transactions.","title":"Ghost Vertices"},{"location":"advanced-topics/executor-service/","text":"Backend ExecutorService By default JanusGraph uses ExecutorService to process some queries in parallel (see storage.parallel-backend-ops configuration option). JanusGraph allows to configure the executor service which is used via configuration options provided in storage.parallel-backend-executor-service configuration section. Currently JanusGraph has the next ExecutorService implementations which can be used (controlled via storage.parallel-backend-executor-service.class ): fixed - fixed thread pool size; cached - cached thread pool size; Custom ExecutorService; Custom ExecutorService To use custom ExecutorService the configuration option storage.parallel-backend-executor-service.class must be provided which must be the full class name of the ExecutorService implementation class. The provided class which implements ExecutorService must have either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. When both constructors are available the constructor with ExecutorServiceConfiguration argument will be used. When custom ExecutorService is provided with ExecutorServiceConfiguration it isn't required to use any of the configuration options provided in ExecutorServiceConfiguration but sometimes it is convenient to use the provided configurations. The configuration options provided in ExecutorServiceConfiguration are typically those configuration options which are provided via configurations from storage.parallel-backend-executor-service . Cassandra backend ExecutorService Apart from the general executor service mentioned in the previous section, Cassandra backend uses an additional ExecutorService to process CQL queries by default (see storage.cql.executor-service.enabled configuration option). The rules by which the Cassandra backend ExecutorService is built are the same as the rules which are used to build parallel backend queries ExecutorService (described above). The only difference is that the configuration for Cassandra backend ExecutorService are provided via configuration options under storage.cql.executor-service . Disabling CQL executor service reduces overhead of thread pool but requires the user to tune maximum throughput thoroughly. With disabled CQL executor service the parallelism will be controlled internally by the CQL driver via the next properties: storage.cql.max-requests-per-connection , storage.cql.local-max-connections-per-host , storage.cql.remote-max-connections-per-host . Info It is recommended to disable CQL executor service in a production environment and properly configure maximum throughput. CQL executor service does not provide any benefits other than limiting the amount of parallel requests per JanusGraph instance. Warning Improper tuning of maximum throughput might result in failures under heavy workloads.","title":"ExecutorService"},{"location":"advanced-topics/executor-service/#backend-executorservice","text":"By default JanusGraph uses ExecutorService to process some queries in parallel (see storage.parallel-backend-ops configuration option). JanusGraph allows to configure the executor service which is used via configuration options provided in storage.parallel-backend-executor-service configuration section. Currently JanusGraph has the next ExecutorService implementations which can be used (controlled via storage.parallel-backend-executor-service.class ): fixed - fixed thread pool size; cached - cached thread pool size; Custom ExecutorService;","title":"Backend ExecutorService"},{"location":"advanced-topics/executor-service/#custom-executorservice","text":"To use custom ExecutorService the configuration option storage.parallel-backend-executor-service.class must be provided which must be the full class name of the ExecutorService implementation class. The provided class which implements ExecutorService must have either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. When both constructors are available the constructor with ExecutorServiceConfiguration argument will be used. When custom ExecutorService is provided with ExecutorServiceConfiguration it isn't required to use any of the configuration options provided in ExecutorServiceConfiguration but sometimes it is convenient to use the provided configurations. The configuration options provided in ExecutorServiceConfiguration are typically those configuration options which are provided via configurations from storage.parallel-backend-executor-service .","title":"Custom ExecutorService"},{"location":"advanced-topics/executor-service/#cassandra-backend-executorservice","text":"Apart from the general executor service mentioned in the previous section, Cassandra backend uses an additional ExecutorService to process CQL queries by default (see storage.cql.executor-service.enabled configuration option). The rules by which the Cassandra backend ExecutorService is built are the same as the rules which are used to build parallel backend queries ExecutorService (described above). The only difference is that the configuration for Cassandra backend ExecutorService are provided via configuration options under storage.cql.executor-service . Disabling CQL executor service reduces overhead of thread pool but requires the user to tune maximum throughput thoroughly. With disabled CQL executor service the parallelism will be controlled internally by the CQL driver via the next properties: storage.cql.max-requests-per-connection , storage.cql.local-max-connections-per-host , storage.cql.remote-max-connections-per-host . Info It is recommended to disable CQL executor service in a production environment and properly configure maximum throughput. CQL executor service does not provide any benefits other than limiting the amount of parallel requests per JanusGraph instance. Warning Improper tuning of maximum throughput might result in failures under heavy workloads.","title":"Cassandra backend ExecutorService"},{"location":"advanced-topics/hadoop/","text":"JanusGraph with TinkerPop\u2019s Hadoop-Gremlin This chapter describes how to leverage Apache Hadoop and Apache Spark to configure JanusGraph for distributed graph processing. These steps will provide an overview on how to get started with those projects, but please refer to those project communities to become more deeply familiar with them. JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP. For the scope of the example below, Apache Spark is the computing framework and Apache Cassandra is the storage backend. The directions can be followed with other packages with minor changes to the configuration properties. Note The examples in this chapter are based on running Spark in local mode or standalone cluster mode. Additional configuration is required when using Spark on YARN or Mesos. Configuring Hadoop for Running OLAP For running OLAP queries from the Gremlin Console, a few prerequisites need to be fulfilled. You will need to add the Hadoop configuration directory into the CLASSPATH , and the configuration directory needs to point to a live Hadoop cluster. Hadoop provides a distributed access-controlled file system. The Hadoop file system is used by Spark workers running on different machines to have a common source for file based operations. The intermediate computations of various OLAP queries may be persisted on the Hadoop file system. For configuring a single node Hadoop cluster, please refer to official Apache Hadoop Docs Once you have a Hadoop cluster up and running, we will need to specify the Hadoop configuration files in the CLASSPATH . The below document expects that you have those configuration files located under /etc/hadoop/conf . Once verified, follow the below steps to add the Hadoop configuration to the CLASSPATH and start the Gremlin Console, which will play the role of the Spark driver program. export HADOOP_CONF_DIR = /etc/hadoop/conf export CLASSPATH = $HADOOP_CONF_DIR bin/gremlin.sh Once the path to Hadoop configuration has been added to the CLASSPATH , we can verify whether the Gremlin Console can access the Hadoop cluster by following these quick steps: gremlin > hdfs ==> storage [ org . apache . hadoop . fs . LocalFileSystem @ 65 bb9029 ] // BAD gremlin > hdfs ==> storage [ DFS [ DFSClient [ clientName = DFSClient_NONMAPREDUCE_1229457199_1 , ugi = user ( auth: SIMPLE )]]] // GOOD OLAP Traversals JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP to traverse over the graph, and parallelize queries by leveraging Apache Spark. OLAP Traversals with Spark Local OLAP Examples below are showing configuration examples for directly supported backends by JanusGraph. Additional configuration will be needed that is specific to that storage backend. The configuration is specified by the gremlin.hadoop.graphReader property which specifies the class to read data from the storage backend. JanusGraph directly supports following graphReader classes: CqlInputFormat for use with Cassandra HBaseInputFormat and HBaseSnapshotInputFormat for use with HBase The following .properties files can be used to connect a JanusGraph instance such that it can be used with HadoopGraph to run OLAP queries. read-cql.properties # Copyright 2019 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.cql.CqlInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph Cassandra InputFormat configuration # # These properties defines the connection properties which were used while write data to JanusGraph. janusgraphmr.ioformat.conf.storage.backend = cql # This specifies the hostname & port for Cassandra data store. janusgraphmr.ioformat.conf.storage.hostname = 127.0.0.1 janusgraphmr.ioformat.conf.storage.port = 9042 # This specifies the keyspace where data is stored. janusgraphmr.ioformat.conf.storage.cql.keyspace = janusgraph # This defines the indexing backend configuration used while writing data to JanusGraph. janusgraphmr.ioformat.conf.index.search.backend = elasticsearch janusgraphmr.ioformat.conf.index.search.hostname = 127.0.0.1 # Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr). # # Apache Cassandra InputFormat configuration # cassandra.input.partitioner.class = org.apache.cassandra.dht.Murmur3Partitioner cassandra.input.widerows = true # # SparkGraphComputer Configuration # spark.master = local[*] spark.executor.memory = 1g spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator read-hbase.properties # Copyright 2019 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.hbase.HBaseInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph HBase InputFormat configuration # janusgraphmr.ioformat.conf.storage.backend = hbase janusgraphmr.ioformat.conf.storage.hostname = localhost janusgraphmr.ioformat.conf.storage.hbase.table = janusgraph # # SparkGraphComputer Configuration # spark.master = local[*] spark.executor.memory = 1g spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator First create a properties file with above configurations, and load the same on the Gremlin Console to run OLAP queries as follows: read-cql.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from Cassandra 3 gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-cql.properties' ) == >hadoopgraph [ cqlinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ cqlinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046 read-hbase.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from HBase gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-hbase.properties' ) == >hadoopgraph [ hbaseinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ hbaseinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046 OLAP Traversals with Spark Standalone Cluster The steps followed in the previous section can also be used with a Spark standalone cluster with only minor changes: Update the spark.master property to point to the Spark master URL instead of local Update the spark.executor.extraClassPath to enable the Spark executor to find the JanusGraph dependency jars Copy the JanusGraph dependency jars into the location specified in the previous step on each Spark executor machine Note We have copied all the jars under janusgraph-distribution/lib into /opt/lib/janusgraph/ and the same directory structure is created across all workers, and jars are manually copied across all workers. The final properties file used for OLAP traversal is as follows: read-cql-standalone-cluster.properties # Copyright 2020 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.cql.CqlInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph Cassandra InputFormat configuration # # These properties defines the connection properties which were used while write data to JanusGraph. janusgraphmr.ioformat.conf.storage.backend = cql # This specifies the hostname & port for Cassandra data store. janusgraphmr.ioformat.conf.storage.hostname = 127.0.0.1 janusgraphmr.ioformat.conf.storage.port = 9042 # This specifies the keyspace where data is stored. janusgraphmr.ioformat.conf.storage.cql.keyspace = janusgraph # This defines the indexing backend configuration used while writing data to JanusGraph. janusgraphmr.ioformat.conf.index.search.backend = elasticsearch janusgraphmr.ioformat.conf.index.search.hostname = 127.0.0.1 # Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr). # # Apache Cassandra InputFormat configuration # cassandra.input.partitioner.class = org.apache.cassandra.dht.Murmur3Partitioner cassandra.input.widerows = true # # SparkGraphComputer Configuration # spark.master = spark://127.0.0.1:7077 spark.executor.memory = 1g spark.executor.extraClassPath = /opt/lib/janusgraph/* spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator read-hbase-standalone-cluster.properties # Copyright 2020 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.hbase.HBaseInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph HBase InputFormat configuration # janusgraphmr.ioformat.conf.storage.backend = hbase janusgraphmr.ioformat.conf.storage.hostname = localhost janusgraphmr.ioformat.conf.storage.hbase.table = janusgraph # # SparkGraphComputer Configuration # spark.master = spark://127.0.0.1:7077 spark.executor.memory = 1g spark.executor.extraClassPath = /opt/lib/janusgraph/* spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator Then use the properties file as follows from the Gremlin Console: read-cql-standalone-cluster.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from Cassandra 3 gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-cql-standalone-cluster.properties' ) == >hadoopgraph [ cqlinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ cqlinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046 read-hbase-standalone-cluster.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from HBase gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-hbase-standalone-cluster.properties' ) == >hadoopgraph [ hbaseinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ hbaseinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046 Other Vertex Programs Apache TinkerPop provides various vertex programs. A vertex program runs on each vertex until either a termination criteria is attained or a fixed number of iterations has been reached. Due to the parallel nature of vertex programs, they can leverage parallel computing framework like Spark to improve their performance. Once you are familiar with how to configure JanusGraph to work with Spark, you can run all the other vertex programs provided by Apache TinkerPop, like Page Rank, Bulk Loading and Peer Pressure. See the TinkerPop VertexProgram docs for more details.","title":"JanusGraph with TinkerPop\u2019s Hadoop-Gremlin"},{"location":"advanced-topics/hadoop/#janusgraph-with-tinkerpops-hadoop-gremlin","text":"This chapter describes how to leverage Apache Hadoop and Apache Spark to configure JanusGraph for distributed graph processing. These steps will provide an overview on how to get started with those projects, but please refer to those project communities to become more deeply familiar with them. JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP. For the scope of the example below, Apache Spark is the computing framework and Apache Cassandra is the storage backend. The directions can be followed with other packages with minor changes to the configuration properties. Note The examples in this chapter are based on running Spark in local mode or standalone cluster mode. Additional configuration is required when using Spark on YARN or Mesos.","title":"JanusGraph with TinkerPop\u2019s Hadoop-Gremlin"},{"location":"advanced-topics/hadoop/#configuring-hadoop-for-running-olap","text":"For running OLAP queries from the Gremlin Console, a few prerequisites need to be fulfilled. You will need to add the Hadoop configuration directory into the CLASSPATH , and the configuration directory needs to point to a live Hadoop cluster. Hadoop provides a distributed access-controlled file system. The Hadoop file system is used by Spark workers running on different machines to have a common source for file based operations. The intermediate computations of various OLAP queries may be persisted on the Hadoop file system. For configuring a single node Hadoop cluster, please refer to official Apache Hadoop Docs Once you have a Hadoop cluster up and running, we will need to specify the Hadoop configuration files in the CLASSPATH . The below document expects that you have those configuration files located under /etc/hadoop/conf . Once verified, follow the below steps to add the Hadoop configuration to the CLASSPATH and start the Gremlin Console, which will play the role of the Spark driver program. export HADOOP_CONF_DIR = /etc/hadoop/conf export CLASSPATH = $HADOOP_CONF_DIR bin/gremlin.sh Once the path to Hadoop configuration has been added to the CLASSPATH , we can verify whether the Gremlin Console can access the Hadoop cluster by following these quick steps: gremlin > hdfs ==> storage [ org . apache . hadoop . fs . LocalFileSystem @ 65 bb9029 ] // BAD gremlin > hdfs ==> storage [ DFS [ DFSClient [ clientName = DFSClient_NONMAPREDUCE_1229457199_1 , ugi = user ( auth: SIMPLE )]]] // GOOD","title":"Configuring Hadoop for Running OLAP"},{"location":"advanced-topics/hadoop/#olap-traversals","text":"JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP to traverse over the graph, and parallelize queries by leveraging Apache Spark.","title":"OLAP Traversals"},{"location":"advanced-topics/hadoop/#olap-traversals-with-spark-local","text":"OLAP Examples below are showing configuration examples for directly supported backends by JanusGraph. Additional configuration will be needed that is specific to that storage backend. The configuration is specified by the gremlin.hadoop.graphReader property which specifies the class to read data from the storage backend. JanusGraph directly supports following graphReader classes: CqlInputFormat for use with Cassandra HBaseInputFormat and HBaseSnapshotInputFormat for use with HBase The following .properties files can be used to connect a JanusGraph instance such that it can be used with HadoopGraph to run OLAP queries. read-cql.properties # Copyright 2019 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.cql.CqlInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph Cassandra InputFormat configuration # # These properties defines the connection properties which were used while write data to JanusGraph. janusgraphmr.ioformat.conf.storage.backend = cql # This specifies the hostname & port for Cassandra data store. janusgraphmr.ioformat.conf.storage.hostname = 127.0.0.1 janusgraphmr.ioformat.conf.storage.port = 9042 # This specifies the keyspace where data is stored. janusgraphmr.ioformat.conf.storage.cql.keyspace = janusgraph # This defines the indexing backend configuration used while writing data to JanusGraph. janusgraphmr.ioformat.conf.index.search.backend = elasticsearch janusgraphmr.ioformat.conf.index.search.hostname = 127.0.0.1 # Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr). # # Apache Cassandra InputFormat configuration # cassandra.input.partitioner.class = org.apache.cassandra.dht.Murmur3Partitioner cassandra.input.widerows = true # # SparkGraphComputer Configuration # spark.master = local[*] spark.executor.memory = 1g spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator read-hbase.properties # Copyright 2019 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.hbase.HBaseInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph HBase InputFormat configuration # janusgraphmr.ioformat.conf.storage.backend = hbase janusgraphmr.ioformat.conf.storage.hostname = localhost janusgraphmr.ioformat.conf.storage.hbase.table = janusgraph # # SparkGraphComputer Configuration # spark.master = local[*] spark.executor.memory = 1g spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator First create a properties file with above configurations, and load the same on the Gremlin Console to run OLAP queries as follows: read-cql.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from Cassandra 3 gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-cql.properties' ) == >hadoopgraph [ cqlinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ cqlinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046 read-hbase.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from HBase gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-hbase.properties' ) == >hadoopgraph [ hbaseinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ hbaseinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046","title":"OLAP Traversals with Spark Local"},{"location":"advanced-topics/hadoop/#olap-traversals-with-spark-standalone-cluster","text":"The steps followed in the previous section can also be used with a Spark standalone cluster with only minor changes: Update the spark.master property to point to the Spark master URL instead of local Update the spark.executor.extraClassPath to enable the Spark executor to find the JanusGraph dependency jars Copy the JanusGraph dependency jars into the location specified in the previous step on each Spark executor machine Note We have copied all the jars under janusgraph-distribution/lib into /opt/lib/janusgraph/ and the same directory structure is created across all workers, and jars are manually copied across all workers. The final properties file used for OLAP traversal is as follows: read-cql-standalone-cluster.properties # Copyright 2020 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.cql.CqlInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph Cassandra InputFormat configuration # # These properties defines the connection properties which were used while write data to JanusGraph. janusgraphmr.ioformat.conf.storage.backend = cql # This specifies the hostname & port for Cassandra data store. janusgraphmr.ioformat.conf.storage.hostname = 127.0.0.1 janusgraphmr.ioformat.conf.storage.port = 9042 # This specifies the keyspace where data is stored. janusgraphmr.ioformat.conf.storage.cql.keyspace = janusgraph # This defines the indexing backend configuration used while writing data to JanusGraph. janusgraphmr.ioformat.conf.index.search.backend = elasticsearch janusgraphmr.ioformat.conf.index.search.hostname = 127.0.0.1 # Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr). # # Apache Cassandra InputFormat configuration # cassandra.input.partitioner.class = org.apache.cassandra.dht.Murmur3Partitioner cassandra.input.widerows = true # # SparkGraphComputer Configuration # spark.master = spark://127.0.0.1:7077 spark.executor.memory = 1g spark.executor.extraClassPath = /opt/lib/janusgraph/* spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator read-hbase-standalone-cluster.properties # Copyright 2020 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # # Hadoop Graph Configuration # gremlin.graph = org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph gremlin.hadoop.graphReader = org.janusgraph.hadoop.formats.hbase.HBaseInputFormat gremlin.hadoop.graphWriter = org.apache.hadoop.mapreduce.lib.output.NullOutputFormat gremlin.hadoop.jarsInDistributedCache = true gremlin.hadoop.inputLocation = none gremlin.hadoop.outputLocation = output gremlin.spark.persistContext = true # # JanusGraph HBase InputFormat configuration # janusgraphmr.ioformat.conf.storage.backend = hbase janusgraphmr.ioformat.conf.storage.hostname = localhost janusgraphmr.ioformat.conf.storage.hbase.table = janusgraph # # SparkGraphComputer Configuration # spark.master = spark://127.0.0.1:7077 spark.executor.memory = 1g spark.executor.extraClassPath = /opt/lib/janusgraph/* spark.serializer = org.apache.spark.serializer.KryoSerializer spark.kryo.registrator = org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator Then use the properties file as follows from the Gremlin Console: read-cql-standalone-cluster.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from Cassandra 3 gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-cql-standalone-cluster.properties' ) == >hadoopgraph [ cqlinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ cqlinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046 read-hbase-standalone-cluster.properties bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- plugin activated: janusgraph.imports gremlin> :plugin use tinkerpop.hadoop == >tinkerpop.hadoop activated gremlin> :plugin use tinkerpop.spark == >tinkerpop.spark activated gremlin> // 1 . Open a the graph for OLAP processing reading in from HBase gremlin> graph = GraphFactory.open ( 'conf/hadoop-graph/read-hbase-standalone-cluster.properties' ) == >hadoopgraph [ hbaseinputformat->gryooutputformat ] gremlin> // 2 . Configure the traversal to run with Spark gremlin> g = graph.traversal () .withComputer ( SparkGraphComputer ) == >graphtraversalsource [ hadoopgraph [ hbaseinputformat->gryooutputformat ] , sparkgraphcomputer ] gremlin> // 3 . Run some OLAP traversals gremlin> g.V () .count () ...... == >808 gremlin> g.E () .count () ...... == > 8046","title":"OLAP Traversals with Spark Standalone Cluster"},{"location":"advanced-topics/hadoop/#other-vertex-programs","text":"Apache TinkerPop provides various vertex programs. A vertex program runs on each vertex until either a termination criteria is attained or a fixed number of iterations has been reached. Due to the parallel nature of vertex programs, they can leverage parallel computing framework like Spark to improve their performance. Once you are familiar with how to configure JanusGraph to work with Spark, you can run all the other vertex programs provided by Apache TinkerPop, like Page Rank, Bulk Loading and Peer Pressure. See the TinkerPop VertexProgram docs for more details.","title":"Other Vertex Programs"},{"location":"advanced-topics/janusgraph-bus/","text":"JanusGraph Bus The JanusGraph Bus describes a collection of configurable logs to which JanusGraph writes changes to the graph and its management. The JanusGraph Bus is used for internal (i.e. between multiple JanusGraph instances) and external (i.e. integration with other systems) communication. In particular, JanusGraph maintains three separate logs: Trigger Log The purpose of the trigger log is to capture the mutations of a transaction so that the resulting changes to the graph can trigger events in other system. Such events may be propagating the change to other data stores, view maintenance, or aggregate computation. The trigger log consists of multiple sub-logs as configured by the user. When opening a transaction, the identifier for the trigger sub-log can be specified: tx = g . buildTransaction (). logIdentifier ( \"purchase\" ). start (); In this case, the identifier is \"purchase\" which means that the mutations of this transaction will be written to a log with the name \"trigger_purchase\". This gives the user control over where transactional mutations are logged. If no trigger log is specified, no trigger log entry will be created. Transaction Log The transaction log is maintained by JanusGraph and contains two entries for each transaction if enabled: 1. Pre-Commit: Before the changes are persisted to the storage and indexing backends, the changes are compiled and written to the log. 2. Post-Commit: The success status of the transaction is written to the log. In this way, the transaction log functions as a Write-Ahead-Log (WAL). This log is not meant for consumption by the user or external systems - use trigger logs for that. It is used internally to store partial transaction persistence against eventually consistent backends. The transaction log can be enabled via the root-level configuration option \"log-tx\". Management Log The management log is maintained by JanusGraph internally to communicate and persist all changes to global configuration options or the graph schema.","title":"JanusGraph Bus"},{"location":"advanced-topics/janusgraph-bus/#janusgraph-bus","text":"The JanusGraph Bus describes a collection of configurable logs to which JanusGraph writes changes to the graph and its management. The JanusGraph Bus is used for internal (i.e. between multiple JanusGraph instances) and external (i.e. integration with other systems) communication. In particular, JanusGraph maintains three separate logs:","title":"JanusGraph Bus"},{"location":"advanced-topics/janusgraph-bus/#trigger-log","text":"The purpose of the trigger log is to capture the mutations of a transaction so that the resulting changes to the graph can trigger events in other system. Such events may be propagating the change to other data stores, view maintenance, or aggregate computation. The trigger log consists of multiple sub-logs as configured by the user. When opening a transaction, the identifier for the trigger sub-log can be specified: tx = g . buildTransaction (). logIdentifier ( \"purchase\" ). start (); In this case, the identifier is \"purchase\" which means that the mutations of this transaction will be written to a log with the name \"trigger_purchase\". This gives the user control over where transactional mutations are logged. If no trigger log is specified, no trigger log entry will be created.","title":"Trigger Log"},{"location":"advanced-topics/janusgraph-bus/#transaction-log","text":"The transaction log is maintained by JanusGraph and contains two entries for each transaction if enabled: 1. Pre-Commit: Before the changes are persisted to the storage and indexing backends, the changes are compiled and written to the log. 2. Post-Commit: The success status of the transaction is written to the log. In this way, the transaction log functions as a Write-Ahead-Log (WAL). This log is not meant for consumption by the user or external systems - use trigger logs for that. It is used internally to store partial transaction persistence against eventually consistent backends. The transaction log can be enabled via the root-level configuration option \"log-tx\".","title":"Transaction Log"},{"location":"advanced-topics/janusgraph-bus/#management-log","text":"The management log is maintained by JanusGraph internally to communicate and persist all changes to global configuration options or the graph schema.","title":"Management Log"},{"location":"advanced-topics/partitioning/","text":"Graph Partitioning When JanusGraph is deployed on a cluster of multiple storage backend instances, the graph is partitioned across those machines. Since JanusGraph stores the graph in an adjacency list representation the assignment of vertices to machines determines the partitioning. By default, JanusGraph uses a random partitioning strategy that randomly assigns vertices to machines. Random partitioning is very efficient, requires no configuration, and results in balanced partitions. Currently explicit partitioning is not supported. cluster.max-partitions = 32 ids.placement = simple The configuration option max-partitions controls how many virtual partitions JanusGraph creates. This number should be roughly twice the number of storage backend instances. If the cluster of storage backend instances is expected to grow, estimate the size of the cluster in the foreseeable future and take this number as the baseline. Setting this number too large will unnecessarily fragment the cluster which can lead to poor performance. This number should be larger than the maximum expected number of nodes in the JanusGraph graph. It must be greater than 1 and a power of 2. There are two aspects to graph partitioning which can be individually controlled: edge cuts and vertex cuts. Edge Cut In assigning vertices to partitions one strives to optimize the assignment such that frequently co-traversed vertices are hosted on the same machine. Assume vertex A is assigned to machine 1 and vertex B is assigned to machine 2. An edge between the vertices is called a cut edge because its end points are hosted on separate machines. Traversing this edge as part of a graph query requires communication between the machines which slows down query processing. Hence, it is desirable to reduce the edge cut for frequently traversed edges. That, in turn, requires placing the adjacent vertices of frequently traversed edges in the same partition. Vertices are placed in a partition by way of the assigned vertex id. A partition is essentially a sequential range of vertex ids. To place a vertex in a particular partition, JanusGraph chooses an id from the partition\u2019s range of vertex ids. JanusGraph controls the vertex-to-partition assignment through the configured placement strategy. By default, vertices created in the same transaction are assigned to the same partition. This strategy is easy to reason about and works well in situations where frequently co-traversed vertices are created in the same transaction - either by optimizing the loading strategy to that effect or because vertices are naturally added to the graph that way. However, the strategy is limited, leads to imbalanced partitions when data is loaded in large transactions and not the optimal strategy for many use cases. The user can provide a use case specific vertex placement strategy by implementing the IDPlacementStrategy interface and registering it in the configuration through the ids.placement option. When implementing IDPlacementStrategy , note that partitions are identified by an integer id in the range from 0 to the number of configured virtual partitions minus 1. For our example configuration, there are partitions 0, 1, 2, 3, ..31. Partition ids are not the same as vertex ids. Edge cuts are more meaningful when the JanusGraph servers are on the same hosts as the storage backend. If you have to make a network call to a different host on each hop of a traversal, the benefit of edge cuts and custom placement strategies can be largely nullified. Vertex Cut While edge cut optimization aims to reduce the cross communication and thereby improve query execution, vertex cuts address the hotspot issue caused by vertices with a large number of incident edges. While vertex-centric indexes effectively address query performance for large degree vertices, vertex cuts are needed to address the hot spot issue on very large graphs. Cutting a vertex means storing a subset of that vertex\u2019s adjacency list on each partition in the graph. In other words, the vertex and its adjacency list is partitioned thereby effectively distributing the load on that single vertex across all of the instances in the cluster and removing the hot spot. JanusGraph cuts vertices by label. A vertex label can be defined as partitioned which means that all vertices of that label will be partitioned across the cluster in the manner described above. mgmt = graph . openManagement () mgmt . makeVertexLabel ( ' user ' ). make () mgmt . makeVertexLabel ( ' product ' ). partition (). make () mgmt . commit () In the example above, product is defined as a partitioned vertex label whereas user is a normal label. This configuration is beneficial for situations where there are thousands of products but millions of users and one records transactions between users and products. In that case, the product vertices will have a very high degree and the popular products turns into hot spots if they are not partitioned. However, products that don't have high degree are also partitioned, causing unnecessary overhead. Warning Vertex partition feature has high overhead and is not production-ready. We generally discourage usage of this feature. For more detail, see Discussion#2717 . Graph Partitioning FAQ Random vs. Explicit Partitioning When the graph is small or accommodated by a few storage instances, it is best to use random partitioning for its simplicity. As a rule of thumb, one should strongly consider enabling explicit graph partitioning and configure a suitable partitioning heuristic when the graph grows into the 10s of billions of edges.","title":"Graph Partitioning"},{"location":"advanced-topics/partitioning/#graph-partitioning","text":"When JanusGraph is deployed on a cluster of multiple storage backend instances, the graph is partitioned across those machines. Since JanusGraph stores the graph in an adjacency list representation the assignment of vertices to machines determines the partitioning. By default, JanusGraph uses a random partitioning strategy that randomly assigns vertices to machines. Random partitioning is very efficient, requires no configuration, and results in balanced partitions. Currently explicit partitioning is not supported. cluster.max-partitions = 32 ids.placement = simple The configuration option max-partitions controls how many virtual partitions JanusGraph creates. This number should be roughly twice the number of storage backend instances. If the cluster of storage backend instances is expected to grow, estimate the size of the cluster in the foreseeable future and take this number as the baseline. Setting this number too large will unnecessarily fragment the cluster which can lead to poor performance. This number should be larger than the maximum expected number of nodes in the JanusGraph graph. It must be greater than 1 and a power of 2. There are two aspects to graph partitioning which can be individually controlled: edge cuts and vertex cuts.","title":"Graph Partitioning"},{"location":"advanced-topics/partitioning/#edge-cut","text":"In assigning vertices to partitions one strives to optimize the assignment such that frequently co-traversed vertices are hosted on the same machine. Assume vertex A is assigned to machine 1 and vertex B is assigned to machine 2. An edge between the vertices is called a cut edge because its end points are hosted on separate machines. Traversing this edge as part of a graph query requires communication between the machines which slows down query processing. Hence, it is desirable to reduce the edge cut for frequently traversed edges. That, in turn, requires placing the adjacent vertices of frequently traversed edges in the same partition. Vertices are placed in a partition by way of the assigned vertex id. A partition is essentially a sequential range of vertex ids. To place a vertex in a particular partition, JanusGraph chooses an id from the partition\u2019s range of vertex ids. JanusGraph controls the vertex-to-partition assignment through the configured placement strategy. By default, vertices created in the same transaction are assigned to the same partition. This strategy is easy to reason about and works well in situations where frequently co-traversed vertices are created in the same transaction - either by optimizing the loading strategy to that effect or because vertices are naturally added to the graph that way. However, the strategy is limited, leads to imbalanced partitions when data is loaded in large transactions and not the optimal strategy for many use cases. The user can provide a use case specific vertex placement strategy by implementing the IDPlacementStrategy interface and registering it in the configuration through the ids.placement option. When implementing IDPlacementStrategy , note that partitions are identified by an integer id in the range from 0 to the number of configured virtual partitions minus 1. For our example configuration, there are partitions 0, 1, 2, 3, ..31. Partition ids are not the same as vertex ids. Edge cuts are more meaningful when the JanusGraph servers are on the same hosts as the storage backend. If you have to make a network call to a different host on each hop of a traversal, the benefit of edge cuts and custom placement strategies can be largely nullified.","title":"Edge Cut"},{"location":"advanced-topics/partitioning/#vertex-cut","text":"While edge cut optimization aims to reduce the cross communication and thereby improve query execution, vertex cuts address the hotspot issue caused by vertices with a large number of incident edges. While vertex-centric indexes effectively address query performance for large degree vertices, vertex cuts are needed to address the hot spot issue on very large graphs. Cutting a vertex means storing a subset of that vertex\u2019s adjacency list on each partition in the graph. In other words, the vertex and its adjacency list is partitioned thereby effectively distributing the load on that single vertex across all of the instances in the cluster and removing the hot spot. JanusGraph cuts vertices by label. A vertex label can be defined as partitioned which means that all vertices of that label will be partitioned across the cluster in the manner described above. mgmt = graph . openManagement () mgmt . makeVertexLabel ( ' user ' ). make () mgmt . makeVertexLabel ( ' product ' ). partition (). make () mgmt . commit () In the example above, product is defined as a partitioned vertex label whereas user is a normal label. This configuration is beneficial for situations where there are thousands of products but millions of users and one records transactions between users and products. In that case, the product vertices will have a very high degree and the popular products turns into hot spots if they are not partitioned. However, products that don't have high degree are also partitioned, causing unnecessary overhead. Warning Vertex partition feature has high overhead and is not production-ready. We generally discourage usage of this feature. For more detail, see Discussion#2717 .","title":"Vertex Cut"},{"location":"advanced-topics/partitioning/#graph-partitioning-faq","text":"","title":"Graph Partitioning FAQ"},{"location":"advanced-topics/partitioning/#random-vs-explicit-partitioning","text":"When the graph is small or accommodated by a few storage instances, it is best to use random partitioning for its simplicity. As a rule of thumb, one should strongly consider enabling explicit graph partitioning and configure a suitable partitioning heuristic when the graph grows into the 10s of billions of edges.","title":"Random vs. Explicit Partitioning"},{"location":"advanced-topics/serializer/","text":"Datatype and Attribute Serializer Configuration JanusGraph supports a number of classes for attribute values on properties. JanusGraph efficiently serializes primitives, primitive arrays and Geoshape , UUID , Date , ObjectNode and ArrayNode . JanusGraph supports serializing arbitrary objects as attribute values, but these require custom serializers to be defined. To configure a custom attribute class with a custom serializer, follow these steps: Implement a custom AttributeSerializer for the custom attribute class Add the following configuration options where [X] is the custom attribute id that must be larger than all attribute ids for already configured custom attributes: attributes.custom.attribute[X].attribute-class = [Full attribute class name] attributes.custom.attribute[X].serializer-class = [Full serializer class name] For example, suppose we want to register a special integer attribute class called SpecialInt and have implemented a custom serializer SpecialIntSerializer that implements AttributeSerializer . We already have 9 custom attributes configured in the configuration file, so we would add the following lines attributes.custom.attribute10.attribute-class = com.example.SpecialInt attributes.custom.attribute10.serializer-class = com.example.SpecialIntSerializer Custom Object Serialization JanusGraph supports arbitrary objects as property attributes and can serialize such objects to disk. For this default serializer to work for a custom class, the following conditions must be fulfilled: The class must implement AttributeSerializer The class must have a no-argument constructor The class must implement the equals(Object) method The last requirement is needed because JanusGraph will test both serialization and deserialization of a custom class before persisting data to disk.","title":"Datatype and Attribute Serializer Configuration"},{"location":"advanced-topics/serializer/#datatype-and-attribute-serializer-configuration","text":"JanusGraph supports a number of classes for attribute values on properties. JanusGraph efficiently serializes primitives, primitive arrays and Geoshape , UUID , Date , ObjectNode and ArrayNode . JanusGraph supports serializing arbitrary objects as attribute values, but these require custom serializers to be defined. To configure a custom attribute class with a custom serializer, follow these steps: Implement a custom AttributeSerializer for the custom attribute class Add the following configuration options where [X] is the custom attribute id that must be larger than all attribute ids for already configured custom attributes: attributes.custom.attribute[X].attribute-class = [Full attribute class name] attributes.custom.attribute[X].serializer-class = [Full serializer class name] For example, suppose we want to register a special integer attribute class called SpecialInt and have implemented a custom serializer SpecialIntSerializer that implements AttributeSerializer . We already have 9 custom attributes configured in the configuration file, so we would add the following lines attributes.custom.attribute10.attribute-class = com.example.SpecialInt attributes.custom.attribute10.serializer-class = com.example.SpecialIntSerializer","title":"Datatype and Attribute Serializer Configuration"},{"location":"advanced-topics/serializer/#custom-object-serialization","text":"JanusGraph supports arbitrary objects as property attributes and can serialize such objects to disk. For this default serializer to work for a custom class, the following conditions must be fulfilled: The class must implement AttributeSerializer The class must have a no-argument constructor The class must implement the equals(Object) method The last requirement is needed because JanusGraph will test both serialization and deserialization of a custom class before persisting data to disk.","title":"Custom Object Serialization"},{"location":"advanced-topics/stale-index/","text":"Permanent stale index inconsistency In some situations due to crashes of storage database, index database, or JanusGraph instances permanent stale index may appear. One of such cases could be a vertex removal from the graph but due to a crash during index persistence there is a chance that index won't be updated ever. In case we try to remove a vertex which is already gone from the storage database but which is still indexed then the IllegalStateException will be thrown with the message Vertex with id %vertexId% was removed. . For example, using g.V().has(\"name\", \"HelloWorld\").drop().iterate(); g.tx().commit(); we may receive an exception noting that some vertex has already been removed nevertheless it's record is still in the index. The exception will be thrown anytime we try to remove such vertex from the graph. This problem is known and should be temporal limitation until this issue is fixed in JanusGraph. As for now there is the utility tool which may be used to fix permanent stale indices. StaleIndexRecordUtil StaleIndexRecordUtil.class is available in janusgraph-core module and is meant to be used as a helper class to fix permanent stale index entries. StaleIndexRecordUtil.forceRemoveVertexFromGraphIndex can be used to force remove an index record for any vertex from a graph index. An example of using this method is below: // Let's say we want to remove non-existent vertex from a stale index. // We will assume the next constraints: // Vertex id is: `12345` // Index name is: `nameAgeIndex` // There are two indexed properties: `name` and `age` // Value of the name property is: `HelloWorld` // Value of the age property is: `123` JanusGraph graph = JanusGraphFactory . open ( configuration ); Map < String , Object > indexRecordPropertyValues = new HashMap <> (); indexRecordPropertyValues . put ( \"name\" , \"HelloWorld\" ); indexRecordPropertyValues . put ( \"age\" , 123 ); // After the below method is executed index entry of the vertex 12345 should be removed from the index which // effectively fixes permanent stale index inconsistency StaleIndexRecordUtil . forceRemoveVertexFromGraphIndex ( 12345L , // vertex id of the index record to be removed indexRecordPropertyValues , // index record property values graph , \"nameAgeIndex\" // graph index name for which to remove the index record ); For more in-depth information about usage of this tool as well as explanations of additional methods of this tool see JavaDoc for StaleIndexRecordUtil .","title":"Permanent stale index inconsistency"},{"location":"advanced-topics/stale-index/#permanent-stale-index-inconsistency","text":"In some situations due to crashes of storage database, index database, or JanusGraph instances permanent stale index may appear. One of such cases could be a vertex removal from the graph but due to a crash during index persistence there is a chance that index won't be updated ever. In case we try to remove a vertex which is already gone from the storage database but which is still indexed then the IllegalStateException will be thrown with the message Vertex with id %vertexId% was removed. . For example, using g.V().has(\"name\", \"HelloWorld\").drop().iterate(); g.tx().commit(); we may receive an exception noting that some vertex has already been removed nevertheless it's record is still in the index. The exception will be thrown anytime we try to remove such vertex from the graph. This problem is known and should be temporal limitation until this issue is fixed in JanusGraph. As for now there is the utility tool which may be used to fix permanent stale indices.","title":"Permanent stale index inconsistency"},{"location":"advanced-topics/stale-index/#staleindexrecordutil","text":"StaleIndexRecordUtil.class is available in janusgraph-core module and is meant to be used as a helper class to fix permanent stale index entries. StaleIndexRecordUtil.forceRemoveVertexFromGraphIndex can be used to force remove an index record for any vertex from a graph index. An example of using this method is below: // Let's say we want to remove non-existent vertex from a stale index. // We will assume the next constraints: // Vertex id is: `12345` // Index name is: `nameAgeIndex` // There are two indexed properties: `name` and `age` // Value of the name property is: `HelloWorld` // Value of the age property is: `123` JanusGraph graph = JanusGraphFactory . open ( configuration ); Map < String , Object > indexRecordPropertyValues = new HashMap <> (); indexRecordPropertyValues . put ( \"name\" , \"HelloWorld\" ); indexRecordPropertyValues . put ( \"age\" , 123 ); // After the below method is executed index entry of the vertex 12345 should be removed from the index which // effectively fixes permanent stale index inconsistency StaleIndexRecordUtil . forceRemoveVertexFromGraphIndex ( 12345L , // vertex id of the index record to be removed indexRecordPropertyValues , // index record property values graph , \"nameAgeIndex\" // graph index name for which to remove the index record ); For more in-depth information about usage of this tool as well as explanations of additional methods of this tool see JavaDoc for StaleIndexRecordUtil .","title":"StaleIndexRecordUtil"},{"location":"advanced-topics/technical-limitations/","text":"Technical Limitations There are various limitations and \"gotchas\" that one should be aware of when using JanusGraph. Some of these limitations are necessary design choices and others are issues that will be rectified as JanusGraph development continues. Finally, the last section provides solutions to common issues. Design Limitations These limitations reflect long-term tradeoffs design tradeoffs which are either difficult or impractical to change. These limitations are unlikely to be removed in the near future. Size Limitation JanusGraph can store up to a quintillion edges (2^60) and half as many vertices. That limitation is imposed by JanusGraph\u2019s id scheme. DataType Definitions When declaring the data type of a property key using dataType(Class) JanusGraph will enforce that all properties for that key have the declared type, unless that type is Object.class . This is an equality type check, meaning that sub-classes will not be allowed. For instance, one cannot declare the data type to be Number.class and use Integer or Long . For efficiency reasons, the type needs to match exactly. Hence, use Object.class as the data type for type flexibility. In all other cases, declare the actual data type to benefit from increased performance and type safety. Edge Retrievals are O(log(k)) Retrieving an edge by id, e.g tx.getEdge(edge.getId()) , is not a constant time operation because it requires an index call on one of its adjacent vertices. Hence, the cost of retrieving an individual edge by its id is O(log(k)) where k is the number of incident edges on the adjacent vertex. JanusGraph will attempt to pick the adjacent vertex with the smaller degree. This also applies to index retrievals for edges via a standard or external index. Type Definitions cannot be changed The definition of an edge label, property key, or vertex label cannot be changed once it has been committed to the graph. However, a type can be renamed and new types can be created at runtime to accommodate an evolving schema. Reserved Keywords There are certain keywords that JanusGraph uses internally for types that cannot be used otherwise. These types include vertex labels, edge labels, and property keys. The following are keywords that cannot be used: vertex element edge property label key For example, if you attempt to create a vertex with the label of property , you will receive an exception regarding protected system types. Temporary Limitations These are limitations in JanusGraph\u2019s current implementation. These limitations could reasonably be removed in upcoming versions of JanusGraph. Limited Mixed Index Support Mixed indexes only support a subset of the data types that JanusGraph supports. See Mixed Index Data Types for a current listing. Also, mixed indexes do not currently support property keys with SET or LIST cardinality. Batch Loading Speed JanusGraph provides a batch loading mode that can be enabled through the graph configuration . However, this batch mode only facilitates faster loading into the storage backend, it does not use storage backend specific batch loading techniques that prepare the data in memory for disk storage. As such, batch loading in JanusGraph is currently slower than batch loading modes provided by single machine databases. Bulk Loading contains information on speeding up batch loading in JanusGraph. Another limitation related to batch loading is the failure to load millions of edges into a single vertex at once or in a short time of period. Such supernode loading can fail for some storage backends. This limitation also applies to dense index entries.","title":"Technical Limitations"},{"location":"advanced-topics/technical-limitations/#technical-limitations","text":"There are various limitations and \"gotchas\" that one should be aware of when using JanusGraph. Some of these limitations are necessary design choices and others are issues that will be rectified as JanusGraph development continues. Finally, the last section provides solutions to common issues.","title":"Technical Limitations"},{"location":"advanced-topics/technical-limitations/#design-limitations","text":"These limitations reflect long-term tradeoffs design tradeoffs which are either difficult or impractical to change. These limitations are unlikely to be removed in the near future.","title":"Design Limitations"},{"location":"advanced-topics/technical-limitations/#size-limitation","text":"JanusGraph can store up to a quintillion edges (2^60) and half as many vertices. That limitation is imposed by JanusGraph\u2019s id scheme.","title":"Size Limitation"},{"location":"advanced-topics/technical-limitations/#datatype-definitions","text":"When declaring the data type of a property key using dataType(Class) JanusGraph will enforce that all properties for that key have the declared type, unless that type is Object.class . This is an equality type check, meaning that sub-classes will not be allowed. For instance, one cannot declare the data type to be Number.class and use Integer or Long . For efficiency reasons, the type needs to match exactly. Hence, use Object.class as the data type for type flexibility. In all other cases, declare the actual data type to benefit from increased performance and type safety.","title":"DataType Definitions"},{"location":"advanced-topics/technical-limitations/#edge-retrievals-are-ologk","text":"Retrieving an edge by id, e.g tx.getEdge(edge.getId()) , is not a constant time operation because it requires an index call on one of its adjacent vertices. Hence, the cost of retrieving an individual edge by its id is O(log(k)) where k is the number of incident edges on the adjacent vertex. JanusGraph will attempt to pick the adjacent vertex with the smaller degree. This also applies to index retrievals for edges via a standard or external index.","title":"Edge Retrievals are O(log(k))"},{"location":"advanced-topics/technical-limitations/#type-definitions-cannot-be-changed","text":"The definition of an edge label, property key, or vertex label cannot be changed once it has been committed to the graph. However, a type can be renamed and new types can be created at runtime to accommodate an evolving schema.","title":"Type Definitions cannot be changed"},{"location":"advanced-topics/technical-limitations/#reserved-keywords","text":"There are certain keywords that JanusGraph uses internally for types that cannot be used otherwise. These types include vertex labels, edge labels, and property keys. The following are keywords that cannot be used: vertex element edge property label key For example, if you attempt to create a vertex with the label of property , you will receive an exception regarding protected system types.","title":"Reserved Keywords"},{"location":"advanced-topics/technical-limitations/#temporary-limitations","text":"These are limitations in JanusGraph\u2019s current implementation. These limitations could reasonably be removed in upcoming versions of JanusGraph.","title":"Temporary Limitations"},{"location":"advanced-topics/technical-limitations/#limited-mixed-index-support","text":"Mixed indexes only support a subset of the data types that JanusGraph supports. See Mixed Index Data Types for a current listing. Also, mixed indexes do not currently support property keys with SET or LIST cardinality.","title":"Limited Mixed Index Support"},{"location":"advanced-topics/technical-limitations/#batch-loading-speed","text":"JanusGraph provides a batch loading mode that can be enabled through the graph configuration . However, this batch mode only facilitates faster loading into the storage backend, it does not use storage backend specific batch loading techniques that prepare the data in memory for disk storage. As such, batch loading in JanusGraph is currently slower than batch loading modes provided by single machine databases. Bulk Loading contains information on speeding up batch loading in JanusGraph. Another limitation related to batch loading is the failure to load millions of edges into a single vertex at once or in a short time of period. Such supernode loading can fail for some storage backends. This limitation also applies to dense index entries.","title":"Batch Loading Speed"},{"location":"advanced-topics/transaction-log/","text":"Transaction Log JanusGraph can automatically log transactional changes for additional processing or as a record of change. To enable logging for a particular transaction, specify the name of the target log during the start of the transaction. tx = graph . buildTransaction (). logIdentifier ( 'addedPerson' ). start () u = tx . addVertex ( label , 'human' ) u . property ( 'name' , 'proteros' ) u . property ( 'age' , 36 ) tx . commit () Upon commit, any changes made during the transaction are logged to the user logging system into a log named addedPerson . The user logging system is a configurable logging backend with a JanusGraph compatible log interface. By default, the log is written to a separate store in the primary storage backend which can be configured as described below. The log identifier specified during the start of the transaction identifies the log in which the changes are recorded thereby allowing different types of changes to be recorded in separate logs for individual processing. tx = graph . buildTransaction (). logIdentifier ( 'battle' ). start () h = tx . traversal (). V (). has ( 'name' , 'hercules' ). next () m = tx . addVertex ( label , 'monster' ) m . property ( 'name' , 'phylatax' ) h . addEdge ( 'battled' , m , 'time' , 22 ) tx . commit () JanusGraph provides a user transaction log processor framework to process the recorded transactional changes. The transaction log processor is opened via JanusGraphFactory.openTransactionLog(JanusGraph) against a previously opened JanusGraph graph instance. One can then add processors for a particular log which holds transactional changes. import java.util.concurrent.atomic.* ; import org.janusgraph.core.log.* ; import java.util.concurrent.* ; logProcessor = JanusGraphFactory . openTransactionLog ( g ); totalHumansAdded = new AtomicInteger ( 0 ); totalGodsAdded = new AtomicInteger ( 0 ); logProcessor . addLogProcessor ( \"addedPerson\" ). setProcessorIdentifier ( \"addedPersonCounter\" ). setStartTimeNow (). addProcessor ( new ChangeProcessor () { @Override public void process ( JanusGraphTransaction tx , TransactionId txId , ChangeState changeState ) { for ( v in changeState . getVertices ( Change . ADDED )) { if ( v . label (). equals ( \"human\" )) totalHumansAdded . incrementAndGet (); } } }). addProcessor ( new ChangeProcessor () { @Override public void process ( JanusGraphTransaction tx , TransactionId txId , ChangeState changeState ) { for ( v in changeState . getVertices ( Change . ADDED )) { if ( v . label (). equals ( \"god\" )) totalGodsAdded . incrementAndGet (); } } }). build (); In this example, a log processor is built for the user transaction log named addedPerson to process the changes made in transactions which used the addedPerson log identifier. Two change processors are added to this log processor. The first processor counts the number of humans added and the second counts the number of gods added to the graph. When a log processor is built against a particular log, such as the addedPerson log in the example above, it will start reading transactional change records from the log immediately upon successful construction and initialization up to the head of the log. The start time specified in the builder marks the time point in the log where the log processor will start reading records. Optionally, one can specify an identifier for the log processor in the builder. The log processor will use the identifier to regularly persist its state of processing, i.e. it will maintain a marker on the last read log record. If the log processor is later restarted with the same identifier, it will continue reading from the last read record. This is particularly useful when the log processor is supposed to run for long periods of time and is therefore likely to fail. In such failure situations, the log processor can simply be restarted with the same identifier. It must be ensured that log processor identifiers are unique in a JanusGraph cluster in order to avoid conflicts on the persisted read markers. A change processor must implement the ChangeProcessor interface. It\u2019s process() method is invoked for each change record read from the log with a JanusGraphTransaction handle, the id of the transaction that caused the change, and a ChangeState container which holds the transactional changes. The change state container can be queried to retrieve individual elements that were part of the change state. In the example, all added vertices are retrieved. Refer to the API documentation for a description of all the query methods on ChangeState . The provided transaction id can be used to investigate the origin of the transaction which is uniquely identified by the combination of the id of the JanusGraph instance that executed the transaction ( txId.getInstanceId() ) and the instance specific transaction id ( txId.getTransactionId() ). In addition, the time of the transaction is available through txId.getTransactionTime() . Change processors are executed individually and in multiple threads. If a change processor accesses global state it must be ensured that such state allows concurrent access. While the log processor reads log records sequentially, the changes are processed in multiple threads so it cannot be guaranteed that the log order is preserved in the change processors. Note, that log processors run each registered change processor at least once for each record in the log which means that a single transactional change record may be processed multiple times under certain failure conditions. One cannot add or remove change processor from a running log processor. In other words, a log processor is immutable after it is built. To change log processing, start a new log processor and shut down an existing one. logProcessor . addLogProcessor ( \"battle\" ). setProcessorIdentifier ( \"battleTimer\" ). setStartTimeNow (). addProcessor ( new ChangeProcessor () { @Override public void process ( JanusGraphTransaction tx , TransactionId txId , ChangeState changeState ) { h = tx . V (). has ( \"name\" , \"hercules\" ). toList (). iterator (). next (); for ( edge in changeState . getEdges ( h , Change . ADDED , Direction . OUT , \"battled\" )) { if ( edge .< Integer > value ( \"time\" )> 1000 ) h . property ( \"oldFighter\" , true ); } } }). build (); The log processor above processes transactions for the battle log identifier with a single change processor which evaluates battled edges that were added to Hercules. This example demonstrates that the transaction handle passed into the change processor is a normal JanusGraphTransaction which query the JanusGraph graph and make changes to it. Transaction Log Use Cases Record of Change The user transaction log can be used to keep a record of all changes made against the graph. By using separate log identifiers, changes can be recorded in different logs to distinguish separate transaction types. At any time, a log processor can be built which can processes all recorded changes starting from the desired start time. This can be used for forensic analysis, to replay changes against a different graph, or to compute an aggregate. Downstream Updates It is often the case that a JanusGraph graph cluster is part of a larger architecture. The user transaction log and the log processor framework provide the tools needed to broadcast changes to other components of the overall system without slowing down the original transactions causing the change. This is particularly useful when transaction latencies need to be low and/or there are a number of other systems that need to be alerted to a change in the graph. Triggers The user transaction log provides the basic infrastructure to implement triggers that can scale to a large number of concurrent transactions and very large graphs. A trigger is registered with a particular change of data and either triggers an event in an external system or additional changes to the graph. At scale, it is not advisable to implement triggers in the original transaction but rather process triggers with a slight delay through the log processor framework. The second example shows how changes to the graph can be evaluated and trigger additional modifications. Log Configuration There are a number of configuration options to fine tune how the log processor reads from the log. Refer to the complete list of configuration options Configuration Reference for the options under the log namespace. To configure the user transaction log, use the log.user namespace. The options listed there allow the configuration of the number of threads to be used, the number of log records read in each batch, the read interval, and whether the transaction change records should automatically expire and be removed from the log after a configurable amount of time (TTL). An example configuration for user transaction log could look like below: log.user.backend = default log.user.fixed-partition = false log.user.max-read-time = 10000 ms log.user.max-write-time = 5000 ms log.user.read-batch-size = 1024 log.user.read-interval = 3000 ms log.user.read-lag-time = 2000 ms log.user.read-threads = 1 log.user.send-batch-size = 256 log.user.send-delay = 0 ms log.user.ttl = 120000 ms Custom storage backend By default JanusGraph supports a single log backend implementation which is defined by a reserved shortcut default in log.user.backend configuration option and has the following class implementation org.janusgraph.diskstorage.log.kcvs.KCVSLogManager . KCVSLogManager reuses the graph's storage backend (defined by storage.backend ) to store all logs. Usually for default management-only operations the default storage backend is good enough to manage logs and custom log storage backend is not necessary. That said, in case User Transaction Log is used the underlying graph's storage backend might be far from optimal in highly loaded applications for log.user log and a better storage might be preferred (i.e. Kafka or else). Thus, JanusGraph allows providing custom log implementations via log.[x].backend option. The custom log implementation needs to implement org.janusgraph.diskstorage.log.LogManager interface and the implementation class has to have a public constructor which accepts org.janusgraph.diskstorage.configuration.Configuration as a single parameter. The main purpose of that LogManager implementation is to open new logging implementations which extend org.janusgraph.diskstorage.log.Log interface. For example, logs opened by KCVSLogManager have org.janusgraph.diskstorage.log.kcvs.KCVSLog implementations which manage logs inside graph's storage backend but the user may implement Log which uses Kafka storage backend or else. To specify any custom log implementation it's necessary to provide full class path of the LogManager implementing class via log.[x].backend configuration option.","title":"Transaction Log"},{"location":"advanced-topics/transaction-log/#transaction-log","text":"JanusGraph can automatically log transactional changes for additional processing or as a record of change. To enable logging for a particular transaction, specify the name of the target log during the start of the transaction. tx = graph . buildTransaction (). logIdentifier ( 'addedPerson' ). start () u = tx . addVertex ( label , 'human' ) u . property ( 'name' , 'proteros' ) u . property ( 'age' , 36 ) tx . commit () Upon commit, any changes made during the transaction are logged to the user logging system into a log named addedPerson . The user logging system is a configurable logging backend with a JanusGraph compatible log interface. By default, the log is written to a separate store in the primary storage backend which can be configured as described below. The log identifier specified during the start of the transaction identifies the log in which the changes are recorded thereby allowing different types of changes to be recorded in separate logs for individual processing. tx = graph . buildTransaction (). logIdentifier ( 'battle' ). start () h = tx . traversal (). V (). has ( 'name' , 'hercules' ). next () m = tx . addVertex ( label , 'monster' ) m . property ( 'name' , 'phylatax' ) h . addEdge ( 'battled' , m , 'time' , 22 ) tx . commit () JanusGraph provides a user transaction log processor framework to process the recorded transactional changes. The transaction log processor is opened via JanusGraphFactory.openTransactionLog(JanusGraph) against a previously opened JanusGraph graph instance. One can then add processors for a particular log which holds transactional changes. import java.util.concurrent.atomic.* ; import org.janusgraph.core.log.* ; import java.util.concurrent.* ; logProcessor = JanusGraphFactory . openTransactionLog ( g ); totalHumansAdded = new AtomicInteger ( 0 ); totalGodsAdded = new AtomicInteger ( 0 ); logProcessor . addLogProcessor ( \"addedPerson\" ). setProcessorIdentifier ( \"addedPersonCounter\" ). setStartTimeNow (). addProcessor ( new ChangeProcessor () { @Override public void process ( JanusGraphTransaction tx , TransactionId txId , ChangeState changeState ) { for ( v in changeState . getVertices ( Change . ADDED )) { if ( v . label (). equals ( \"human\" )) totalHumansAdded . incrementAndGet (); } } }). addProcessor ( new ChangeProcessor () { @Override public void process ( JanusGraphTransaction tx , TransactionId txId , ChangeState changeState ) { for ( v in changeState . getVertices ( Change . ADDED )) { if ( v . label (). equals ( \"god\" )) totalGodsAdded . incrementAndGet (); } } }). build (); In this example, a log processor is built for the user transaction log named addedPerson to process the changes made in transactions which used the addedPerson log identifier. Two change processors are added to this log processor. The first processor counts the number of humans added and the second counts the number of gods added to the graph. When a log processor is built against a particular log, such as the addedPerson log in the example above, it will start reading transactional change records from the log immediately upon successful construction and initialization up to the head of the log. The start time specified in the builder marks the time point in the log where the log processor will start reading records. Optionally, one can specify an identifier for the log processor in the builder. The log processor will use the identifier to regularly persist its state of processing, i.e. it will maintain a marker on the last read log record. If the log processor is later restarted with the same identifier, it will continue reading from the last read record. This is particularly useful when the log processor is supposed to run for long periods of time and is therefore likely to fail. In such failure situations, the log processor can simply be restarted with the same identifier. It must be ensured that log processor identifiers are unique in a JanusGraph cluster in order to avoid conflicts on the persisted read markers. A change processor must implement the ChangeProcessor interface. It\u2019s process() method is invoked for each change record read from the log with a JanusGraphTransaction handle, the id of the transaction that caused the change, and a ChangeState container which holds the transactional changes. The change state container can be queried to retrieve individual elements that were part of the change state. In the example, all added vertices are retrieved. Refer to the API documentation for a description of all the query methods on ChangeState . The provided transaction id can be used to investigate the origin of the transaction which is uniquely identified by the combination of the id of the JanusGraph instance that executed the transaction ( txId.getInstanceId() ) and the instance specific transaction id ( txId.getTransactionId() ). In addition, the time of the transaction is available through txId.getTransactionTime() . Change processors are executed individually and in multiple threads. If a change processor accesses global state it must be ensured that such state allows concurrent access. While the log processor reads log records sequentially, the changes are processed in multiple threads so it cannot be guaranteed that the log order is preserved in the change processors. Note, that log processors run each registered change processor at least once for each record in the log which means that a single transactional change record may be processed multiple times under certain failure conditions. One cannot add or remove change processor from a running log processor. In other words, a log processor is immutable after it is built. To change log processing, start a new log processor and shut down an existing one. logProcessor . addLogProcessor ( \"battle\" ). setProcessorIdentifier ( \"battleTimer\" ). setStartTimeNow (). addProcessor ( new ChangeProcessor () { @Override public void process ( JanusGraphTransaction tx , TransactionId txId , ChangeState changeState ) { h = tx . V (). has ( \"name\" , \"hercules\" ). toList (). iterator (). next (); for ( edge in changeState . getEdges ( h , Change . ADDED , Direction . OUT , \"battled\" )) { if ( edge .< Integer > value ( \"time\" )> 1000 ) h . property ( \"oldFighter\" , true ); } } }). build (); The log processor above processes transactions for the battle log identifier with a single change processor which evaluates battled edges that were added to Hercules. This example demonstrates that the transaction handle passed into the change processor is a normal JanusGraphTransaction which query the JanusGraph graph and make changes to it.","title":"Transaction Log"},{"location":"advanced-topics/transaction-log/#transaction-log-use-cases","text":"","title":"Transaction Log Use Cases"},{"location":"advanced-topics/transaction-log/#record-of-change","text":"The user transaction log can be used to keep a record of all changes made against the graph. By using separate log identifiers, changes can be recorded in different logs to distinguish separate transaction types. At any time, a log processor can be built which can processes all recorded changes starting from the desired start time. This can be used for forensic analysis, to replay changes against a different graph, or to compute an aggregate.","title":"Record of Change"},{"location":"advanced-topics/transaction-log/#downstream-updates","text":"It is often the case that a JanusGraph graph cluster is part of a larger architecture. The user transaction log and the log processor framework provide the tools needed to broadcast changes to other components of the overall system without slowing down the original transactions causing the change. This is particularly useful when transaction latencies need to be low and/or there are a number of other systems that need to be alerted to a change in the graph.","title":"Downstream Updates"},{"location":"advanced-topics/transaction-log/#triggers","text":"The user transaction log provides the basic infrastructure to implement triggers that can scale to a large number of concurrent transactions and very large graphs. A trigger is registered with a particular change of data and either triggers an event in an external system or additional changes to the graph. At scale, it is not advisable to implement triggers in the original transaction but rather process triggers with a slight delay through the log processor framework. The second example shows how changes to the graph can be evaluated and trigger additional modifications.","title":"Triggers"},{"location":"advanced-topics/transaction-log/#log-configuration","text":"There are a number of configuration options to fine tune how the log processor reads from the log. Refer to the complete list of configuration options Configuration Reference for the options under the log namespace. To configure the user transaction log, use the log.user namespace. The options listed there allow the configuration of the number of threads to be used, the number of log records read in each batch, the read interval, and whether the transaction change records should automatically expire and be removed from the log after a configurable amount of time (TTL). An example configuration for user transaction log could look like below: log.user.backend = default log.user.fixed-partition = false log.user.max-read-time = 10000 ms log.user.max-write-time = 5000 ms log.user.read-batch-size = 1024 log.user.read-interval = 3000 ms log.user.read-lag-time = 2000 ms log.user.read-threads = 1 log.user.send-batch-size = 256 log.user.send-delay = 0 ms log.user.ttl = 120000 ms","title":"Log Configuration"},{"location":"advanced-topics/transaction-log/#custom-storage-backend","text":"By default JanusGraph supports a single log backend implementation which is defined by a reserved shortcut default in log.user.backend configuration option and has the following class implementation org.janusgraph.diskstorage.log.kcvs.KCVSLogManager . KCVSLogManager reuses the graph's storage backend (defined by storage.backend ) to store all logs. Usually for default management-only operations the default storage backend is good enough to manage logs and custom log storage backend is not necessary. That said, in case User Transaction Log is used the underlying graph's storage backend might be far from optimal in highly loaded applications for log.user log and a better storage might be preferred (i.e. Kafka or else). Thus, JanusGraph allows providing custom log implementations via log.[x].backend option. The custom log implementation needs to implement org.janusgraph.diskstorage.log.LogManager interface and the implementation class has to have a public constructor which accepts org.janusgraph.diskstorage.configuration.Configuration as a single parameter. The main purpose of that LogManager implementation is to open new logging implementations which extend org.janusgraph.diskstorage.log.Log interface. For example, logs opened by KCVSLogManager have org.janusgraph.diskstorage.log.kcvs.KCVSLog implementations which manage logs inside graph's storage backend but the user may implement Log which uses Kafka storage backend or else. To specify any custom log implementation it's necessary to provide full class path of the LogManager implementing class via log.[x].backend configuration option.","title":"Custom storage backend"},{"location":"configs/","text":"Configuration A JanusGraph graph database cluster consists of one or multiple JanusGraph instances. To open a JanusGraph instance, a configuration has to be provided which specifies how JanusGraph should be set up. A JanusGraph configuration specifies which components JanusGraph should use, controls all operational aspects of a JanusGraph deployment, and provides a number of tuning options to get maximum performance from a JanusGraph cluster. At a minimum, a JanusGraph configuration must define the persistence engine that JanusGraph should use as a storage backend. Storage Backends lists all supported persistence engines and how to configure them respectively. If advanced graph query support (e.g full-text search, geo search, or range queries) is required an additional indexing backend must be configured. See Index Backends for details. If query performance is a concern, then caching should be enabled. Cache configuration and tuning is described in JanusGraph Cache . Example Configurations Below are some example configuration files to demonstrate how to configure the most commonly used storage backends, indexing systems, and performance components. This covers only a tiny portion of the available configuration options. Refer to Configuration Reference for the complete list of all options. Cassandra+Elasticsearch Sets up JanusGraph to use the Cassandra persistence engine running locally and a remote Elastic search indexing system: storage.backend = cql storage.hostname = localhost index.search.backend = elasticsearch index.search.hostname = 100.100.101.1, 100.100.101.2 index.search.elasticsearch.client-only = true HBase+Caching Sets up JanusGraph to use the HBase persistence engine running remotely and uses JanusGraph\u2019s caching component for better performance. storage.backend = hbase storage.hostname = 100.100.101.1 storage.port = 2181 cache.db-cache = true cache.db-cache-clean-wait = 20 cache.db-cache-time = 180000 cache.db-cache-size = 0.5 BerkeleyDB Sets up JanusGraph to use BerkeleyDB as an embedded persistence engine with Elasticsearch as an embedded indexing system. storage.backend = berkeleyje storage.directory = /tmp/graph index.search.backend = elasticsearch index.search.directory = /tmp/searchindex index.search.elasticsearch.client-only = false index.search.elasticsearch.local-mode = true Configuration Reference describes all of these configuration options in detail. The conf directory of the JanusGraph distribution contains additional configuration examples. Further Examples There are several example configuration files in the conf/ directory that can be used to get started with JanusGraph quickly. Paths to these files can be passed to JanusGraphFactory.open(...) as shown below: // Connect to Cassandra on localhost using a default configuration graph = JanusGraphFactory . open ( \"conf/janusgraph-cql.properties\" ) // Connect to HBase on localhost using a default configuration graph = JanusGraphFactory . open ( \"conf/janusgraph-hbase.properties\" ) Using Configuration How the configuration is provided to JanusGraph depends on the instantiation mode. JanusGraphFactory Gremlin Console The JanusGraph distribution contains a command line Gremlin Console which makes it easy to get started and interact with JanusGraph. Invoke bin/gremlin.sh (Unix/Linux) or bin/gremlin.bat (Windows) to start the Console and then open a JanusGraph graph using the factory with the configuration stored in an accessible properties configuration file: graph = JanusGraphFactory . open ( 'path/to/configuration.properties' ) JanusGraph Embedded JanusGraphFactory can also be used to open an embedded JanusGraph graph instance from within a JVM-based user application. In that case, JanusGraph is part of the user application and the application can call upon JanusGraph directly through its public API. Short Codes If the JanusGraph graph cluster has been previously configured and/or only the storage backend needs to be defined, JanusGraphFactory accepts a colon-separated string representation of the storage backend name and hostname or directory. graph = JanusGraphFactory . open ( 'cql:localhost' ) graph = JanusGraphFactory . open ( 'berkeleyje:/tmp/graph' ) JanusGraph Server JanusGraph, by itself, is simply a set of jar files with no thread of execution. There are two basic patterns for connecting to, and using a JanusGraph database: JanusGraph can be used by embedding JanusGraph calls in a client program where the program provides the thread of execution. JanusGraph packages a long running server process that, when started, allows a remote client or logic running in a separate program to make JanusGraph calls. This long running server process is called JanusGraph Server . For the JanusGraph Server, JanusGraph uses Gremlin Server of the Apache TinkerPop stack to service client requests. JanusGraph provides an out-of-the-box configuration for a quick start with JanusGraph Server, but the configuration can be changed to provide a wide range of server capabilities. Configuring JanusGraph Server is accomplished through a JanusGraph Server yaml configuration file located in the ./conf/gremlin-server directory in the JanusGraph distribution. To configure JanusGraph Server with a graph instance ( JanusGraph ), the JanusGraph Server configuration file requires the following settings: ... graphs : { graph : conf/janusgraph-berkeleyje.properties } scriptEngines : { gremlin-groovy : { plugins : { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin : {}, org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin : { classImports : [ java.lang.Math ], methodImports : [ java.lang.Math#* ]}, org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin : { files : [ scripts/empty-sample.groovy ]}}}} ... The entry for graphs defines the bindings to specific JanusGraph configurations. In the above case it binds graph to a JanusGraph configuration at conf/janusgraph-berkeleyje.properties . The plugins entry enables the JanusGraph Gremlin Plugin, which enables auto-imports of JanusGraph classes so that they can be referenced in remotely submitted scripts. Learn more about configuring and using JanusGraph Server in JanusGraph Server . Server Distribution The JanusGraph zip file contains a quick start server component that helps make it easier to get started with Gremlin Server and JanusGraph. Invoke bin/janusgraph.sh start to start Gremlin Server with Cassandra and Elasticsearch. Note For security reasons Elasticsearch and therefore janusgraph.sh must be run under a non-root account Note Starting with 0.5.1, this is just included in the full package version. Global Configuration JanusGraph distinguishes between local and global configuration options. Local configuration options apply to an individual JanusGraph instance. Global configuration options apply to all instances in a cluster. More specifically, JanusGraph distinguishes the following five scopes for configuration options: LOCAL : These options only apply to an individual JanusGraph instance and are specified in the configuration provided when initializing the JanusGraph instance. MASKABLE : These configuration options can be overwritten for an individual JanusGraph instance by the local configuration file. If the local configuration file does not specify the option, its value is read from the global JanusGraph cluster configuration. GLOBAL : These options are always read from the cluster configuration and cannot be overwritten on an instance basis. GLOBAL_OFFLINE : Like GLOBAL , but changing these options requires a cluster restart to ensure that the value is the same across the entire cluster. FIXED : Like GLOBAL , but the value cannot be changed once the JanusGraph cluster is initialized. When the first JanusGraph instance in a cluster is started, the global configuration options are initialized from the provided local configuration file. Subsequently changing global configuration options is done through JanusGraph\u2019s management API. To access the management API, call g.getManagementSystem() on an open JanusGraph instance handle g . For example, to change the default caching behavior on a JanusGraph cluster: mgmt = graph . openManagement () mgmt . get ( 'cache.db-cache' ) // Prints the current config setting mgmt . set ( 'cache.db-cache' , true ) // Changes option mgmt . get ( 'cache.db-cache' ) // Prints 'true' mgmt . commit () // Changes take effect Changing Offline Options Changing configuration options does not affect running instances and only applies to newly started ones. Changing GLOBAL_OFFLINE configuration options requires restarting the cluster so that the changes take effect immediately for all instances. To change GLOBAL_OFFLINE options follow these steps: Close all but one JanusGraph instance in the cluster Connect to the single instance Ensure all running transactions are closed Ensure no new transactions are started (i.e. the cluster must be offline) Open the management API Change the configuration option(s) Call commit which will automatically shut down the graph instance Restart all instances Refer to the full list of configuration options in Configuration Reference for more information including the configuration scope of each option.","title":"Introduction"},{"location":"configs/#configuration","text":"A JanusGraph graph database cluster consists of one or multiple JanusGraph instances. To open a JanusGraph instance, a configuration has to be provided which specifies how JanusGraph should be set up. A JanusGraph configuration specifies which components JanusGraph should use, controls all operational aspects of a JanusGraph deployment, and provides a number of tuning options to get maximum performance from a JanusGraph cluster. At a minimum, a JanusGraph configuration must define the persistence engine that JanusGraph should use as a storage backend. Storage Backends lists all supported persistence engines and how to configure them respectively. If advanced graph query support (e.g full-text search, geo search, or range queries) is required an additional indexing backend must be configured. See Index Backends for details. If query performance is a concern, then caching should be enabled. Cache configuration and tuning is described in JanusGraph Cache .","title":"Configuration"},{"location":"configs/#example-configurations","text":"Below are some example configuration files to demonstrate how to configure the most commonly used storage backends, indexing systems, and performance components. This covers only a tiny portion of the available configuration options. Refer to Configuration Reference for the complete list of all options.","title":"Example Configurations"},{"location":"configs/#cassandraelasticsearch","text":"Sets up JanusGraph to use the Cassandra persistence engine running locally and a remote Elastic search indexing system: storage.backend = cql storage.hostname = localhost index.search.backend = elasticsearch index.search.hostname = 100.100.101.1, 100.100.101.2 index.search.elasticsearch.client-only = true","title":"Cassandra+Elasticsearch"},{"location":"configs/#hbasecaching","text":"Sets up JanusGraph to use the HBase persistence engine running remotely and uses JanusGraph\u2019s caching component for better performance. storage.backend = hbase storage.hostname = 100.100.101.1 storage.port = 2181 cache.db-cache = true cache.db-cache-clean-wait = 20 cache.db-cache-time = 180000 cache.db-cache-size = 0.5","title":"HBase+Caching"},{"location":"configs/#berkeleydb","text":"Sets up JanusGraph to use BerkeleyDB as an embedded persistence engine with Elasticsearch as an embedded indexing system. storage.backend = berkeleyje storage.directory = /tmp/graph index.search.backend = elasticsearch index.search.directory = /tmp/searchindex index.search.elasticsearch.client-only = false index.search.elasticsearch.local-mode = true Configuration Reference describes all of these configuration options in detail. The conf directory of the JanusGraph distribution contains additional configuration examples.","title":"BerkeleyDB"},{"location":"configs/#further-examples","text":"There are several example configuration files in the conf/ directory that can be used to get started with JanusGraph quickly. Paths to these files can be passed to JanusGraphFactory.open(...) as shown below: // Connect to Cassandra on localhost using a default configuration graph = JanusGraphFactory . open ( \"conf/janusgraph-cql.properties\" ) // Connect to HBase on localhost using a default configuration graph = JanusGraphFactory . open ( \"conf/janusgraph-hbase.properties\" )","title":"Further Examples"},{"location":"configs/#using-configuration","text":"How the configuration is provided to JanusGraph depends on the instantiation mode.","title":"Using Configuration"},{"location":"configs/#janusgraphfactory","text":"","title":"JanusGraphFactory"},{"location":"configs/#gremlin-console","text":"The JanusGraph distribution contains a command line Gremlin Console which makes it easy to get started and interact with JanusGraph. Invoke bin/gremlin.sh (Unix/Linux) or bin/gremlin.bat (Windows) to start the Console and then open a JanusGraph graph using the factory with the configuration stored in an accessible properties configuration file: graph = JanusGraphFactory . open ( 'path/to/configuration.properties' )","title":"Gremlin Console"},{"location":"configs/#janusgraph-embedded","text":"JanusGraphFactory can also be used to open an embedded JanusGraph graph instance from within a JVM-based user application. In that case, JanusGraph is part of the user application and the application can call upon JanusGraph directly through its public API.","title":"JanusGraph Embedded"},{"location":"configs/#short-codes","text":"If the JanusGraph graph cluster has been previously configured and/or only the storage backend needs to be defined, JanusGraphFactory accepts a colon-separated string representation of the storage backend name and hostname or directory. graph = JanusGraphFactory . open ( 'cql:localhost' ) graph = JanusGraphFactory . open ( 'berkeleyje:/tmp/graph' )","title":"Short Codes"},{"location":"configs/#janusgraph-server","text":"JanusGraph, by itself, is simply a set of jar files with no thread of execution. There are two basic patterns for connecting to, and using a JanusGraph database: JanusGraph can be used by embedding JanusGraph calls in a client program where the program provides the thread of execution. JanusGraph packages a long running server process that, when started, allows a remote client or logic running in a separate program to make JanusGraph calls. This long running server process is called JanusGraph Server . For the JanusGraph Server, JanusGraph uses Gremlin Server of the Apache TinkerPop stack to service client requests. JanusGraph provides an out-of-the-box configuration for a quick start with JanusGraph Server, but the configuration can be changed to provide a wide range of server capabilities. Configuring JanusGraph Server is accomplished through a JanusGraph Server yaml configuration file located in the ./conf/gremlin-server directory in the JanusGraph distribution. To configure JanusGraph Server with a graph instance ( JanusGraph ), the JanusGraph Server configuration file requires the following settings: ... graphs : { graph : conf/janusgraph-berkeleyje.properties } scriptEngines : { gremlin-groovy : { plugins : { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin : {}, org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin : { classImports : [ java.lang.Math ], methodImports : [ java.lang.Math#* ]}, org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin : { files : [ scripts/empty-sample.groovy ]}}}} ... The entry for graphs defines the bindings to specific JanusGraph configurations. In the above case it binds graph to a JanusGraph configuration at conf/janusgraph-berkeleyje.properties . The plugins entry enables the JanusGraph Gremlin Plugin, which enables auto-imports of JanusGraph classes so that they can be referenced in remotely submitted scripts. Learn more about configuring and using JanusGraph Server in JanusGraph Server .","title":"JanusGraph Server"},{"location":"configs/#server-distribution","text":"The JanusGraph zip file contains a quick start server component that helps make it easier to get started with Gremlin Server and JanusGraph. Invoke bin/janusgraph.sh start to start Gremlin Server with Cassandra and Elasticsearch. Note For security reasons Elasticsearch and therefore janusgraph.sh must be run under a non-root account Note Starting with 0.5.1, this is just included in the full package version.","title":"Server Distribution"},{"location":"configs/#global-configuration","text":"JanusGraph distinguishes between local and global configuration options. Local configuration options apply to an individual JanusGraph instance. Global configuration options apply to all instances in a cluster. More specifically, JanusGraph distinguishes the following five scopes for configuration options: LOCAL : These options only apply to an individual JanusGraph instance and are specified in the configuration provided when initializing the JanusGraph instance. MASKABLE : These configuration options can be overwritten for an individual JanusGraph instance by the local configuration file. If the local configuration file does not specify the option, its value is read from the global JanusGraph cluster configuration. GLOBAL : These options are always read from the cluster configuration and cannot be overwritten on an instance basis. GLOBAL_OFFLINE : Like GLOBAL , but changing these options requires a cluster restart to ensure that the value is the same across the entire cluster. FIXED : Like GLOBAL , but the value cannot be changed once the JanusGraph cluster is initialized. When the first JanusGraph instance in a cluster is started, the global configuration options are initialized from the provided local configuration file. Subsequently changing global configuration options is done through JanusGraph\u2019s management API. To access the management API, call g.getManagementSystem() on an open JanusGraph instance handle g . For example, to change the default caching behavior on a JanusGraph cluster: mgmt = graph . openManagement () mgmt . get ( 'cache.db-cache' ) // Prints the current config setting mgmt . set ( 'cache.db-cache' , true ) // Changes option mgmt . get ( 'cache.db-cache' ) // Prints 'true' mgmt . commit () // Changes take effect","title":"Global Configuration"},{"location":"configs/#changing-offline-options","text":"Changing configuration options does not affect running instances and only applies to newly started ones. Changing GLOBAL_OFFLINE configuration options requires restarting the cluster so that the changes take effect immediately for all instances. To change GLOBAL_OFFLINE options follow these steps: Close all but one JanusGraph instance in the cluster Connect to the single instance Ensure all running transactions are closed Ensure no new transactions are started (i.e. the cluster must be offline) Open the management API Change the configuration option(s) Call commit which will automatically shut down the graph instance Restart all instances Refer to the full list of configuration options in Configuration Reference for more information including the configuration scope of each option.","title":"Changing Offline Options"},{"location":"configs/configuration-reference/","text":"Configuration Reference This section is the authoritative reference for JanusGraph configuration options. It includes all options for storage and indexing backends that are part of the official JanusGraph distribution. The table is automatically generated by traversing the keys and namespaces in JanusGraph\u2019s internal configuration management API. Hence, the configuration options as listed on this page are synchronized with a particular JanusGraph release. If a reference to a configuration option in other parts of this documentation is in conflict with its representation on this page, assume the version listed here to be correct. Mutability Levels Each configuration option has a certain mutability level that governs whether and how it can be modified after the database is opened for the first time. The following listing describes the mutability levels. FIXED Once the database has been opened, these configuration options cannot be changed for the entire life of the database GLOBAL_OFFLINE These options can only be changed for the entire database cluster at once when all instances are shut down GLOBAL These options can only be changed globally across the entire database cluster MASKABLE These options are global but can be overwritten by a local configuration file LOCAL These options can only be provided through a local configuration file Refer to Global Configuration for information on how to change non-local configuration options. Umbrella Namespace Namespaces marked with an asterisk are umbrella namespaces which means that they can accommodate an arbitrary number of sub-namespaces - each of which uniquely identified by its name. The configuration options listed under an umbrella namespace apply only to those sub-namespaces. Umbrella namespaces are used to configure multiple system components that are of the same type and hence have the same configuration options. For example, the log namespace is an umbrella namespace because JanusGraph can interface with multiple logging backends, such as the user log, each of which has the same core set of configuration options. To configure the send batch size of the user log to 100 transaction changes, one would have to set the following option in the configuration log.user.send-batch-size = 100 Configuration Namespaces and Options attributes.custom * Custom attribute serialization and handling Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE cache Configuration options that modify JanusGraph's caching behavior Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data. Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them. This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 MASKABLE cache.db-cache-size Size of JanusGraph's database level cache. Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 MASKABLE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE cluster Configuration options for multi-machine deployments Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodes in the JanusGraph graph cluster. Must be greater than 1 and a power of 2. Integer 32 FIXED computer GraphComputer related configuration Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE graph General configuration options Name Description Datatype Default Value Mutability graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL. These types are managed globally through the storage backend and cannot be overridden by changing the local configuration. This type of conflict usually indicates misconfiguration. When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option. When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.allow-upgrade Setting this to true will allow certain fixed values to be updated such as storage-version. This should only be used for upgrading. Boolean false MASKABLE graph.assign-timestamp Whether to use JanusGraph generated client-side timestamp in mutations if the backend supports it. When enabled, JanusGraph assigns one timestamp to all insertions and another slightly earlier timestamp to all deletions in the same batch. When this is disabled, mutation behavior depends on the backend. Some might use server-side timestamp (e.g. HBase) while others might use client-side timestamp generated by driver (CQL). Boolean true LOCAL graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagement APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anyway. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false FIXED graph.storage-version The version of JanusGraph storage schema with which this database was created. Automatically set on first start of graph. Should only ever be changed if upgrading to a new major release version of JanusGraph that contains schema changes String (no default value) FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored. An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance. This must be unique among all instances concurrently accessing the same stores or indexes. It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL graph.script-eval Configuration options for gremlin script engine. Name Description Datatype Default Value Mutability graph.script-eval.enabled Whether to enable Gremlin script evaluation. If it is enabled, a gremlin script engine will be instantiated together with the JanusGraph instance, with which one can use eval method to evaluate a gremlin script in plain string format. This is usually only useful when JanusGraph is used as an embedded Java library. Boolean false MASKABLE graph.script-eval.engine Full class name of script engine that implements GremlinScriptEngine interface. Following shorthands can be used: - GremlinLangScriptEngine (A script engine that only accepts standard gremlin queries. Anything else including lambda function is not accepted. We recommend using this because it's generally safer, but it is not guaranteed that it has no security problem.)- GremlinGroovyScriptEngine (A script engine that accepts arbitrary groovy code. This can be dangerous and you should use it at your own risk. See https://tinkerpop.apache.org/docs/current/reference/#script-execution for potential security problems.) String GremlinLangScriptEngine MASKABLE gremlin Gremlin configuration options Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL ids General configuration options for graph element IDs Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size. Setting this too low will make commits frequently block on slow reservation requests. Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation. When false, IDs are assigned only when the transaction commits. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE ids.authority Configuration options for graph element ID reservation/allocation Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE index * Configuration options for the individual indexing backends Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional. JanusGraph can use multiple heterogeneous index backends. Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers. This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that the indexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maximum number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE index.[X].bkd-circle-processor Configuration for BKD circle processors which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.class Full class name of circle processor that implements CircleProcessor interface. The class is used for transformation of a Circle shape to another shape when BKD mapping is used. The provided implementation class should have either a public constructor which accepts configuration as a parameter ( org.janusgraph.diskstorage.configuration.Configuration ) or a public constructor with no parameters. Usually the transforming shape is a Polygon. Following shorthands can be used: - noTransformation Circle processor which is not transforming a circle, but instead keep the circle shape unchanged. This implementation may be useful in situations when the user wants to control circle transformation logic on ElasticSearch side instead of application side. For example, using ElasticSearch Circle Processor or any custom plugin. - fixedErrorDistance Circle processor which transforms the provided Circle into Polygon, Box, or Point depending on the configuration provided in index.bkd-circle-processor.fixed . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. - dynamicErrorDistance Circle processor which calculates error distance dynamically depending on the circle radius and the specified multiplier value. The error distance calculation formula is log(radius) * multiplier . Configuration for this class can be provided via index.bkd-circle-processor.dynamic . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. String dynamicErrorDistance MASKABLE index.[X].bkd-circle-processor.dynamic Configuration for Elasticsearch dynamic circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.dynamic.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.dynamic.error-distance-multiplier Multiplier variable for dynamic error distance calculation in the formula log(radius) * multiplier . Radius and error distance specified in meters. Double 2.0 MASKABLE index.[X].bkd-circle-processor.fixed Configuration for Elasticsearch fixed circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.fixed.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.fixed.error-distance The difference between the resulting inscribed distance from center to side and the circle\u2019s radius. Specified in meters. Double 10.0 MASKABLE index.[X].elasticsearch Elasticsearch index configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.client-keep-alive Set a keep-alive timeout (in milliseconds) Long (no default value) GLOBAL_OFFLINE index.[X].elasticsearch.connect-timeout Sets the maximum connection timeout (in milliseconds). Integer 1000 MASKABLE index.[X].elasticsearch.enable_index_names_cache Enables cache for generated index store names. It is recommended to always enable index store names cache unless you have more then 50000 indexes per index store. Boolean true MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status. This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.retry_on_conflict Specify how many times should the operation be retried when a conflict occurs. Integer 0 MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in seconds) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.setup-max-open-scroll-contexts Whether JanusGraph should setup max_open_scroll_context to maximum value for the cluster or not. Boolean true MASKABLE index.[X].elasticsearch.socket-timeout Sets the maximum socket timeout (in milliseconds). Integer 30000 MASKABLE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-mapping-for-es7 Mapping types are deprecated in ElasticSearch 7 and JanusGraph will not use mapping types by default for ElasticSearch 7 but if you want to preserve mapping types, you can setup this parameter to true. If you are updating ElasticSearch from 6 to 7 and you don't want to reindex your indexes, you may setup this parameter to true but we do recommend to reindex your indexes and don't use this parameter. Boolean false MASKABLE index.[X].elasticsearch.create Settings related to index creation Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index. This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE index.[X].elasticsearch.create.ext Overrides for arbitrary settings applied at index creation. See Elasticsearch , The full list of possible setting is available at Elasticsearch index settings . Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.ext.number_of_replicas The number of replicas each primary shard has Integer 1 MASKABLE index.[X].elasticsearch.create.ext.number_of_shards The number of primary shards that an index should have.Default value is 5 on ES 6 and 1 on ES 7 Integer (no default value) MASKABLE index.[X].elasticsearch.http.auth Configuration options for HTTP(S) authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.type Authentication type to be used for HTTP(S) access. Available options are NONE , BASIC and CUSTOM . String NONE LOCAL index.[X].elasticsearch.http.auth.basic Configuration options for HTTP(S) Basic authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.basic.password Password for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.basic.realm Realm value for HTTP(S) authentication. If empty, any realm is accepted. String LOCAL index.[X].elasticsearch.http.auth.basic.username Username for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.custom Configuration options for custom HTTP(S) authenticator. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.custom.authenticator-args Comma-separated custom authenticator constructor arguments. String[] LOCAL index.[X].elasticsearch.http.auth.custom.authenticator-class Authenticator fully qualified class name. String LOCAL index.[X].elasticsearch.ssl Elasticsearch SSL configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.allow-self-signed-certificates Controls the accepting of the self-signed SSL certificates. Boolean false LOCAL index.[X].elasticsearch.ssl.disable-hostname-verification Disables the SSL hostname verification if set to true. Hostname verification is enabled by default. Boolean false LOCAL index.[X].elasticsearch.ssl.enabled Controls use of the SSL connection to Elasticsearch. Boolean false LOCAL index.[X].elasticsearch.ssl.keystore Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.keystore.keypassword The password to access the key in the SSL Keystore. If the option is not present, the value of \"storepassword\" is used. String LOCAL index.[X].elasticsearch.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.keystore.storepassword The password to access SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.truststore Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL index.[X].elasticsearch.ssl.truststore.password The password to access SSL Truststore. String LOCAL index.[X].solr Solr index configuration Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be reused for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabled, the user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.kerberos-enabled Whether SOLR instance is Kerberized or not. Boolean false MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of collection=field . String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP ( http ) or using SolrCloud ( cloud ) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE log * Configuration options for JanusGraph's logging system Name Description Datatype Default Value Mutability log.[X].backend Define the log backend to use. A reserved shortcut default can be used to use graph's storage backend to manage logs. A custom log implementation can be specified by providing full class path which implements org.janusgraph.diskstorage.log.LogManager and accepts a single parameter org.janusgraph.diskstorage.configuration.Configuration in the public constructor. String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not become visible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaning that all entries added to this log expire after the configured amount of time. Requires that the log implementation supports TTL. Duration (no default value) GLOBAL metrics Configuration options for metrics reporting Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE metrics.console Configuration options for metrics reporting to console Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE metrics.csv Configuration options for metrics reporting to CSV file Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.graphite Configuration options for metrics reporting through Graphite Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE metrics.jmx Configuration options for metrics reporting through JMX Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE metrics.slf4j Configuration options for metrics reporting through slf4j Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE query Configuration options for query processing Name Description Datatype Default Value Mutability query.batch Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. Boolean true MASKABLE query.batch-property-prefetch Whether to do a batched pre-fetch of all properties on adjacent vertices against the storage backend prior to evaluating a has condition against those vertices. Because these vertex properties will be loaded into the transaction-level cache of recently-used vertices when the condition is evaluated this can lead to significant performance improvement if there are many edges to adjacent vertices and there is a non-trivial latency to the backend. Boolean false MASKABLE query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequent property access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing so limits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.hard-max-limit If smart-limit is disabled and no limit is given in the query, query optimizer adds a limit in light of possibly large result sets. It works in the same way as smart-limit except that hard-max-limit is usually a large number. Default value is Integer.MAX_VALUE which effectively disables this behavior. This option does not take effect when smart-limit is enabled. Integer 2147483647 MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.index-select-strategy Name of the index selection strategy or full class name. Following shorthands can be used: - brute-force (Try all combinations of index candidates and pick up optimal one) - approximate (Use greedy algorithm to pick up approximately optimal index candidate) - threshold-based (Use index-select-threshold to pick up either approximate or threshold-based strategy on runtime) String threshold-based MASKABLE query.index-select-threshold Threshold of deciding whether to use brute force enumeration algorithm or fast approximation algorithm for selecting suitable indexes. Selecting optimal indexes for a query is a NP-complete set cover problem. When number of suitable index candidates is no larger than threshold, JanusGraph uses brute force search with exponential time complexity to ensure the best combination of indexes is selected. Only effective when threshold-based index select strategy is chosen. Integer 10 MASKABLE query.limit-batch-size Configure a maximum batch size for queries against the storage backend. This can be used to ensure responsiveness if batches tend to grow very large. The used batch size is equivalent to the barrier size of a preceding barrier() step. If a step has no preceding barrier(), the default barrier of TinkerPop will be inserted. This option only takes effect if query.batch is enabled. Boolean true MASKABLE query.optimizer-backend-access Whether the optimizer should be allowed to fire backend queries during the optimization phase. Allowing these will give the optimizer a chance to find more efficient execution plan but also increase the optimization overhead. Boolean true MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean false MASKABLE schema Schema related configuration options Name Description Datatype Default Value Mutability schema.constraints Configures the schema constraints to be used by this graph. If config 'schema.constraints' is set to 'true' and 'schema.default' is set to 'none', then an 'IllegalArgumentException' is thrown for schema constraint violations. If 'schema.constraints' is set to 'true' and 'schema.default' is not set 'none', schema constraints are automatically created as described in the config option 'schema.default'. If 'schema.constraints' is set to 'false' which is the default, then no schema constraints are applied. Boolean false GLOBAL_OFFLINE schema.default Configures the DefaultSchemaMaker to be used by this graph. Either one of the following shorthands can be used: - default (a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys), - tp3 (same as default, but has LIST property keys), - none (automatic schema creation is disabled) - ignore-prop (same as none, but simply ignore unknown properties rather than throw exceptions) - or to the full package and classname of a custom/third-party implementing the interface org.janusgraph.core.schema.DefaultSchemaMaker String default MASKABLE schema.logging Controls whether logging is enabled for schema makers. This only takes effect if you set schema.default to default or ignore-prop . For default schema maker, warning messages will be logged before schema types are created automatically. For ignore-prop schema maker, warning messages will be logged before unknown properties are ignored. Boolean false MASKABLE storage Configuration options for the storage backend. Some options are applicable only for certain backends. Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph. This is required. It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cql, hbase, inmemory, scylla) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers. This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to / . String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE storage.berkeleyje BerkeleyDB JE configuration options Name Description Datatype Default Value Mutability storage.berkeleyje.cache-mode Modes that can be specified for control over caching of records in the JE in-memory cache String DEFAULT MASKABLE storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE storage.berkeleyje.shared-cache If true, the shared cache is used for all graph instances Boolean true MASKABLE storage.cql CQL storage backend options Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.gc-grace-seconds The number of seconds before tombstones (deletion markers) are eligible for garbage-collection. Integer (no default value) FIXED storage.cql.heartbeat-interval The connection heartbeat interval in milliseconds. Long (no default value) MASKABLE storage.cql.heartbeat-timeout How long the driver waits for the response (in milliseconds) to a heartbeat. Long (no default value) MASKABLE storage.cql.keyspace The name of JanusGraph's keyspace. It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter. This value will be passed into CqlSessionBuilder.withLocalDatacenter. String datacenter1 MASKABLE storage.cql.local-max-connections-per-host The maximum number of connections that can be created per host for local datacenter Integer 1 MASKABLE storage.cql.max-requests-per-connection The maximum number of requests that can be executed concurrently on a connection. Integer 1024 MASKABLE storage.cql.metadata-schema-enabled Whether schema metadata is enabled. Boolean (no default value) MASKABLE storage.cql.metadata-token-map-enabled Whether token metadata is enabled. If disabled, partitioner-name must be provided. Boolean (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.partitioner-name The name of Cassandra cluster's partitioner. It will be retrieved by client if not provided. If provided, it must match the cluster's partitioner name. It can be the full class name such as org.apache.cassandra.dht.ByteOrderedPartitioner or the simple name such as ByteOrderedPartitioner String (no default value) MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database. If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.remote-max-connections-per-host The maximum number of connections that can be created per host for remote datacenter Integer 1 MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept. This options is used when storage.cql.replication-strategy-class is set to SimpleStrategy Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace. Available strategies: SimpleStrategy,NetworkTopologyStrategy. String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. This options is used when storage.cql.replication-strategy-class is set to NetworkTopologyStrategy. replication_factor can be used to specify a replication factor. String[] (no default value) FIXED storage.cql.request-timeout Timeout for CQL requests in milliseconds. See DataStax Java Driver option basic.request.timeout for more information. Long 12000 MASKABLE storage.cql.session-leak-threshold The maximum number of live sessions that are allowed to coexist in a given VM until the warning starts to log for every new session. If the value is less than or equal to 0, the feature is disabled: no warning will be issued. See DataStax Java Driver option advanced.session-leak.threshold for more information. Integer (no default value) MASKABLE storage.cql.session-name Default name for the Cassandra session String JanusGraph Session MASKABLE storage.cql.speculative-retry The speculative retry policy. One of: NONE, ALWAYS, percentile, ms. String (no default value) FIXED storage.cql.ttl-enabled Whether TTL should be enabled or not. Must be turned off if the storage does not support TTL. Amazon Keyspace, for example, does not support TTL by default unless otherwise enabled. Boolean true LOCAL storage.cql.use-external-locking True to prevent JanusGraph from using its own locking mechanism. Setting this to true eliminates redundant checks when using an external locking mechanism outside of JanusGraph. Be aware that when use-external-locking is set to true, that failure to employ a locking algorithm which locks all columns that participate in a transaction upfront and unlocks them when the transaction ends, will result in a 'read uncommitted' transaction isolation level guarantee. If set to true without an appropriate external locking mechanism in place side effects such as dirty/non-repeatable/phantom reads should be expected. Boolean false MASKABLE storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE storage.cql.executor-service Configuration options for CQL executor service which is used to process CQL queries. Name Description Datatype Default Value Mutability storage.cql.executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.cql.executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 10 LOCAL storage.cql.executor-service.enabled Whether to use CQL executor service to process queries or not. If not used, the parallelism will be controlled internally by the CQL driver via storage.cql.max-requests-per-connection parameter which may be preferable in production environments. Disabling executor service reduces overhead of thread pool but might be more difficult to tune. Boolean false LOCAL storage.cql.executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.cql.executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.cql.executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL storage.cql.internal Advanced configuration of internal DataStax driver. Notice, all available configurations will be composed in the order. Non specified configurations will be skipped. By default only base configuration is enabled (which has the smallest priority. It means that you can overwrite any configuration used in base programmatic configuration by using any other configuration type). The configurations are composed in the next order (sorted by priority in descending order): file-configuration , resource-configuration , string-configuration , url-configuration , base-programmatic-configuration (which is controlled by base-programmatic-configuration-enabled property). Configurations with higher priority always overwrite configurations with lower priority. I.e. if the same configuration parameter is used in both file-configuration and string-configuration the configuration parameter from file-configuration will be used and configuration parameter from string-configuration will be ignored. See available configuration options and configurations structure here: https://docs.datastax.com/en/developer/java-driver/4.13/manual/core/configuration/reference/ Name Description Datatype Default Value Mutability storage.cql.internal.base-programmatic-configuration-enabled Whether to use main programmatic configuration provided by JanusGraph properties or not. We don't recommend to disable this property unless you want to disable usage of all storage.cql properties and use default configurations or other configurations. If programmatic configuration options miss some important configuration options you can provide those missing configurations with other configuration types which will be applied with programmatic configuration (see other configuration types in this section). For most use cases this option should always be true . JanusGraph behaviour might be unpredictable when using unspecified configuration options. Boolean true MASKABLE storage.cql.internal.file-configuration Path to file with DataStax configuration. String (no default value) LOCAL storage.cql.internal.resource-configuration Classpath resource with DataStax configuration. String (no default value) MASKABLE storage.cql.internal.string-configuration String representing DataStax configuration. String (no default value) MASKABLE storage.cql.internal.url-configuration Url where to get DataStax configuration. String (no default value) MASKABLE storage.cql.metrics Configuration options for CQL metrics Name Description Datatype Default Value Mutability storage.cql.metrics.cql-messages-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-messages-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-messages-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.significant-digits Integer (no default value) LOCAL storage.cql.metrics.cql-requests-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-requests-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-requests-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.significant-digits Integer (no default value) LOCAL storage.cql.metrics.node-enabled Comma separated list of enabled node metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: pool.open-connections, pool.available-streams, bytes-sent). String[] (no default value) LOCAL storage.cql.metrics.node-expire-after The time after which the node level metrics will be evicted in milliseconds. Long (no default value) LOCAL storage.cql.metrics.session-enabled Comma separated list of enabled session metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: bytes-sent, bytes-received, connected-nodes). String[] (no default value) LOCAL storage.cql.metrics.throttling-delay-highest-latency The largest latency that we expect to record for throttling in milliseconds. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.highest-latency Long (no default value) LOCAL storage.cql.metrics.throttling-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for throttling. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.refresh-interval Long (no default value) LOCAL storage.cql.metrics.throttling-delay-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for throttling. This must be between 0 and 5. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.significant-digits Integer (no default value) LOCAL storage.cql.netty Configuration options related to the Netty event loop groups used internally by the CQL driver. Name Description Datatype Default Value Mutability storage.cql.netty.admin-size The number of threads for the event loop group used for admin tasks not related to request I/O (handle cluster events, refresh metadata, schedule reconnections, etc.). If this is not set, the driver will use 2. Integer (no default value) LOCAL storage.cql.netty.io-size The number of threads for the event loop group used for I/O operations (reading and writing to Cassandra nodes). If this is not set, the driver will use Runtime.getRuntime().availableProcessors() * 2 . Integer (no default value) LOCAL storage.cql.netty.timer-tick-duration The timer tick duration in milliseconds. This is how frequent the timer should wake up to check for timed-out tasks or speculative executions. See DataStax Java Driver option advanced.netty.timer.tick-duration for more information. Long (no default value) LOCAL storage.cql.netty.timer-ticks-per-wheel Number of ticks in a Timer wheel. See DataStax Java Driver option advanced.netty.timer.ticks-per-wheel for more information. Integer (no default value) LOCAL storage.cql.request-tracker Configuration options for CQL request tracker and builtin request logger Name Description Datatype Default Value Mutability storage.cql.request-tracker.class It is either a predefined DataStax driver value for a builtin request tracker or a full qualified class name which implements com.datastax.oss.driver.internal.core.tracker.RequestTracker interface. If no any value provided, the default DataStax request tracker is used, which is NoopRequestTracker which doesn't do anything. If RequestLogger value is provided, the DataStax RequestLogger is used. String (no default value) LOCAL storage.cql.request-tracker.logs-error-enabled Whether to log failed requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-max-query-length The maximum length of the query string in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-value-length The maximum length for bound values in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-values The maximum number of bound values to log. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-show-stack-traces Whether to log stack traces for failed queries. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-show-values Whether to log bound values in addition to the query string. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-enabled Whether to log slow requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-threshold The threshold to classify a successful request as slow . In milliseconds. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Long (no default value) LOCAL storage.cql.request-tracker.logs-success-enabled Whether to log successful requests. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.ssl Configuration options for SSL Name Description Datatype Default Value Mutability storage.cql.ssl.client-authentication-enabled Enables use of a client key to authenticate with Cassandra Boolean false LOCAL storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL storage.cql.ssl.hostname_validation Enable / disable SSL hostname validation. Boolean false LOCAL storage.cql.ssl.keystore Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability storage.cql.ssl.keystore.keypassword The password to access the key in SSL Keystore. String LOCAL storage.cql.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL storage.cql.ssl.keystore.storepassword The password to access the SSL Keystore. String LOCAL storage.cql.ssl.truststore Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL storage.hbase HBase storage options Name Description Datatype Default Value Mutability storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster. JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances. This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.snapshot-name The name of an existing HBase snapshot to be used by HBaseSnapshotInputFormat String janusgraph-snapshot LOCAL storage.hbase.snapshot-restore-dir The temporary directory to be used by HBaseSnapshotInputFormat to restore a snapshot. This directory should be on the same File System as the HBase root dir. String /tmp LOCAL storage.hbase.table The name of the table JanusGraph will use. When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL storage.lock Options for locking on eventually-consistent stores Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend. JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code. JanusGraph generates an appropriate default value for this option at startup. Overriding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Although this value is maskable, it is highly recommended to use the same value across JanusGraph instances in production environments. Duration 100 ms MASKABLE storage.meta * Meta data to include in storage backend retrievals Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps. If enabled, timestamp can be retrieved by element.value(ImplicitKey.TIMESTAMP.name()) or equivalently, element.value(\"~timestamp\") . Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL. If enabled, ttl can be retrieved by element.value(ImplicitKey.TTL.name()) or equivalently, element.value(\"~ttl\") . Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility. If enabled, visibility can be retrieved by element.value(ImplicitKey.VISIBILITY.name()) or equivalently, element.value(\"~visibility\") . Boolean true GLOBAL storage.parallel-backend-executor-service Configuration options for executor service which is used for parallel requests when storage.parallel-backend-ops is enabled. Name Description Datatype Default Value Mutability storage.parallel-backend-executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.parallel-backend-executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.parallel-backend-executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.parallel-backend-executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.parallel-backend-executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL tx Configuration options for transaction handling Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL tx.recovery Configuration options for transaction recovery processes Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE","title":"Configuration Reference"},{"location":"configs/configuration-reference/#configuration-reference","text":"This section is the authoritative reference for JanusGraph configuration options. It includes all options for storage and indexing backends that are part of the official JanusGraph distribution. The table is automatically generated by traversing the keys and namespaces in JanusGraph\u2019s internal configuration management API. Hence, the configuration options as listed on this page are synchronized with a particular JanusGraph release. If a reference to a configuration option in other parts of this documentation is in conflict with its representation on this page, assume the version listed here to be correct.","title":"Configuration Reference"},{"location":"configs/configuration-reference/#mutability-levels","text":"Each configuration option has a certain mutability level that governs whether and how it can be modified after the database is opened for the first time. The following listing describes the mutability levels. FIXED Once the database has been opened, these configuration options cannot be changed for the entire life of the database GLOBAL_OFFLINE These options can only be changed for the entire database cluster at once when all instances are shut down GLOBAL These options can only be changed globally across the entire database cluster MASKABLE These options are global but can be overwritten by a local configuration file LOCAL These options can only be provided through a local configuration file Refer to Global Configuration for information on how to change non-local configuration options.","title":"Mutability Levels"},{"location":"configs/configuration-reference/#umbrella-namespace","text":"Namespaces marked with an asterisk are umbrella namespaces which means that they can accommodate an arbitrary number of sub-namespaces - each of which uniquely identified by its name. The configuration options listed under an umbrella namespace apply only to those sub-namespaces. Umbrella namespaces are used to configure multiple system components that are of the same type and hence have the same configuration options. For example, the log namespace is an umbrella namespace because JanusGraph can interface with multiple logging backends, such as the user log, each of which has the same core set of configuration options. To configure the send batch size of the user log to 100 transaction changes, one would have to set the following option in the configuration log.user.send-batch-size = 100","title":"Umbrella Namespace"},{"location":"configs/configuration-reference/#configuration-namespaces-and-options","text":"","title":"Configuration Namespaces and Options"},{"location":"configs/configuration-reference/#attributescustom","text":"Custom attribute serialization and handling Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE","title":"attributes.custom *"},{"location":"configs/configuration-reference/#cache","text":"Configuration options that modify JanusGraph's caching behavior Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data. Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them. This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 MASKABLE cache.db-cache-size Size of JanusGraph's database level cache. Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 MASKABLE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE","title":"cache"},{"location":"configs/configuration-reference/#cluster","text":"Configuration options for multi-machine deployments Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodes in the JanusGraph graph cluster. Must be greater than 1 and a power of 2. Integer 32 FIXED","title":"cluster"},{"location":"configs/configuration-reference/#computer","text":"GraphComputer related configuration Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE","title":"computer"},{"location":"configs/configuration-reference/#graph","text":"General configuration options Name Description Datatype Default Value Mutability graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL. These types are managed globally through the storage backend and cannot be overridden by changing the local configuration. This type of conflict usually indicates misconfiguration. When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option. When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.allow-upgrade Setting this to true will allow certain fixed values to be updated such as storage-version. This should only be used for upgrading. Boolean false MASKABLE graph.assign-timestamp Whether to use JanusGraph generated client-side timestamp in mutations if the backend supports it. When enabled, JanusGraph assigns one timestamp to all insertions and another slightly earlier timestamp to all deletions in the same batch. When this is disabled, mutation behavior depends on the backend. Some might use server-side timestamp (e.g. HBase) while others might use client-side timestamp generated by driver (CQL). Boolean true LOCAL graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagement APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anyway. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false FIXED graph.storage-version The version of JanusGraph storage schema with which this database was created. Automatically set on first start of graph. Should only ever be changed if upgrading to a new major release version of JanusGraph that contains schema changes String (no default value) FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored. An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance. This must be unique among all instances concurrently accessing the same stores or indexes. It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL","title":"graph"},{"location":"configs/configuration-reference/#graphscript-eval","text":"Configuration options for gremlin script engine. Name Description Datatype Default Value Mutability graph.script-eval.enabled Whether to enable Gremlin script evaluation. If it is enabled, a gremlin script engine will be instantiated together with the JanusGraph instance, with which one can use eval method to evaluate a gremlin script in plain string format. This is usually only useful when JanusGraph is used as an embedded Java library. Boolean false MASKABLE graph.script-eval.engine Full class name of script engine that implements GremlinScriptEngine interface. Following shorthands can be used: - GremlinLangScriptEngine (A script engine that only accepts standard gremlin queries. Anything else including lambda function is not accepted. We recommend using this because it's generally safer, but it is not guaranteed that it has no security problem.)- GremlinGroovyScriptEngine (A script engine that accepts arbitrary groovy code. This can be dangerous and you should use it at your own risk. See https://tinkerpop.apache.org/docs/current/reference/#script-execution for potential security problems.) String GremlinLangScriptEngine MASKABLE","title":"graph.script-eval"},{"location":"configs/configuration-reference/#gremlin","text":"Gremlin configuration options Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL","title":"gremlin"},{"location":"configs/configuration-reference/#ids","text":"General configuration options for graph element IDs Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size. Setting this too low will make commits frequently block on slow reservation requests. Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation. When false, IDs are assigned only when the transaction commits. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE","title":"ids"},{"location":"configs/configuration-reference/#idsauthority","text":"Configuration options for graph element ID reservation/allocation Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE","title":"ids.authority"},{"location":"configs/configuration-reference/#index","text":"Configuration options for the individual indexing backends Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional. JanusGraph can use multiple heterogeneous index backends. Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers. This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that the indexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maximum number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE","title":"index *"},{"location":"configs/configuration-reference/#indexxbkd-circle-processor","text":"Configuration for BKD circle processors which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.class Full class name of circle processor that implements CircleProcessor interface. The class is used for transformation of a Circle shape to another shape when BKD mapping is used. The provided implementation class should have either a public constructor which accepts configuration as a parameter ( org.janusgraph.diskstorage.configuration.Configuration ) or a public constructor with no parameters. Usually the transforming shape is a Polygon. Following shorthands can be used: - noTransformation Circle processor which is not transforming a circle, but instead keep the circle shape unchanged. This implementation may be useful in situations when the user wants to control circle transformation logic on ElasticSearch side instead of application side. For example, using ElasticSearch Circle Processor or any custom plugin. - fixedErrorDistance Circle processor which transforms the provided Circle into Polygon, Box, or Point depending on the configuration provided in index.bkd-circle-processor.fixed . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. - dynamicErrorDistance Circle processor which calculates error distance dynamically depending on the circle radius and the specified multiplier value. The error distance calculation formula is log(radius) * multiplier . Configuration for this class can be provided via index.bkd-circle-processor.dynamic . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. String dynamicErrorDistance MASKABLE","title":"index.[X].bkd-circle-processor"},{"location":"configs/configuration-reference/#indexxbkd-circle-processordynamic","text":"Configuration for Elasticsearch dynamic circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.dynamic.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.dynamic.error-distance-multiplier Multiplier variable for dynamic error distance calculation in the formula log(radius) * multiplier . Radius and error distance specified in meters. Double 2.0 MASKABLE","title":"index.[X].bkd-circle-processor.dynamic"},{"location":"configs/configuration-reference/#indexxbkd-circle-processorfixed","text":"Configuration for Elasticsearch fixed circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.fixed.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.fixed.error-distance The difference between the resulting inscribed distance from center to side and the circle\u2019s radius. Specified in meters. Double 10.0 MASKABLE","title":"index.[X].bkd-circle-processor.fixed"},{"location":"configs/configuration-reference/#indexxelasticsearch","text":"Elasticsearch index configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.client-keep-alive Set a keep-alive timeout (in milliseconds) Long (no default value) GLOBAL_OFFLINE index.[X].elasticsearch.connect-timeout Sets the maximum connection timeout (in milliseconds). Integer 1000 MASKABLE index.[X].elasticsearch.enable_index_names_cache Enables cache for generated index store names. It is recommended to always enable index store names cache unless you have more then 50000 indexes per index store. Boolean true MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status. This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.retry_on_conflict Specify how many times should the operation be retried when a conflict occurs. Integer 0 MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in seconds) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.setup-max-open-scroll-contexts Whether JanusGraph should setup max_open_scroll_context to maximum value for the cluster or not. Boolean true MASKABLE index.[X].elasticsearch.socket-timeout Sets the maximum socket timeout (in milliseconds). Integer 30000 MASKABLE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-mapping-for-es7 Mapping types are deprecated in ElasticSearch 7 and JanusGraph will not use mapping types by default for ElasticSearch 7 but if you want to preserve mapping types, you can setup this parameter to true. If you are updating ElasticSearch from 6 to 7 and you don't want to reindex your indexes, you may setup this parameter to true but we do recommend to reindex your indexes and don't use this parameter. Boolean false MASKABLE","title":"index.[X].elasticsearch"},{"location":"configs/configuration-reference/#indexxelasticsearchcreate","text":"Settings related to index creation Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index. This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE","title":"index.[X].elasticsearch.create"},{"location":"configs/configuration-reference/#indexxelasticsearchcreateext","text":"Overrides for arbitrary settings applied at index creation. See Elasticsearch , The full list of possible setting is available at Elasticsearch index settings . Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.ext.number_of_replicas The number of replicas each primary shard has Integer 1 MASKABLE index.[X].elasticsearch.create.ext.number_of_shards The number of primary shards that an index should have.Default value is 5 on ES 6 and 1 on ES 7 Integer (no default value) MASKABLE","title":"index.[X].elasticsearch.create.ext"},{"location":"configs/configuration-reference/#indexxelasticsearchhttpauth","text":"Configuration options for HTTP(S) authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.type Authentication type to be used for HTTP(S) access. Available options are NONE , BASIC and CUSTOM . String NONE LOCAL","title":"index.[X].elasticsearch.http.auth"},{"location":"configs/configuration-reference/#indexxelasticsearchhttpauthbasic","text":"Configuration options for HTTP(S) Basic authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.basic.password Password for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.basic.realm Realm value for HTTP(S) authentication. If empty, any realm is accepted. String LOCAL index.[X].elasticsearch.http.auth.basic.username Username for HTTP(S) authentication. String LOCAL","title":"index.[X].elasticsearch.http.auth.basic"},{"location":"configs/configuration-reference/#indexxelasticsearchhttpauthcustom","text":"Configuration options for custom HTTP(S) authenticator. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.custom.authenticator-args Comma-separated custom authenticator constructor arguments. String[] LOCAL index.[X].elasticsearch.http.auth.custom.authenticator-class Authenticator fully qualified class name. String LOCAL","title":"index.[X].elasticsearch.http.auth.custom"},{"location":"configs/configuration-reference/#indexxelasticsearchssl","text":"Elasticsearch SSL configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.allow-self-signed-certificates Controls the accepting of the self-signed SSL certificates. Boolean false LOCAL index.[X].elasticsearch.ssl.disable-hostname-verification Disables the SSL hostname verification if set to true. Hostname verification is enabled by default. Boolean false LOCAL index.[X].elasticsearch.ssl.enabled Controls use of the SSL connection to Elasticsearch. Boolean false LOCAL","title":"index.[X].elasticsearch.ssl"},{"location":"configs/configuration-reference/#indexxelasticsearchsslkeystore","text":"Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.keystore.keypassword The password to access the key in the SSL Keystore. If the option is not present, the value of \"storepassword\" is used. String LOCAL index.[X].elasticsearch.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.keystore.storepassword The password to access SSL Keystore. String LOCAL","title":"index.[X].elasticsearch.ssl.keystore"},{"location":"configs/configuration-reference/#indexxelasticsearchssltruststore","text":"Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL index.[X].elasticsearch.ssl.truststore.password The password to access SSL Truststore. String LOCAL","title":"index.[X].elasticsearch.ssl.truststore"},{"location":"configs/configuration-reference/#indexxsolr","text":"Solr index configuration Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be reused for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabled, the user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.kerberos-enabled Whether SOLR instance is Kerberized or not. Boolean false MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of collection=field . String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP ( http ) or using SolrCloud ( cloud ) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE","title":"index.[X].solr"},{"location":"configs/configuration-reference/#log","text":"Configuration options for JanusGraph's logging system Name Description Datatype Default Value Mutability log.[X].backend Define the log backend to use. A reserved shortcut default can be used to use graph's storage backend to manage logs. A custom log implementation can be specified by providing full class path which implements org.janusgraph.diskstorage.log.LogManager and accepts a single parameter org.janusgraph.diskstorage.configuration.Configuration in the public constructor. String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not become visible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaning that all entries added to this log expire after the configured amount of time. Requires that the log implementation supports TTL. Duration (no default value) GLOBAL","title":"log *"},{"location":"configs/configuration-reference/#metrics","text":"Configuration options for metrics reporting Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE","title":"metrics"},{"location":"configs/configuration-reference/#metricsconsole","text":"Configuration options for metrics reporting to console Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE","title":"metrics.console"},{"location":"configs/configuration-reference/#metricscsv","text":"Configuration options for metrics reporting to CSV file Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE","title":"metrics.csv"},{"location":"configs/configuration-reference/#metricsgraphite","text":"Configuration options for metrics reporting through Graphite Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE","title":"metrics.graphite"},{"location":"configs/configuration-reference/#metricsjmx","text":"Configuration options for metrics reporting through JMX Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE","title":"metrics.jmx"},{"location":"configs/configuration-reference/#metricsslf4j","text":"Configuration options for metrics reporting through slf4j Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE","title":"metrics.slf4j"},{"location":"configs/configuration-reference/#query","text":"Configuration options for query processing Name Description Datatype Default Value Mutability query.batch Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. Boolean true MASKABLE query.batch-property-prefetch Whether to do a batched pre-fetch of all properties on adjacent vertices against the storage backend prior to evaluating a has condition against those vertices. Because these vertex properties will be loaded into the transaction-level cache of recently-used vertices when the condition is evaluated this can lead to significant performance improvement if there are many edges to adjacent vertices and there is a non-trivial latency to the backend. Boolean false MASKABLE query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequent property access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing so limits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.hard-max-limit If smart-limit is disabled and no limit is given in the query, query optimizer adds a limit in light of possibly large result sets. It works in the same way as smart-limit except that hard-max-limit is usually a large number. Default value is Integer.MAX_VALUE which effectively disables this behavior. This option does not take effect when smart-limit is enabled. Integer 2147483647 MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.index-select-strategy Name of the index selection strategy or full class name. Following shorthands can be used: - brute-force (Try all combinations of index candidates and pick up optimal one) - approximate (Use greedy algorithm to pick up approximately optimal index candidate) - threshold-based (Use index-select-threshold to pick up either approximate or threshold-based strategy on runtime) String threshold-based MASKABLE query.index-select-threshold Threshold of deciding whether to use brute force enumeration algorithm or fast approximation algorithm for selecting suitable indexes. Selecting optimal indexes for a query is a NP-complete set cover problem. When number of suitable index candidates is no larger than threshold, JanusGraph uses brute force search with exponential time complexity to ensure the best combination of indexes is selected. Only effective when threshold-based index select strategy is chosen. Integer 10 MASKABLE query.limit-batch-size Configure a maximum batch size for queries against the storage backend. This can be used to ensure responsiveness if batches tend to grow very large. The used batch size is equivalent to the barrier size of a preceding barrier() step. If a step has no preceding barrier(), the default barrier of TinkerPop will be inserted. This option only takes effect if query.batch is enabled. Boolean true MASKABLE query.optimizer-backend-access Whether the optimizer should be allowed to fire backend queries during the optimization phase. Allowing these will give the optimizer a chance to find more efficient execution plan but also increase the optimization overhead. Boolean true MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean false MASKABLE","title":"query"},{"location":"configs/configuration-reference/#schema","text":"Schema related configuration options Name Description Datatype Default Value Mutability schema.constraints Configures the schema constraints to be used by this graph. If config 'schema.constraints' is set to 'true' and 'schema.default' is set to 'none', then an 'IllegalArgumentException' is thrown for schema constraint violations. If 'schema.constraints' is set to 'true' and 'schema.default' is not set 'none', schema constraints are automatically created as described in the config option 'schema.default'. If 'schema.constraints' is set to 'false' which is the default, then no schema constraints are applied. Boolean false GLOBAL_OFFLINE schema.default Configures the DefaultSchemaMaker to be used by this graph. Either one of the following shorthands can be used: - default (a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys), - tp3 (same as default, but has LIST property keys), - none (automatic schema creation is disabled) - ignore-prop (same as none, but simply ignore unknown properties rather than throw exceptions) - or to the full package and classname of a custom/third-party implementing the interface org.janusgraph.core.schema.DefaultSchemaMaker String default MASKABLE schema.logging Controls whether logging is enabled for schema makers. This only takes effect if you set schema.default to default or ignore-prop . For default schema maker, warning messages will be logged before schema types are created automatically. For ignore-prop schema maker, warning messages will be logged before unknown properties are ignored. Boolean false MASKABLE","title":"schema"},{"location":"configs/configuration-reference/#storage","text":"Configuration options for the storage backend. Some options are applicable only for certain backends. Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph. This is required. It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cql, hbase, inmemory, scylla) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers. This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to / . String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE","title":"storage"},{"location":"configs/configuration-reference/#storageberkeleyje","text":"BerkeleyDB JE configuration options Name Description Datatype Default Value Mutability storage.berkeleyje.cache-mode Modes that can be specified for control over caching of records in the JE in-memory cache String DEFAULT MASKABLE storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE storage.berkeleyje.shared-cache If true, the shared cache is used for all graph instances Boolean true MASKABLE","title":"storage.berkeleyje"},{"location":"configs/configuration-reference/#storagecql","text":"CQL storage backend options Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.gc-grace-seconds The number of seconds before tombstones (deletion markers) are eligible for garbage-collection. Integer (no default value) FIXED storage.cql.heartbeat-interval The connection heartbeat interval in milliseconds. Long (no default value) MASKABLE storage.cql.heartbeat-timeout How long the driver waits for the response (in milliseconds) to a heartbeat. Long (no default value) MASKABLE storage.cql.keyspace The name of JanusGraph's keyspace. It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter. This value will be passed into CqlSessionBuilder.withLocalDatacenter. String datacenter1 MASKABLE storage.cql.local-max-connections-per-host The maximum number of connections that can be created per host for local datacenter Integer 1 MASKABLE storage.cql.max-requests-per-connection The maximum number of requests that can be executed concurrently on a connection. Integer 1024 MASKABLE storage.cql.metadata-schema-enabled Whether schema metadata is enabled. Boolean (no default value) MASKABLE storage.cql.metadata-token-map-enabled Whether token metadata is enabled. If disabled, partitioner-name must be provided. Boolean (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.partitioner-name The name of Cassandra cluster's partitioner. It will be retrieved by client if not provided. If provided, it must match the cluster's partitioner name. It can be the full class name such as org.apache.cassandra.dht.ByteOrderedPartitioner or the simple name such as ByteOrderedPartitioner String (no default value) MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database. If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.remote-max-connections-per-host The maximum number of connections that can be created per host for remote datacenter Integer 1 MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept. This options is used when storage.cql.replication-strategy-class is set to SimpleStrategy Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace. Available strategies: SimpleStrategy,NetworkTopologyStrategy. String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. This options is used when storage.cql.replication-strategy-class is set to NetworkTopologyStrategy. replication_factor can be used to specify a replication factor. String[] (no default value) FIXED storage.cql.request-timeout Timeout for CQL requests in milliseconds. See DataStax Java Driver option basic.request.timeout for more information. Long 12000 MASKABLE storage.cql.session-leak-threshold The maximum number of live sessions that are allowed to coexist in a given VM until the warning starts to log for every new session. If the value is less than or equal to 0, the feature is disabled: no warning will be issued. See DataStax Java Driver option advanced.session-leak.threshold for more information. Integer (no default value) MASKABLE storage.cql.session-name Default name for the Cassandra session String JanusGraph Session MASKABLE storage.cql.speculative-retry The speculative retry policy. One of: NONE, ALWAYS, percentile, ms. String (no default value) FIXED storage.cql.ttl-enabled Whether TTL should be enabled or not. Must be turned off if the storage does not support TTL. Amazon Keyspace, for example, does not support TTL by default unless otherwise enabled. Boolean true LOCAL storage.cql.use-external-locking True to prevent JanusGraph from using its own locking mechanism. Setting this to true eliminates redundant checks when using an external locking mechanism outside of JanusGraph. Be aware that when use-external-locking is set to true, that failure to employ a locking algorithm which locks all columns that participate in a transaction upfront and unlocks them when the transaction ends, will result in a 'read uncommitted' transaction isolation level guarantee. If set to true without an appropriate external locking mechanism in place side effects such as dirty/non-repeatable/phantom reads should be expected. Boolean false MASKABLE storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE","title":"storage.cql"},{"location":"configs/configuration-reference/#storagecqlexecutor-service","text":"Configuration options for CQL executor service which is used to process CQL queries. Name Description Datatype Default Value Mutability storage.cql.executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.cql.executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 10 LOCAL storage.cql.executor-service.enabled Whether to use CQL executor service to process queries or not. If not used, the parallelism will be controlled internally by the CQL driver via storage.cql.max-requests-per-connection parameter which may be preferable in production environments. Disabling executor service reduces overhead of thread pool but might be more difficult to tune. Boolean false LOCAL storage.cql.executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.cql.executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.cql.executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL","title":"storage.cql.executor-service"},{"location":"configs/configuration-reference/#storagecqlinternal","text":"Advanced configuration of internal DataStax driver. Notice, all available configurations will be composed in the order. Non specified configurations will be skipped. By default only base configuration is enabled (which has the smallest priority. It means that you can overwrite any configuration used in base programmatic configuration by using any other configuration type). The configurations are composed in the next order (sorted by priority in descending order): file-configuration , resource-configuration , string-configuration , url-configuration , base-programmatic-configuration (which is controlled by base-programmatic-configuration-enabled property). Configurations with higher priority always overwrite configurations with lower priority. I.e. if the same configuration parameter is used in both file-configuration and string-configuration the configuration parameter from file-configuration will be used and configuration parameter from string-configuration will be ignored. See available configuration options and configurations structure here: https://docs.datastax.com/en/developer/java-driver/4.13/manual/core/configuration/reference/ Name Description Datatype Default Value Mutability storage.cql.internal.base-programmatic-configuration-enabled Whether to use main programmatic configuration provided by JanusGraph properties or not. We don't recommend to disable this property unless you want to disable usage of all storage.cql properties and use default configurations or other configurations. If programmatic configuration options miss some important configuration options you can provide those missing configurations with other configuration types which will be applied with programmatic configuration (see other configuration types in this section). For most use cases this option should always be true . JanusGraph behaviour might be unpredictable when using unspecified configuration options. Boolean true MASKABLE storage.cql.internal.file-configuration Path to file with DataStax configuration. String (no default value) LOCAL storage.cql.internal.resource-configuration Classpath resource with DataStax configuration. String (no default value) MASKABLE storage.cql.internal.string-configuration String representing DataStax configuration. String (no default value) MASKABLE storage.cql.internal.url-configuration Url where to get DataStax configuration. String (no default value) MASKABLE","title":"storage.cql.internal"},{"location":"configs/configuration-reference/#storagecqlmetrics","text":"Configuration options for CQL metrics Name Description Datatype Default Value Mutability storage.cql.metrics.cql-messages-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-messages-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-messages-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.significant-digits Integer (no default value) LOCAL storage.cql.metrics.cql-requests-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-requests-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-requests-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.significant-digits Integer (no default value) LOCAL storage.cql.metrics.node-enabled Comma separated list of enabled node metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: pool.open-connections, pool.available-streams, bytes-sent). String[] (no default value) LOCAL storage.cql.metrics.node-expire-after The time after which the node level metrics will be evicted in milliseconds. Long (no default value) LOCAL storage.cql.metrics.session-enabled Comma separated list of enabled session metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: bytes-sent, bytes-received, connected-nodes). String[] (no default value) LOCAL storage.cql.metrics.throttling-delay-highest-latency The largest latency that we expect to record for throttling in milliseconds. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.highest-latency Long (no default value) LOCAL storage.cql.metrics.throttling-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for throttling. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.refresh-interval Long (no default value) LOCAL storage.cql.metrics.throttling-delay-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for throttling. This must be between 0 and 5. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.significant-digits Integer (no default value) LOCAL","title":"storage.cql.metrics"},{"location":"configs/configuration-reference/#storagecqlnetty","text":"Configuration options related to the Netty event loop groups used internally by the CQL driver. Name Description Datatype Default Value Mutability storage.cql.netty.admin-size The number of threads for the event loop group used for admin tasks not related to request I/O (handle cluster events, refresh metadata, schedule reconnections, etc.). If this is not set, the driver will use 2. Integer (no default value) LOCAL storage.cql.netty.io-size The number of threads for the event loop group used for I/O operations (reading and writing to Cassandra nodes). If this is not set, the driver will use Runtime.getRuntime().availableProcessors() * 2 . Integer (no default value) LOCAL storage.cql.netty.timer-tick-duration The timer tick duration in milliseconds. This is how frequent the timer should wake up to check for timed-out tasks or speculative executions. See DataStax Java Driver option advanced.netty.timer.tick-duration for more information. Long (no default value) LOCAL storage.cql.netty.timer-ticks-per-wheel Number of ticks in a Timer wheel. See DataStax Java Driver option advanced.netty.timer.ticks-per-wheel for more information. Integer (no default value) LOCAL","title":"storage.cql.netty"},{"location":"configs/configuration-reference/#storagecqlrequest-tracker","text":"Configuration options for CQL request tracker and builtin request logger Name Description Datatype Default Value Mutability storage.cql.request-tracker.class It is either a predefined DataStax driver value for a builtin request tracker or a full qualified class name which implements com.datastax.oss.driver.internal.core.tracker.RequestTracker interface. If no any value provided, the default DataStax request tracker is used, which is NoopRequestTracker which doesn't do anything. If RequestLogger value is provided, the DataStax RequestLogger is used. String (no default value) LOCAL storage.cql.request-tracker.logs-error-enabled Whether to log failed requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-max-query-length The maximum length of the query string in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-value-length The maximum length for bound values in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-values The maximum number of bound values to log. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-show-stack-traces Whether to log stack traces for failed queries. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-show-values Whether to log bound values in addition to the query string. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-enabled Whether to log slow requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-threshold The threshold to classify a successful request as slow . In milliseconds. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Long (no default value) LOCAL storage.cql.request-tracker.logs-success-enabled Whether to log successful requests. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL","title":"storage.cql.request-tracker"},{"location":"configs/configuration-reference/#storagecqlssl","text":"Configuration options for SSL Name Description Datatype Default Value Mutability storage.cql.ssl.client-authentication-enabled Enables use of a client key to authenticate with Cassandra Boolean false LOCAL storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL storage.cql.ssl.hostname_validation Enable / disable SSL hostname validation. Boolean false LOCAL","title":"storage.cql.ssl"},{"location":"configs/configuration-reference/#storagecqlsslkeystore","text":"Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability storage.cql.ssl.keystore.keypassword The password to access the key in SSL Keystore. String LOCAL storage.cql.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL storage.cql.ssl.keystore.storepassword The password to access the SSL Keystore. String LOCAL","title":"storage.cql.ssl.keystore"},{"location":"configs/configuration-reference/#storagecqlssltruststore","text":"Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL","title":"storage.cql.ssl.truststore"},{"location":"configs/configuration-reference/#storagehbase","text":"HBase storage options Name Description Datatype Default Value Mutability storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster. JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances. This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.snapshot-name The name of an existing HBase snapshot to be used by HBaseSnapshotInputFormat String janusgraph-snapshot LOCAL storage.hbase.snapshot-restore-dir The temporary directory to be used by HBaseSnapshotInputFormat to restore a snapshot. This directory should be on the same File System as the HBase root dir. String /tmp LOCAL storage.hbase.table The name of the table JanusGraph will use. When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL","title":"storage.hbase"},{"location":"configs/configuration-reference/#storagelock","text":"Options for locking on eventually-consistent stores Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend. JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code. JanusGraph generates an appropriate default value for this option at startup. Overriding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Although this value is maskable, it is highly recommended to use the same value across JanusGraph instances in production environments. Duration 100 ms MASKABLE","title":"storage.lock"},{"location":"configs/configuration-reference/#storagemeta","text":"Meta data to include in storage backend retrievals Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps. If enabled, timestamp can be retrieved by element.value(ImplicitKey.TIMESTAMP.name()) or equivalently, element.value(\"~timestamp\") . Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL. If enabled, ttl can be retrieved by element.value(ImplicitKey.TTL.name()) or equivalently, element.value(\"~ttl\") . Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility. If enabled, visibility can be retrieved by element.value(ImplicitKey.VISIBILITY.name()) or equivalently, element.value(\"~visibility\") . Boolean true GLOBAL","title":"storage.meta *"},{"location":"configs/configuration-reference/#storageparallel-backend-executor-service","text":"Configuration options for executor service which is used for parallel requests when storage.parallel-backend-ops is enabled. Name Description Datatype Default Value Mutability storage.parallel-backend-executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.parallel-backend-executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.parallel-backend-executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.parallel-backend-executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.parallel-backend-executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL","title":"storage.parallel-backend-executor-service"},{"location":"configs/configuration-reference/#tx","text":"Configuration options for transaction handling Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL","title":"tx"},{"location":"configs/configuration-reference/#txrecovery","text":"Configuration options for transaction recovery processes Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE","title":"tx.recovery"},{"location":"configs/example-config/","text":"Example Graph Configuration This page illustrates a number of common graph configurations. Please refer to Configuration Reference and the pages of the respective storage backend , index backend for more information. Also, note that the JanusGraph distribution includes local configuration files in the conf/ directory. BerkeleyDB storage.backend = berkeleyje storage.directory = /tmp/graph index.search.backend = elasticsearch index.search.directory = /tmp/searchindex index.search.elasticsearch.client-only = false index.search.elasticsearch.local-mode = true This configuration file configures JanusGraph to use BerkeleyDB as an embedded storage backend, meaning, JanusGraph will start BerkeleyDB internally. The primary data will be stored in the directory /tmp/graph . In addition, this configures an embedded Elasticsearch index backend with the name search . JanusGraph will start Elasticsearch internally and it will not be externally accessible since local-mode is enabled. Elasticsearch stores all data for the search index in /tmp/searchindex . Configuring an index backend is optional. Cassandra Cassandra Remote storage.backend = cql storage.hostname = 100.100.100.1, 100.100.100.2 index.search.backend = elasticsearch index.search.hostname = 100.100.101.1, 100.100.101.2 index.search.elasticsearch.client-only = true This configuration file configures JanusGraph to use Cassandra as a remote storage backend. It assumes that a Cassandra cluster is running and accessible at the given IP addresses. If Cassandra is running locally, use the IP address 127.0.0.1 . In addition, this configures a remote Elasticsearch index backend with the name search . It assumes that an Elasticsearch cluster is running and accessible at the given IP addresses. Enabling client-only ensures that the local instance does not join the existing Elasticsearch cluster as another node but only connects to it. Configuring an index backend is optional. HBase storage.backend = hbase storage.hostname = 127.0.0.1 storage.port = 2181 index.search.backend = elasticsearch index.search.hostname = 127.0.0.1 index.search.elasticsearch.client-only = true This configuration file configures JanusGraph to use HBase as a remote storage backend. It assumes that an HBase cluster is running and accessible at the given IP addresses through the configured port. If HBase is running locally, use the IP address 127.0.0.1 . The optional index backend configuration is identical to remote index configuration described above.","title":"Config Example"},{"location":"configs/example-config/#example-graph-configuration","text":"This page illustrates a number of common graph configurations. Please refer to Configuration Reference and the pages of the respective storage backend , index backend for more information. Also, note that the JanusGraph distribution includes local configuration files in the conf/ directory.","title":"Example Graph Configuration"},{"location":"configs/example-config/#berkeleydb","text":"storage.backend = berkeleyje storage.directory = /tmp/graph index.search.backend = elasticsearch index.search.directory = /tmp/searchindex index.search.elasticsearch.client-only = false index.search.elasticsearch.local-mode = true This configuration file configures JanusGraph to use BerkeleyDB as an embedded storage backend, meaning, JanusGraph will start BerkeleyDB internally. The primary data will be stored in the directory /tmp/graph . In addition, this configures an embedded Elasticsearch index backend with the name search . JanusGraph will start Elasticsearch internally and it will not be externally accessible since local-mode is enabled. Elasticsearch stores all data for the search index in /tmp/searchindex . Configuring an index backend is optional.","title":"BerkeleyDB"},{"location":"configs/example-config/#cassandra","text":"","title":"Cassandra"},{"location":"configs/example-config/#cassandra-remote","text":"storage.backend = cql storage.hostname = 100.100.100.1, 100.100.100.2 index.search.backend = elasticsearch index.search.hostname = 100.100.101.1, 100.100.101.2 index.search.elasticsearch.client-only = true This configuration file configures JanusGraph to use Cassandra as a remote storage backend. It assumes that a Cassandra cluster is running and accessible at the given IP addresses. If Cassandra is running locally, use the IP address 127.0.0.1 . In addition, this configures a remote Elasticsearch index backend with the name search . It assumes that an Elasticsearch cluster is running and accessible at the given IP addresses. Enabling client-only ensures that the local instance does not join the existing Elasticsearch cluster as another node but only connects to it. Configuring an index backend is optional.","title":"Cassandra Remote"},{"location":"configs/example-config/#hbase","text":"storage.backend = hbase storage.hostname = 127.0.0.1 storage.port = 2181 index.search.backend = elasticsearch index.search.hostname = 127.0.0.1 index.search.elasticsearch.client-only = true This configuration file configures JanusGraph to use HBase as a remote storage backend. It assumes that an HBase cluster is running and accessible at the given IP addresses through the configured port. If HBase is running locally, use the IP address 127.0.0.1 . The optional index backend configuration is identical to remote index configuration described above.","title":"HBase"},{"location":"configs/janusgraph-cfg/","text":"attributes.custom * Custom attribute serialization and handling Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE cache Configuration options that modify JanusGraph's caching behavior Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data. Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them. This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 MASKABLE cache.db-cache-size Size of JanusGraph's database level cache. Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 MASKABLE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE cluster Configuration options for multi-machine deployments Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodes in the JanusGraph graph cluster. Must be greater than 1 and a power of 2. Integer 32 FIXED computer GraphComputer related configuration Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE graph General configuration options Name Description Datatype Default Value Mutability graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL. These types are managed globally through the storage backend and cannot be overridden by changing the local configuration. This type of conflict usually indicates misconfiguration. When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option. When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.allow-upgrade Setting this to true will allow certain fixed values to be updated such as storage-version. This should only be used for upgrading. Boolean false MASKABLE graph.assign-timestamp Whether to use JanusGraph generated client-side timestamp in mutations if the backend supports it. When enabled, JanusGraph assigns one timestamp to all insertions and another slightly earlier timestamp to all deletions in the same batch. When this is disabled, mutation behavior depends on the backend. Some might use server-side timestamp (e.g. HBase) while others might use client-side timestamp generated by driver (CQL). Boolean true LOCAL graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagement APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anyway. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false FIXED graph.storage-version The version of JanusGraph storage schema with which this database was created. Automatically set on first start of graph. Should only ever be changed if upgrading to a new major release version of JanusGraph that contains schema changes String (no default value) FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored. An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance. This must be unique among all instances concurrently accessing the same stores or indexes. It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL graph.script-eval Configuration options for gremlin script engine. Name Description Datatype Default Value Mutability graph.script-eval.enabled Whether to enable Gremlin script evaluation. If it is enabled, a gremlin script engine will be instantiated together with the JanusGraph instance, with which one can use eval method to evaluate a gremlin script in plain string format. This is usually only useful when JanusGraph is used as an embedded Java library. Boolean false MASKABLE graph.script-eval.engine Full class name of script engine that implements GremlinScriptEngine interface. Following shorthands can be used: - GremlinLangScriptEngine (A script engine that only accepts standard gremlin queries. Anything else including lambda function is not accepted. We recommend using this because it's generally safer, but it is not guaranteed that it has no security problem.)- GremlinGroovyScriptEngine (A script engine that accepts arbitrary groovy code. This can be dangerous and you should use it at your own risk. See https://tinkerpop.apache.org/docs/current/reference/#script-execution for potential security problems.) String GremlinLangScriptEngine MASKABLE gremlin Gremlin configuration options Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL ids General configuration options for graph element IDs Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size. Setting this too low will make commits frequently block on slow reservation requests. Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation. When false, IDs are assigned only when the transaction commits. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE ids.authority Configuration options for graph element ID reservation/allocation Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE index * Configuration options for the individual indexing backends Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional. JanusGraph can use multiple heterogeneous index backends. Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers. This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that the indexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maximum number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE index.[X].bkd-circle-processor Configuration for BKD circle processors which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.class Full class name of circle processor that implements CircleProcessor interface. The class is used for transformation of a Circle shape to another shape when BKD mapping is used. The provided implementation class should have either a public constructor which accepts configuration as a parameter ( org.janusgraph.diskstorage.configuration.Configuration ) or a public constructor with no parameters. Usually the transforming shape is a Polygon. Following shorthands can be used: - noTransformation Circle processor which is not transforming a circle, but instead keep the circle shape unchanged. This implementation may be useful in situations when the user wants to control circle transformation logic on ElasticSearch side instead of application side. For example, using ElasticSearch Circle Processor or any custom plugin. - fixedErrorDistance Circle processor which transforms the provided Circle into Polygon, Box, or Point depending on the configuration provided in index.bkd-circle-processor.fixed . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. - dynamicErrorDistance Circle processor which calculates error distance dynamically depending on the circle radius and the specified multiplier value. The error distance calculation formula is log(radius) * multiplier . Configuration for this class can be provided via index.bkd-circle-processor.dynamic . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. String dynamicErrorDistance MASKABLE index.[X].bkd-circle-processor.dynamic Configuration for Elasticsearch dynamic circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.dynamic.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.dynamic.error-distance-multiplier Multiplier variable for dynamic error distance calculation in the formula log(radius) * multiplier . Radius and error distance specified in meters. Double 2.0 MASKABLE index.[X].bkd-circle-processor.fixed Configuration for Elasticsearch fixed circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.fixed.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.fixed.error-distance The difference between the resulting inscribed distance from center to side and the circle\u2019s radius. Specified in meters. Double 10.0 MASKABLE index.[X].elasticsearch Elasticsearch index configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.client-keep-alive Set a keep-alive timeout (in milliseconds) Long (no default value) GLOBAL_OFFLINE index.[X].elasticsearch.connect-timeout Sets the maximum connection timeout (in milliseconds). Integer 1000 MASKABLE index.[X].elasticsearch.enable_index_names_cache Enables cache for generated index store names. It is recommended to always enable index store names cache unless you have more then 50000 indexes per index store. Boolean true MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status. This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.retry_on_conflict Specify how many times should the operation be retried when a conflict occurs. Integer 0 MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in seconds) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.setup-max-open-scroll-contexts Whether JanusGraph should setup max_open_scroll_context to maximum value for the cluster or not. Boolean true MASKABLE index.[X].elasticsearch.socket-timeout Sets the maximum socket timeout (in milliseconds). Integer 30000 MASKABLE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-mapping-for-es7 Mapping types are deprecated in ElasticSearch 7 and JanusGraph will not use mapping types by default for ElasticSearch 7 but if you want to preserve mapping types, you can setup this parameter to true. If you are updating ElasticSearch from 6 to 7 and you don't want to reindex your indexes, you may setup this parameter to true but we do recommend to reindex your indexes and don't use this parameter. Boolean false MASKABLE index.[X].elasticsearch.create Settings related to index creation Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index. This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE index.[X].elasticsearch.create.ext Overrides for arbitrary settings applied at index creation. See Elasticsearch , The full list of possible setting is available at Elasticsearch index settings . Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.ext.number_of_replicas The number of replicas each primary shard has Integer 1 MASKABLE index.[X].elasticsearch.create.ext.number_of_shards The number of primary shards that an index should have.Default value is 5 on ES 6 and 1 on ES 7 Integer (no default value) MASKABLE index.[X].elasticsearch.http.auth Configuration options for HTTP(S) authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.type Authentication type to be used for HTTP(S) access. Available options are NONE , BASIC and CUSTOM . String NONE LOCAL index.[X].elasticsearch.http.auth.basic Configuration options for HTTP(S) Basic authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.basic.password Password for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.basic.realm Realm value for HTTP(S) authentication. If empty, any realm is accepted. String LOCAL index.[X].elasticsearch.http.auth.basic.username Username for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.custom Configuration options for custom HTTP(S) authenticator. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.custom.authenticator-args Comma-separated custom authenticator constructor arguments. String[] LOCAL index.[X].elasticsearch.http.auth.custom.authenticator-class Authenticator fully qualified class name. String LOCAL index.[X].elasticsearch.ssl Elasticsearch SSL configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.allow-self-signed-certificates Controls the accepting of the self-signed SSL certificates. Boolean false LOCAL index.[X].elasticsearch.ssl.disable-hostname-verification Disables the SSL hostname verification if set to true. Hostname verification is enabled by default. Boolean false LOCAL index.[X].elasticsearch.ssl.enabled Controls use of the SSL connection to Elasticsearch. Boolean false LOCAL index.[X].elasticsearch.ssl.keystore Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.keystore.keypassword The password to access the key in the SSL Keystore. If the option is not present, the value of \"storepassword\" is used. String LOCAL index.[X].elasticsearch.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.keystore.storepassword The password to access SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.truststore Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL index.[X].elasticsearch.ssl.truststore.password The password to access SSL Truststore. String LOCAL index.[X].solr Solr index configuration Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be reused for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabled, the user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.kerberos-enabled Whether SOLR instance is Kerberized or not. Boolean false MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of collection=field . String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP ( http ) or using SolrCloud ( cloud ) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE log * Configuration options for JanusGraph's logging system Name Description Datatype Default Value Mutability log.[X].backend Define the log backend to use. A reserved shortcut default can be used to use graph's storage backend to manage logs. A custom log implementation can be specified by providing full class path which implements org.janusgraph.diskstorage.log.LogManager and accepts a single parameter org.janusgraph.diskstorage.configuration.Configuration in the public constructor. String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not become visible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaning that all entries added to this log expire after the configured amount of time. Requires that the log implementation supports TTL. Duration (no default value) GLOBAL metrics Configuration options for metrics reporting Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE metrics.console Configuration options for metrics reporting to console Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE metrics.csv Configuration options for metrics reporting to CSV file Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.graphite Configuration options for metrics reporting through Graphite Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE metrics.jmx Configuration options for metrics reporting through JMX Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE metrics.slf4j Configuration options for metrics reporting through slf4j Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE query Configuration options for query processing Name Description Datatype Default Value Mutability query.batch Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. Boolean true MASKABLE query.batch-property-prefetch Whether to do a batched pre-fetch of all properties on adjacent vertices against the storage backend prior to evaluating a has condition against those vertices. Because these vertex properties will be loaded into the transaction-level cache of recently-used vertices when the condition is evaluated this can lead to significant performance improvement if there are many edges to adjacent vertices and there is a non-trivial latency to the backend. Boolean false MASKABLE query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequent property access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing so limits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.hard-max-limit If smart-limit is disabled and no limit is given in the query, query optimizer adds a limit in light of possibly large result sets. It works in the same way as smart-limit except that hard-max-limit is usually a large number. Default value is Integer.MAX_VALUE which effectively disables this behavior. This option does not take effect when smart-limit is enabled. Integer 2147483647 MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.index-select-strategy Name of the index selection strategy or full class name. Following shorthands can be used: - brute-force (Try all combinations of index candidates and pick up optimal one) - approximate (Use greedy algorithm to pick up approximately optimal index candidate) - threshold-based (Use index-select-threshold to pick up either approximate or threshold-based strategy on runtime) String threshold-based MASKABLE query.index-select-threshold Threshold of deciding whether to use brute force enumeration algorithm or fast approximation algorithm for selecting suitable indexes. Selecting optimal indexes for a query is a NP-complete set cover problem. When number of suitable index candidates is no larger than threshold, JanusGraph uses brute force search with exponential time complexity to ensure the best combination of indexes is selected. Only effective when threshold-based index select strategy is chosen. Integer 10 MASKABLE query.limit-batch-size Configure a maximum batch size for queries against the storage backend. This can be used to ensure responsiveness if batches tend to grow very large. The used batch size is equivalent to the barrier size of a preceding barrier() step. If a step has no preceding barrier(), the default barrier of TinkerPop will be inserted. This option only takes effect if query.batch is enabled. Boolean true MASKABLE query.optimizer-backend-access Whether the optimizer should be allowed to fire backend queries during the optimization phase. Allowing these will give the optimizer a chance to find more efficient execution plan but also increase the optimization overhead. Boolean true MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean false MASKABLE schema Schema related configuration options Name Description Datatype Default Value Mutability schema.constraints Configures the schema constraints to be used by this graph. If config 'schema.constraints' is set to 'true' and 'schema.default' is set to 'none', then an 'IllegalArgumentException' is thrown for schema constraint violations. If 'schema.constraints' is set to 'true' and 'schema.default' is not set 'none', schema constraints are automatically created as described in the config option 'schema.default'. If 'schema.constraints' is set to 'false' which is the default, then no schema constraints are applied. Boolean false GLOBAL_OFFLINE schema.default Configures the DefaultSchemaMaker to be used by this graph. Either one of the following shorthands can be used: - default (a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys), - tp3 (same as default, but has LIST property keys), - none (automatic schema creation is disabled) - ignore-prop (same as none, but simply ignore unknown properties rather than throw exceptions) - or to the full package and classname of a custom/third-party implementing the interface org.janusgraph.core.schema.DefaultSchemaMaker String default MASKABLE schema.logging Controls whether logging is enabled for schema makers. This only takes effect if you set schema.default to default or ignore-prop . For default schema maker, warning messages will be logged before schema types are created automatically. For ignore-prop schema maker, warning messages will be logged before unknown properties are ignored. Boolean false MASKABLE storage Configuration options for the storage backend. Some options are applicable only for certain backends. Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph. This is required. It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cql, hbase, inmemory, scylla) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers. This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to / . String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE storage.berkeleyje BerkeleyDB JE configuration options Name Description Datatype Default Value Mutability storage.berkeleyje.cache-mode Modes that can be specified for control over caching of records in the JE in-memory cache String DEFAULT MASKABLE storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE storage.berkeleyje.shared-cache If true, the shared cache is used for all graph instances Boolean true MASKABLE storage.cql CQL storage backend options Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.gc-grace-seconds The number of seconds before tombstones (deletion markers) are eligible for garbage-collection. Integer (no default value) FIXED storage.cql.heartbeat-interval The connection heartbeat interval in milliseconds. Long (no default value) MASKABLE storage.cql.heartbeat-timeout How long the driver waits for the response (in milliseconds) to a heartbeat. Long (no default value) MASKABLE storage.cql.keyspace The name of JanusGraph's keyspace. It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter. This value will be passed into CqlSessionBuilder.withLocalDatacenter. String datacenter1 MASKABLE storage.cql.local-max-connections-per-host The maximum number of connections that can be created per host for local datacenter Integer 1 MASKABLE storage.cql.max-requests-per-connection The maximum number of requests that can be executed concurrently on a connection. Integer 1024 MASKABLE storage.cql.metadata-schema-enabled Whether schema metadata is enabled. Boolean (no default value) MASKABLE storage.cql.metadata-token-map-enabled Whether token metadata is enabled. If disabled, partitioner-name must be provided. Boolean (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.partitioner-name The name of Cassandra cluster's partitioner. It will be retrieved by client if not provided. If provided, it must match the cluster's partitioner name. It can be the full class name such as org.apache.cassandra.dht.ByteOrderedPartitioner or the simple name such as ByteOrderedPartitioner String (no default value) MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database. If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.remote-max-connections-per-host The maximum number of connections that can be created per host for remote datacenter Integer 1 MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept. This options is used when storage.cql.replication-strategy-class is set to SimpleStrategy Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace. Available strategies: SimpleStrategy,NetworkTopologyStrategy. String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. This options is used when storage.cql.replication-strategy-class is set to NetworkTopologyStrategy. replication_factor can be used to specify a replication factor. String[] (no default value) FIXED storage.cql.request-timeout Timeout for CQL requests in milliseconds. See DataStax Java Driver option basic.request.timeout for more information. Long 12000 MASKABLE storage.cql.session-leak-threshold The maximum number of live sessions that are allowed to coexist in a given VM until the warning starts to log for every new session. If the value is less than or equal to 0, the feature is disabled: no warning will be issued. See DataStax Java Driver option advanced.session-leak.threshold for more information. Integer (no default value) MASKABLE storage.cql.session-name Default name for the Cassandra session String JanusGraph Session MASKABLE storage.cql.speculative-retry The speculative retry policy. One of: NONE, ALWAYS, percentile, ms. String (no default value) FIXED storage.cql.ttl-enabled Whether TTL should be enabled or not. Must be turned off if the storage does not support TTL. Amazon Keyspace, for example, does not support TTL by default unless otherwise enabled. Boolean true LOCAL storage.cql.use-external-locking True to prevent JanusGraph from using its own locking mechanism. Setting this to true eliminates redundant checks when using an external locking mechanism outside of JanusGraph. Be aware that when use-external-locking is set to true, that failure to employ a locking algorithm which locks all columns that participate in a transaction upfront and unlocks them when the transaction ends, will result in a 'read uncommitted' transaction isolation level guarantee. If set to true without an appropriate external locking mechanism in place side effects such as dirty/non-repeatable/phantom reads should be expected. Boolean false MASKABLE storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE storage.cql.executor-service Configuration options for CQL executor service which is used to process CQL queries. Name Description Datatype Default Value Mutability storage.cql.executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.cql.executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 10 LOCAL storage.cql.executor-service.enabled Whether to use CQL executor service to process queries or not. If not used, the parallelism will be controlled internally by the CQL driver via storage.cql.max-requests-per-connection parameter which may be preferable in production environments. Disabling executor service reduces overhead of thread pool but might be more difficult to tune. Boolean false LOCAL storage.cql.executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.cql.executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.cql.executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL storage.cql.internal Advanced configuration of internal DataStax driver. Notice, all available configurations will be composed in the order. Non specified configurations will be skipped. By default only base configuration is enabled (which has the smallest priority. It means that you can overwrite any configuration used in base programmatic configuration by using any other configuration type). The configurations are composed in the next order (sorted by priority in descending order): file-configuration , resource-configuration , string-configuration , url-configuration , base-programmatic-configuration (which is controlled by base-programmatic-configuration-enabled property). Configurations with higher priority always overwrite configurations with lower priority. I.e. if the same configuration parameter is used in both file-configuration and string-configuration the configuration parameter from file-configuration will be used and configuration parameter from string-configuration will be ignored. See available configuration options and configurations structure here: https://docs.datastax.com/en/developer/java-driver/4.13/manual/core/configuration/reference/ Name Description Datatype Default Value Mutability storage.cql.internal.base-programmatic-configuration-enabled Whether to use main programmatic configuration provided by JanusGraph properties or not. We don't recommend to disable this property unless you want to disable usage of all storage.cql properties and use default configurations or other configurations. If programmatic configuration options miss some important configuration options you can provide those missing configurations with other configuration types which will be applied with programmatic configuration (see other configuration types in this section). For most use cases this option should always be true . JanusGraph behaviour might be unpredictable when using unspecified configuration options. Boolean true MASKABLE storage.cql.internal.file-configuration Path to file with DataStax configuration. String (no default value) LOCAL storage.cql.internal.resource-configuration Classpath resource with DataStax configuration. String (no default value) MASKABLE storage.cql.internal.string-configuration String representing DataStax configuration. String (no default value) MASKABLE storage.cql.internal.url-configuration Url where to get DataStax configuration. String (no default value) MASKABLE storage.cql.metrics Configuration options for CQL metrics Name Description Datatype Default Value Mutability storage.cql.metrics.cql-messages-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-messages-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-messages-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.significant-digits Integer (no default value) LOCAL storage.cql.metrics.cql-requests-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-requests-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-requests-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.significant-digits Integer (no default value) LOCAL storage.cql.metrics.node-enabled Comma separated list of enabled node metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: pool.open-connections, pool.available-streams, bytes-sent). String[] (no default value) LOCAL storage.cql.metrics.node-expire-after The time after which the node level metrics will be evicted in milliseconds. Long (no default value) LOCAL storage.cql.metrics.session-enabled Comma separated list of enabled session metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: bytes-sent, bytes-received, connected-nodes). String[] (no default value) LOCAL storage.cql.metrics.throttling-delay-highest-latency The largest latency that we expect to record for throttling in milliseconds. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.highest-latency Long (no default value) LOCAL storage.cql.metrics.throttling-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for throttling. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.refresh-interval Long (no default value) LOCAL storage.cql.metrics.throttling-delay-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for throttling. This must be between 0 and 5. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.significant-digits Integer (no default value) LOCAL storage.cql.netty Configuration options related to the Netty event loop groups used internally by the CQL driver. Name Description Datatype Default Value Mutability storage.cql.netty.admin-size The number of threads for the event loop group used for admin tasks not related to request I/O (handle cluster events, refresh metadata, schedule reconnections, etc.). If this is not set, the driver will use 2. Integer (no default value) LOCAL storage.cql.netty.io-size The number of threads for the event loop group used for I/O operations (reading and writing to Cassandra nodes). If this is not set, the driver will use Runtime.getRuntime().availableProcessors() * 2 . Integer (no default value) LOCAL storage.cql.netty.timer-tick-duration The timer tick duration in milliseconds. This is how frequent the timer should wake up to check for timed-out tasks or speculative executions. See DataStax Java Driver option advanced.netty.timer.tick-duration for more information. Long (no default value) LOCAL storage.cql.netty.timer-ticks-per-wheel Number of ticks in a Timer wheel. See DataStax Java Driver option advanced.netty.timer.ticks-per-wheel for more information. Integer (no default value) LOCAL storage.cql.request-tracker Configuration options for CQL request tracker and builtin request logger Name Description Datatype Default Value Mutability storage.cql.request-tracker.class It is either a predefined DataStax driver value for a builtin request tracker or a full qualified class name which implements com.datastax.oss.driver.internal.core.tracker.RequestTracker interface. If no any value provided, the default DataStax request tracker is used, which is NoopRequestTracker which doesn't do anything. If RequestLogger value is provided, the DataStax RequestLogger is used. String (no default value) LOCAL storage.cql.request-tracker.logs-error-enabled Whether to log failed requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-max-query-length The maximum length of the query string in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-value-length The maximum length for bound values in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-values The maximum number of bound values to log. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-show-stack-traces Whether to log stack traces for failed queries. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-show-values Whether to log bound values in addition to the query string. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-enabled Whether to log slow requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-threshold The threshold to classify a successful request as slow . In milliseconds. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Long (no default value) LOCAL storage.cql.request-tracker.logs-success-enabled Whether to log successful requests. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.ssl Configuration options for SSL Name Description Datatype Default Value Mutability storage.cql.ssl.client-authentication-enabled Enables use of a client key to authenticate with Cassandra Boolean false LOCAL storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL storage.cql.ssl.hostname_validation Enable / disable SSL hostname validation. Boolean false LOCAL storage.cql.ssl.keystore Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability storage.cql.ssl.keystore.keypassword The password to access the key in SSL Keystore. String LOCAL storage.cql.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL storage.cql.ssl.keystore.storepassword The password to access the SSL Keystore. String LOCAL storage.cql.ssl.truststore Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL storage.hbase HBase storage options Name Description Datatype Default Value Mutability storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster. JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances. This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.snapshot-name The name of an existing HBase snapshot to be used by HBaseSnapshotInputFormat String janusgraph-snapshot LOCAL storage.hbase.snapshot-restore-dir The temporary directory to be used by HBaseSnapshotInputFormat to restore a snapshot. This directory should be on the same File System as the HBase root dir. String /tmp LOCAL storage.hbase.table The name of the table JanusGraph will use. When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL storage.lock Options for locking on eventually-consistent stores Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend. JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code. JanusGraph generates an appropriate default value for this option at startup. Overriding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Although this value is maskable, it is highly recommended to use the same value across JanusGraph instances in production environments. Duration 100 ms MASKABLE storage.meta * Meta data to include in storage backend retrievals Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps. If enabled, timestamp can be retrieved by element.value(ImplicitKey.TIMESTAMP.name()) or equivalently, element.value(\"~timestamp\") . Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL. If enabled, ttl can be retrieved by element.value(ImplicitKey.TTL.name()) or equivalently, element.value(\"~ttl\") . Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility. If enabled, visibility can be retrieved by element.value(ImplicitKey.VISIBILITY.name()) or equivalently, element.value(\"~visibility\") . Boolean true GLOBAL storage.parallel-backend-executor-service Configuration options for executor service which is used for parallel requests when storage.parallel-backend-ops is enabled. Name Description Datatype Default Value Mutability storage.parallel-backend-executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.parallel-backend-executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.parallel-backend-executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.parallel-backend-executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.parallel-backend-executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL tx Configuration options for transaction handling Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL tx.recovery Configuration options for transaction recovery processes Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE","title":"Janusgraph cfg"},{"location":"configs/janusgraph-cfg/#attributescustom","text":"Custom attribute serialization and handling Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE","title":"attributes.custom *"},{"location":"configs/janusgraph-cfg/#cache","text":"Configuration options that modify JanusGraph's caching behavior Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data. Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them. This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 MASKABLE cache.db-cache-size Size of JanusGraph's database level cache. Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 MASKABLE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE","title":"cache"},{"location":"configs/janusgraph-cfg/#cluster","text":"Configuration options for multi-machine deployments Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodes in the JanusGraph graph cluster. Must be greater than 1 and a power of 2. Integer 32 FIXED","title":"cluster"},{"location":"configs/janusgraph-cfg/#computer","text":"GraphComputer related configuration Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE","title":"computer"},{"location":"configs/janusgraph-cfg/#graph","text":"General configuration options Name Description Datatype Default Value Mutability graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL. These types are managed globally through the storage backend and cannot be overridden by changing the local configuration. This type of conflict usually indicates misconfiguration. When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option. When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.allow-upgrade Setting this to true will allow certain fixed values to be updated such as storage-version. This should only be used for upgrading. Boolean false MASKABLE graph.assign-timestamp Whether to use JanusGraph generated client-side timestamp in mutations if the backend supports it. When enabled, JanusGraph assigns one timestamp to all insertions and another slightly earlier timestamp to all deletions in the same batch. When this is disabled, mutation behavior depends on the backend. Some might use server-side timestamp (e.g. HBase) while others might use client-side timestamp generated by driver (CQL). Boolean true LOCAL graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagement APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anyway. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false FIXED graph.storage-version The version of JanusGraph storage schema with which this database was created. Automatically set on first start of graph. Should only ever be changed if upgrading to a new major release version of JanusGraph that contains schema changes String (no default value) FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored. An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance. This must be unique among all instances concurrently accessing the same stores or indexes. It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL","title":"graph"},{"location":"configs/janusgraph-cfg/#graphscript-eval","text":"Configuration options for gremlin script engine. Name Description Datatype Default Value Mutability graph.script-eval.enabled Whether to enable Gremlin script evaluation. If it is enabled, a gremlin script engine will be instantiated together with the JanusGraph instance, with which one can use eval method to evaluate a gremlin script in plain string format. This is usually only useful when JanusGraph is used as an embedded Java library. Boolean false MASKABLE graph.script-eval.engine Full class name of script engine that implements GremlinScriptEngine interface. Following shorthands can be used: - GremlinLangScriptEngine (A script engine that only accepts standard gremlin queries. Anything else including lambda function is not accepted. We recommend using this because it's generally safer, but it is not guaranteed that it has no security problem.)- GremlinGroovyScriptEngine (A script engine that accepts arbitrary groovy code. This can be dangerous and you should use it at your own risk. See https://tinkerpop.apache.org/docs/current/reference/#script-execution for potential security problems.) String GremlinLangScriptEngine MASKABLE","title":"graph.script-eval"},{"location":"configs/janusgraph-cfg/#gremlin","text":"Gremlin configuration options Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL","title":"gremlin"},{"location":"configs/janusgraph-cfg/#ids","text":"General configuration options for graph element IDs Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size. Setting this too low will make commits frequently block on slow reservation requests. Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation. When false, IDs are assigned only when the transaction commits. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE","title":"ids"},{"location":"configs/janusgraph-cfg/#idsauthority","text":"Configuration options for graph element ID reservation/allocation Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE","title":"ids.authority"},{"location":"configs/janusgraph-cfg/#index","text":"Configuration options for the individual indexing backends Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional. JanusGraph can use multiple heterogeneous index backends. Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers. This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that the indexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maximum number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE","title":"index *"},{"location":"configs/janusgraph-cfg/#indexxbkd-circle-processor","text":"Configuration for BKD circle processors which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.class Full class name of circle processor that implements CircleProcessor interface. The class is used for transformation of a Circle shape to another shape when BKD mapping is used. The provided implementation class should have either a public constructor which accepts configuration as a parameter ( org.janusgraph.diskstorage.configuration.Configuration ) or a public constructor with no parameters. Usually the transforming shape is a Polygon. Following shorthands can be used: - noTransformation Circle processor which is not transforming a circle, but instead keep the circle shape unchanged. This implementation may be useful in situations when the user wants to control circle transformation logic on ElasticSearch side instead of application side. For example, using ElasticSearch Circle Processor or any custom plugin. - fixedErrorDistance Circle processor which transforms the provided Circle into Polygon, Box, or Point depending on the configuration provided in index.bkd-circle-processor.fixed . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. - dynamicErrorDistance Circle processor which calculates error distance dynamically depending on the circle radius and the specified multiplier value. The error distance calculation formula is log(radius) * multiplier . Configuration for this class can be provided via index.bkd-circle-processor.dynamic . The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. String dynamicErrorDistance MASKABLE","title":"index.[X].bkd-circle-processor"},{"location":"configs/janusgraph-cfg/#indexxbkd-circle-processordynamic","text":"Configuration for Elasticsearch dynamic circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.dynamic.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.dynamic.error-distance-multiplier Multiplier variable for dynamic error distance calculation in the formula log(radius) * multiplier . Radius and error distance specified in meters. Double 2.0 MASKABLE","title":"index.[X].bkd-circle-processor.dynamic"},{"location":"configs/janusgraph-cfg/#indexxbkd-circle-processorfixed","text":"Configuration for Elasticsearch fixed circle processor which is used for BKD Geoshape mapping. Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.fixed.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case false is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.fixed.error-distance The difference between the resulting inscribed distance from center to side and the circle\u2019s radius. Specified in meters. Double 10.0 MASKABLE","title":"index.[X].bkd-circle-processor.fixed"},{"location":"configs/janusgraph-cfg/#indexxelasticsearch","text":"Elasticsearch index configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.client-keep-alive Set a keep-alive timeout (in milliseconds) Long (no default value) GLOBAL_OFFLINE index.[X].elasticsearch.connect-timeout Sets the maximum connection timeout (in milliseconds). Integer 1000 MASKABLE index.[X].elasticsearch.enable_index_names_cache Enables cache for generated index store names. It is recommended to always enable index store names cache unless you have more then 50000 indexes per index store. Boolean true MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status. This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.retry_on_conflict Specify how many times should the operation be retried when a conflict occurs. Integer 0 MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in seconds) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.setup-max-open-scroll-contexts Whether JanusGraph should setup max_open_scroll_context to maximum value for the cluster or not. Boolean true MASKABLE index.[X].elasticsearch.socket-timeout Sets the maximum socket timeout (in milliseconds). Integer 30000 MASKABLE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-mapping-for-es7 Mapping types are deprecated in ElasticSearch 7 and JanusGraph will not use mapping types by default for ElasticSearch 7 but if you want to preserve mapping types, you can setup this parameter to true. If you are updating ElasticSearch from 6 to 7 and you don't want to reindex your indexes, you may setup this parameter to true but we do recommend to reindex your indexes and don't use this parameter. Boolean false MASKABLE","title":"index.[X].elasticsearch"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchcreate","text":"Settings related to index creation Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index. This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE","title":"index.[X].elasticsearch.create"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchcreateext","text":"Overrides for arbitrary settings applied at index creation. See Elasticsearch , The full list of possible setting is available at Elasticsearch index settings . Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.ext.number_of_replicas The number of replicas each primary shard has Integer 1 MASKABLE index.[X].elasticsearch.create.ext.number_of_shards The number of primary shards that an index should have.Default value is 5 on ES 6 and 1 on ES 7 Integer (no default value) MASKABLE","title":"index.[X].elasticsearch.create.ext"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchhttpauth","text":"Configuration options for HTTP(S) authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.type Authentication type to be used for HTTP(S) access. Available options are NONE , BASIC and CUSTOM . String NONE LOCAL","title":"index.[X].elasticsearch.http.auth"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchhttpauthbasic","text":"Configuration options for HTTP(S) Basic authentication. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.basic.password Password for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.basic.realm Realm value for HTTP(S) authentication. If empty, any realm is accepted. String LOCAL index.[X].elasticsearch.http.auth.basic.username Username for HTTP(S) authentication. String LOCAL","title":"index.[X].elasticsearch.http.auth.basic"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchhttpauthcustom","text":"Configuration options for custom HTTP(S) authenticator. Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.custom.authenticator-args Comma-separated custom authenticator constructor arguments. String[] LOCAL index.[X].elasticsearch.http.auth.custom.authenticator-class Authenticator fully qualified class name. String LOCAL","title":"index.[X].elasticsearch.http.auth.custom"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchssl","text":"Elasticsearch SSL configuration Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.allow-self-signed-certificates Controls the accepting of the self-signed SSL certificates. Boolean false LOCAL index.[X].elasticsearch.ssl.disable-hostname-verification Disables the SSL hostname verification if set to true. Hostname verification is enabled by default. Boolean false LOCAL index.[X].elasticsearch.ssl.enabled Controls use of the SSL connection to Elasticsearch. Boolean false LOCAL","title":"index.[X].elasticsearch.ssl"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchsslkeystore","text":"Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.keystore.keypassword The password to access the key in the SSL Keystore. If the option is not present, the value of \"storepassword\" is used. String LOCAL index.[X].elasticsearch.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.keystore.storepassword The password to access SSL Keystore. String LOCAL","title":"index.[X].elasticsearch.ssl.keystore"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchssltruststore","text":"Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL index.[X].elasticsearch.ssl.truststore.password The password to access SSL Truststore. String LOCAL","title":"index.[X].elasticsearch.ssl.truststore"},{"location":"configs/janusgraph-cfg/#indexxsolr","text":"Solr index configuration Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be reused for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabled, the user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.kerberos-enabled Whether SOLR instance is Kerberized or not. Boolean false MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of collection=field . String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP ( http ) or using SolrCloud ( cloud ) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE","title":"index.[X].solr"},{"location":"configs/janusgraph-cfg/#log","text":"Configuration options for JanusGraph's logging system Name Description Datatype Default Value Mutability log.[X].backend Define the log backend to use. A reserved shortcut default can be used to use graph's storage backend to manage logs. A custom log implementation can be specified by providing full class path which implements org.janusgraph.diskstorage.log.LogManager and accepts a single parameter org.janusgraph.diskstorage.configuration.Configuration in the public constructor. String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not become visible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaning that all entries added to this log expire after the configured amount of time. Requires that the log implementation supports TTL. Duration (no default value) GLOBAL","title":"log *"},{"location":"configs/janusgraph-cfg/#metrics","text":"Configuration options for metrics reporting Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE","title":"metrics"},{"location":"configs/janusgraph-cfg/#metricsconsole","text":"Configuration options for metrics reporting to console Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE","title":"metrics.console"},{"location":"configs/janusgraph-cfg/#metricscsv","text":"Configuration options for metrics reporting to CSV file Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE","title":"metrics.csv"},{"location":"configs/janusgraph-cfg/#metricsgraphite","text":"Configuration options for metrics reporting through Graphite Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE","title":"metrics.graphite"},{"location":"configs/janusgraph-cfg/#metricsjmx","text":"Configuration options for metrics reporting through JMX Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE","title":"metrics.jmx"},{"location":"configs/janusgraph-cfg/#metricsslf4j","text":"Configuration options for metrics reporting through slf4j Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE","title":"metrics.slf4j"},{"location":"configs/janusgraph-cfg/#query","text":"Configuration options for query processing Name Description Datatype Default Value Mutability query.batch Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. Boolean true MASKABLE query.batch-property-prefetch Whether to do a batched pre-fetch of all properties on adjacent vertices against the storage backend prior to evaluating a has condition against those vertices. Because these vertex properties will be loaded into the transaction-level cache of recently-used vertices when the condition is evaluated this can lead to significant performance improvement if there are many edges to adjacent vertices and there is a non-trivial latency to the backend. Boolean false MASKABLE query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequent property access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing so limits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.hard-max-limit If smart-limit is disabled and no limit is given in the query, query optimizer adds a limit in light of possibly large result sets. It works in the same way as smart-limit except that hard-max-limit is usually a large number. Default value is Integer.MAX_VALUE which effectively disables this behavior. This option does not take effect when smart-limit is enabled. Integer 2147483647 MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.index-select-strategy Name of the index selection strategy or full class name. Following shorthands can be used: - brute-force (Try all combinations of index candidates and pick up optimal one) - approximate (Use greedy algorithm to pick up approximately optimal index candidate) - threshold-based (Use index-select-threshold to pick up either approximate or threshold-based strategy on runtime) String threshold-based MASKABLE query.index-select-threshold Threshold of deciding whether to use brute force enumeration algorithm or fast approximation algorithm for selecting suitable indexes. Selecting optimal indexes for a query is a NP-complete set cover problem. When number of suitable index candidates is no larger than threshold, JanusGraph uses brute force search with exponential time complexity to ensure the best combination of indexes is selected. Only effective when threshold-based index select strategy is chosen. Integer 10 MASKABLE query.limit-batch-size Configure a maximum batch size for queries against the storage backend. This can be used to ensure responsiveness if batches tend to grow very large. The used batch size is equivalent to the barrier size of a preceding barrier() step. If a step has no preceding barrier(), the default barrier of TinkerPop will be inserted. This option only takes effect if query.batch is enabled. Boolean true MASKABLE query.optimizer-backend-access Whether the optimizer should be allowed to fire backend queries during the optimization phase. Allowing these will give the optimizer a chance to find more efficient execution plan but also increase the optimization overhead. Boolean true MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean false MASKABLE","title":"query"},{"location":"configs/janusgraph-cfg/#schema","text":"Schema related configuration options Name Description Datatype Default Value Mutability schema.constraints Configures the schema constraints to be used by this graph. If config 'schema.constraints' is set to 'true' and 'schema.default' is set to 'none', then an 'IllegalArgumentException' is thrown for schema constraint violations. If 'schema.constraints' is set to 'true' and 'schema.default' is not set 'none', schema constraints are automatically created as described in the config option 'schema.default'. If 'schema.constraints' is set to 'false' which is the default, then no schema constraints are applied. Boolean false GLOBAL_OFFLINE schema.default Configures the DefaultSchemaMaker to be used by this graph. Either one of the following shorthands can be used: - default (a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys), - tp3 (same as default, but has LIST property keys), - none (automatic schema creation is disabled) - ignore-prop (same as none, but simply ignore unknown properties rather than throw exceptions) - or to the full package and classname of a custom/third-party implementing the interface org.janusgraph.core.schema.DefaultSchemaMaker String default MASKABLE schema.logging Controls whether logging is enabled for schema makers. This only takes effect if you set schema.default to default or ignore-prop . For default schema maker, warning messages will be logged before schema types are created automatically. For ignore-prop schema maker, warning messages will be logged before unknown properties are ignored. Boolean false MASKABLE","title":"schema"},{"location":"configs/janusgraph-cfg/#storage","text":"Configuration options for the storage backend. Some options are applicable only for certain backends. Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph. This is required. It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cql, hbase, inmemory, scylla) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers. This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to / . String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE","title":"storage"},{"location":"configs/janusgraph-cfg/#storageberkeleyje","text":"BerkeleyDB JE configuration options Name Description Datatype Default Value Mutability storage.berkeleyje.cache-mode Modes that can be specified for control over caching of records in the JE in-memory cache String DEFAULT MASKABLE storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE storage.berkeleyje.shared-cache If true, the shared cache is used for all graph instances Boolean true MASKABLE","title":"storage.berkeleyje"},{"location":"configs/janusgraph-cfg/#storagecql","text":"CQL storage backend options Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.gc-grace-seconds The number of seconds before tombstones (deletion markers) are eligible for garbage-collection. Integer (no default value) FIXED storage.cql.heartbeat-interval The connection heartbeat interval in milliseconds. Long (no default value) MASKABLE storage.cql.heartbeat-timeout How long the driver waits for the response (in milliseconds) to a heartbeat. Long (no default value) MASKABLE storage.cql.keyspace The name of JanusGraph's keyspace. It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter. This value will be passed into CqlSessionBuilder.withLocalDatacenter. String datacenter1 MASKABLE storage.cql.local-max-connections-per-host The maximum number of connections that can be created per host for local datacenter Integer 1 MASKABLE storage.cql.max-requests-per-connection The maximum number of requests that can be executed concurrently on a connection. Integer 1024 MASKABLE storage.cql.metadata-schema-enabled Whether schema metadata is enabled. Boolean (no default value) MASKABLE storage.cql.metadata-token-map-enabled Whether token metadata is enabled. If disabled, partitioner-name must be provided. Boolean (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.partitioner-name The name of Cassandra cluster's partitioner. It will be retrieved by client if not provided. If provided, it must match the cluster's partitioner name. It can be the full class name such as org.apache.cassandra.dht.ByteOrderedPartitioner or the simple name such as ByteOrderedPartitioner String (no default value) MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database. If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.remote-max-connections-per-host The maximum number of connections that can be created per host for remote datacenter Integer 1 MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept. This options is used when storage.cql.replication-strategy-class is set to SimpleStrategy Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace. Available strategies: SimpleStrategy,NetworkTopologyStrategy. String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter. This list is interpreted as a map. It must have an even number of elements in [key,val,key,val,...] form. This options is used when storage.cql.replication-strategy-class is set to NetworkTopologyStrategy. replication_factor can be used to specify a replication factor. String[] (no default value) FIXED storage.cql.request-timeout Timeout for CQL requests in milliseconds. See DataStax Java Driver option basic.request.timeout for more information. Long 12000 MASKABLE storage.cql.session-leak-threshold The maximum number of live sessions that are allowed to coexist in a given VM until the warning starts to log for every new session. If the value is less than or equal to 0, the feature is disabled: no warning will be issued. See DataStax Java Driver option advanced.session-leak.threshold for more information. Integer (no default value) MASKABLE storage.cql.session-name Default name for the Cassandra session String JanusGraph Session MASKABLE storage.cql.speculative-retry The speculative retry policy. One of: NONE, ALWAYS, percentile, ms. String (no default value) FIXED storage.cql.ttl-enabled Whether TTL should be enabled or not. Must be turned off if the storage does not support TTL. Amazon Keyspace, for example, does not support TTL by default unless otherwise enabled. Boolean true LOCAL storage.cql.use-external-locking True to prevent JanusGraph from using its own locking mechanism. Setting this to true eliminates redundant checks when using an external locking mechanism outside of JanusGraph. Be aware that when use-external-locking is set to true, that failure to employ a locking algorithm which locks all columns that participate in a transaction upfront and unlocks them when the transaction ends, will result in a 'read uncommitted' transaction isolation level guarantee. If set to true without an appropriate external locking mechanism in place side effects such as dirty/non-repeatable/phantom reads should be expected. Boolean false MASKABLE storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE","title":"storage.cql"},{"location":"configs/janusgraph-cfg/#storagecqlexecutor-service","text":"Configuration options for CQL executor service which is used to process CQL queries. Name Description Datatype Default Value Mutability storage.cql.executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.cql.executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 10 LOCAL storage.cql.executor-service.enabled Whether to use CQL executor service to process queries or not. If not used, the parallelism will be controlled internally by the CQL driver via storage.cql.max-requests-per-connection parameter which may be preferable in production environments. Disabling executor service reduces overhead of thread pool but might be more difficult to tune. Boolean false LOCAL storage.cql.executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.cql.executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.cql.executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL","title":"storage.cql.executor-service"},{"location":"configs/janusgraph-cfg/#storagecqlinternal","text":"Advanced configuration of internal DataStax driver. Notice, all available configurations will be composed in the order. Non specified configurations will be skipped. By default only base configuration is enabled (which has the smallest priority. It means that you can overwrite any configuration used in base programmatic configuration by using any other configuration type). The configurations are composed in the next order (sorted by priority in descending order): file-configuration , resource-configuration , string-configuration , url-configuration , base-programmatic-configuration (which is controlled by base-programmatic-configuration-enabled property). Configurations with higher priority always overwrite configurations with lower priority. I.e. if the same configuration parameter is used in both file-configuration and string-configuration the configuration parameter from file-configuration will be used and configuration parameter from string-configuration will be ignored. See available configuration options and configurations structure here: https://docs.datastax.com/en/developer/java-driver/4.13/manual/core/configuration/reference/ Name Description Datatype Default Value Mutability storage.cql.internal.base-programmatic-configuration-enabled Whether to use main programmatic configuration provided by JanusGraph properties or not. We don't recommend to disable this property unless you want to disable usage of all storage.cql properties and use default configurations or other configurations. If programmatic configuration options miss some important configuration options you can provide those missing configurations with other configuration types which will be applied with programmatic configuration (see other configuration types in this section). For most use cases this option should always be true . JanusGraph behaviour might be unpredictable when using unspecified configuration options. Boolean true MASKABLE storage.cql.internal.file-configuration Path to file with DataStax configuration. String (no default value) LOCAL storage.cql.internal.resource-configuration Classpath resource with DataStax configuration. String (no default value) MASKABLE storage.cql.internal.string-configuration String representing DataStax configuration. String (no default value) MASKABLE storage.cql.internal.url-configuration Url where to get DataStax configuration. String (no default value) MASKABLE","title":"storage.cql.internal"},{"location":"configs/janusgraph-cfg/#storagecqlmetrics","text":"Configuration options for CQL metrics Name Description Datatype Default Value Mutability storage.cql.metrics.cql-messages-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-messages-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-messages-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option advanced.metrics.node.cql-messages.significant-digits Integer (no default value) LOCAL storage.cql.metrics.cql-requests-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.highest-latency Long (no default value) LOCAL storage.cql.metrics.cql-requests-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.refresh-interval Long (no default value) LOCAL storage.cql.metrics.cql-requests-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.cql-requests.significant-digits Integer (no default value) LOCAL storage.cql.metrics.node-enabled Comma separated list of enabled node metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: pool.open-connections, pool.available-streams, bytes-sent). String[] (no default value) LOCAL storage.cql.metrics.node-expire-after The time after which the node level metrics will be evicted in milliseconds. Long (no default value) LOCAL storage.cql.metrics.session-enabled Comma separated list of enabled session metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: bytes-sent, bytes-received, connected-nodes). String[] (no default value) LOCAL storage.cql.metrics.throttling-delay-highest-latency The largest latency that we expect to record for throttling in milliseconds. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.highest-latency Long (no default value) LOCAL storage.cql.metrics.throttling-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for throttling. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.refresh-interval Long (no default value) LOCAL storage.cql.metrics.throttling-delay-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for throttling. This must be between 0 and 5. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option advanced.metrics.session.throttling.delay.significant-digits Integer (no default value) LOCAL","title":"storage.cql.metrics"},{"location":"configs/janusgraph-cfg/#storagecqlnetty","text":"Configuration options related to the Netty event loop groups used internally by the CQL driver. Name Description Datatype Default Value Mutability storage.cql.netty.admin-size The number of threads for the event loop group used for admin tasks not related to request I/O (handle cluster events, refresh metadata, schedule reconnections, etc.). If this is not set, the driver will use 2. Integer (no default value) LOCAL storage.cql.netty.io-size The number of threads for the event loop group used for I/O operations (reading and writing to Cassandra nodes). If this is not set, the driver will use Runtime.getRuntime().availableProcessors() * 2 . Integer (no default value) LOCAL storage.cql.netty.timer-tick-duration The timer tick duration in milliseconds. This is how frequent the timer should wake up to check for timed-out tasks or speculative executions. See DataStax Java Driver option advanced.netty.timer.tick-duration for more information. Long (no default value) LOCAL storage.cql.netty.timer-ticks-per-wheel Number of ticks in a Timer wheel. See DataStax Java Driver option advanced.netty.timer.ticks-per-wheel for more information. Integer (no default value) LOCAL","title":"storage.cql.netty"},{"location":"configs/janusgraph-cfg/#storagecqlrequest-tracker","text":"Configuration options for CQL request tracker and builtin request logger Name Description Datatype Default Value Mutability storage.cql.request-tracker.class It is either a predefined DataStax driver value for a builtin request tracker or a full qualified class name which implements com.datastax.oss.driver.internal.core.tracker.RequestTracker interface. If no any value provided, the default DataStax request tracker is used, which is NoopRequestTracker which doesn't do anything. If RequestLogger value is provided, the DataStax RequestLogger is used. String (no default value) LOCAL storage.cql.request-tracker.logs-error-enabled Whether to log failed requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-max-query-length The maximum length of the query string in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-value-length The maximum length for bound values in the log message. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-values The maximum number of bound values to log. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Integer (no default value) LOCAL storage.cql.request-tracker.logs-show-stack-traces Whether to log stack traces for failed queries. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-show-values Whether to log bound values in addition to the query string. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-enabled Whether to log slow requests.Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-threshold The threshold to classify a successful request as slow . In milliseconds. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Long (no default value) LOCAL storage.cql.request-tracker.logs-success-enabled Whether to log successful requests. Can be used when root.storage.cql.request-tracker.class is set to RequestLogger . Boolean (no default value) LOCAL","title":"storage.cql.request-tracker"},{"location":"configs/janusgraph-cfg/#storagecqlssl","text":"Configuration options for SSL Name Description Datatype Default Value Mutability storage.cql.ssl.client-authentication-enabled Enables use of a client key to authenticate with Cassandra Boolean false LOCAL storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL storage.cql.ssl.hostname_validation Enable / disable SSL hostname validation. Boolean false LOCAL","title":"storage.cql.ssl"},{"location":"configs/janusgraph-cfg/#storagecqlsslkeystore","text":"Configuration options for SSL Keystore. Name Description Datatype Default Value Mutability storage.cql.ssl.keystore.keypassword The password to access the key in SSL Keystore. String LOCAL storage.cql.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL storage.cql.ssl.keystore.storepassword The password to access the SSL Keystore. String LOCAL","title":"storage.cql.ssl.keystore"},{"location":"configs/janusgraph-cfg/#storagecqlssltruststore","text":"Configuration options for SSL Truststore. Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL","title":"storage.cql.ssl.truststore"},{"location":"configs/janusgraph-cfg/#storagehbase","text":"HBase storage options Name Description Datatype Default Value Mutability storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster. JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances. This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.snapshot-name The name of an existing HBase snapshot to be used by HBaseSnapshotInputFormat String janusgraph-snapshot LOCAL storage.hbase.snapshot-restore-dir The temporary directory to be used by HBaseSnapshotInputFormat to restore a snapshot. This directory should be on the same File System as the HBase root dir. String /tmp LOCAL storage.hbase.table The name of the table JanusGraph will use. When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL","title":"storage.hbase"},{"location":"configs/janusgraph-cfg/#storagelock","text":"Options for locking on eventually-consistent stores Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend. JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code. JanusGraph generates an appropriate default value for this option at startup. Overriding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Although this value is maskable, it is highly recommended to use the same value across JanusGraph instances in production environments. Duration 100 ms MASKABLE","title":"storage.lock"},{"location":"configs/janusgraph-cfg/#storagemeta","text":"Meta data to include in storage backend retrievals Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps. If enabled, timestamp can be retrieved by element.value(ImplicitKey.TIMESTAMP.name()) or equivalently, element.value(\"~timestamp\") . Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL. If enabled, ttl can be retrieved by element.value(ImplicitKey.TTL.name()) or equivalently, element.value(\"~ttl\") . Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility. If enabled, visibility can be retrieved by element.value(ImplicitKey.VISIBILITY.name()) or equivalently, element.value(\"~visibility\") . Boolean true GLOBAL","title":"storage.meta *"},{"location":"configs/janusgraph-cfg/#storageparallel-backend-executor-service","text":"Configuration options for executor service which is used for parallel requests when storage.parallel-backend-ops is enabled. Name Description Datatype Default Value Mutability storage.parallel-backend-executor-service.class The implementation of ExecutorService to use. The full name of the class which extends ExecutorService which has either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor. Other accepted options are: fixed - fixed thread pool of size core-pool-size ; cached - cached thread pool; String fixed LOCAL storage.parallel-backend-executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.parallel-backend-executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the core-pool-size , this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for fixed executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.parallel-backend-executor-service.max-pool-size Maximum pool size for executor service. Ignored for fixed and cached executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.parallel-backend-executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL","title":"storage.parallel-backend-executor-service"},{"location":"configs/janusgraph-cfg/#tx","text":"Configuration options for transaction handling Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL","title":"tx"},{"location":"configs/janusgraph-cfg/#txrecovery","text":"Configuration options for transaction recovery processes Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE","title":"tx.recovery"},{"location":"getting-started/architecture/","text":"JanusGraph is a graph database engine. JanusGraph itself is focused on compact graph serialization, rich graph data modeling, and efficient query execution. In addition, JanusGraph utilizes Hadoop for graph analytics and batch graph processing. JanusGraph implements robust, modular interfaces for data persistence, data indexing, and client access. JanusGraph\u2019s modular architecture allows it to interoperate with a wide range of storage, index, and client technologies; it also eases the process of extending JanusGraph to support new ones. Between JanusGraph and the disks sits one or more storage and indexing adapters. JanusGraph comes standard with the following adapters, but JanusGraph\u2019s modular architecture supports third-party adapters. Data storage: Apache Cassandra Apache HBase Oracle Berkeley DB Java Edition Indices, which speed up and enable more complex queries: Elasticsearch Apache Solr Apache Lucene Broadly speaking, applications can interact with JanusGraph in two ways: Embed JanusGraph inside the application executing Gremlin queries directly against the graph within the same JVM. Query execution, JanusGraph\u2019s caches, and transaction handling all happen in the same JVM as the application while data retrieval from the storage backend may be local or remote. Interact with a local or remote JanusGraph instance by submitting Gremlin queries to the server. JanusGraph natively supports the Gremlin Server component of the Apache TinkerPop stack.","title":"Architectural Overview"},{"location":"getting-started/basic-usage/","text":"Basic Usage This section offers a very short introduction to Gremlin's feature set. For a closer look at the topic, refer to Gremlin Query Language . The examples in this section make extensive use of a toy graph distributed with JanusGraph called The Graph of the Gods . This graph is diagrammed below. The abstract data model is known as a Property Graph Model and this particular instance describes the relationships between the beings and places of the Roman pantheon. Moreover, special text and symbol modifiers in the diagram (e.g. bold, underline, etc.) denote different schematics/typings in the graph. visual symbol meaning bold key a graph indexed key bold key with star a graph indexed key that must have a unique value underlined key a vertex-centric indexed key hollow-head edge a functional/unique edge (no duplicates) tail-crossed edge a unidirectional edge (can only traverse in one direction) Loading the Graph of the Gods Into JanusGraph The example below will open a JanusGraph graph instance and load The Graph of the Gods dataset diagrammed above. JanusGraphFactory provides a set of static open methods, each of which takes a configuration as its argument and returns a graph instance. This tutorial demonstrates loading The Graph of the Gods using the helper class GraphOfTheGodsFactory with different configurations. This section skips over the configuration details, but additional information about storage backends, index backends, and their configuration are available in Storage Backends , Index Backends , and Configuration Reference . Loading with an index backend The below example calls one of these open methods on a configuration that uses the BerkeleyDB storage backend and the Elasticsearch index backend: gremlin > graph = JanusGraphFactory . open ( 'conf/janusgraph-berkeleyje-es.properties' ) ==> standardjanusgraph [ berkeleyje: .. /db/ berkeley ] gremlin > GraphOfTheGodsFactory . load ( graph ) ==> null gremlin > g = graph . traversal () ==> graphtraversalsource [ standardjanusgraph [ berkeleyje: .. /db/ berkeley ], standard ] The JanusGraphFactory.open() and GraphOfTheGodsFactory.load() methods do the following to the newly constructed graph prior to returning it: Creates a collection of global and vertex-centric indices on the graph. Adds all the vertices to the graph along with their properties. Adds all the edges to the graph along with their properties. Please see the GraphOfTheGodsFactory source code for details. For those using JanusGraph/Cassandra (or JanusGraph/HBase), be sure to make use of conf/janusgraph-cql-es.properties (or conf/janusgraph-hbase-es.properties ) and GraphOfTheGodsFactory.load() . gremlin > graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > GraphOfTheGodsFactory . load ( graph ) ==> null gremlin > g = graph . traversal () ==> graphtraversalsource [ standardjanusgraph [ cql: [ 127.0 . 0.1 ]], standard ] Loading without an index backend You may also use the conf/janusgraph-cql.properties , conf/janusgraph-berkeleyje.properties , conf/janusgraph-hbase.properties , or conf/janusgraph-inmemory.properties configuration files to open a graph without an indexing backend configured. In such cases, you will need to use the GraphOfTheGodsFactory.loadWithoutMixedIndex() method to load the Graph of the Gods so that it doesn\u2019t attempt to make use of an indexing backend. gremlin > graph = JanusGraphFactory . open ( 'conf/janusgraph-cql.properties' ) ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > GraphOfTheGodsFactory . loadWithoutMixedIndex ( graph , true ) ==> null gremlin > g = graph . traversal () ==> graphtraversalsource [ standardjanusgraph [ cql: [ 127.0 . 0.1 ]], standard ] Info Using any configuration file other than conf/janusgraph-inmemory.properties requires that you have a dedicated backend configured and running. If you just want to quickly open a graph instance and explore some of the JanusGraph features, you could simply choose conf/janusgraph-inmemory.properties to open an in-memory backend . Global Graph Indices The typical pattern for accessing data in a graph database is to first locate the entry point into the graph using a graph index. That entry point is an element (or set of elements)\u2009\u2014\u2009i.e. a vertex or edge. From the entry elements, a Gremlin path description describes how to traverse to other elements in the graph via the explicit graph structure. Given that there is a unique index on name property, the Saturn vertex can be retrieved. The property map (i.e. the key/value pairs of Saturn) can then be examined. As demonstrated, the Saturn vertex has a name of \"saturn, \" an age of 10000, and a type of \"titan.\" The grandchild of Saturn can be retrieved with a traversal that expresses: \"Who is Saturn\u2019s grandchild?\" (the inverse of \"father\" is \"child\"). The result is Hercules. gremlin > saturn = g . V (). has ( 'name' , 'saturn' ). next () ==> v [ 256 ] gremlin > g . V ( saturn ). valueMap () ==>[ name: [ saturn ], age: [ 10000 ]] gremlin > g . V ( saturn ). in ( 'father' ). in ( 'father' ). values ( 'name' ) ==> hercules The property place is also in a graph index. The property place is an edge property. Therefore, JanusGraph can index edges in a graph index. It is possible to query The Graph of the Gods for all events that have happened within 50 kilometers of Athens (latitude:37.97 and long:23.72). Then, given that information, which vertices were involved in those events. gremlin > g . E (). has ( 'place' , geoWithin ( Geoshape . circle ( 37.97 , 23.72 , 50 ))) ==> e [ a9x - co8 - 9 hx - 39 s ][ 16424 - battled -> 4240 ] ==> e [ 9 vp - co8 - 9 hx - 9 ns ][ 16424 - battled -> 12520 ] gremlin > g . E (). has ( 'place' , geoWithin ( Geoshape . circle ( 37.97 , 23.72 , 50 ))). as ( 'source' ). inV (). as ( 'god2' ). select ( 'source' ). outV (). as ( 'god1' ). select ( 'god1' , 'god2' ). by ( 'name' ) ==>[ god1: hercules , god2: hydra ] ==>[ god1: hercules , god2: nemean ] Graph indices are one type of index structure in JanusGraph. Graph indices are automatically chosen by JanusGraph to answer which ask for all vertices ( g.V ) or all edges ( g.E ) that satisfy one or multiple constraints (e.g. has or interval ). The second aspect of indexing in JanusGraph is known as vertex-centric indices. Vertex-centric indices are utilized to speed up traversals inside the graph. Vertex-centric indices are described later. Graph Traversal Examples Hercules , son of Jupiter and Alcmene , bore super human strength. Hercules was a Demigod because his father was a god and his mother was a human. Juno , wife of Jupiter, was furious with Jupiter\u2019s infidelity. In revenge, she blinded Hercules with temporary insanity and caused him to kill his wife and children. To atone for the slaying, Hercules was ordered by the Oracle of Delphi to serve Eurystheus . Eurystheus appointed Hercules to 12 labors. In the previous section, it was demonstrated that Saturn\u2019s grandchild was Hercules. This can be expressed using a loop . In essence, Hercules is the vertex that is 2-steps away from Saturn along the in('father') path. gremlin > hercules = g . V ( saturn ). repeat ( __ . in ( 'father' )). times ( 2 ). next () ==> v [ 1536 ] Hercules is a demigod. To prove that Hercules is half human and half god, his parent\u2019s origins must be examined. It is possible to traverse from the Hercules vertex to his mother and father. Finally, it is possible to determine the type of each of them\u2009\u2014\u2009yielding \"god\" and \"human.\" gremlin > g . V ( hercules ). out ( 'father' , 'mother' ) ==> v [ 1024 ] ==> v [ 1792 ] gremlin > g . V ( hercules ). out ( 'father' , 'mother' ). values ( 'name' ) ==> jupiter ==> alcmene gremlin > g . V ( hercules ). out ( 'father' , 'mother' ). label () ==> god ==> human gremlin > hercules . label () ==> demigod The examples thus far have been with respect to the genetic lines of the various actors in the Roman pantheon. The Property Graph Model is expressive enough to represent multiple types of things and relationships. In this way, The Graph of the Gods also identifies Hercules' various heroic exploits --- his famous 12 labors. In the previous section, it was discovered that Hercules was involved in two battles near Athens. It is possible to explore these events by traversing battled edges out of the Hercules vertex. gremlin > g . V ( hercules ). out ( 'battled' ) ==> v [ 2304 ] ==> v [ 2560 ] ==> v [ 2816 ] gremlin > g . V ( hercules ). out ( 'battled' ). valueMap () ==>[ name: [ nemean ]] ==>[ name: [ hydra ]] ==>[ name: [ cerberus ]] gremlin > g . V ( hercules ). outE ( 'battled' ). has ( 'time' , gt ( 1 )). inV (). values ( 'name' ) ==> cerberus ==> hydra The edge property time on battled edges is indexed by the vertex-centric indices of a vertex. Retrieving battled edges incident to Hercules according to a constraint/filter on time is faster than doing a linear scan of all edges and filtering (typically O(log n) , where n is the number incident edges). JanusGraph is intelligent enough to use vertex-centric indices when available. A toString() of a Gremlin expression shows a decomposition into individual steps. gremlin > g . V ( hercules ). outE ( 'battled' ). has ( 'time' , gt ( 1 )). inV (). values ( 'name' ). toString () ==>[ GraphStep ([ v [ 24744 ]], vertex ), VertexStep ( OUT ,[ battled ], edge ), HasStep ([ time . gt ( 1 )]), EdgeVertexStep ( IN ), PropertiesStep ([ name ], value )] More Complex Graph Traversal Examples In the depths of Tartarus lives Pluto. His relationship with Hercules was strained by the fact that Hercules battled his pet, Cerberus. However, Hercules is his nephew\u2009\u2014\u2009how should he make Hercules pay for his insolence? The Gremlin traversals below provide more examples over The Graph of the Gods . The explanation of each traversal is provided in the prior line as a // comment. Cohabitors of Tartarus gremlin > pluto = g . V (). has ( 'name' , 'pluto' ). next () ==> v [ 2048 ] gremlin > // who are pluto's cohabitants? gremlin > g . V ( pluto ). out ( 'lives' ). in ( 'lives' ). values ( 'name' ) ==> pluto ==> cerberus gremlin > // pluto can't be his own cohabitant gremlin > g . V ( pluto ). out ( 'lives' ). in ( 'lives' ). where ( is ( neq ( pluto ))). values ( 'name' ) ==> cerberus gremlin > g . V ( pluto ). as ( 'x' ). out ( 'lives' ). in ( 'lives' ). where ( neq ( 'x' )). values ( 'name' ) ==> cerberus Pluto\u2019s Brothers gremlin > // where do pluto's brothers live? gremlin > g . V ( pluto ). out ( 'brother' ). out ( 'lives' ). values ( 'name' ) ==> sky ==> sea gremlin > // which brother lives in which place? gremlin > g . V ( pluto ). out ( 'brother' ). as ( 'god' ). out ( 'lives' ). as ( 'place' ). select ( 'god' , 'place' ) ==>[ god: v [ 1024 ], place: v [ 512 ]] ==>[ god: v [ 1280 ], place: v [ 768 ]] gremlin > // what is the name of the brother and the name of the place? gremlin > g . V ( pluto ). out ( 'brother' ). as ( 'god' ). out ( 'lives' ). as ( 'place' ). select ( 'god' , 'place' ). by ( 'name' ) ==>[ god: jupiter , place: sky ] ==>[ god: neptune , place: sea ] Finally, Pluto lives in Tartarus because he shows no concern for death. His brothers, on the other hand, chose their locations based upon their love for certain qualities of those locations.! gremlin > g . V ( pluto ). outE ( 'lives' ). values ( 'reason' ) ==> no fear of death gremlin > g . E (). has ( 'reason' , textContains ( 'loves' )) ==> e [ 6 xs - sg - m51 - e8 ][ 1024 - lives -> 512 ] ==> e [ 70 g - zk - m51 - lc ][ 1280 - lives -> 768 ] gremlin > g . E (). has ( 'reason' , textContains ( 'loves' )). as ( 'source' ). values ( 'reason' ). as ( 'reason' ). select ( 'source' ). outV (). values ( 'name' ). as ( 'god' ). select ( 'source' ). inV (). values ( 'name' ). as ( 'thing' ). select ( 'god' , 'reason' , 'thing' ) ==>[ god: neptune , reason: loves waves , thing: sea ] ==>[ god: jupiter , reason: loves fresh breezes , thing: sky ]","title":"Basic Usage"},{"location":"getting-started/basic-usage/#basic-usage","text":"This section offers a very short introduction to Gremlin's feature set. For a closer look at the topic, refer to Gremlin Query Language . The examples in this section make extensive use of a toy graph distributed with JanusGraph called The Graph of the Gods . This graph is diagrammed below. The abstract data model is known as a Property Graph Model and this particular instance describes the relationships between the beings and places of the Roman pantheon. Moreover, special text and symbol modifiers in the diagram (e.g. bold, underline, etc.) denote different schematics/typings in the graph. visual symbol meaning bold key a graph indexed key bold key with star a graph indexed key that must have a unique value underlined key a vertex-centric indexed key hollow-head edge a functional/unique edge (no duplicates) tail-crossed edge a unidirectional edge (can only traverse in one direction)","title":"Basic Usage"},{"location":"getting-started/basic-usage/#loading-the-graph-of-the-gods-into-janusgraph","text":"The example below will open a JanusGraph graph instance and load The Graph of the Gods dataset diagrammed above. JanusGraphFactory provides a set of static open methods, each of which takes a configuration as its argument and returns a graph instance. This tutorial demonstrates loading The Graph of the Gods using the helper class GraphOfTheGodsFactory with different configurations. This section skips over the configuration details, but additional information about storage backends, index backends, and their configuration are available in Storage Backends , Index Backends , and Configuration Reference .","title":"Loading the Graph of the Gods Into JanusGraph"},{"location":"getting-started/basic-usage/#loading-with-an-index-backend","text":"The below example calls one of these open methods on a configuration that uses the BerkeleyDB storage backend and the Elasticsearch index backend: gremlin > graph = JanusGraphFactory . open ( 'conf/janusgraph-berkeleyje-es.properties' ) ==> standardjanusgraph [ berkeleyje: .. /db/ berkeley ] gremlin > GraphOfTheGodsFactory . load ( graph ) ==> null gremlin > g = graph . traversal () ==> graphtraversalsource [ standardjanusgraph [ berkeleyje: .. /db/ berkeley ], standard ] The JanusGraphFactory.open() and GraphOfTheGodsFactory.load() methods do the following to the newly constructed graph prior to returning it: Creates a collection of global and vertex-centric indices on the graph. Adds all the vertices to the graph along with their properties. Adds all the edges to the graph along with their properties. Please see the GraphOfTheGodsFactory source code for details. For those using JanusGraph/Cassandra (or JanusGraph/HBase), be sure to make use of conf/janusgraph-cql-es.properties (or conf/janusgraph-hbase-es.properties ) and GraphOfTheGodsFactory.load() . gremlin > graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > GraphOfTheGodsFactory . load ( graph ) ==> null gremlin > g = graph . traversal () ==> graphtraversalsource [ standardjanusgraph [ cql: [ 127.0 . 0.1 ]], standard ]","title":"Loading with an index backend"},{"location":"getting-started/basic-usage/#loading-without-an-index-backend","text":"You may also use the conf/janusgraph-cql.properties , conf/janusgraph-berkeleyje.properties , conf/janusgraph-hbase.properties , or conf/janusgraph-inmemory.properties configuration files to open a graph without an indexing backend configured. In such cases, you will need to use the GraphOfTheGodsFactory.loadWithoutMixedIndex() method to load the Graph of the Gods so that it doesn\u2019t attempt to make use of an indexing backend. gremlin > graph = JanusGraphFactory . open ( 'conf/janusgraph-cql.properties' ) ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > GraphOfTheGodsFactory . loadWithoutMixedIndex ( graph , true ) ==> null gremlin > g = graph . traversal () ==> graphtraversalsource [ standardjanusgraph [ cql: [ 127.0 . 0.1 ]], standard ] Info Using any configuration file other than conf/janusgraph-inmemory.properties requires that you have a dedicated backend configured and running. If you just want to quickly open a graph instance and explore some of the JanusGraph features, you could simply choose conf/janusgraph-inmemory.properties to open an in-memory backend .","title":"Loading without an index backend"},{"location":"getting-started/basic-usage/#global-graph-indices","text":"The typical pattern for accessing data in a graph database is to first locate the entry point into the graph using a graph index. That entry point is an element (or set of elements)\u2009\u2014\u2009i.e. a vertex or edge. From the entry elements, a Gremlin path description describes how to traverse to other elements in the graph via the explicit graph structure. Given that there is a unique index on name property, the Saturn vertex can be retrieved. The property map (i.e. the key/value pairs of Saturn) can then be examined. As demonstrated, the Saturn vertex has a name of \"saturn, \" an age of 10000, and a type of \"titan.\" The grandchild of Saturn can be retrieved with a traversal that expresses: \"Who is Saturn\u2019s grandchild?\" (the inverse of \"father\" is \"child\"). The result is Hercules. gremlin > saturn = g . V (). has ( 'name' , 'saturn' ). next () ==> v [ 256 ] gremlin > g . V ( saturn ). valueMap () ==>[ name: [ saturn ], age: [ 10000 ]] gremlin > g . V ( saturn ). in ( 'father' ). in ( 'father' ). values ( 'name' ) ==> hercules The property place is also in a graph index. The property place is an edge property. Therefore, JanusGraph can index edges in a graph index. It is possible to query The Graph of the Gods for all events that have happened within 50 kilometers of Athens (latitude:37.97 and long:23.72). Then, given that information, which vertices were involved in those events. gremlin > g . E (). has ( 'place' , geoWithin ( Geoshape . circle ( 37.97 , 23.72 , 50 ))) ==> e [ a9x - co8 - 9 hx - 39 s ][ 16424 - battled -> 4240 ] ==> e [ 9 vp - co8 - 9 hx - 9 ns ][ 16424 - battled -> 12520 ] gremlin > g . E (). has ( 'place' , geoWithin ( Geoshape . circle ( 37.97 , 23.72 , 50 ))). as ( 'source' ). inV (). as ( 'god2' ). select ( 'source' ). outV (). as ( 'god1' ). select ( 'god1' , 'god2' ). by ( 'name' ) ==>[ god1: hercules , god2: hydra ] ==>[ god1: hercules , god2: nemean ] Graph indices are one type of index structure in JanusGraph. Graph indices are automatically chosen by JanusGraph to answer which ask for all vertices ( g.V ) or all edges ( g.E ) that satisfy one or multiple constraints (e.g. has or interval ). The second aspect of indexing in JanusGraph is known as vertex-centric indices. Vertex-centric indices are utilized to speed up traversals inside the graph. Vertex-centric indices are described later.","title":"Global Graph Indices"},{"location":"getting-started/basic-usage/#graph-traversal-examples","text":"Hercules , son of Jupiter and Alcmene , bore super human strength. Hercules was a Demigod because his father was a god and his mother was a human. Juno , wife of Jupiter, was furious with Jupiter\u2019s infidelity. In revenge, she blinded Hercules with temporary insanity and caused him to kill his wife and children. To atone for the slaying, Hercules was ordered by the Oracle of Delphi to serve Eurystheus . Eurystheus appointed Hercules to 12 labors. In the previous section, it was demonstrated that Saturn\u2019s grandchild was Hercules. This can be expressed using a loop . In essence, Hercules is the vertex that is 2-steps away from Saturn along the in('father') path. gremlin > hercules = g . V ( saturn ). repeat ( __ . in ( 'father' )). times ( 2 ). next () ==> v [ 1536 ] Hercules is a demigod. To prove that Hercules is half human and half god, his parent\u2019s origins must be examined. It is possible to traverse from the Hercules vertex to his mother and father. Finally, it is possible to determine the type of each of them\u2009\u2014\u2009yielding \"god\" and \"human.\" gremlin > g . V ( hercules ). out ( 'father' , 'mother' ) ==> v [ 1024 ] ==> v [ 1792 ] gremlin > g . V ( hercules ). out ( 'father' , 'mother' ). values ( 'name' ) ==> jupiter ==> alcmene gremlin > g . V ( hercules ). out ( 'father' , 'mother' ). label () ==> god ==> human gremlin > hercules . label () ==> demigod The examples thus far have been with respect to the genetic lines of the various actors in the Roman pantheon. The Property Graph Model is expressive enough to represent multiple types of things and relationships. In this way, The Graph of the Gods also identifies Hercules' various heroic exploits --- his famous 12 labors. In the previous section, it was discovered that Hercules was involved in two battles near Athens. It is possible to explore these events by traversing battled edges out of the Hercules vertex. gremlin > g . V ( hercules ). out ( 'battled' ) ==> v [ 2304 ] ==> v [ 2560 ] ==> v [ 2816 ] gremlin > g . V ( hercules ). out ( 'battled' ). valueMap () ==>[ name: [ nemean ]] ==>[ name: [ hydra ]] ==>[ name: [ cerberus ]] gremlin > g . V ( hercules ). outE ( 'battled' ). has ( 'time' , gt ( 1 )). inV (). values ( 'name' ) ==> cerberus ==> hydra The edge property time on battled edges is indexed by the vertex-centric indices of a vertex. Retrieving battled edges incident to Hercules according to a constraint/filter on time is faster than doing a linear scan of all edges and filtering (typically O(log n) , where n is the number incident edges). JanusGraph is intelligent enough to use vertex-centric indices when available. A toString() of a Gremlin expression shows a decomposition into individual steps. gremlin > g . V ( hercules ). outE ( 'battled' ). has ( 'time' , gt ( 1 )). inV (). values ( 'name' ). toString () ==>[ GraphStep ([ v [ 24744 ]], vertex ), VertexStep ( OUT ,[ battled ], edge ), HasStep ([ time . gt ( 1 )]), EdgeVertexStep ( IN ), PropertiesStep ([ name ], value )]","title":"Graph Traversal Examples"},{"location":"getting-started/basic-usage/#more-complex-graph-traversal-examples","text":"In the depths of Tartarus lives Pluto. His relationship with Hercules was strained by the fact that Hercules battled his pet, Cerberus. However, Hercules is his nephew\u2009\u2014\u2009how should he make Hercules pay for his insolence? The Gremlin traversals below provide more examples over The Graph of the Gods . The explanation of each traversal is provided in the prior line as a // comment.","title":"More Complex Graph Traversal Examples"},{"location":"getting-started/basic-usage/#cohabitors-of-tartarus","text":"gremlin > pluto = g . V (). has ( 'name' , 'pluto' ). next () ==> v [ 2048 ] gremlin > // who are pluto's cohabitants? gremlin > g . V ( pluto ). out ( 'lives' ). in ( 'lives' ). values ( 'name' ) ==> pluto ==> cerberus gremlin > // pluto can't be his own cohabitant gremlin > g . V ( pluto ). out ( 'lives' ). in ( 'lives' ). where ( is ( neq ( pluto ))). values ( 'name' ) ==> cerberus gremlin > g . V ( pluto ). as ( 'x' ). out ( 'lives' ). in ( 'lives' ). where ( neq ( 'x' )). values ( 'name' ) ==> cerberus","title":"Cohabitors of Tartarus"},{"location":"getting-started/basic-usage/#plutos-brothers","text":"gremlin > // where do pluto's brothers live? gremlin > g . V ( pluto ). out ( 'brother' ). out ( 'lives' ). values ( 'name' ) ==> sky ==> sea gremlin > // which brother lives in which place? gremlin > g . V ( pluto ). out ( 'brother' ). as ( 'god' ). out ( 'lives' ). as ( 'place' ). select ( 'god' , 'place' ) ==>[ god: v [ 1024 ], place: v [ 512 ]] ==>[ god: v [ 1280 ], place: v [ 768 ]] gremlin > // what is the name of the brother and the name of the place? gremlin > g . V ( pluto ). out ( 'brother' ). as ( 'god' ). out ( 'lives' ). as ( 'place' ). select ( 'god' , 'place' ). by ( 'name' ) ==>[ god: jupiter , place: sky ] ==>[ god: neptune , place: sea ] Finally, Pluto lives in Tartarus because he shows no concern for death. His brothers, on the other hand, chose their locations based upon their love for certain qualities of those locations.! gremlin > g . V ( pluto ). outE ( 'lives' ). values ( 'reason' ) ==> no fear of death gremlin > g . E (). has ( 'reason' , textContains ( 'loves' )) ==> e [ 6 xs - sg - m51 - e8 ][ 1024 - lives -> 512 ] ==> e [ 70 g - zk - m51 - lc ][ 1280 - lives -> 768 ] gremlin > g . E (). has ( 'reason' , textContains ( 'loves' )). as ( 'source' ). values ( 'reason' ). as ( 'reason' ). select ( 'source' ). outV (). values ( 'name' ). as ( 'god' ). select ( 'source' ). inV (). values ( 'name' ). as ( 'thing' ). select ( 'god' , 'reason' , 'thing' ) ==>[ god: neptune , reason: loves waves , thing: sea ] ==>[ god: jupiter , reason: loves fresh breezes , thing: sky ]","title":"Pluto\u2019s Brothers"},{"location":"getting-started/gremlin/","text":"Gremlin Query Language Gremlin is JanusGraph\u2019s query language used to retrieve data from and modify data in the graph. Gremlin is a path-oriented language which succinctly expresses complex graph traversals and mutation operations. Gremlin is a functional language whereby traversal operators are chained together to form path-like expressions. For example, \"from Hercules, traverse to his father and then his father\u2019s father and return the grandfather\u2019s name.\" Gremlin is a component of Apache TinkerPop . It is developed independently from JanusGraph and is supported by most graph databases. By building applications on top of JanusGraph through the Gremlin query language, users avoid vendor-lock in because their application can be migrated to other graph databases supporting Gremlin. This section is a brief overview of the Gremlin query language. For more information on Gremlin, refer to the following resources: Practical Gremlin : An online book by Kelvin R. Lawrence providing an in-depth overview of Gremlin and it's interaction with JanusGraph. Complete Gremlin Manual : Reference manual for all of the Gremlin steps. Gremlin Console Tutorial : Learn how to use the Gremlin Console effectively to traverse and analyze a graph interactively. Gremlin Recipes : A collection of best practices and common traversal patterns for Gremlin. Gremlin Language Drivers : Connect to a Gremlin Server with different programming languages, including Go, JavaScript, .NET/C#, PHP, Python, Ruby, Scala, and TypeScript. Gremlin Language Variants : Learn how to embed Gremlin in a host programming language. Gremlin for SQL developers : Learn Gremlin using typical patterns found when querying data with SQL. In addition to these resources, Connecting to JanusGraph explains how Gremlin can be used in different programming languages to query a JanusGraph Server. Introductory Traversals A Gremlin query is a chain of operations/functions that are evaluated from left to right. A simple grandfather query is provided below over the Graph of the Gods dataset discussed in Getting Started . gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ). values ( 'name' ) ==> saturn The query above can be read: g : for the current graph traversal. V : for all vertices in the graph has('name', 'hercules') : filters the vertices down to those with name property \"hercules\" (there is only one). out('father') : traverse outgoing father edge\u2019s from Hercules. \u2018out('father')`: traverse outgoing father edge\u2019s from Hercules\u2019 father\u2019s vertex (i.e. Jupiter). name : get the name property of the \"hercules\" vertex\u2019s grandfather. Taken together, these steps form a path-like traversal query. Each step can be decomposed and its results demonstrated. This style of building up a traversal/query is useful when constructing larger, complex query chains. gremlin > g ==> graphtraversalsource [ janusgraph [ cql: 127.0 . 0.1 ], standard ] gremlin > g . V (). has ( 'name' , 'hercules' ) ==> v [ 24 ] gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ) ==> v [ 16 ] gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ) ==> v [ 20 ] gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ). values ( 'name' ) ==> saturn For a sanity check, it is usually good to look at the properties of each return, not the assigned long id. gremlin > g . V (). has ( 'name' , 'hercules' ). values ( 'name' ) ==> hercules gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). values ( 'name' ) ==> jupiter gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ). values ( 'name' ) ==> saturn Note the related traversal that shows the entire father family tree branch of Hercules. This more complicated traversal is provided in order to demonstrate the flexibility and expressivity of the language. A competent grasp of Gremlin provides the JanusGraph user the ability to fluently navigate the underlying graph structure. gremlin > g . V (). has ( 'name' , 'hercules' ). repeat ( out ( 'father' )). emit (). values ( 'name' ) ==> jupiter ==> saturn Some more traversal examples are provided below. gremlin > hercules = g . V (). has ( 'name' , 'hercules' ). next () ==> v [ 1536 ] gremlin > g . V ( hercules ). out ( 'father' , 'mother' ). label () ==> god ==> human gremlin > g . V ( hercules ). out ( 'battled' ). label () ==> monster ==> monster ==> monster gremlin > g . V ( hercules ). out ( 'battled' ). valueMap () ==>{ name = nemean } ==>{ name = hydra } ==>{ name = cerberus } Each step (denoted by a separating .) is a function that operates on the objects emitted from the previous step. There are numerous steps in the Gremlin language (see Gremlin Steps ). By simply changing a step or order of the steps, different traversal semantics are enacted. The example below returns the name of all the people that have battled the same monsters as Hercules who themselves are not Hercules (i.e. \"co-battlers\" or perhaps, \"allies\"). Given that The Graph of the Gods only has one battler (Hercules), another battler (for the sake of example) is added to the graph with Gremlin showcasing how vertices and edges are added to the graph. gremlin > theseus = graph . addVertex ( 'human' ) ==> v [ 3328 ] gremlin > theseus . property ( 'name' , 'theseus' ) ==> null gremlin > cerberus = g . V (). has ( 'name' , 'cerberus' ). next () ==> v [ 2816 ] gremlin > battle = theseus . addEdge ( 'battled' , cerberus , 'time' , 22 ) ==> e [ 7 eo - 2 kg - iz9 - 268 ][ 3328 - battled -> 2816 ] gremlin > battle . values ( 'time' ) ==> 22 When adding a vertex, an optional vertex label can be provided. An edge label must be specified when adding edges. Properties as key-value pairs can be set on both vertices and edges. When a property key is defined with SET or LIST cardinality, addProperty must be used when adding a respective property to a vertex. gremlin > g . V ( hercules ). as ( 'h' ). out ( 'battled' ). in ( 'battled' ). where ( neq ( 'h' )). values ( 'name' ) ==> theseus The example above has 4 chained functions: out , in , except , and values (i.e. name is shorthand for values('name') ). The function signatures of each are itemized below, where V is vertex and U is any object, where V is a subset of U . out: V -> V in: V -> V except: U -> U values: V -> U When chaining together functions, the incoming type must match the outgoing type, where U matches anything. Thus, the \"co-battled/ally\" traversal above is correct. Note The Gremlin overview presented in this section focused on the Gremlin-Groovy language implementation used in the Gremlin Console. Refer to Connecting to JanusGraph for information about connecting to JanusGraph with other languages than Groovy and independent of the Gremlin Console. Iterating the Traversal One convenient feature of the Gremlin Console is that it automatically iterates all results from a query executed from the gremlin> prompt. This works well within the REPL environment as it shows you the results as a String. As you transition towards writing a Gremlin application, it is important to understand how to iterate a traversal explicitly because your application\u2019s traversals will not iterate automatically. These are some of the common ways to iterate the Traversal : iterate() - Zero results are expected or can be ignored. next() - Get one result. Make sure to check hasNext() first. next(int n) - Get the next n results. Make sure to check hasNext() first. toList() - Get all results as a list. If there are no results, an empty list is returned. A Java code example is shown below to demonstrate these concepts: Traversal t = g . V (). has ( \"name\" , \"pluto\" ); // Define a traversal // Note the traversal is not executed/iterated yet Vertex pluto = null ; if ( t . hasNext ()) { // Check if results are available pluto = g . V (). has ( \"name\" , \"pluto\" ). next (); // Get one result g . V ( pluto ). drop (). iterate (); // Execute a traversal to drop pluto from graph } // Note the traversal can be cloned for reuse Traversal tt = t . asAdmin (). clone (); if ( tt . hasNext ()) { System . err . println ( \"pluto was not dropped!\" ); } List < Vertex > gods = g . V (). hasLabel ( \"god\" ). toList (); // Find all the gods","title":"Gremlin Query Language"},{"location":"getting-started/gremlin/#gremlin-query-language","text":"Gremlin is JanusGraph\u2019s query language used to retrieve data from and modify data in the graph. Gremlin is a path-oriented language which succinctly expresses complex graph traversals and mutation operations. Gremlin is a functional language whereby traversal operators are chained together to form path-like expressions. For example, \"from Hercules, traverse to his father and then his father\u2019s father and return the grandfather\u2019s name.\" Gremlin is a component of Apache TinkerPop . It is developed independently from JanusGraph and is supported by most graph databases. By building applications on top of JanusGraph through the Gremlin query language, users avoid vendor-lock in because their application can be migrated to other graph databases supporting Gremlin. This section is a brief overview of the Gremlin query language. For more information on Gremlin, refer to the following resources: Practical Gremlin : An online book by Kelvin R. Lawrence providing an in-depth overview of Gremlin and it's interaction with JanusGraph. Complete Gremlin Manual : Reference manual for all of the Gremlin steps. Gremlin Console Tutorial : Learn how to use the Gremlin Console effectively to traverse and analyze a graph interactively. Gremlin Recipes : A collection of best practices and common traversal patterns for Gremlin. Gremlin Language Drivers : Connect to a Gremlin Server with different programming languages, including Go, JavaScript, .NET/C#, PHP, Python, Ruby, Scala, and TypeScript. Gremlin Language Variants : Learn how to embed Gremlin in a host programming language. Gremlin for SQL developers : Learn Gremlin using typical patterns found when querying data with SQL. In addition to these resources, Connecting to JanusGraph explains how Gremlin can be used in different programming languages to query a JanusGraph Server.","title":"Gremlin Query Language"},{"location":"getting-started/gremlin/#introductory-traversals","text":"A Gremlin query is a chain of operations/functions that are evaluated from left to right. A simple grandfather query is provided below over the Graph of the Gods dataset discussed in Getting Started . gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ). values ( 'name' ) ==> saturn The query above can be read: g : for the current graph traversal. V : for all vertices in the graph has('name', 'hercules') : filters the vertices down to those with name property \"hercules\" (there is only one). out('father') : traverse outgoing father edge\u2019s from Hercules. \u2018out('father')`: traverse outgoing father edge\u2019s from Hercules\u2019 father\u2019s vertex (i.e. Jupiter). name : get the name property of the \"hercules\" vertex\u2019s grandfather. Taken together, these steps form a path-like traversal query. Each step can be decomposed and its results demonstrated. This style of building up a traversal/query is useful when constructing larger, complex query chains. gremlin > g ==> graphtraversalsource [ janusgraph [ cql: 127.0 . 0.1 ], standard ] gremlin > g . V (). has ( 'name' , 'hercules' ) ==> v [ 24 ] gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ) ==> v [ 16 ] gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ) ==> v [ 20 ] gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ). values ( 'name' ) ==> saturn For a sanity check, it is usually good to look at the properties of each return, not the assigned long id. gremlin > g . V (). has ( 'name' , 'hercules' ). values ( 'name' ) ==> hercules gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). values ( 'name' ) ==> jupiter gremlin > g . V (). has ( 'name' , 'hercules' ). out ( 'father' ). out ( 'father' ). values ( 'name' ) ==> saturn Note the related traversal that shows the entire father family tree branch of Hercules. This more complicated traversal is provided in order to demonstrate the flexibility and expressivity of the language. A competent grasp of Gremlin provides the JanusGraph user the ability to fluently navigate the underlying graph structure. gremlin > g . V (). has ( 'name' , 'hercules' ). repeat ( out ( 'father' )). emit (). values ( 'name' ) ==> jupiter ==> saturn Some more traversal examples are provided below. gremlin > hercules = g . V (). has ( 'name' , 'hercules' ). next () ==> v [ 1536 ] gremlin > g . V ( hercules ). out ( 'father' , 'mother' ). label () ==> god ==> human gremlin > g . V ( hercules ). out ( 'battled' ). label () ==> monster ==> monster ==> monster gremlin > g . V ( hercules ). out ( 'battled' ). valueMap () ==>{ name = nemean } ==>{ name = hydra } ==>{ name = cerberus } Each step (denoted by a separating .) is a function that operates on the objects emitted from the previous step. There are numerous steps in the Gremlin language (see Gremlin Steps ). By simply changing a step or order of the steps, different traversal semantics are enacted. The example below returns the name of all the people that have battled the same monsters as Hercules who themselves are not Hercules (i.e. \"co-battlers\" or perhaps, \"allies\"). Given that The Graph of the Gods only has one battler (Hercules), another battler (for the sake of example) is added to the graph with Gremlin showcasing how vertices and edges are added to the graph. gremlin > theseus = graph . addVertex ( 'human' ) ==> v [ 3328 ] gremlin > theseus . property ( 'name' , 'theseus' ) ==> null gremlin > cerberus = g . V (). has ( 'name' , 'cerberus' ). next () ==> v [ 2816 ] gremlin > battle = theseus . addEdge ( 'battled' , cerberus , 'time' , 22 ) ==> e [ 7 eo - 2 kg - iz9 - 268 ][ 3328 - battled -> 2816 ] gremlin > battle . values ( 'time' ) ==> 22 When adding a vertex, an optional vertex label can be provided. An edge label must be specified when adding edges. Properties as key-value pairs can be set on both vertices and edges. When a property key is defined with SET or LIST cardinality, addProperty must be used when adding a respective property to a vertex. gremlin > g . V ( hercules ). as ( 'h' ). out ( 'battled' ). in ( 'battled' ). where ( neq ( 'h' )). values ( 'name' ) ==> theseus The example above has 4 chained functions: out , in , except , and values (i.e. name is shorthand for values('name') ). The function signatures of each are itemized below, where V is vertex and U is any object, where V is a subset of U . out: V -> V in: V -> V except: U -> U values: V -> U When chaining together functions, the incoming type must match the outgoing type, where U matches anything. Thus, the \"co-battled/ally\" traversal above is correct. Note The Gremlin overview presented in this section focused on the Gremlin-Groovy language implementation used in the Gremlin Console. Refer to Connecting to JanusGraph for information about connecting to JanusGraph with other languages than Groovy and independent of the Gremlin Console.","title":"Introductory Traversals"},{"location":"getting-started/gremlin/#iterating-the-traversal","text":"One convenient feature of the Gremlin Console is that it automatically iterates all results from a query executed from the gremlin> prompt. This works well within the REPL environment as it shows you the results as a String. As you transition towards writing a Gremlin application, it is important to understand how to iterate a traversal explicitly because your application\u2019s traversals will not iterate automatically. These are some of the common ways to iterate the Traversal : iterate() - Zero results are expected or can be ignored. next() - Get one result. Make sure to check hasNext() first. next(int n) - Get the next n results. Make sure to check hasNext() first. toList() - Get all results as a list. If there are no results, an empty list is returned. A Java code example is shown below to demonstrate these concepts: Traversal t = g . V (). has ( \"name\" , \"pluto\" ); // Define a traversal // Note the traversal is not executed/iterated yet Vertex pluto = null ; if ( t . hasNext ()) { // Check if results are available pluto = g . V (). has ( \"name\" , \"pluto\" ). next (); // Get one result g . V ( pluto ). drop (). iterate (); // Execute a traversal to drop pluto from graph } // Note the traversal can be cloned for reuse Traversal tt = t . asAdmin (). clone (); if ( tt . hasNext ()) { System . err . println ( \"pluto was not dropped!\" ); } List < Vertex > gods = g . V (). hasLabel ( \"god\" ). toList (); // Find all the gods","title":"Iterating the Traversal"},{"location":"getting-started/installation/","text":"Installation Running JanusGraph inside a Docker container For virtualization and easy access, JanusGraph provides a Docker image . Docker makes it easier to run servers and clients on a single machine without dealing with multiple installations. For instructions on installing and using Docker, please refer to the docker guide . With JanusGraph Server running in one container, we can either run the client in another container, or on our bare-metal machine. In the examples below, we use Gremlin Console as the client. Running Gremlin Console inside another Docker container Let's start by running a simple JanusGraph instance in Docker: $ docker run --name janusgraph-default janusgraph/janusgraph:latest The above command launches a JanusGraph Server instance. We name the container so that we can link a second container to it. The server may need a few seconds to start up so be patient and wait for the corresponding log messages to appear. Example log SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/opt/janusgraph/lib/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/janusgraph/lib/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 0 [main] INFO com.jcabi.manifests.Manifests - 110 attributes loaded from 283 stream(s) in 130ms, 110 saved, 3770 ignored: [\"Agent-Class\", \"Ant-Version\", \"Archiver-Version\", \"Automatic-Module-Name\", \"Bnd-LastModified\", \"Boot-Class-Path\", \"Branch\", \"Build-Date\", \"Build-Host\", \"Build-Id\", \"Build-Java-Version\", \"Build-Jdk\", \"Build-Job\", \"Build-Number\", \"Build-Timestamp\", \"Build-Version\", \"Built-At\", \"Built-By\", \"Built-Date\", \"Built-OS\", \"Built-On\", \"Built-Status\", \"Bundle-ActivationPolicy\", \"Bundle-Activator\", \"Bundle-BuddyPolicy\", \"Bundle-Category\", \"Bundle-ClassPath\", \"Bundle-ContactAddress\", \"Bundle-Description\", \"Bundle-DocURL\", \"Bundle-License\", \"Bundle-ManifestVersion\", \"Bundle-Name\", \"Bundle-NativeCode\", \"Bundle-RequiredExecutionEnvironment\", \"Bundle-SymbolicName\", \"Bundle-Vendor\", \"Bundle-Version\", \"Can-Redefine-Classes\", \"Change\", \"Class-Path\", \"Created-By\", \"DSTAMP\", \"DynamicImport-Package\", \"Eclipse-BuddyPolicy\", \"Eclipse-ExtensibleAPI\", \"Embed-Dependency\", \"Embed-Transitive\", \"Export-Package\", \"Extension-Name\", \"Extension-name\", \"Fragment-Host\", \"Gradle-Version\", \"Gremlin-Lib-Paths\", \"Gremlin-Plugin-Dependencies\", \"Gremlin-Plugin-Paths\", \"Ignore-Package\", \"Implementation-Build\", \"Implementation-Build-Date\", \"Implementation-Title\", \"Implementation-URL\", \"Implementation-Vendor\", \"Implementation-Vendor-Id\", \"Implementation-Version\", \"Import-Package\", \"Include-Resource\", \"JCabi-Build\", \"JCabi-Date\", \"JCabi-Version\", \"Java-Vendor\", \"Java-Version\", \"Main-Class\", \"Manifest-Version\", \"Maven-Version\", \"Module-Email\", \"Module-Origin\", \"Module-Owner\", \"Module-Source\", \"Originally-Created-By\", \"Os-Arch\", \"Os-Name\", \"Os-Version\", \"Package\", \"Premain-Class\", \"Private-Package\", \"Provide-Capability\", \"Require-Bundle\", \"Require-Capability\", \"Scm-Connection\", \"Scm-Revision\", \"Scm-Url\", \"Specification-Title\", \"Specification-Vendor\", \"Specification-Version\", \"TODAY\", \"TSTAMP\", \"Time-Zone-Database-Version\", \"Tool\", \"X-Compile-Elasticsearch-Snapshot\", \"X-Compile-Elasticsearch-Version\", \"X-Compile-Lucene-Version\", \"X-Compile-Source-JDK\", \"X-Compile-Target-JDK\", \"hash\", \"implementation-version\", \"mode\", \"package\", \"service\", \"url\", \"version\"] 1 [main] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - 3.4.1 \\,,,/ (o o) -----oOOo-(3)-oOOo----- 100 [main] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Configuring Gremlin Server from /etc/opt/janusgraph/gremlin-server.yaml ... ... 3965 [gremlin-server-boss-1] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Gremlin Server configured with worker thread pool of 1, gremlin pool of 8 and boss thread pool of 1. 3965 [gremlin-server-boss-1] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Channel started at port 8182. We can now instruct Docker to start a second container for the client and link it to the already running server. Here we use Gremlin Console ( gremlin.sh ) as the client. $ docker run --rm --link janusgraph-default:janusgraph -e GREMLIN_REMOTE_HOSTS = janusgraph \\ -it janusgraph/janusgraph:latest ./bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- gremlin> :remote connect tinkerpop.server conf/remote.yaml == >Configured janusgraph/172.17.0.2:8182 Warning The above command only establishes the connection to the server. It does not forward the following commands to the server by default! As a result, further commands will still be executed locally unless preceeded by :> . To forward every command to the remote server, use the :remote console command. Further documentation can be found in the TinkerPop reference docs Running Gremlin Console on Bare-metal We can also run the Gremlin Console on our bare-metal machine. To make the server able to communicate with the gremlin console, we need to expose the 8182 port when launching JanusGraph Server: $ docker run -it -p 8182 :8182 janusgraph/janusgraph We can now start a Gremlin Console and try to connect to the server. We need to download the released zip archive and launch the gremlin console (see local installation section for details): $ bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- gremlin> :remote connect tinkerpop.server conf/remote.yaml == >Configured localhost/127.0.0.1:8182 For further reading, see the JanusGraph Server section as well as the JanusGraph Docker documentation . Local Installation In order to run JanusGraph, Java 8 SE is required. Make sure the $JAVA_HOME environment variable points to the correct location where either JRE or JDK is installed. JanusGraph can be downloaded as a .zip archive from the Releases section of the project repository. $ unzip janusgraph-1.0.0-rc2.zip Archive: janusgraph-1.0.0-rc2.zip creating: janusgraph-1.0.0-rc2/ ... Once you have unzipped the downloaded archive, you are ready to go. Running the Gremlin Console The Gremlin Console is an interactive shell that gives you access to the data managed by JanusGraph. You can reach it by running the gremlin.sh script which is located in the project's bin directory. $ cd janusgraph-1.0.0-rc2 $ bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- 09 :12:24 INFO org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph - HADOOP_GREMLIN_LIBS is set to: /usr/local/janusgraph/lib plugin activated: tinkerpop.hadoop plugin activated: janusgraph.imports gremlin> The Gremlin Console interprets commands using Apache Groovy , which is a superset of Java. Gremlin-Groovy extends Groovy by providing a set of methods for basic and advanced graph traversal functionality. For a deeper dive into Gremlin language's features, please refer to our introduction to Gremlin . Running the JanusGraph Server In most real-world use cases, queries to a database will not be run from the exact same server the data is stored on. Instead, there will be some sort of client-server hierarchy in which the server runs the database and handles requests while multiple clients create these requests and thereby read and write entries within the database independently of one another. This behavior can also be achieved with JanusGraph. In order to start a server on your local machine, simply run the janusgraph-server.sh script instead of the gremlin.sh script. You can optionally pass a configuration file as a parameter. The default configuration is located at conf/gremlin-server/gremlin-server.yaml . $ ./bin/janusgraph-server.sh start or $ ./bin/janusgraph-server.sh console ./conf/gremlin-server/gremlin-server- [ ... ] .yaml Info The default configuration ( gremlin-server.yaml ) uses it's own inmemory backend instead of a dedicated database server. No search backend is used by default, so mixed indices aren't supported as search backend isn't specified (Make sure you are using GraphOfTheGodsFactory.loadWithoutMixedIndex(graph, true) instead of GraphOfTheGodsFactory.load(graph) if you follow Basic Usage example ). For further information about storage backends, visit the corresponding section of the documentation. A Gremlin server is now running on your local machine and waiting for clients to connect on the default port 8182 . To instantiate a client -- as done before -- run the gremlin.sh script. Again, a local Gremlin Console will show up. This time, instead of using it locally, we will connect the Gremlin Console to a remote server and redirect all of it's queries to this server. This is done by using the :remote command: gremlin> :remote connect tinkerpop.server conf/remote.yaml == >Configured localhost/127.0.0.1:8182 As you can probably tell from the log, the client and server are running on the same machine in this case. When using a different setup, all you have to do is modify the parameters in the conf/remote.yaml file. Using the Pre-Packaged Distribution Note Starting with 0.5.1, this requires to download janusgraph-full-1.0.0-rc2.zip instead of the default janusgraph-1.0.0-rc2.zip . The JanusGraph release comes pre-configured to run JanusGraph Server out of the box leveraging a sample Cassandra and Elasticsearch configuration to allow users to get started quickly with JanusGraph Server. This configuration defaults to client applications that can connect to JanusGraph Server via WebSocket with a custom subprotocol. There are a number of clients developed in different languages to help support the subprotocol. The most familiar client to use the WebSocket interface is the Gremlin Console. The quick-start bundle is not intended to be representative of a production installation, but does provide a way to perform development with JanusGraph Server, run tests and see how the components are wired together. To use this default configuration: Download a copy of the current janusgraph-full-$VERSION.zip file from the Releases page Unzip it and enter the janusgraph--full-$VERSION directory Run bin/janusgraph.sh start . This step will start Gremlin Server with Cassandra/ES forked into a separate process. Note for security reasons Elasticsearch and therefore janusgraph.sh must be run under a non-root account. $ bin/janusgraph.sh start Forking Cassandra... Running ` nodetool statusthrift ` .. OK ( returned exit status 0 and printed string \"running\" ) . Forking Elasticsearch... Connecting to Elasticsearch ( 127 .0.0.1:9300 ) ... OK ( connected to 127 .0.0.1:9300 ) . Forking Gremlin-Server... Connecting to Gremlin-Server ( 127 .0.0.1:8182 ) ... OK ( connected to 127 .0.0.1:8182 ) . Run gremlin.sh to connect. Cleaning up after the Pre-Packaged Distribution If you want to start fresh and remove the database and logs you can use the clean command with janusgraph.sh . The server should be stopped before running the clean operation. $ cd /Path/to/janusgraph/janusgraph- { project.version } / $ ./bin/janusgraph.sh stop Killing Gremlin-Server ( pid 91505 ) ... Killing Elasticsearch ( pid 91402 ) ... Killing Cassandra ( pid 91219 ) ... $ ./bin/janusgraph.sh clean Are you sure you want to delete all stored data and logs? [ y/N ] y Deleted data in /Path/to/janusgraph/janusgraph- { project.version } /db Deleted logs in /Path/to/janusgraph/janusgraph- { project.version } /log","title":"Installation"},{"location":"getting-started/installation/#installation","text":"","title":"Installation"},{"location":"getting-started/installation/#running-janusgraph-inside-a-docker-container","text":"For virtualization and easy access, JanusGraph provides a Docker image . Docker makes it easier to run servers and clients on a single machine without dealing with multiple installations. For instructions on installing and using Docker, please refer to the docker guide . With JanusGraph Server running in one container, we can either run the client in another container, or on our bare-metal machine. In the examples below, we use Gremlin Console as the client.","title":"Running JanusGraph inside a Docker container"},{"location":"getting-started/installation/#running-gremlin-console-inside-another-docker-container","text":"Let's start by running a simple JanusGraph instance in Docker: $ docker run --name janusgraph-default janusgraph/janusgraph:latest The above command launches a JanusGraph Server instance. We name the container so that we can link a second container to it. The server may need a few seconds to start up so be patient and wait for the corresponding log messages to appear. Example log SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/opt/janusgraph/lib/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/opt/janusgraph/lib/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 0 [main] INFO com.jcabi.manifests.Manifests - 110 attributes loaded from 283 stream(s) in 130ms, 110 saved, 3770 ignored: [\"Agent-Class\", \"Ant-Version\", \"Archiver-Version\", \"Automatic-Module-Name\", \"Bnd-LastModified\", \"Boot-Class-Path\", \"Branch\", \"Build-Date\", \"Build-Host\", \"Build-Id\", \"Build-Java-Version\", \"Build-Jdk\", \"Build-Job\", \"Build-Number\", \"Build-Timestamp\", \"Build-Version\", \"Built-At\", \"Built-By\", \"Built-Date\", \"Built-OS\", \"Built-On\", \"Built-Status\", \"Bundle-ActivationPolicy\", \"Bundle-Activator\", \"Bundle-BuddyPolicy\", \"Bundle-Category\", \"Bundle-ClassPath\", \"Bundle-ContactAddress\", \"Bundle-Description\", \"Bundle-DocURL\", \"Bundle-License\", \"Bundle-ManifestVersion\", \"Bundle-Name\", \"Bundle-NativeCode\", \"Bundle-RequiredExecutionEnvironment\", \"Bundle-SymbolicName\", \"Bundle-Vendor\", \"Bundle-Version\", \"Can-Redefine-Classes\", \"Change\", \"Class-Path\", \"Created-By\", \"DSTAMP\", \"DynamicImport-Package\", \"Eclipse-BuddyPolicy\", \"Eclipse-ExtensibleAPI\", \"Embed-Dependency\", \"Embed-Transitive\", \"Export-Package\", \"Extension-Name\", \"Extension-name\", \"Fragment-Host\", \"Gradle-Version\", \"Gremlin-Lib-Paths\", \"Gremlin-Plugin-Dependencies\", \"Gremlin-Plugin-Paths\", \"Ignore-Package\", \"Implementation-Build\", \"Implementation-Build-Date\", \"Implementation-Title\", \"Implementation-URL\", \"Implementation-Vendor\", \"Implementation-Vendor-Id\", \"Implementation-Version\", \"Import-Package\", \"Include-Resource\", \"JCabi-Build\", \"JCabi-Date\", \"JCabi-Version\", \"Java-Vendor\", \"Java-Version\", \"Main-Class\", \"Manifest-Version\", \"Maven-Version\", \"Module-Email\", \"Module-Origin\", \"Module-Owner\", \"Module-Source\", \"Originally-Created-By\", \"Os-Arch\", \"Os-Name\", \"Os-Version\", \"Package\", \"Premain-Class\", \"Private-Package\", \"Provide-Capability\", \"Require-Bundle\", \"Require-Capability\", \"Scm-Connection\", \"Scm-Revision\", \"Scm-Url\", \"Specification-Title\", \"Specification-Vendor\", \"Specification-Version\", \"TODAY\", \"TSTAMP\", \"Time-Zone-Database-Version\", \"Tool\", \"X-Compile-Elasticsearch-Snapshot\", \"X-Compile-Elasticsearch-Version\", \"X-Compile-Lucene-Version\", \"X-Compile-Source-JDK\", \"X-Compile-Target-JDK\", \"hash\", \"implementation-version\", \"mode\", \"package\", \"service\", \"url\", \"version\"] 1 [main] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - 3.4.1 \\,,,/ (o o) -----oOOo-(3)-oOOo----- 100 [main] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Configuring Gremlin Server from /etc/opt/janusgraph/gremlin-server.yaml ... ... 3965 [gremlin-server-boss-1] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Gremlin Server configured with worker thread pool of 1, gremlin pool of 8 and boss thread pool of 1. 3965 [gremlin-server-boss-1] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Channel started at port 8182. We can now instruct Docker to start a second container for the client and link it to the already running server. Here we use Gremlin Console ( gremlin.sh ) as the client. $ docker run --rm --link janusgraph-default:janusgraph -e GREMLIN_REMOTE_HOSTS = janusgraph \\ -it janusgraph/janusgraph:latest ./bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- gremlin> :remote connect tinkerpop.server conf/remote.yaml == >Configured janusgraph/172.17.0.2:8182 Warning The above command only establishes the connection to the server. It does not forward the following commands to the server by default! As a result, further commands will still be executed locally unless preceeded by :> . To forward every command to the remote server, use the :remote console command. Further documentation can be found in the TinkerPop reference docs","title":"Running Gremlin Console inside another Docker container"},{"location":"getting-started/installation/#running-gremlin-console-on-bare-metal","text":"We can also run the Gremlin Console on our bare-metal machine. To make the server able to communicate with the gremlin console, we need to expose the 8182 port when launching JanusGraph Server: $ docker run -it -p 8182 :8182 janusgraph/janusgraph We can now start a Gremlin Console and try to connect to the server. We need to download the released zip archive and launch the gremlin console (see local installation section for details): $ bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- gremlin> :remote connect tinkerpop.server conf/remote.yaml == >Configured localhost/127.0.0.1:8182 For further reading, see the JanusGraph Server section as well as the JanusGraph Docker documentation .","title":"Running Gremlin Console on Bare-metal"},{"location":"getting-started/installation/#local-installation","text":"In order to run JanusGraph, Java 8 SE is required. Make sure the $JAVA_HOME environment variable points to the correct location where either JRE or JDK is installed. JanusGraph can be downloaded as a .zip archive from the Releases section of the project repository. $ unzip janusgraph-1.0.0-rc2.zip Archive: janusgraph-1.0.0-rc2.zip creating: janusgraph-1.0.0-rc2/ ... Once you have unzipped the downloaded archive, you are ready to go.","title":"Local Installation"},{"location":"getting-started/installation/#running-the-gremlin-console","text":"The Gremlin Console is an interactive shell that gives you access to the data managed by JanusGraph. You can reach it by running the gremlin.sh script which is located in the project's bin directory. $ cd janusgraph-1.0.0-rc2 $ bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- 09 :12:24 INFO org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph - HADOOP_GREMLIN_LIBS is set to: /usr/local/janusgraph/lib plugin activated: tinkerpop.hadoop plugin activated: janusgraph.imports gremlin> The Gremlin Console interprets commands using Apache Groovy , which is a superset of Java. Gremlin-Groovy extends Groovy by providing a set of methods for basic and advanced graph traversal functionality. For a deeper dive into Gremlin language's features, please refer to our introduction to Gremlin .","title":"Running the Gremlin Console"},{"location":"getting-started/installation/#running-the-janusgraph-server","text":"In most real-world use cases, queries to a database will not be run from the exact same server the data is stored on. Instead, there will be some sort of client-server hierarchy in which the server runs the database and handles requests while multiple clients create these requests and thereby read and write entries within the database independently of one another. This behavior can also be achieved with JanusGraph. In order to start a server on your local machine, simply run the janusgraph-server.sh script instead of the gremlin.sh script. You can optionally pass a configuration file as a parameter. The default configuration is located at conf/gremlin-server/gremlin-server.yaml . $ ./bin/janusgraph-server.sh start or $ ./bin/janusgraph-server.sh console ./conf/gremlin-server/gremlin-server- [ ... ] .yaml Info The default configuration ( gremlin-server.yaml ) uses it's own inmemory backend instead of a dedicated database server. No search backend is used by default, so mixed indices aren't supported as search backend isn't specified (Make sure you are using GraphOfTheGodsFactory.loadWithoutMixedIndex(graph, true) instead of GraphOfTheGodsFactory.load(graph) if you follow Basic Usage example ). For further information about storage backends, visit the corresponding section of the documentation. A Gremlin server is now running on your local machine and waiting for clients to connect on the default port 8182 . To instantiate a client -- as done before -- run the gremlin.sh script. Again, a local Gremlin Console will show up. This time, instead of using it locally, we will connect the Gremlin Console to a remote server and redirect all of it's queries to this server. This is done by using the :remote command: gremlin> :remote connect tinkerpop.server conf/remote.yaml == >Configured localhost/127.0.0.1:8182 As you can probably tell from the log, the client and server are running on the same machine in this case. When using a different setup, all you have to do is modify the parameters in the conf/remote.yaml file.","title":"Running the JanusGraph Server"},{"location":"getting-started/installation/#using-the-pre-packaged-distribution","text":"Note Starting with 0.5.1, this requires to download janusgraph-full-1.0.0-rc2.zip instead of the default janusgraph-1.0.0-rc2.zip . The JanusGraph release comes pre-configured to run JanusGraph Server out of the box leveraging a sample Cassandra and Elasticsearch configuration to allow users to get started quickly with JanusGraph Server. This configuration defaults to client applications that can connect to JanusGraph Server via WebSocket with a custom subprotocol. There are a number of clients developed in different languages to help support the subprotocol. The most familiar client to use the WebSocket interface is the Gremlin Console. The quick-start bundle is not intended to be representative of a production installation, but does provide a way to perform development with JanusGraph Server, run tests and see how the components are wired together. To use this default configuration: Download a copy of the current janusgraph-full-$VERSION.zip file from the Releases page Unzip it and enter the janusgraph--full-$VERSION directory Run bin/janusgraph.sh start . This step will start Gremlin Server with Cassandra/ES forked into a separate process. Note for security reasons Elasticsearch and therefore janusgraph.sh must be run under a non-root account. $ bin/janusgraph.sh start Forking Cassandra... Running ` nodetool statusthrift ` .. OK ( returned exit status 0 and printed string \"running\" ) . Forking Elasticsearch... Connecting to Elasticsearch ( 127 .0.0.1:9300 ) ... OK ( connected to 127 .0.0.1:9300 ) . Forking Gremlin-Server... Connecting to Gremlin-Server ( 127 .0.0.1:8182 ) ... OK ( connected to 127 .0.0.1:8182 ) . Run gremlin.sh to connect.","title":"Using the Pre-Packaged Distribution"},{"location":"getting-started/installation/#cleaning-up-after-the-pre-packaged-distribution","text":"If you want to start fresh and remove the database and logs you can use the clean command with janusgraph.sh . The server should be stopped before running the clean operation. $ cd /Path/to/janusgraph/janusgraph- { project.version } / $ ./bin/janusgraph.sh stop Killing Gremlin-Server ( pid 91505 ) ... Killing Elasticsearch ( pid 91402 ) ... Killing Cassandra ( pid 91219 ) ... $ ./bin/janusgraph.sh clean Are you sure you want to delete all stored data and logs? [ y/N ] y Deleted data in /Path/to/janusgraph/janusgraph- { project.version } /db Deleted logs in /Path/to/janusgraph/janusgraph- { project.version } /log","title":"Cleaning up after the Pre-Packaged Distribution"},{"location":"index-backend/","text":"While JanusGraph's composite graph indexes are natively supported through the primary storage backend, mixed graph indexes require that an indexing backend is configured. Mixed indexes provide support for geo, numeric range, and full-text search. The choice of index backend determines which search features are supported, as well as the performance and scalability of the index. JanusGraph currently supports three index backends: Elasticsearch , Apache Solr and Apache Lucene . Use Elasticsearch or Apache Solr when there is an expectation that JanusGraph will be distributed across multiple machines. Apache Lucene performs better in small scale, single machine applications. It performs better in unit tests, for instance.","title":"Introduction"},{"location":"index-backend/direct-index-query/","text":"Direct Index Query JanusGraph\u2019s standard global graph querying mechanism supports boolean queries for vertices or edges. In other words, an element either matches the query or it does not. There are no partial matches or result scoring. Some indexing backends additionally support fuzzy search queries. For those queries, a score is computed for each match to indicate the \"goodness\" of the match and results are returned in the order of their score. Fuzzy search is particularly useful when dealing with full-text search queries where matching more words is considered to be better. Since fuzzy search implementations and scoring algorithms differ significantly between indexing backends, JanusGraph does not support fuzzy search natively. However, JanusGraph provides a direct index query mechanism that allows search queries to be directly send to the indexing backend for evaluation (for those backends that support it). Use Graph.indexQuery() to compose a query that is executed directly against an indexing backend. This query builder expects two parameters: The name of the indexing backend to query. This must be the name configured in JanusGraph\u2019s configuration and used in the property key indexing definitions The query string The builder allows configuration of the maximum number of elements to be returned via its limit(int) method. The builder\u2019s offset(int) controls number of initial matches in the result set to skip. To retrieve all vertex or edges matching the given query in the specified indexing backend, invoke vertexStream() or edgeStream() , respectively. It is not possible to query for both vertices and edges at the same time. These methods return a Stream over Result objects. A result object contains the matched handle, retrievable via getElement() , and the associated score - getScore() . Consider the following example: ManagementSystem mgmt = graph . openManagement (); PropertyKey text = mgmt . makePropertyKey ( \"text\" ). dataType ( String . class ). make (); mgmt . buildIndex ( \"vertexByText\" , Vertex . class ). addKey ( text ). buildMixedIndex ( \"search\" ); mgmt . commit (); // ... Load vertices ... for ( Result < Vertex > result : graph . indexQuery ( \"vertexByText\" , \"v.text:(farm uncle berry)\" ). vertexStream ()) { System . out . println ( result . getElement () + \": \" + result . getScore ()); } Query String The query string is handed directly to the indexing backend for processing and hence the query string syntax depends on what is supported by the indexing backend. For vertex queries, JanusGraph will analyze the query string for property key references starting with \"v.\" and replace those by a handle to the indexing field that corresponds to the property key. Likewise, for edge queries, JanusGraph will replace property key references starting with \"e.\". Hence, to refer to a property of a vertex, use \"v.[KEY_NAME]\" in the query string. Likewise, for edges write \"e.[KEY_NAME]\". Elasticsearch and Lucene support the Lucene query syntax . Refer to the Lucene documentation or the Elasticsearch documentation for more information. The query used in the example above follows the Lucene query syntax. graph . indexQuery ( \"vertexByText\" , \"v.text:(farm uncle berry)\" ). vertexStream () This query matches all vertices where the text contains any of the three words (grouped by parentheses) and score matches higher the more words are matched in the text. In addition Elasticsearch supports wildcard queries, use \"v.*\" or \"e.*\" in the query string to query if any of the properties on the element match. Query Totals It is sometimes useful to know how many total results were returned from a query without having to retrieve all results. Fortunately, Elasticsearch and Solr provide a shortcut that does not involve retrieving and ranking all documents. This shortcut is exposed through the \".vertexTotals()\", \".edgeTotals()\", and \".propertyTotals()\" methods. The totals can be retrieved using the same query syntax as the indexQuery builder. graph . indexQuery ( \"vertexByText\" , \"v.text:(farm uncle berry)\" ). vertexTotals () Note In JanusGraph versions earlier than 1.0.0 provided limit and offset was ignored for any totals queries. Starting from JanusGraph 1.0.0 provided limit and offset are applied to the final totals result ( vertexTotals() , edgeTotals() , propertyTotals() as well as internal JanusGraph method IndexProvider.totals ). This was made to have consistent behaviour between totals and stream direct index queries. Gotchas Property Key Names Names of property keys that contain non-alphanumeric characters must be placed in quotation marks to ensure that the query is parsed correctly. graph . indexQuery ( \"vertexByText\" , \"v.\\\"first_name\\\":john\" ). vertexStream () Some property key names may be transformed by the JanusGraph indexing backend implementation. For instance, an indexing backend that does not permit spaces in field names may transform \"My Field Name\" to \"My\u2022Field\u2022Name\", or an indexing backend like Solr may append type information to the name, transforming \"myBooleanField\" to \"myBooleanField_b\". These transformations happen in the index backend\u2019s implementation of IndexProvider, in the \"mapKey2Field\" method. Indexing backends may reserve special characters (such as \u2022 ) and prohibit indexing of fields that contain them. For this reason it is recommended to avoid spaces and special characters in property names. In general, making direct index queries depends on implementation details of JanusGraph indexing backends that are normally hidden from users, so it\u2019s best to verify a query empirically against the indexing backend in use. Element Identifier Collision The strings \"v.\", \"e.\", and \"p.\" are used to identify a vertex, edge or property element respectively in a query. If the field name or the query value contains the same sequence of characters, this can cause a collision in the query string and parsing errors as in the following example: graph . indexQuery ( \"vertexByText\" , \"v.name:v.john\" ). vertexStream () //DOES NOT WORK! To avoid such identifier collisions, use the setElementIdentifier method to define a unique element identifier string that does not occur in any other parts of the query: graph . indexQuery ( \"vertexByText\" , \"$v$name:v.john\" ). setElementIdentifier ( \"$v$\" ). vertexStream () Mixed Index Availability Delay When a query traverses a mixed index immediately after data is inserted the changes may not be visible. In Elasticsearch the configuration option that determines this delay is index refresh interval . In Solr the primary configuration option is max time .","title":"Direct Index Query"},{"location":"index-backend/direct-index-query/#direct-index-query","text":"JanusGraph\u2019s standard global graph querying mechanism supports boolean queries for vertices or edges. In other words, an element either matches the query or it does not. There are no partial matches or result scoring. Some indexing backends additionally support fuzzy search queries. For those queries, a score is computed for each match to indicate the \"goodness\" of the match and results are returned in the order of their score. Fuzzy search is particularly useful when dealing with full-text search queries where matching more words is considered to be better. Since fuzzy search implementations and scoring algorithms differ significantly between indexing backends, JanusGraph does not support fuzzy search natively. However, JanusGraph provides a direct index query mechanism that allows search queries to be directly send to the indexing backend for evaluation (for those backends that support it). Use Graph.indexQuery() to compose a query that is executed directly against an indexing backend. This query builder expects two parameters: The name of the indexing backend to query. This must be the name configured in JanusGraph\u2019s configuration and used in the property key indexing definitions The query string The builder allows configuration of the maximum number of elements to be returned via its limit(int) method. The builder\u2019s offset(int) controls number of initial matches in the result set to skip. To retrieve all vertex or edges matching the given query in the specified indexing backend, invoke vertexStream() or edgeStream() , respectively. It is not possible to query for both vertices and edges at the same time. These methods return a Stream over Result objects. A result object contains the matched handle, retrievable via getElement() , and the associated score - getScore() . Consider the following example: ManagementSystem mgmt = graph . openManagement (); PropertyKey text = mgmt . makePropertyKey ( \"text\" ). dataType ( String . class ). make (); mgmt . buildIndex ( \"vertexByText\" , Vertex . class ). addKey ( text ). buildMixedIndex ( \"search\" ); mgmt . commit (); // ... Load vertices ... for ( Result < Vertex > result : graph . indexQuery ( \"vertexByText\" , \"v.text:(farm uncle berry)\" ). vertexStream ()) { System . out . println ( result . getElement () + \": \" + result . getScore ()); }","title":"Direct Index Query"},{"location":"index-backend/direct-index-query/#query-string","text":"The query string is handed directly to the indexing backend for processing and hence the query string syntax depends on what is supported by the indexing backend. For vertex queries, JanusGraph will analyze the query string for property key references starting with \"v.\" and replace those by a handle to the indexing field that corresponds to the property key. Likewise, for edge queries, JanusGraph will replace property key references starting with \"e.\". Hence, to refer to a property of a vertex, use \"v.[KEY_NAME]\" in the query string. Likewise, for edges write \"e.[KEY_NAME]\". Elasticsearch and Lucene support the Lucene query syntax . Refer to the Lucene documentation or the Elasticsearch documentation for more information. The query used in the example above follows the Lucene query syntax. graph . indexQuery ( \"vertexByText\" , \"v.text:(farm uncle berry)\" ). vertexStream () This query matches all vertices where the text contains any of the three words (grouped by parentheses) and score matches higher the more words are matched in the text. In addition Elasticsearch supports wildcard queries, use \"v.*\" or \"e.*\" in the query string to query if any of the properties on the element match.","title":"Query String"},{"location":"index-backend/direct-index-query/#query-totals","text":"It is sometimes useful to know how many total results were returned from a query without having to retrieve all results. Fortunately, Elasticsearch and Solr provide a shortcut that does not involve retrieving and ranking all documents. This shortcut is exposed through the \".vertexTotals()\", \".edgeTotals()\", and \".propertyTotals()\" methods. The totals can be retrieved using the same query syntax as the indexQuery builder. graph . indexQuery ( \"vertexByText\" , \"v.text:(farm uncle berry)\" ). vertexTotals () Note In JanusGraph versions earlier than 1.0.0 provided limit and offset was ignored for any totals queries. Starting from JanusGraph 1.0.0 provided limit and offset are applied to the final totals result ( vertexTotals() , edgeTotals() , propertyTotals() as well as internal JanusGraph method IndexProvider.totals ). This was made to have consistent behaviour between totals and stream direct index queries.","title":"Query Totals"},{"location":"index-backend/direct-index-query/#gotchas","text":"","title":"Gotchas"},{"location":"index-backend/direct-index-query/#property-key-names","text":"Names of property keys that contain non-alphanumeric characters must be placed in quotation marks to ensure that the query is parsed correctly. graph . indexQuery ( \"vertexByText\" , \"v.\\\"first_name\\\":john\" ). vertexStream () Some property key names may be transformed by the JanusGraph indexing backend implementation. For instance, an indexing backend that does not permit spaces in field names may transform \"My Field Name\" to \"My\u2022Field\u2022Name\", or an indexing backend like Solr may append type information to the name, transforming \"myBooleanField\" to \"myBooleanField_b\". These transformations happen in the index backend\u2019s implementation of IndexProvider, in the \"mapKey2Field\" method. Indexing backends may reserve special characters (such as \u2022 ) and prohibit indexing of fields that contain them. For this reason it is recommended to avoid spaces and special characters in property names. In general, making direct index queries depends on implementation details of JanusGraph indexing backends that are normally hidden from users, so it\u2019s best to verify a query empirically against the indexing backend in use.","title":"Property Key Names"},{"location":"index-backend/direct-index-query/#element-identifier-collision","text":"The strings \"v.\", \"e.\", and \"p.\" are used to identify a vertex, edge or property element respectively in a query. If the field name or the query value contains the same sequence of characters, this can cause a collision in the query string and parsing errors as in the following example: graph . indexQuery ( \"vertexByText\" , \"v.name:v.john\" ). vertexStream () //DOES NOT WORK! To avoid such identifier collisions, use the setElementIdentifier method to define a unique element identifier string that does not occur in any other parts of the query: graph . indexQuery ( \"vertexByText\" , \"$v$name:v.john\" ). setElementIdentifier ( \"$v$\" ). vertexStream ()","title":"Element Identifier Collision"},{"location":"index-backend/direct-index-query/#mixed-index-availability-delay","text":"When a query traverses a mixed index immediately after data is inserted the changes may not be visible. In Elasticsearch the configuration option that determines this delay is index refresh interval . In Solr the primary configuration option is max time .","title":"Mixed Index Availability Delay"},{"location":"index-backend/elasticsearch/","text":"Elasticsearch Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. \u2014 Elasticsearch Overview JanusGraph supports Elasticsearch as an index backend. Here are some of the Elasticsearch features supported by JanusGraph: Full-Text : Supports all Text predicates to search for text properties that matches a given word, prefix or regular expression. Geo : Supports all Geo predicates to search for geo properties that are intersecting, within, disjoint to or contained in a given query geometry. Supports points, circles, boxes, lines and polygons for indexing. Supports circles, boxes and polygons for querying point properties and all shapes for querying non-point properties. Numeric Range : Supports all numeric comparisons in Compare . Flexible Configuration : Supports remote operation and open-ended settings customization. Collections : Supports indexing SET and LIST cardinality properties. Temporal : Nanosecond granularity temporal indexing. Custom Analyzer : Choose to use a custom analyzer Please see Version Compatibility for details on what versions of Elasticsearch will work with JanusGraph. Important JanusGraph uses sandboxed https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-painless.html[Painless scripts] for inline updates, which are enabled by default in Elasticsearch. Running Elasticsearch JanusGraph supports connections to a running Elasticsearch cluster. JanusGraph provides two options for running local Elasticsearch instances for getting started quickly. JanusGraph server (see Getting started ) automatically starts a local Elasticsearch instance. Alternatively JanusGraph releases include a full Elasticsearch distribution to allow users to manually start a local Elasticsearch instance (see this page for more information). $ elasticsearch/bin/elasticsearch Note For security reasons Elasticsearch must be run under a non-root account Elasticsearch Configuration Overview JanusGraph supports HTTP(S) client connections to a running Elasticsearch cluster. Please see Version Compatibility for details on what versions of Elasticsearch will work with the different client types in JanusGraph. Note JanusGraph\u2019s index options start with the string \" index.[X]. \" where \" [X] \" is a user-defined name for the backend. This user-defined name must be passed to JanusGraph\u2019s ManagementSystem interface when building a mixed index, as described in Mixed Index , so that JanusGraph knows which of potentially multiple configured index backends to use. Configuration snippets in this chapter use the name search , whereas prose discussion of options typically write [X] in the same position. The exact index name is not significant as long as it is used consistently in JanusGraph\u2019s configuration and when administering indices. Tip It\u2019s recommended that index names contain only alphanumeric lowercase characters and hyphens, and that they start with a lowercase letter. Connecting to Elasticsearch The Elasticsearch client is specified as follows: index.search.backend = elasticsearch When connecting to Elasticsearch a single or list of hostnames for the Elasticsearch instances must be provided. These are supplied via JanusGraph\u2019s index.[X].hostname key. index.search.backend = elasticsearch index.search.hostname = 10.0.0.10:9200 Each host or host:port pair specified here will be added to the HTTP client\u2019s round-robin list of request targets. Here\u2019s a minimal configuration that will round-robin over 10.0.0.10 on the default Elasticsearch HTTP port (9200) and 10.0.0.20 on port 7777: index.search.backend = elasticsearch index.search.hostname = 10.0.0.10, 10.0.0.20:7777 JanusGraph index.[X] and index.[X].elasticsearch options JanusGraph only uses default values for index-name and health-request-timeout . See Configuration Reference for descriptions of these options and their accepted values. index.[X].elasticsearch.index-name index.[X].elasticsearch.health-request-timeout REST Client Options The REST client accepts the index.[X].bulk-refresh option. This option controls when changes are made visible to search. See ?refresh documentation for more information. REST Client HTTPS Configuration SSL support for HTTP can be enabled by setting the index.[X].elasticsearch.ssl.enabled configuration option to true . Note that depending on your configuration you may need to change the value of index.[X].port if your HTTPS port number is different from the default one for the REST API (9200). When SSL is enabled you may also configure the location and password of the truststore. This can be done as follows: index.search.elasticsearch.ssl.truststore.location = /path/to/your/truststore.jks index.search.elasticsearch.ssl.truststore.password = truststorepwd Note that these settings apply only to Elasticsearch REST client and do not affect any other SSL connections in JanusGraph. Configuration of the client keystore is also supported: index.search.elasticsearch.ssl.keystore.location = /path/to/your/keystore.jks index.search.elasticsearch.ssl.keystore.storepassword = keystorepwd index.search.elasticsearch.ssl.keystore.keypassword = keypwd Any of the passwords can be empty. If needed, the SSL hostname verification can be disabled by setting the index.[X].elasticsearch.ssl.disable-hostname-verification property value to true and the support for self-signed SSL certificates can be enabled by setting index.[X].elasticsearch.ssl.allow-self-signed-certificates property value to true . Tip It is not recommended to rely on the self-signed SSL certificates or to disable the hostname verification for a production system as it significantly limits the client's ability to provide the secure communication channel with the Elasticsearch server(s). This may result in leaking the confidential data which may be a part of your JanusGraph index. REST Client HTTP Authentication REST client supports the following authentication options: Basic HTTP Authentication (username/password) and custom authentication based on the user-provided implementation. These authentication methods are independent from SSL client authentication described above. REST Client Basic HTTP Authentication Basic HTTP Authentication is available regardless of the state of SSL support. Optionally, an authentication realm can be specified via index.[X].elasticsearch.http.auth.basic.realm property. index.search.elasticsearch.http.auth.type = basic index.search.elasticsearch.http.auth.basic.username = httpuser index.search.elasticsearch.http.auth.basic.password = httppassword Tip It is highly recommended to use SSL (e.g. setting index.[X].elasticsearch.ssl.enabled to true ) when using this option as the credentials can be intercepted when sent over an unencrypted connection! REST Client Custom HTTP Authentication Additional authentication methods can be implemented by providing your own implementation. The custom authenticator is configured as follows: index.search.elasticsearch.http.auth.type = custom index.search.elasticsearch.http.auth.custom.authenticator-class = fully.qualified.class.Name index.search.elasticsearch.http.auth.custom.authenticator-args = arg1,arg2,... Argument list is optional and can be empty. The class specified there has to implement the org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticator interface or extend org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticatorBase convenience class. The implementation gets access to HTTP client configuration and can customize the client as needed. Refer to < > for more information. For example, the following code snippet implements an authenticator allowing the Elasticsearch REST client to authenticate and get authorized against AWS IAM: import java.io.IOException ; import java.time.LocalDateTime ; import java.time.ZoneOffset ; import org.apache.http.HttpRequestInterceptor ; import org.apache.http.impl.nio.client.HttpAsyncClientBuilder ; import org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticatorBase ; import com.amazonaws.auth.DefaultAWSCredentialsProviderChain ; import com.amazonaws.regions.DefaultAwsRegionProviderChain ; import com.google.common.base.Supplier ; import vc.inreach.aws.request.AWSSigner ; import vc.inreach.aws.request.AWSSigningRequestInterceptor ; /** * <p> * Elasticsearch REST HTTP(S) client callback implementing AWS request signing. * </p> * <p> * The signer is based on AWS SDK default provider chain, allowing multiple options for providing * the caller credentials. See {@link DefaultAWSCredentialsProviderChain} documentation for the details. * </p> */ public class AWSV4AuthHttpClientConfigCallback extends RestClientAuthenticatorBase { private static final String AWS_SERVICE_NAME = \"es\" ; private HttpRequestInterceptor awsSigningInterceptor ; public AWSV4AuthHttpClientConfigCallback ( final String [] args ) { // does not require any configuration } @Override public void init () throws IOException { DefaultAWSCredentialsProviderChain awsCredentialsProvider = new DefaultAWSCredentialsProviderChain (); final Supplier < LocalDateTime > clock = () -> LocalDateTime . now ( ZoneOffset . UTC ); // using default region provider chain // (https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/java-dg-region-selection.html) DefaultAwsRegionProviderChain regionProviderChain = new DefaultAwsRegionProviderChain (); final String awsRegion = regionProviderChain . getRegion (); final AWSSigner awsSigner = new AWSSigner ( awsCredentialsProvider , awsRegion , AWS_SERVICE_NAME , clock ); this . awsSigningInterceptor = new AWSSigningRequestInterceptor ( awsSigner ); } @Override public HttpAsyncClientBuilder customizeHttpClient ( HttpAsyncClientBuilder httpClientBuilder ) { return httpClientBuilder . addInterceptorLast ( awsSigningInterceptor ); / } } This custom authenticator does not use any constructor arguments. Ingest Pipelines Different ingest pipelines can be set for each mixed index. Ingest pipeline can be use to pre-process documents before indexing. A pipeline is composed by a series of processors. Each processor transforms the document in some way. For example date processor can extract a date from a text to a date field. So you can query this date with JanusGraph without it being physically in the primary storage. index.[X].elasticsearch.ingest-pipeline.[mixedIndexName] = pipeline_id See ingest documentation for more information about ingest pipelines and processors documentation for more information about ingest processors. Secure Elasticsearch Elasticsearch does not perform authentication or authorization. A client that can connect to Elasticsearch is trusted by Elasticsearch. When Elasticsearch runs on an unsecured or public network, particularly the Internet, it should be deployed with some type of external security. This is generally done with a combination of firewalling, tunneling of Elasticsearch\u2019s ports or by using Elasticsearch extensions such as X-Pack . Elasticsearch has two client-facing ports to consider: The HTTP REST API, usually on port 9200 The native \"transport\" protocol, usually on port 9300 A client uses either one protocol/port or the other, but not both simultaneously. Securing the HTTP protocol port is generally done with a combination of firewalling and a reverse proxy with SSL encryption and HTTP authentication. There are a couple of ways to approach security on the native \"transport\" protocol port: In addition to that, some hosted Elasticsearch services offer other methods of authentication and authorization. For example, AWS Elasticsearch Service requires the use of HTTPS and offers an option for using IAM-based access control. For that the requests sent to this service must be signed. This can be achieved by using a custom authenticator (see above). Tunnel Elasticsearch's native \"transport\" protocol:: This approach can be implemented with SSL/TLS tunneling (for instance via stunnel ), a VPN, or SSH port forwarding. SSL/TLS tunnels require non-trivial setup and monitoring: one or both ends of the tunnel need a certificate, and the stunnel processes need to be configured and running continuously. The setup for most secure VPNs is likewise non-trivial. Some Elasticsearch service providers handle server-side tunnel management and provide a custom Elasticsearch transport.type to simplify the client setup. Add a firewall rule that allows only trusted clients to connect on Elasticsearch\u2019s native protocol port This is typically done at the host firewall level. Easy to configure, but very weak security by itself. Index Creation Options JanusGraph supports customization of the index settings it uses when creating its Elasticsearch index. It allows setting arbitrary key-value pairs on the settings object in the Elasticsearch create index request issued by JanusGraph. Here is a non-exhaustive sample of Elasticsearch index settings that can be customized using this mechanism: index.number_of_replicas index.number_of_shards index.refresh_interval Settings customized through this mechanism are only applied when JanusGraph attempts to create its index in Elasticsearch. If JanusGraph finds that its index already exists, then it does not attempt to recreate it, and these settings have no effect. Embedding Elasticsearch index creation settings with create.ext JanusGraph iterates over all properties prefixed with index.[X].elasticsearch.create.ext. , where [X] is an index name such as search . It strips the prefix from each property key. The remainder of the stripped key will be interpreted as an Elasticsearch index creation setting. The value associated with the key is not modified. The stripped key and unmodified value are passed as part of the settings object in the Elasticsearch create index request that JanusGraph issues when bootstrapping on Elasticsearch. This allows embedding arbitrary index creation settings settings in JanusGraph\u2019s properties. Here\u2019s an example configuration fragment that customizes three Elasticsearch index settings using the create.ext config mechanism: index.search.backend = elasticsearch index.search.elasticsearch.create.ext.number_of_shards = 15 index.search.elasticsearch.create.ext.number_of_replicas = 3 index.search.elasticsearch.create.ext.shard.check_on_startup = true The configuration fragment listed above takes advantage of Elasticsearch\u2019s assumption, implemented server-side, that unqualified create index setting keys have an index. prefix. It\u2019s also possible to spell out the index prefix explicitly. Here\u2019s a JanusGraph config file functionally equivalent to the one listed above, except that the index. prefix before the index creation settings is explicit: index.search.backend = elasticsearch index.search.elasticsearch.create.ext.index.number_of_shards = 15 index.search.elasticsearch.create.ext.index.number_of_replicas = 3 index.search.elasticsearch.create.ext.index.shard.check_on_startup = false Tip The create.ext mechanism for specifying index creation settings is compatible with JanusGraph\u2019s Elasticsearch configuration. Troubleshooting Connection Issues to remote Elasticsearch cluster Check that the Elasticsearch cluster nodes are reachable on the HTTP protocol port from the JanusGraph nodes. Check the node listen port by examining the Elasticsearch node configuration logs or using a general diagnostic utility like netstat . Check the JanusGraph configuration. Optimizing Elasticsearch Write Optimization For bulk loading or other write-intense applications, consider increasing Elasticsearch\u2019s refresh interval. Refer to this discussion on how to increase the refresh interval and its impact on write performance. Note, that a higher refresh interval means that it takes a longer time for graph mutations to be available in the index. For additional suggestions on how to increase write performance in Elasticsearch with detailed instructions, please read this blog post . Further Reading Please refer to the Elasticsearch homepage and available documentation for more information on Elasticsearch and how to setup an Elasticsearch cluster.","title":"Elasticsearch"},{"location":"index-backend/elasticsearch/#elasticsearch","text":"Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected. \u2014 Elasticsearch Overview JanusGraph supports Elasticsearch as an index backend. Here are some of the Elasticsearch features supported by JanusGraph: Full-Text : Supports all Text predicates to search for text properties that matches a given word, prefix or regular expression. Geo : Supports all Geo predicates to search for geo properties that are intersecting, within, disjoint to or contained in a given query geometry. Supports points, circles, boxes, lines and polygons for indexing. Supports circles, boxes and polygons for querying point properties and all shapes for querying non-point properties. Numeric Range : Supports all numeric comparisons in Compare . Flexible Configuration : Supports remote operation and open-ended settings customization. Collections : Supports indexing SET and LIST cardinality properties. Temporal : Nanosecond granularity temporal indexing. Custom Analyzer : Choose to use a custom analyzer Please see Version Compatibility for details on what versions of Elasticsearch will work with JanusGraph. Important JanusGraph uses sandboxed https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-painless.html[Painless scripts] for inline updates, which are enabled by default in Elasticsearch.","title":"Elasticsearch"},{"location":"index-backend/elasticsearch/#running-elasticsearch","text":"JanusGraph supports connections to a running Elasticsearch cluster. JanusGraph provides two options for running local Elasticsearch instances for getting started quickly. JanusGraph server (see Getting started ) automatically starts a local Elasticsearch instance. Alternatively JanusGraph releases include a full Elasticsearch distribution to allow users to manually start a local Elasticsearch instance (see this page for more information). $ elasticsearch/bin/elasticsearch Note For security reasons Elasticsearch must be run under a non-root account","title":"Running Elasticsearch"},{"location":"index-backend/elasticsearch/#elasticsearch-configuration-overview","text":"JanusGraph supports HTTP(S) client connections to a running Elasticsearch cluster. Please see Version Compatibility for details on what versions of Elasticsearch will work with the different client types in JanusGraph. Note JanusGraph\u2019s index options start with the string \" index.[X]. \" where \" [X] \" is a user-defined name for the backend. This user-defined name must be passed to JanusGraph\u2019s ManagementSystem interface when building a mixed index, as described in Mixed Index , so that JanusGraph knows which of potentially multiple configured index backends to use. Configuration snippets in this chapter use the name search , whereas prose discussion of options typically write [X] in the same position. The exact index name is not significant as long as it is used consistently in JanusGraph\u2019s configuration and when administering indices. Tip It\u2019s recommended that index names contain only alphanumeric lowercase characters and hyphens, and that they start with a lowercase letter.","title":"Elasticsearch Configuration Overview"},{"location":"index-backend/elasticsearch/#connecting-to-elasticsearch","text":"The Elasticsearch client is specified as follows: index.search.backend = elasticsearch When connecting to Elasticsearch a single or list of hostnames for the Elasticsearch instances must be provided. These are supplied via JanusGraph\u2019s index.[X].hostname key. index.search.backend = elasticsearch index.search.hostname = 10.0.0.10:9200 Each host or host:port pair specified here will be added to the HTTP client\u2019s round-robin list of request targets. Here\u2019s a minimal configuration that will round-robin over 10.0.0.10 on the default Elasticsearch HTTP port (9200) and 10.0.0.20 on port 7777: index.search.backend = elasticsearch index.search.hostname = 10.0.0.10, 10.0.0.20:7777","title":"Connecting to Elasticsearch"},{"location":"index-backend/elasticsearch/#janusgraph-indexx-and-indexxelasticsearch-options","text":"JanusGraph only uses default values for index-name and health-request-timeout . See Configuration Reference for descriptions of these options and their accepted values. index.[X].elasticsearch.index-name index.[X].elasticsearch.health-request-timeout","title":"JanusGraph index.[X] and index.[X].elasticsearch options"},{"location":"index-backend/elasticsearch/#rest-client-options","text":"The REST client accepts the index.[X].bulk-refresh option. This option controls when changes are made visible to search. See ?refresh documentation for more information.","title":"REST Client Options"},{"location":"index-backend/elasticsearch/#rest-client-https-configuration","text":"SSL support for HTTP can be enabled by setting the index.[X].elasticsearch.ssl.enabled configuration option to true . Note that depending on your configuration you may need to change the value of index.[X].port if your HTTPS port number is different from the default one for the REST API (9200). When SSL is enabled you may also configure the location and password of the truststore. This can be done as follows: index.search.elasticsearch.ssl.truststore.location = /path/to/your/truststore.jks index.search.elasticsearch.ssl.truststore.password = truststorepwd Note that these settings apply only to Elasticsearch REST client and do not affect any other SSL connections in JanusGraph. Configuration of the client keystore is also supported: index.search.elasticsearch.ssl.keystore.location = /path/to/your/keystore.jks index.search.elasticsearch.ssl.keystore.storepassword = keystorepwd index.search.elasticsearch.ssl.keystore.keypassword = keypwd Any of the passwords can be empty. If needed, the SSL hostname verification can be disabled by setting the index.[X].elasticsearch.ssl.disable-hostname-verification property value to true and the support for self-signed SSL certificates can be enabled by setting index.[X].elasticsearch.ssl.allow-self-signed-certificates property value to true . Tip It is not recommended to rely on the self-signed SSL certificates or to disable the hostname verification for a production system as it significantly limits the client's ability to provide the secure communication channel with the Elasticsearch server(s). This may result in leaking the confidential data which may be a part of your JanusGraph index.","title":"REST Client HTTPS Configuration"},{"location":"index-backend/elasticsearch/#rest-client-http-authentication","text":"REST client supports the following authentication options: Basic HTTP Authentication (username/password) and custom authentication based on the user-provided implementation. These authentication methods are independent from SSL client authentication described above.","title":"REST Client HTTP Authentication"},{"location":"index-backend/elasticsearch/#rest-client-basic-http-authentication","text":"Basic HTTP Authentication is available regardless of the state of SSL support. Optionally, an authentication realm can be specified via index.[X].elasticsearch.http.auth.basic.realm property. index.search.elasticsearch.http.auth.type = basic index.search.elasticsearch.http.auth.basic.username = httpuser index.search.elasticsearch.http.auth.basic.password = httppassword Tip It is highly recommended to use SSL (e.g. setting index.[X].elasticsearch.ssl.enabled to true ) when using this option as the credentials can be intercepted when sent over an unencrypted connection!","title":"REST Client Basic HTTP Authentication"},{"location":"index-backend/elasticsearch/#rest-client-custom-http-authentication","text":"Additional authentication methods can be implemented by providing your own implementation. The custom authenticator is configured as follows: index.search.elasticsearch.http.auth.type = custom index.search.elasticsearch.http.auth.custom.authenticator-class = fully.qualified.class.Name index.search.elasticsearch.http.auth.custom.authenticator-args = arg1,arg2,... Argument list is optional and can be empty. The class specified there has to implement the org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticator interface or extend org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticatorBase convenience class. The implementation gets access to HTTP client configuration and can customize the client as needed. Refer to < > for more information. For example, the following code snippet implements an authenticator allowing the Elasticsearch REST client to authenticate and get authorized against AWS IAM: import java.io.IOException ; import java.time.LocalDateTime ; import java.time.ZoneOffset ; import org.apache.http.HttpRequestInterceptor ; import org.apache.http.impl.nio.client.HttpAsyncClientBuilder ; import org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticatorBase ; import com.amazonaws.auth.DefaultAWSCredentialsProviderChain ; import com.amazonaws.regions.DefaultAwsRegionProviderChain ; import com.google.common.base.Supplier ; import vc.inreach.aws.request.AWSSigner ; import vc.inreach.aws.request.AWSSigningRequestInterceptor ; /** * <p> * Elasticsearch REST HTTP(S) client callback implementing AWS request signing. * </p> * <p> * The signer is based on AWS SDK default provider chain, allowing multiple options for providing * the caller credentials. See {@link DefaultAWSCredentialsProviderChain} documentation for the details. * </p> */ public class AWSV4AuthHttpClientConfigCallback extends RestClientAuthenticatorBase { private static final String AWS_SERVICE_NAME = \"es\" ; private HttpRequestInterceptor awsSigningInterceptor ; public AWSV4AuthHttpClientConfigCallback ( final String [] args ) { // does not require any configuration } @Override public void init () throws IOException { DefaultAWSCredentialsProviderChain awsCredentialsProvider = new DefaultAWSCredentialsProviderChain (); final Supplier < LocalDateTime > clock = () -> LocalDateTime . now ( ZoneOffset . UTC ); // using default region provider chain // (https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/java-dg-region-selection.html) DefaultAwsRegionProviderChain regionProviderChain = new DefaultAwsRegionProviderChain (); final String awsRegion = regionProviderChain . getRegion (); final AWSSigner awsSigner = new AWSSigner ( awsCredentialsProvider , awsRegion , AWS_SERVICE_NAME , clock ); this . awsSigningInterceptor = new AWSSigningRequestInterceptor ( awsSigner ); } @Override public HttpAsyncClientBuilder customizeHttpClient ( HttpAsyncClientBuilder httpClientBuilder ) { return httpClientBuilder . addInterceptorLast ( awsSigningInterceptor ); / } } This custom authenticator does not use any constructor arguments.","title":"REST Client Custom HTTP Authentication"},{"location":"index-backend/elasticsearch/#ingest-pipelines","text":"Different ingest pipelines can be set for each mixed index. Ingest pipeline can be use to pre-process documents before indexing. A pipeline is composed by a series of processors. Each processor transforms the document in some way. For example date processor can extract a date from a text to a date field. So you can query this date with JanusGraph without it being physically in the primary storage. index.[X].elasticsearch.ingest-pipeline.[mixedIndexName] = pipeline_id See ingest documentation for more information about ingest pipelines and processors documentation for more information about ingest processors.","title":"Ingest Pipelines"},{"location":"index-backend/elasticsearch/#secure-elasticsearch","text":"Elasticsearch does not perform authentication or authorization. A client that can connect to Elasticsearch is trusted by Elasticsearch. When Elasticsearch runs on an unsecured or public network, particularly the Internet, it should be deployed with some type of external security. This is generally done with a combination of firewalling, tunneling of Elasticsearch\u2019s ports or by using Elasticsearch extensions such as X-Pack . Elasticsearch has two client-facing ports to consider: The HTTP REST API, usually on port 9200 The native \"transport\" protocol, usually on port 9300 A client uses either one protocol/port or the other, but not both simultaneously. Securing the HTTP protocol port is generally done with a combination of firewalling and a reverse proxy with SSL encryption and HTTP authentication. There are a couple of ways to approach security on the native \"transport\" protocol port: In addition to that, some hosted Elasticsearch services offer other methods of authentication and authorization. For example, AWS Elasticsearch Service requires the use of HTTPS and offers an option for using IAM-based access control. For that the requests sent to this service must be signed. This can be achieved by using a custom authenticator (see above). Tunnel Elasticsearch's native \"transport\" protocol:: This approach can be implemented with SSL/TLS tunneling (for instance via stunnel ), a VPN, or SSH port forwarding. SSL/TLS tunnels require non-trivial setup and monitoring: one or both ends of the tunnel need a certificate, and the stunnel processes need to be configured and running continuously. The setup for most secure VPNs is likewise non-trivial. Some Elasticsearch service providers handle server-side tunnel management and provide a custom Elasticsearch transport.type to simplify the client setup. Add a firewall rule that allows only trusted clients to connect on Elasticsearch\u2019s native protocol port This is typically done at the host firewall level. Easy to configure, but very weak security by itself.","title":"Secure Elasticsearch"},{"location":"index-backend/elasticsearch/#index-creation-options","text":"JanusGraph supports customization of the index settings it uses when creating its Elasticsearch index. It allows setting arbitrary key-value pairs on the settings object in the Elasticsearch create index request issued by JanusGraph. Here is a non-exhaustive sample of Elasticsearch index settings that can be customized using this mechanism: index.number_of_replicas index.number_of_shards index.refresh_interval Settings customized through this mechanism are only applied when JanusGraph attempts to create its index in Elasticsearch. If JanusGraph finds that its index already exists, then it does not attempt to recreate it, and these settings have no effect.","title":"Index Creation Options"},{"location":"index-backend/elasticsearch/#embedding-elasticsearch-index-creation-settings-with-createext","text":"JanusGraph iterates over all properties prefixed with index.[X].elasticsearch.create.ext. , where [X] is an index name such as search . It strips the prefix from each property key. The remainder of the stripped key will be interpreted as an Elasticsearch index creation setting. The value associated with the key is not modified. The stripped key and unmodified value are passed as part of the settings object in the Elasticsearch create index request that JanusGraph issues when bootstrapping on Elasticsearch. This allows embedding arbitrary index creation settings settings in JanusGraph\u2019s properties. Here\u2019s an example configuration fragment that customizes three Elasticsearch index settings using the create.ext config mechanism: index.search.backend = elasticsearch index.search.elasticsearch.create.ext.number_of_shards = 15 index.search.elasticsearch.create.ext.number_of_replicas = 3 index.search.elasticsearch.create.ext.shard.check_on_startup = true The configuration fragment listed above takes advantage of Elasticsearch\u2019s assumption, implemented server-side, that unqualified create index setting keys have an index. prefix. It\u2019s also possible to spell out the index prefix explicitly. Here\u2019s a JanusGraph config file functionally equivalent to the one listed above, except that the index. prefix before the index creation settings is explicit: index.search.backend = elasticsearch index.search.elasticsearch.create.ext.index.number_of_shards = 15 index.search.elasticsearch.create.ext.index.number_of_replicas = 3 index.search.elasticsearch.create.ext.index.shard.check_on_startup = false Tip The create.ext mechanism for specifying index creation settings is compatible with JanusGraph\u2019s Elasticsearch configuration.","title":"Embedding Elasticsearch index creation settings with create.ext"},{"location":"index-backend/elasticsearch/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"index-backend/elasticsearch/#connection-issues-to-remote-elasticsearch-cluster","text":"Check that the Elasticsearch cluster nodes are reachable on the HTTP protocol port from the JanusGraph nodes. Check the node listen port by examining the Elasticsearch node configuration logs or using a general diagnostic utility like netstat . Check the JanusGraph configuration.","title":"Connection Issues to remote Elasticsearch cluster"},{"location":"index-backend/elasticsearch/#optimizing-elasticsearch","text":"","title":"Optimizing Elasticsearch"},{"location":"index-backend/elasticsearch/#write-optimization","text":"For bulk loading or other write-intense applications, consider increasing Elasticsearch\u2019s refresh interval. Refer to this discussion on how to increase the refresh interval and its impact on write performance. Note, that a higher refresh interval means that it takes a longer time for graph mutations to be available in the index. For additional suggestions on how to increase write performance in Elasticsearch with detailed instructions, please read this blog post .","title":"Write Optimization"},{"location":"index-backend/elasticsearch/#further-reading","text":"Please refer to the Elasticsearch homepage and available documentation for more information on Elasticsearch and how to setup an Elasticsearch cluster.","title":"Further Reading"},{"location":"index-backend/field-mapping/","text":"Field Mapping Individual Field Mapping By default, JanusGraph will encode property keys to generate a unique field name for the property key in the mixed index. If one wants to query the mixed index directly in the external index backend can be difficult to deal with and are illegible. For this use case, the field name can be explicitly specified through a parameter. mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'bookname' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( name , Parameter . of ( 'mapped-name' , 'bookname' )). buildMixedIndex ( \"search\" ) mgmt . commit () With this field mapping defined as a parameter, JanusGraph will use the same name for the field in the booksBySummary index created in the external index system as for the property key. Note, that it must be ensured that the given field name is unique in the index. Global Field Mapping Instead of individually adjusting the field mapping for every key added to a mixed index, one can instruct JanusGraph to always set the field name in the external index to be identical to the property key name. This is accomplished by enabling the configuration option map-name which is configured per indexing backend. If this option is enabled for a particular indexing backend, then all mixed indexes defined against said backend will use field names identical to the property key names. However, this approach has two limitations: 1) The user has to ensure that the property key names are valid field names for the indexing backend and 2) renaming the property key will NOT rename the field name in the index which can lead to naming collisions that the user has to be aware of and avoid. Note, that individual field mappings as described above can be used to overwrite the default name for a particular key. Custom Analyzer By default, JanusGraph will use the default analyzer from the indexing backend for properties with Mapping.TEXT, and no analyzer for properties with Mapping.STRING. If one wants to use another analyzer, it can be explicitly specified through a parameter : ParameterType.TEXT_ANALYZER for Mapping.TEXT and ParameterType.STRING_ANALYZER for Mapping.STRING. For Elasticsearch The name of the analyzer must be set as parameter value. mgmt = graph . openManagement () string = mgmt . makePropertyKey ( 'string' ). dataType ( String . class ). make () text = mgmt . makePropertyKey ( 'text' ). dataType ( String . class ). make () textString = mgmt . makePropertyKey ( 'textString' ). dataType ( String . class ). make () mgmt . buildIndex ( 'string' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), 'standard' )). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( 'text' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), 'english' )). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( 'textString' , Vertex . class ). addKey ( text , Mapping . TEXTSTRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), 'standard' ), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), 'english' )). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use the standard analyzer for property key string and the english analyzer for property key text . For Solr The class of the tokenizer must be set as parameter value. mgmt = graph . openManagement () string = mgmt . makePropertyKey ( ' string ' ). dataType ( String . class ). make () text = mgmt . makePropertyKey ( ' text ' ). dataType ( String . class ). make () mgmt . buildIndex ( ' string ' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), ' org . apache . lucene . analysis . standard . StandardTokenizer ' )). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' text ' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), ' org . apache . lucene . analysis . core . WhitespaceTokenizer ' )). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use the standard tokenizer for property key string and the whitespace tokenizer for property key text . For Lucene The name of the analyzer must be set as parameter value or it defaults to KeywordAnalyzer for Mapping.STRING and to StandardAnalyzer for Mapping.TEXT . mgmt = graph . openManagement () string = mgmt . makePropertyKey ( ' string ' ). dataType ( String . class ). make () text = mgmt . makePropertyKey ( ' text ' ). dataType ( String . class ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). make () document = mgmt . makePropertyKey ( ' document ' ). dataType ( String . class ). make () mgmt . buildIndex ( ' string ' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), org . apache . lucene . analysis . core . SimpleAnalyzer . class . getName ())). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' text ' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), org . apache . lucene . analysis . en . EnglishAnalyzer . class . getName ())). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' name ' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' document ' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use a SimpleAnalyzer analyzer for property key string , an EnglishAnalyzer analyzer for property key text , a KeywordAnalyzer analyzer for property name and a StandardAnalyzer analyzer for property 'document'. Custom parameters Sometimes it is required to set additional parameters on mappings (other than mapping type, mapping name and analyzer). For example, when we would like to use a different similarity algorithm (to modify the scoring algorithm of full text search) or if we want to use a custom boosting on some fields in Elasticsearch we can set custom parameters (right now only Elasticsearch supports custom parameters). The name of the custom parameter must be set through ParameterType.customParameterName(\"yourProperty\") . For Elasticsearch mgmt = graph . openManagement () myProperty = mgmt . makePropertyKey ( ' my_property ' ). dataType ( String . class ). make () mgmt . buildIndex ( ' custom_property_test ' , Vertex . class ). addKey ( myProperty , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . customParameterName ( \"boost\" ), 5 ), Parameter . of ( ParameterType . customParameterName ( \"similarity\" ), \"boolean\" )). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use the boost 5 and boolean similarity algorithm for property key my_property . Possible mapping parameters depend on Elasticsearch version. See mapping parameters for current Elasticsearch version.","title":"Field Mapping"},{"location":"index-backend/field-mapping/#field-mapping","text":"","title":"Field Mapping"},{"location":"index-backend/field-mapping/#individual-field-mapping","text":"By default, JanusGraph will encode property keys to generate a unique field name for the property key in the mixed index. If one wants to query the mixed index directly in the external index backend can be difficult to deal with and are illegible. For this use case, the field name can be explicitly specified through a parameter. mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'bookname' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( name , Parameter . of ( 'mapped-name' , 'bookname' )). buildMixedIndex ( \"search\" ) mgmt . commit () With this field mapping defined as a parameter, JanusGraph will use the same name for the field in the booksBySummary index created in the external index system as for the property key. Note, that it must be ensured that the given field name is unique in the index.","title":"Individual Field Mapping"},{"location":"index-backend/field-mapping/#global-field-mapping","text":"Instead of individually adjusting the field mapping for every key added to a mixed index, one can instruct JanusGraph to always set the field name in the external index to be identical to the property key name. This is accomplished by enabling the configuration option map-name which is configured per indexing backend. If this option is enabled for a particular indexing backend, then all mixed indexes defined against said backend will use field names identical to the property key names. However, this approach has two limitations: 1) The user has to ensure that the property key names are valid field names for the indexing backend and 2) renaming the property key will NOT rename the field name in the index which can lead to naming collisions that the user has to be aware of and avoid. Note, that individual field mappings as described above can be used to overwrite the default name for a particular key.","title":"Global Field Mapping"},{"location":"index-backend/field-mapping/#custom-analyzer","text":"By default, JanusGraph will use the default analyzer from the indexing backend for properties with Mapping.TEXT, and no analyzer for properties with Mapping.STRING. If one wants to use another analyzer, it can be explicitly specified through a parameter : ParameterType.TEXT_ANALYZER for Mapping.TEXT and ParameterType.STRING_ANALYZER for Mapping.STRING.","title":"Custom Analyzer"},{"location":"index-backend/field-mapping/#for-elasticsearch","text":"The name of the analyzer must be set as parameter value. mgmt = graph . openManagement () string = mgmt . makePropertyKey ( 'string' ). dataType ( String . class ). make () text = mgmt . makePropertyKey ( 'text' ). dataType ( String . class ). make () textString = mgmt . makePropertyKey ( 'textString' ). dataType ( String . class ). make () mgmt . buildIndex ( 'string' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), 'standard' )). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( 'text' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), 'english' )). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( 'textString' , Vertex . class ). addKey ( text , Mapping . TEXTSTRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), 'standard' ), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), 'english' )). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use the standard analyzer for property key string and the english analyzer for property key text .","title":"For Elasticsearch"},{"location":"index-backend/field-mapping/#for-solr","text":"The class of the tokenizer must be set as parameter value. mgmt = graph . openManagement () string = mgmt . makePropertyKey ( ' string ' ). dataType ( String . class ). make () text = mgmt . makePropertyKey ( ' text ' ). dataType ( String . class ). make () mgmt . buildIndex ( ' string ' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), ' org . apache . lucene . analysis . standard . StandardTokenizer ' )). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' text ' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), ' org . apache . lucene . analysis . core . WhitespaceTokenizer ' )). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use the standard tokenizer for property key string and the whitespace tokenizer for property key text .","title":"For Solr"},{"location":"index-backend/field-mapping/#for-lucene","text":"The name of the analyzer must be set as parameter value or it defaults to KeywordAnalyzer for Mapping.STRING and to StandardAnalyzer for Mapping.TEXT . mgmt = graph . openManagement () string = mgmt . makePropertyKey ( ' string ' ). dataType ( String . class ). make () text = mgmt . makePropertyKey ( ' text ' ). dataType ( String . class ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). make () document = mgmt . makePropertyKey ( ' document ' ). dataType ( String . class ). make () mgmt . buildIndex ( ' string ' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter (), Parameter . of ( ParameterType . STRING_ANALYZER . getName (), org . apache . lucene . analysis . core . SimpleAnalyzer . class . getName ())). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' text ' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . TEXT_ANALYZER . getName (), org . apache . lucene . analysis . en . EnglishAnalyzer . class . getName ())). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' name ' , Vertex . class ). addKey ( string , Mapping . STRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . buildIndex ( ' document ' , Vertex . class ). addKey ( text , Mapping . TEXT . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use a SimpleAnalyzer analyzer for property key string , an EnglishAnalyzer analyzer for property key text , a KeywordAnalyzer analyzer for property name and a StandardAnalyzer analyzer for property 'document'.","title":"For Lucene"},{"location":"index-backend/field-mapping/#custom-parameters","text":"Sometimes it is required to set additional parameters on mappings (other than mapping type, mapping name and analyzer). For example, when we would like to use a different similarity algorithm (to modify the scoring algorithm of full text search) or if we want to use a custom boosting on some fields in Elasticsearch we can set custom parameters (right now only Elasticsearch supports custom parameters). The name of the custom parameter must be set through ParameterType.customParameterName(\"yourProperty\") .","title":"Custom parameters"},{"location":"index-backend/field-mapping/#for-elasticsearch_1","text":"mgmt = graph . openManagement () myProperty = mgmt . makePropertyKey ( ' my_property ' ). dataType ( String . class ). make () mgmt . buildIndex ( ' custom_property_test ' , Vertex . class ). addKey ( myProperty , Mapping . TEXT . asParameter (), Parameter . of ( ParameterType . customParameterName ( \"boost\" ), 5 ), Parameter . of ( ParameterType . customParameterName ( \"similarity\" ), \"boolean\" )). buildMixedIndex ( \"search\" ) mgmt . commit () With these settings, JanusGraph will use the boost 5 and boolean similarity algorithm for property key my_property . Possible mapping parameters depend on Elasticsearch version. See mapping parameters for current Elasticsearch version.","title":"For Elasticsearch"},{"location":"index-backend/lucene/","text":"Apache Lucene Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform. Apache Lucene is an open source project available for free download. \u2014 Apache Lucene Homepage JanusGraph supports Apache Lucene as a single-machine, embedded index backend. Lucene has a slightly extended feature set and performs better in small-scale applications compared to Elasticsearch , but is limited to single-machine deployments. Lucene Embedded Configuration For single machine deployments, Lucene runs embedded with JanusGraph. JanusGraph starts and interfaces with Lucene internally. To run Lucene embedded, add the following configuration options to the graph configuration file where /data/searchindex specifies the directory where Lucene should store the index data: index.search.backend = lucene index.search.directory = /data/searchindex In the above configuration, the index backend is named search . Replace search by a different name to change the name of the index. Feature Support Full-Text : Supports all Text predicates to search for text properties that matches a given word, prefix or regular expression. Geo : Supports Geo predicates to search for geo properties that are intersecting, within, or contained in a given query geometry. Supports points, lines and polygons for indexing. Supports circles and boxes for querying point properties and all shapes for querying non-point properties. Numeric Range : Supports all numeric comparisons in Compare . Collections : Supports indexing SET and LIST cardinality properties, except Geo . Temporal : Nanosecond granularity temporal indexing. Custom Analyzer : Choose to use a custom analyzer Not Query-normal-form : Supports queries other than Query-normal-form (QNF). QNF for JanusGraph is a variant of CNF (conjunctive normal form) with negation inlined where possible. Configuration Options Refer to Configuration Reference for a complete listing of all Lucene specific configuration options in addition to the general JanusGraph configuration options. Note, that each of the index backend options needs to be prefixed with index.[INDEX-NAME]. where [INDEX-NAME] stands for the name of the index backend. For instance, if the index backend is named search then these configuration options need to be prefixed with index.search. . To configure an index backend named search to use Lucene as the index system, set the following configuration option: index.search.backend = lucene Further Reading Please refer to the Apache Lucene homepage and available documentation for more information on Lucene.","title":"Apache Lucene"},{"location":"index-backend/lucene/#apache-lucene","text":"Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform. Apache Lucene is an open source project available for free download. \u2014 Apache Lucene Homepage JanusGraph supports Apache Lucene as a single-machine, embedded index backend. Lucene has a slightly extended feature set and performs better in small-scale applications compared to Elasticsearch , but is limited to single-machine deployments.","title":"Apache Lucene"},{"location":"index-backend/lucene/#lucene-embedded-configuration","text":"For single machine deployments, Lucene runs embedded with JanusGraph. JanusGraph starts and interfaces with Lucene internally. To run Lucene embedded, add the following configuration options to the graph configuration file where /data/searchindex specifies the directory where Lucene should store the index data: index.search.backend = lucene index.search.directory = /data/searchindex In the above configuration, the index backend is named search . Replace search by a different name to change the name of the index.","title":"Lucene Embedded Configuration"},{"location":"index-backend/lucene/#feature-support","text":"Full-Text : Supports all Text predicates to search for text properties that matches a given word, prefix or regular expression. Geo : Supports Geo predicates to search for geo properties that are intersecting, within, or contained in a given query geometry. Supports points, lines and polygons for indexing. Supports circles and boxes for querying point properties and all shapes for querying non-point properties. Numeric Range : Supports all numeric comparisons in Compare . Collections : Supports indexing SET and LIST cardinality properties, except Geo . Temporal : Nanosecond granularity temporal indexing. Custom Analyzer : Choose to use a custom analyzer Not Query-normal-form : Supports queries other than Query-normal-form (QNF). QNF for JanusGraph is a variant of CNF (conjunctive normal form) with negation inlined where possible.","title":"Feature Support"},{"location":"index-backend/lucene/#configuration-options","text":"Refer to Configuration Reference for a complete listing of all Lucene specific configuration options in addition to the general JanusGraph configuration options. Note, that each of the index backend options needs to be prefixed with index.[INDEX-NAME]. where [INDEX-NAME] stands for the name of the index backend. For instance, if the index backend is named search then these configuration options need to be prefixed with index.search. . To configure an index backend named search to use Lucene as the index system, set the following configuration option: index.search.backend = lucene","title":"Configuration Options"},{"location":"index-backend/lucene/#further-reading","text":"Please refer to the Apache Lucene homepage and available documentation for more information on Lucene.","title":"Further Reading"},{"location":"index-backend/solr/","text":"Apache Solr Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more. Solr powers the search and navigation features of many of the world's largest internet sites. \u2014 Apache Solr Homepage JanusGraph supports Apache Solr as an index backend. Here are some of the Solr features supported by JanusGraph: Full-Text : Supports all Text predicates to search for text properties that matches a given word, prefix or regular expression. Geo : Supports all Geo predicates to search for geo properties that are intersecting, within, disjoint to or contained in a given query geometry. Supports points, lines and polygons for indexing. Supports circles, boxes and polygons for querying point properties and all shapes for querying non-point properties. Numeric Range : Supports all numeric comparisons in Compare . TTL : Supports automatically expiring indexed elements. Temporal : Millisecond granularity temporal indexing. Custom Analyzer : Choose to use a custom analyzer Please see Version Compatibility for details on what versions of Solr will work with JanusGraph. Solr Configuration Overview JanusGraph supports Solr running in either a SolrCloud or Solr Standalone (HTTP) configuration for use with a mixed index (see Mixed Index ). The desired connection mode is configured via the parameter mode which must be set to either cloud or http , the former being the default value. For example, to explicitly specify that Solr is running in a SolrCloud configuration the following property is specified as a JanusGraph configuration property: index.search.solr.mode = cloud These are some key Solr terms: Core : A single index on a single machine Configuration : solrconfig.xml , schema.xml , and other files required to define a core. Collection : A single logical index that can span multiple cores on different machines. Configset : A shared configuration that can be reused by multiple cores. Connecting to SolrCloud When connecting to a SolrCloud cluster by setting the mode equal to cloud , the Zookeeper URL (and optionally port) must be specified so that JanusGraph can discover and interact with the Solr cluster. index.search.backend = solr index.search.solr.mode = cloud index.search.solr.zookeeper-url = localhost:2181 A number of additional configuration options pertaining to the creation of new collections (which is only supported in SolrCloud operation mode) can be configured to control sharding behavior among other things. Refer to the Configuration Reference for a complete listing of those options. SolrCloud leverages Zookeeper to coordinate collection and configset information between the Solr servers. The use of Zookeeper with SolrCloud provides the opportunity to significantly reduce the amount of manual configuration required to use Solr as a back end index for JanusGraph. Configset Configuration A configset is required to create a collection. The configset is stored in Zookeeper to enable access to it across the Solr servers. Each collection can provide its own configset when it is created, so that each collection may have a different configuration. With this approach, each collection must be created manually. A shared configset can be uploaded separately to Zookeeper if it will be reused by multiple collections. With this approach, JanusGraph can create collections automatically by using the shared configset. Another benefit is that reusing a configset significantly reduces the amount of data stored in Zookeeper. Using an Individual Configset In this example, a collection named verticesByAge is created manually using the default JanusGraph configuration for Solr that is found in the distribution. When the collection is created, the configuration is uploaded into Zookeeper, using the same collection name verticesByAge for the configset name. Refer to the Solr Reference Guide for available parameters. # create the collection $SOLR_HOME /bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME /conf/solr Define a mixed index using JanusGraphManagement and the same collection name. mgmt = graph . openManagement () age = mgmt . makePropertyKey ( \"age\" ). dataType ( Integer . class ). make () mgmt . buildIndex ( \"verticesByAge\" , Vertex . class ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit () Using a Shared Configset When using a shared configset, it is most convenient to upload the configuration first as a one time operation. In this example, a configset named janusgraph-configset is uploaded in to Zookeeper using the default JanusGraph configuration for Solr that is found in the distribution. Refer to the Solr Reference Guide for available parameters. # upload the shared configset into Zookeeper # Solr 5 $SOLR_HOME /server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -z localhost:2181 \\ -d $JANUSGRAPH_HOME /conf/solr -n janusgraph-configset # Solr 6 and higher $SOLR_HOME /bin/solr zk upconfig -d $JANUSGRAPH_HOME /conf/solr -n janusgraph-configset \\ -z localhost:2181 When configuring the SolrCloud indexing backend for JanusGraph, make sure to provide the name of the shared configset using the index.search.solr.configset property. index.search.backend = solr index.search.solr.mode = cloud index.search.solr.zookeeper-url = localhost:2181 index.search.solr.configset = janusgraph-configset Define a mixed index using JanusGraphManagement and the collection name. mgmt = graph . openManagement () age = mgmt . makePropertyKey ( \"age\" ). dataType ( Integer . class ). make () mgmt . buildIndex ( \"verticesByAge\" , Vertex . class ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit () Connecting to Solr Standalone (HTTP) When connecting to Solr Standalone via HTTP by setting the mode equal to http , a single or list of URLs for the Solr instances must be provided. index.search.backend = solr index.search.solr.mode = http index.search.solr.http-urls = http://localhost:8983/solr Additional configuration options for controlling the maximum number of connections, connection timeout and transmission compression are available for the HTTP mode. Refer to the Configuration Reference for a complete listing of those options. Core Configuration Solr Standalone is used for a single instance, and it keeps configuration information on the file system. A core must be created manually for each mixed index. To create a core, a core_name and a configuration directory is required. Refer to the Solr Reference Guide for available parameters. In this example, a core named verticesByAge is created using the default JanusGraph configuration for Solr that is found in the distribution. $SOLR_HOME /bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME /conf/solr Define a mixed index using JanusGraphManagement and the same core name. mgmt = graph . openManagement () age = mgmt . makePropertyKey ( \"age\" ). dataType ( Integer . class ). make () mgmt . buildIndex ( \"verticesByAge\" , Vertex . class ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit () Kerberos Configuration When connecting to a Solr environment that is protected by Kerberos we must specify that Kerberos is being used and reference a JAAS configuration file to properly configure the Solr Clients. This configuration is required when Kerberos is in use regardless of the mode in which Solr is operating (SolrCloud or Solr Standalone). index.search.solr.kerberos-enabled = true The JAAS configuration file is supplied by ensuring that you set the java system property java.security.auth.login.config with the absolute path to the file. This property should be set using JVM options. For example to run gremlin.sh you would need to set the JAVA_OPTIONS environment variable prior to running the script: export JAVA_OPTIONS = \"-Djava.security.auth.login.config=/absolute/path/jaas.conf\" $JANUSGRAPH_HOME /bin/gremlin.sh For details on the content required in the JAAS configuration file refer to the https://lucene.apache.org/solr/guide/7_0/kerberos-authentication-plugin.html#define-a-jaas-configuration-file[Solr Reference Guide]. Solr Schema Design Dynamic Field Definition By default, JanusGraph uses Solr\u2019s Dynamic Fields feature to define the field types for all indexed keys. This requires no extra configuration when adding property keys to a mixed index backed by Solr and provides better performance than schemaless mode. JanusGraph assumes the following dynamic field tags are defined in the backing Solr collection\u2019s schema.xml file. Please note that there is additional xml definition of the following fields required in a solr schema.xml file in order to use them. Reference the example schema.xml file provided in the ./conf/solr/schema.xml directory in a JanusGraph installation for more information. <dynamicField name= \"*_i\" type= \"int\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_s\" type= \"string\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_l\" type= \"long\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_t\" type= \"text_general\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_b\" type= \"boolean\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_f\" type= \"float\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_d\" type= \"double\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_g\" type= \"geo\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_dt\" type= \"date\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_uuid\" type= \"uuid\" indexed= \"true\" stored= \"true\" /> In JanusGraph\u2019s default configuration, property key names do not have to end with the type-appropriate suffix to take advantage of Solr\u2019s dynamic field feature. JanusGraph generates the Solr field name from the property key name by encoding the property key definition\u2019s numeric identifier and the type-appropriate suffix. This means that JanusGraph uses synthetic field names with type-appropriate suffixes behind the scenes, regardless of the property key names defined and used by application code using JanusGraph. This field name mapping can be overridden through non-default configuration. That\u2019s described in the next section. Manual Field Definition If the user would rather manually define the field types for each of the indexed fields in a collection, the configuration option dyn-fields needs to be disabled. It is important that the field for each indexed property key is defined in the backing Solr schema before the property key is added to the index. In this scenario, it is advisable to enable explicit property key name to field mapping in order to fix the field names for their explicit definition. This can be achieved in one of two ways: Configuring the name of the field by providing a mapped-name parameter when adding the property key to the index. See Individual Field Mapping for more information. By enabling the map-name configuration option for the Solr index which will use the property key name as the field name in Solr. See Global Field Mapping for more information. Schemaless Mode JanusGraph can also interact with a SolrCloud cluster that is configured for schemaless mode . In this scenario, the configuration option dyn-fields should be disabled since Solr will infer the field type from the values and not the field name. Note, however, that schemaless mode is recommended only for prototyping and initial application development and NOT recommended for production use. Troubleshooting Collection Does Not Exist The collection (and all of the required configuration files) must be initialized before a defined index can use the collection. See Connecting to SolrCloud for more information. When using SolrCloud, the Zookeeper zkCli.sh command line tool can be used to inspect the configurations loaded into Zookeeper. Also verify that the default JanusGraph configuration files are copied to the correct location under solr and that the directory where the files are copied is correct. Cannot Find the Specified Configset When using SolrCloud, a configset is required to create a mixed index for JanusGraph. See Configset Configuration for more information. If using an individual configset, the collection must be created manually first. If using a shared configset, the configset must be uploaded into Zookeeper first. You can verify that the configset and its configuration files are in Zookeeper under /configs . Refer to the Solr Reference Guide for other Zookeeper operations. # verify the configset in Zookeeper # Solr 5 $SOLR_HOME /server/scripts/cloud-scripts/zkcli.sh -cmd list -z localhost:2181 # Solr 6 and higher $SOLR_HOME /bin/solr zk ls -r /configs/configset-name -z localhost:2181 HTTP Error 404 This error may be encountered when using Solr Standalone (HTTP) mode. An example of the error: 20:01:22 ERROR org.janusgraph.diskstorage.solr.SolrIndex - Unable to save documents to Solr as one of the shape objects stored were not compatible with Solr. org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://localhost:8983/solr: Expected mime type application/octet-stream but got text/html. <html> <head> <meta http-equiv= \"Content-Type\" content= \"text/html;charset=utf-8\" /> <title> Error 404 Not Found </title> </head> <body><h2> HTTP ERROR 404 </h2> <p> Problem accessing /solr/verticesByAge/update. Reason: <pre> Not Found </pre></p> </body> </html> Make sure to create the core manually before attempting to store data into the index. See Core Configuration for more information. Invalid core or collection name The core or collection name is an identifier. It must consist entirely of periods, underscores, hyphens, and/or alphanumerics, and also it may not start with a hyphen. Connection Problems Irrespective of the operation mode, a Solr instance or a cluster of Solr instances must be running and accessible from the JanusGraph instance(s) in order for JanusGraph to use Solr as an indexing backend. Check that the Solr cluster is running correctly and that it is visible and accessible over the network (or locally) from the JanusGraph instances. JTS ClassNotFoundException with Geo Data Solr relies on Spatial4j for geo processing. Spatial4j declares an optional dependency on JTS (\"JTS Topology Suite\"). JTS is required for some geo field definition and query functionality. If the JTS jar is not on the Solr daemon\u2019s classpath and a field in schema.xml uses a geo type, then Solr may throw a ClassNotFoundException on one of the missing JTS classes. The exception can appear when starting Solr using a schema.xml file designed to work with JanusGraph, but can also appear when invoking CREATE in the Solr CoreAdmin API . The exception appears in slightly different formats on the client and server sides, although the root cause is identical. Here\u2019s a representative example from a Solr server log: ERROR [ http - 8983 - exec - 5 ] 2014 - 10 - 07 02 : 54 : 06 , 665 SolrCoreResourceManager . java ( line 344 ) com / vividsolutions / jts / geom / Geometry java . lang . NoClassDefFoundError : com / vividsolutions / jts / geom / Geometry at com . spatial4j . core . context . jts . JtsSpatialContextFactory . newSpatialContext ( JtsSpatialContextFactory . java : 30 ) at com . spatial4j . core . context . SpatialContextFactory . makeSpatialContext ( SpatialContextFactory . java : 83 ) at org . apache . solr . schema . AbstractSpatialFieldType . init ( AbstractSpatialFieldType . java : 95 ) at org . apache . solr . schema . AbstractSpatialPrefixTreeFieldType . init ( AbstractSpatialPrefixTreeFieldType . java : 43 ) at org . apache . solr . schema . SpatialRecursivePrefixTreeFieldType . init ( SpatialRecursivePrefixTreeFieldType . java : 37 ) at org . apache . solr . schema . FieldType . setArgs ( FieldType . java : 164 ) at org . apache . solr . schema . FieldTypePluginLoader . init ( FieldTypePluginLoader . java : 141 ) at org . apache . solr . schema . FieldTypePluginLoader . init ( FieldTypePluginLoader . java : 43 ) at org . apache . solr . util . plugin . AbstractPluginLoader . load ( AbstractPluginLoader . java : 190 ) at org . apache . solr . schema . IndexSchema . readSchema ( IndexSchema . java : 470 ) at com . datastax . bdp . search . solr . CassandraIndexSchema . readSchema ( CassandraIndexSchema . java : 72 ) at org . apache . solr . schema . IndexSchema . < init > ( IndexSchema . java : 168 ) at com . datastax . bdp . search . solr . CassandraIndexSchema . < init > ( CassandraIndexSchema . java : 54 ) at com . datastax . bdp . search . solr . core . CassandraCoreContainer . create ( CassandraCoreContainer . java : 210 ) at com . datastax . bdp . search . solr . core . SolrCoreResourceManager . createCore ( SolrCoreResourceManager . java : 256 ) at com . datastax . bdp . search . solr . handler . admin . CassandraCoreAdminHandler . handleCreateAction ( CassandraCoreAdminHandler . java : 117 ) ... Here\u2019s what normally appears in the output of the client that issued the associated CREATE command to the CoreAdmin API: org . apache . solr . common . SolrException : com / vividsolutions / jts / geom / Geometry at com . datastax . bdp . search . solr . core . SolrCoreResourceManager . createCore ( SolrCoreResourceManager . java : 345 ) at com . datastax . bdp . search . solr . handler . admin . CassandraCoreAdminHandler . handleCreateAction ( CassandraCoreAdminHandler . java : 117 ) at org . apache . solr . handler . admin . CoreAdminHandler . handleRequestBody ( CoreAdminHandler . java : 152 ) ... This is resolved by adding the JTS jar to the classpath of JanusGraph and/or the Solr server. JTS is not included in JanusGraph distributions by default due to its LGPL license. Users must download the JTS jar file separately and copy it into the JanusGraph and/or Solr server lib directory. If using Solr\u2019s built in web server, the JTS jar may be copied to the example/solr-webapp/webapp/WEB-INF/lib directory to include it in the classpath. Solr can be restarted, and the exception should be gone. Solr must be started once with the correct schema.xml file in place first, for the example/solr-webapp/webapp/WEB-INF/lib directory to exist. To determine the ideal JTS version for Solr server, first check the version of Spatial4j in use by the Solr cluster, then determine the version of JTS against which that Spatial4j version was compiled. Spatial4j declares its target JTS version in the pom for the com.spatial4j:spatial4j artifact . Copy the JTS jar to the server/solr-webapp/webapp/WEB-INF/lib directory in your solr installation. Advanced Solr Configuration DSE Search This section covers installation and configuration of JanusGraph with DataStax Enterprise (DSE) Search. There are multiple ways to install DSE, but this section focuses on DSE\u2019s binary tarball install option on Linux. Most of the steps in this section can be generalized to the other install options for DSE. Install DataStax Enterprise as directed by the page Installing DataStax Enterprise using the binary tarball . Export DSE_HOME and append to PATH in your shell environment. Here\u2019s an example using Bash syntax: export DSE_HOME = /path/to/dse-version.number export PATH = \" $DSE_HOME \" /bin: \" $PATH \" Install JTS for Solr. The appropriate version varies with the Spatial4j version. As of DSE 4.5.2, the appropriate version is 1.13. cd $DSE_HOME /resources/solr/lib curl -O 'http://central.maven.org/maven2/com/vividsolutions/jts/1.13/jts-1.13.jar' Start DSE Cassandra and Solr in a single background daemon: # The \"dse-data\" path below was chosen to match the # \"Installing DataStax Enterprise using the binary tarball\" # documentation page from DataStax. The exact path is not # significant. dse cassandra -s -Ddse.solr.data.dir = \" $DSE_HOME \" /dse-data/solr The previous command will write some startup information to the console and to the logfile path log4j.appender.R.File configured in $DSE_HOME/resources/cassandra/conf/log4j2-server.xml . Once DSE with Cassandra and Solr has started normally, check the cluster health with nodetool status . A single-instance ring should show one node with flags *U*p and *N*ormal: nodetool status Note: Ownership information does not include topology ; for complete information, specify a keyspace = Datacenter: Solr Status = Up/Down | / State = Normal/Leaving/Joining/Moving -- Address Load Owns Host ID Token Rack UN 127 .0.0.1 99 .89 KB 100 .0% 5484ef7b-ebce-4560-80f0-cbdcd9e9f496 -7317038863489909889 rack1 Next, switch to Gremlin Console and open a JanusGraph database against the DSE instance. This will create JanusGraph\u2019s keyspace and column families. cd $JANUSGRAPH_HOME bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- gremlin> graph = JanusGraphFactory.open ( 'conf/janusgraph-cql-solr.properties' ) == >janusgraph [ cql: [ 127 .0.0.1 ]] gremlin> g = graph.traversal () == >graphtraversalsource [ janusgraph [ cql: [ 127 .0.0.1 ]] , standard ] gremlin> Keep this Gremlin Console open. We\u2019ll take a break now to install a Solr core. Then we\u2019ll come back to this console to load some sample data. Next, upload configuration files for JanusGraph\u2019s Solr collection, then create the core in DSE: # Change to the directory where JanusGraph was extracted. Later commands # use relative paths to the Solr config files shipped with the JanusGraph # distribution. cd $JANUSGRAPH_HOME # The name must be URL safe and should contain one dot/full-stop # character. The part of the name after the dot must not conflict with # any of JanusGraph's internal CF names. Starting the part after the dot # \"solr\" will avoid a conflict with JanusGraph's internal CF names. CORE_NAME = janusgraph.solr1 # Where to upload collection configuration and send CoreAdmin requests. SOLR_HOST = localhost:8983 # The value of index.[X].solr.http-urls in JanusGraph's config file # should match $SOLR_HOST and $CORE_NAME. For example, given the # $CORE_NAME and $SOLR_HOST values above, JanusGraph's config file would # contain (assuming \"search\" is the desired index alias): # # index.search.solr.http-urls=http://localhost:8983/solr/janusgraph.solr1 # # The stock JanusGraph config file conf/janusgraph-cql-solr.properties # ships with this http-urls value. # Upload Solr config files to DSE Search daemon for xml in conf/solr/ { solrconfig, schema, elevate } .xml ; do curl -v http:// \" $SOLR_HOST \" /solr/resource/ \" $CORE_NAME / $xml \" \\ --data-binary @ \" $xml \" -H 'Content-type:text/xml; charset=utf-8' done for txt in conf/solr/ { protwords, stopwords, synonyms } .txt ; do curl -v http:// \" $SOLR_HOST \" /solr/resource/ \" $CORE_NAME / $txt \" \\ --data-binary @ \" $txt \" -H 'Content-type:text/plain; charset=utf-8' done sleep 5 # Create core using the Solr config files just uploaded above curl \"http://\" $SOLR_HOST \"/solr/admin/cores?action=CREATE&name= $CORE_NAME \" sleep 5 # Retrieve and print the status of the core we just created curl \"http://localhost:8983/solr/admin/cores?action=STATUS&core= $CORE_NAME \" Now the JanusGraph database and backing Solr core are ready for use. We can test it out with the Graph of the Gods dataset. Picking up the Gremlin Console session started above: // Assuming graph = JanusGraphFactory.open('conf/janusgraph-cql-solr.properties')... gremlin > GraphOfTheGodsFactory . load ( graph ) ==> null Now we can run any of the queries described in Getting started . Queries involving text and geo predicates will be served by Solr. For more verbose reporting from JanusGraph and the Solr client, run gremlin.sh -l DEBUG and issue some index-backed queries.","title":"Apache Solr"},{"location":"index-backend/solr/#apache-solr","text":"Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more. Solr powers the search and navigation features of many of the world's largest internet sites. \u2014 Apache Solr Homepage JanusGraph supports Apache Solr as an index backend. Here are some of the Solr features supported by JanusGraph: Full-Text : Supports all Text predicates to search for text properties that matches a given word, prefix or regular expression. Geo : Supports all Geo predicates to search for geo properties that are intersecting, within, disjoint to or contained in a given query geometry. Supports points, lines and polygons for indexing. Supports circles, boxes and polygons for querying point properties and all shapes for querying non-point properties. Numeric Range : Supports all numeric comparisons in Compare . TTL : Supports automatically expiring indexed elements. Temporal : Millisecond granularity temporal indexing. Custom Analyzer : Choose to use a custom analyzer Please see Version Compatibility for details on what versions of Solr will work with JanusGraph.","title":"Apache Solr"},{"location":"index-backend/solr/#solr-configuration-overview","text":"JanusGraph supports Solr running in either a SolrCloud or Solr Standalone (HTTP) configuration for use with a mixed index (see Mixed Index ). The desired connection mode is configured via the parameter mode which must be set to either cloud or http , the former being the default value. For example, to explicitly specify that Solr is running in a SolrCloud configuration the following property is specified as a JanusGraph configuration property: index.search.solr.mode = cloud These are some key Solr terms: Core : A single index on a single machine Configuration : solrconfig.xml , schema.xml , and other files required to define a core. Collection : A single logical index that can span multiple cores on different machines. Configset : A shared configuration that can be reused by multiple cores.","title":"Solr Configuration Overview"},{"location":"index-backend/solr/#connecting-to-solrcloud","text":"When connecting to a SolrCloud cluster by setting the mode equal to cloud , the Zookeeper URL (and optionally port) must be specified so that JanusGraph can discover and interact with the Solr cluster. index.search.backend = solr index.search.solr.mode = cloud index.search.solr.zookeeper-url = localhost:2181 A number of additional configuration options pertaining to the creation of new collections (which is only supported in SolrCloud operation mode) can be configured to control sharding behavior among other things. Refer to the Configuration Reference for a complete listing of those options. SolrCloud leverages Zookeeper to coordinate collection and configset information between the Solr servers. The use of Zookeeper with SolrCloud provides the opportunity to significantly reduce the amount of manual configuration required to use Solr as a back end index for JanusGraph.","title":"Connecting to SolrCloud"},{"location":"index-backend/solr/#configset-configuration","text":"A configset is required to create a collection. The configset is stored in Zookeeper to enable access to it across the Solr servers. Each collection can provide its own configset when it is created, so that each collection may have a different configuration. With this approach, each collection must be created manually. A shared configset can be uploaded separately to Zookeeper if it will be reused by multiple collections. With this approach, JanusGraph can create collections automatically by using the shared configset. Another benefit is that reusing a configset significantly reduces the amount of data stored in Zookeeper.","title":"Configset Configuration"},{"location":"index-backend/solr/#using-an-individual-configset","text":"In this example, a collection named verticesByAge is created manually using the default JanusGraph configuration for Solr that is found in the distribution. When the collection is created, the configuration is uploaded into Zookeeper, using the same collection name verticesByAge for the configset name. Refer to the Solr Reference Guide for available parameters. # create the collection $SOLR_HOME /bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME /conf/solr Define a mixed index using JanusGraphManagement and the same collection name. mgmt = graph . openManagement () age = mgmt . makePropertyKey ( \"age\" ). dataType ( Integer . class ). make () mgmt . buildIndex ( \"verticesByAge\" , Vertex . class ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit ()","title":"Using an Individual Configset"},{"location":"index-backend/solr/#using-a-shared-configset","text":"When using a shared configset, it is most convenient to upload the configuration first as a one time operation. In this example, a configset named janusgraph-configset is uploaded in to Zookeeper using the default JanusGraph configuration for Solr that is found in the distribution. Refer to the Solr Reference Guide for available parameters. # upload the shared configset into Zookeeper # Solr 5 $SOLR_HOME /server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -z localhost:2181 \\ -d $JANUSGRAPH_HOME /conf/solr -n janusgraph-configset # Solr 6 and higher $SOLR_HOME /bin/solr zk upconfig -d $JANUSGRAPH_HOME /conf/solr -n janusgraph-configset \\ -z localhost:2181 When configuring the SolrCloud indexing backend for JanusGraph, make sure to provide the name of the shared configset using the index.search.solr.configset property. index.search.backend = solr index.search.solr.mode = cloud index.search.solr.zookeeper-url = localhost:2181 index.search.solr.configset = janusgraph-configset Define a mixed index using JanusGraphManagement and the collection name. mgmt = graph . openManagement () age = mgmt . makePropertyKey ( \"age\" ). dataType ( Integer . class ). make () mgmt . buildIndex ( \"verticesByAge\" , Vertex . class ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit ()","title":"Using a Shared Configset"},{"location":"index-backend/solr/#connecting-to-solr-standalone-http","text":"When connecting to Solr Standalone via HTTP by setting the mode equal to http , a single or list of URLs for the Solr instances must be provided. index.search.backend = solr index.search.solr.mode = http index.search.solr.http-urls = http://localhost:8983/solr Additional configuration options for controlling the maximum number of connections, connection timeout and transmission compression are available for the HTTP mode. Refer to the Configuration Reference for a complete listing of those options.","title":"Connecting to Solr Standalone (HTTP)"},{"location":"index-backend/solr/#core-configuration","text":"Solr Standalone is used for a single instance, and it keeps configuration information on the file system. A core must be created manually for each mixed index. To create a core, a core_name and a configuration directory is required. Refer to the Solr Reference Guide for available parameters. In this example, a core named verticesByAge is created using the default JanusGraph configuration for Solr that is found in the distribution. $SOLR_HOME /bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME /conf/solr Define a mixed index using JanusGraphManagement and the same core name. mgmt = graph . openManagement () age = mgmt . makePropertyKey ( \"age\" ). dataType ( Integer . class ). make () mgmt . buildIndex ( \"verticesByAge\" , Vertex . class ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit ()","title":"Core Configuration"},{"location":"index-backend/solr/#kerberos-configuration","text":"When connecting to a Solr environment that is protected by Kerberos we must specify that Kerberos is being used and reference a JAAS configuration file to properly configure the Solr Clients. This configuration is required when Kerberos is in use regardless of the mode in which Solr is operating (SolrCloud or Solr Standalone). index.search.solr.kerberos-enabled = true The JAAS configuration file is supplied by ensuring that you set the java system property java.security.auth.login.config with the absolute path to the file. This property should be set using JVM options. For example to run gremlin.sh you would need to set the JAVA_OPTIONS environment variable prior to running the script: export JAVA_OPTIONS = \"-Djava.security.auth.login.config=/absolute/path/jaas.conf\" $JANUSGRAPH_HOME /bin/gremlin.sh For details on the content required in the JAAS configuration file refer to the https://lucene.apache.org/solr/guide/7_0/kerberos-authentication-plugin.html#define-a-jaas-configuration-file[Solr Reference Guide].","title":"Kerberos Configuration"},{"location":"index-backend/solr/#solr-schema-design","text":"","title":"Solr Schema Design"},{"location":"index-backend/solr/#dynamic-field-definition","text":"By default, JanusGraph uses Solr\u2019s Dynamic Fields feature to define the field types for all indexed keys. This requires no extra configuration when adding property keys to a mixed index backed by Solr and provides better performance than schemaless mode. JanusGraph assumes the following dynamic field tags are defined in the backing Solr collection\u2019s schema.xml file. Please note that there is additional xml definition of the following fields required in a solr schema.xml file in order to use them. Reference the example schema.xml file provided in the ./conf/solr/schema.xml directory in a JanusGraph installation for more information. <dynamicField name= \"*_i\" type= \"int\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_s\" type= \"string\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_l\" type= \"long\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_t\" type= \"text_general\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_b\" type= \"boolean\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_f\" type= \"float\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_d\" type= \"double\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_g\" type= \"geo\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_dt\" type= \"date\" indexed= \"true\" stored= \"true\" /> <dynamicField name= \"*_uuid\" type= \"uuid\" indexed= \"true\" stored= \"true\" /> In JanusGraph\u2019s default configuration, property key names do not have to end with the type-appropriate suffix to take advantage of Solr\u2019s dynamic field feature. JanusGraph generates the Solr field name from the property key name by encoding the property key definition\u2019s numeric identifier and the type-appropriate suffix. This means that JanusGraph uses synthetic field names with type-appropriate suffixes behind the scenes, regardless of the property key names defined and used by application code using JanusGraph. This field name mapping can be overridden through non-default configuration. That\u2019s described in the next section.","title":"Dynamic Field Definition"},{"location":"index-backend/solr/#manual-field-definition","text":"If the user would rather manually define the field types for each of the indexed fields in a collection, the configuration option dyn-fields needs to be disabled. It is important that the field for each indexed property key is defined in the backing Solr schema before the property key is added to the index. In this scenario, it is advisable to enable explicit property key name to field mapping in order to fix the field names for their explicit definition. This can be achieved in one of two ways: Configuring the name of the field by providing a mapped-name parameter when adding the property key to the index. See Individual Field Mapping for more information. By enabling the map-name configuration option for the Solr index which will use the property key name as the field name in Solr. See Global Field Mapping for more information.","title":"Manual Field Definition"},{"location":"index-backend/solr/#schemaless-mode","text":"JanusGraph can also interact with a SolrCloud cluster that is configured for schemaless mode . In this scenario, the configuration option dyn-fields should be disabled since Solr will infer the field type from the values and not the field name. Note, however, that schemaless mode is recommended only for prototyping and initial application development and NOT recommended for production use.","title":"Schemaless Mode"},{"location":"index-backend/solr/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"index-backend/solr/#collection-does-not-exist","text":"The collection (and all of the required configuration files) must be initialized before a defined index can use the collection. See Connecting to SolrCloud for more information. When using SolrCloud, the Zookeeper zkCli.sh command line tool can be used to inspect the configurations loaded into Zookeeper. Also verify that the default JanusGraph configuration files are copied to the correct location under solr and that the directory where the files are copied is correct.","title":"Collection Does Not Exist"},{"location":"index-backend/solr/#cannot-find-the-specified-configset","text":"When using SolrCloud, a configset is required to create a mixed index for JanusGraph. See Configset Configuration for more information. If using an individual configset, the collection must be created manually first. If using a shared configset, the configset must be uploaded into Zookeeper first. You can verify that the configset and its configuration files are in Zookeeper under /configs . Refer to the Solr Reference Guide for other Zookeeper operations. # verify the configset in Zookeeper # Solr 5 $SOLR_HOME /server/scripts/cloud-scripts/zkcli.sh -cmd list -z localhost:2181 # Solr 6 and higher $SOLR_HOME /bin/solr zk ls -r /configs/configset-name -z localhost:2181","title":"Cannot Find the Specified Configset"},{"location":"index-backend/solr/#http-error-404","text":"This error may be encountered when using Solr Standalone (HTTP) mode. An example of the error: 20:01:22 ERROR org.janusgraph.diskstorage.solr.SolrIndex - Unable to save documents to Solr as one of the shape objects stored were not compatible with Solr. org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://localhost:8983/solr: Expected mime type application/octet-stream but got text/html. <html> <head> <meta http-equiv= \"Content-Type\" content= \"text/html;charset=utf-8\" /> <title> Error 404 Not Found </title> </head> <body><h2> HTTP ERROR 404 </h2> <p> Problem accessing /solr/verticesByAge/update. Reason: <pre> Not Found </pre></p> </body> </html> Make sure to create the core manually before attempting to store data into the index. See Core Configuration for more information.","title":"HTTP Error 404"},{"location":"index-backend/solr/#invalid-core-or-collection-name","text":"The core or collection name is an identifier. It must consist entirely of periods, underscores, hyphens, and/or alphanumerics, and also it may not start with a hyphen.","title":"Invalid core or collection name"},{"location":"index-backend/solr/#connection-problems","text":"Irrespective of the operation mode, a Solr instance or a cluster of Solr instances must be running and accessible from the JanusGraph instance(s) in order for JanusGraph to use Solr as an indexing backend. Check that the Solr cluster is running correctly and that it is visible and accessible over the network (or locally) from the JanusGraph instances.","title":"Connection Problems"},{"location":"index-backend/solr/#jts-classnotfoundexception-with-geo-data","text":"Solr relies on Spatial4j for geo processing. Spatial4j declares an optional dependency on JTS (\"JTS Topology Suite\"). JTS is required for some geo field definition and query functionality. If the JTS jar is not on the Solr daemon\u2019s classpath and a field in schema.xml uses a geo type, then Solr may throw a ClassNotFoundException on one of the missing JTS classes. The exception can appear when starting Solr using a schema.xml file designed to work with JanusGraph, but can also appear when invoking CREATE in the Solr CoreAdmin API . The exception appears in slightly different formats on the client and server sides, although the root cause is identical. Here\u2019s a representative example from a Solr server log: ERROR [ http - 8983 - exec - 5 ] 2014 - 10 - 07 02 : 54 : 06 , 665 SolrCoreResourceManager . java ( line 344 ) com / vividsolutions / jts / geom / Geometry java . lang . NoClassDefFoundError : com / vividsolutions / jts / geom / Geometry at com . spatial4j . core . context . jts . JtsSpatialContextFactory . newSpatialContext ( JtsSpatialContextFactory . java : 30 ) at com . spatial4j . core . context . SpatialContextFactory . makeSpatialContext ( SpatialContextFactory . java : 83 ) at org . apache . solr . schema . AbstractSpatialFieldType . init ( AbstractSpatialFieldType . java : 95 ) at org . apache . solr . schema . AbstractSpatialPrefixTreeFieldType . init ( AbstractSpatialPrefixTreeFieldType . java : 43 ) at org . apache . solr . schema . SpatialRecursivePrefixTreeFieldType . init ( SpatialRecursivePrefixTreeFieldType . java : 37 ) at org . apache . solr . schema . FieldType . setArgs ( FieldType . java : 164 ) at org . apache . solr . schema . FieldTypePluginLoader . init ( FieldTypePluginLoader . java : 141 ) at org . apache . solr . schema . FieldTypePluginLoader . init ( FieldTypePluginLoader . java : 43 ) at org . apache . solr . util . plugin . AbstractPluginLoader . load ( AbstractPluginLoader . java : 190 ) at org . apache . solr . schema . IndexSchema . readSchema ( IndexSchema . java : 470 ) at com . datastax . bdp . search . solr . CassandraIndexSchema . readSchema ( CassandraIndexSchema . java : 72 ) at org . apache . solr . schema . IndexSchema . < init > ( IndexSchema . java : 168 ) at com . datastax . bdp . search . solr . CassandraIndexSchema . < init > ( CassandraIndexSchema . java : 54 ) at com . datastax . bdp . search . solr . core . CassandraCoreContainer . create ( CassandraCoreContainer . java : 210 ) at com . datastax . bdp . search . solr . core . SolrCoreResourceManager . createCore ( SolrCoreResourceManager . java : 256 ) at com . datastax . bdp . search . solr . handler . admin . CassandraCoreAdminHandler . handleCreateAction ( CassandraCoreAdminHandler . java : 117 ) ... Here\u2019s what normally appears in the output of the client that issued the associated CREATE command to the CoreAdmin API: org . apache . solr . common . SolrException : com / vividsolutions / jts / geom / Geometry at com . datastax . bdp . search . solr . core . SolrCoreResourceManager . createCore ( SolrCoreResourceManager . java : 345 ) at com . datastax . bdp . search . solr . handler . admin . CassandraCoreAdminHandler . handleCreateAction ( CassandraCoreAdminHandler . java : 117 ) at org . apache . solr . handler . admin . CoreAdminHandler . handleRequestBody ( CoreAdminHandler . java : 152 ) ... This is resolved by adding the JTS jar to the classpath of JanusGraph and/or the Solr server. JTS is not included in JanusGraph distributions by default due to its LGPL license. Users must download the JTS jar file separately and copy it into the JanusGraph and/or Solr server lib directory. If using Solr\u2019s built in web server, the JTS jar may be copied to the example/solr-webapp/webapp/WEB-INF/lib directory to include it in the classpath. Solr can be restarted, and the exception should be gone. Solr must be started once with the correct schema.xml file in place first, for the example/solr-webapp/webapp/WEB-INF/lib directory to exist. To determine the ideal JTS version for Solr server, first check the version of Spatial4j in use by the Solr cluster, then determine the version of JTS against which that Spatial4j version was compiled. Spatial4j declares its target JTS version in the pom for the com.spatial4j:spatial4j artifact . Copy the JTS jar to the server/solr-webapp/webapp/WEB-INF/lib directory in your solr installation.","title":"JTS ClassNotFoundException with Geo Data"},{"location":"index-backend/solr/#advanced-solr-configuration","text":"","title":"Advanced Solr Configuration"},{"location":"index-backend/solr/#dse-search","text":"This section covers installation and configuration of JanusGraph with DataStax Enterprise (DSE) Search. There are multiple ways to install DSE, but this section focuses on DSE\u2019s binary tarball install option on Linux. Most of the steps in this section can be generalized to the other install options for DSE. Install DataStax Enterprise as directed by the page Installing DataStax Enterprise using the binary tarball . Export DSE_HOME and append to PATH in your shell environment. Here\u2019s an example using Bash syntax: export DSE_HOME = /path/to/dse-version.number export PATH = \" $DSE_HOME \" /bin: \" $PATH \" Install JTS for Solr. The appropriate version varies with the Spatial4j version. As of DSE 4.5.2, the appropriate version is 1.13. cd $DSE_HOME /resources/solr/lib curl -O 'http://central.maven.org/maven2/com/vividsolutions/jts/1.13/jts-1.13.jar' Start DSE Cassandra and Solr in a single background daemon: # The \"dse-data\" path below was chosen to match the # \"Installing DataStax Enterprise using the binary tarball\" # documentation page from DataStax. The exact path is not # significant. dse cassandra -s -Ddse.solr.data.dir = \" $DSE_HOME \" /dse-data/solr The previous command will write some startup information to the console and to the logfile path log4j.appender.R.File configured in $DSE_HOME/resources/cassandra/conf/log4j2-server.xml . Once DSE with Cassandra and Solr has started normally, check the cluster health with nodetool status . A single-instance ring should show one node with flags *U*p and *N*ormal: nodetool status Note: Ownership information does not include topology ; for complete information, specify a keyspace = Datacenter: Solr Status = Up/Down | / State = Normal/Leaving/Joining/Moving -- Address Load Owns Host ID Token Rack UN 127 .0.0.1 99 .89 KB 100 .0% 5484ef7b-ebce-4560-80f0-cbdcd9e9f496 -7317038863489909889 rack1 Next, switch to Gremlin Console and open a JanusGraph database against the DSE instance. This will create JanusGraph\u2019s keyspace and column families. cd $JANUSGRAPH_HOME bin/gremlin.sh \\, ,,/ ( o o ) -----oOOo- ( 3 ) -oOOo----- gremlin> graph = JanusGraphFactory.open ( 'conf/janusgraph-cql-solr.properties' ) == >janusgraph [ cql: [ 127 .0.0.1 ]] gremlin> g = graph.traversal () == >graphtraversalsource [ janusgraph [ cql: [ 127 .0.0.1 ]] , standard ] gremlin> Keep this Gremlin Console open. We\u2019ll take a break now to install a Solr core. Then we\u2019ll come back to this console to load some sample data. Next, upload configuration files for JanusGraph\u2019s Solr collection, then create the core in DSE: # Change to the directory where JanusGraph was extracted. Later commands # use relative paths to the Solr config files shipped with the JanusGraph # distribution. cd $JANUSGRAPH_HOME # The name must be URL safe and should contain one dot/full-stop # character. The part of the name after the dot must not conflict with # any of JanusGraph's internal CF names. Starting the part after the dot # \"solr\" will avoid a conflict with JanusGraph's internal CF names. CORE_NAME = janusgraph.solr1 # Where to upload collection configuration and send CoreAdmin requests. SOLR_HOST = localhost:8983 # The value of index.[X].solr.http-urls in JanusGraph's config file # should match $SOLR_HOST and $CORE_NAME. For example, given the # $CORE_NAME and $SOLR_HOST values above, JanusGraph's config file would # contain (assuming \"search\" is the desired index alias): # # index.search.solr.http-urls=http://localhost:8983/solr/janusgraph.solr1 # # The stock JanusGraph config file conf/janusgraph-cql-solr.properties # ships with this http-urls value. # Upload Solr config files to DSE Search daemon for xml in conf/solr/ { solrconfig, schema, elevate } .xml ; do curl -v http:// \" $SOLR_HOST \" /solr/resource/ \" $CORE_NAME / $xml \" \\ --data-binary @ \" $xml \" -H 'Content-type:text/xml; charset=utf-8' done for txt in conf/solr/ { protwords, stopwords, synonyms } .txt ; do curl -v http:// \" $SOLR_HOST \" /solr/resource/ \" $CORE_NAME / $txt \" \\ --data-binary @ \" $txt \" -H 'Content-type:text/plain; charset=utf-8' done sleep 5 # Create core using the Solr config files just uploaded above curl \"http://\" $SOLR_HOST \"/solr/admin/cores?action=CREATE&name= $CORE_NAME \" sleep 5 # Retrieve and print the status of the core we just created curl \"http://localhost:8983/solr/admin/cores?action=STATUS&core= $CORE_NAME \" Now the JanusGraph database and backing Solr core are ready for use. We can test it out with the Graph of the Gods dataset. Picking up the Gremlin Console session started above: // Assuming graph = JanusGraphFactory.open('conf/janusgraph-cql-solr.properties')... gremlin > GraphOfTheGodsFactory . load ( graph ) ==> null Now we can run any of the queries described in Getting started . Queries involving text and geo predicates will be served by Solr. For more verbose reporting from JanusGraph and the Solr client, run gremlin.sh -l DEBUG and issue some index-backed queries.","title":"DSE Search"},{"location":"index-backend/text-search/","text":"Index Parameters and Full-Text Search When defining a mixed index, a list of parameters can be optionally specified for each property key added to the index. These parameters control how the particular key is to be indexed. JanusGraph recognizes the following index parameters. Whether these are supported depends on the configured index backend. A particular index backend might also support custom parameters in addition to the ones listed here. Full-Text Search When indexing string values, that is property keys with String.class data type, one has the choice to either index those as text or character strings which is controlled by the mapping parameter type. When the value is indexed as text, the string is tokenized into a bag of words which allows the user to efficiently query for all matches that contain one or multiple words. This is commonly referred to as full-text search . When the value is indexed as a character string, the string is index \"as-is\" without any further analysis or tokenization. This facilitates queries looking for an exact character sequence match. This is commonly referred to as string search . Full-Text Search By default, strings are indexed as text. To make this indexing option explicit, one can define a mapping when indexing a property key as text. mgmt = graph . openManagement () summary = mgmt . makePropertyKey ( 'booksummary' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( summary , Mapping . TEXT . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () This is identical to a standard mixed index definition with the only addition of an extra parameter that specifies the mapping in the index - in this case Mapping.TEXT . When a string property is indexed as text, the string value is tokenized into a bag of tokens. The exact tokenization depends on the indexing backend and its configuration. JanusGraph\u2019s default tokenization splits the string on non-alphanumeric characters and removes any tokens with less than 2 characters. The tokenization used by an indexing backend may differ (e.g. stop words are removed) which can lead to minor differences in how full-text search queries are handled for modifications inside a transaction and committed data in the indexing backend. When a string property is indexed as text, only full-text search predicates are supported in graph queries by the indexing backend. Full-text search is case-insensitive. textContains : is true if (at least) one word inside the text string matches the query string textContainsPrefix : is true if (at least) one word inside the text string begins with the query string textContainsRegex : is true if (at least) one word inside the text string matches the given regular expression textContainsFuzzy : is true if (at least) one word inside the text string is similar to the query String (based on Levenshtein edit distance) import static org . janusgraph . core . attribute . Text .* g . V (). has ( 'booksummary' , textContains ( 'unicorns' )) g . V (). has ( 'booksummary' , textContainsPrefix ( 'uni' )) g . V (). has ( 'booksummary' , textContainsRegex ( '.*corn.*' )) g . V (). has ( 'booksummary' , textContainsFuzzy ( 'unicorn' )) The Elasticsearch backend extends this functionality and includes support for negations of the above predicates, as well as phrase matching: textNotContains : is true if no words inside the text string match the query string textNotContainsPrefix : is true if no words inside the text string begin with the query string textNotContainsRegex : is true if no words inside the text string match the given regular expression textNotContainsFuzzy : is true if no words inside the text string are similar to the query string (based on Levenshtein edit distance) textNotContainsPhrase : is true if the text string does not contain the sequence of words in the query string String search predicates (see below) may be used in queries, but those require filtering in memory which can be very costly. String Search To index string properties as character sequences without any analysis or tokenization, specify the mapping as Mapping.STRING : mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'bookname' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( name , Mapping . STRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () When a string mapping is configured, the string value is indexed and can be queried \"as-is\" - including stop words and non-letter characters. However, in this case the query must match the entire string value. Hence, the string mapping is useful when indexing short character sequences that are considered to be one token. When a string property is indexed as string, only the following predicates are supported in graph queries by the indexing backend. String search is case-sensitive. eq : if the string is identical to the query string neq : if the string is different than the query string textPrefix : if the string value starts with the given query string textRegex : if the string value matches the given regular expression in its entirety textFuzzy : if the string value is similar to the given query string (based on Levenshtein edit distance) import static org . apache . tinkerpop . gremlin . process . traversal . P .* import static org . janusgraph . core . attribute . Text .* g . V (). has ( 'bookname' , eq ( 'unicorns' )) g . V (). has ( 'bookname' , neq ( 'unicorns' )) g . V (). has ( 'bookname' , textPrefix ( 'uni' )) g . V (). has ( 'bookname' , textRegex ( '.*corn.*' )) g . V (). has ( 'bookname' , textFuzzy ( 'unicorn' )) The Elasticsearch backend extends this functionality and includes support for negations of the above text predicates: textNotPrefix : if the string value does not start with the given query string textNotRegex : if the string value does not match the given regular expression in its entirety textNotFuzzy : if the string value is not similar to the given query string (based on Levenshtein edit distance) Full-text search predicates may be used in queries, but those require filtering in memory which can be very costly. Full text and string search If you are using Elasticsearch it is possible to index properties as both text and string allowing you to use all of the predicates for exact and fuzzy matching. mgmt = graph . openManagement () summary = mgmt . makePropertyKey ( 'booksummary' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( summary , Mapping . TEXTSTRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () Note that the data will be stored in the index twice, once for exact matching and once for fuzzy matching. TinkerPop Text Predicates It is also possible to use the TinkerPop text predicates with JanusGraph, but these predicates do not make use of indices which means that they require filtering in memory which can be very costly. import static org . apache . tinkerpop . gremlin . process . traversal . TextP .*; g . V (). has ( 'bookname' , startingWith ( 'uni' )) g . V (). has ( 'bookname' , endingWith ( 'corn' )) g . V (). has ( 'bookname' , containing ( 'nico' )) Geo Mapping By default, JanusGraph supports indexing geo properties with point type and querying geo properties by circle or box. To index a non-point geo property with support for querying by any geoshape type, specify the mapping as Mapping.BKD (preferred for ElasticSearch) or Mapping.PREFIX_TREE (Supported in Solr, Lucene, and ElasticSearch 6. Deprecated in ElasticSearch 7. Support was removed in ElasticSearch 8): mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'border' ). dataType ( Geoshape . class ). make () mgmt . buildIndex ( 'borderIndex' , Vertex . class ). addKey ( name , Mapping . BKD . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () When Mapping.PREFIX_TREE is used it is possible to specify additional parameters to tune the configuration of the underlying prefix tree mapping. These optional parameters include the number of levels used in the prefix tree as well as the associated precision. mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'border' ). dataType ( Geoshape . class ). make () mgmt . buildIndex ( 'borderIndex' , Vertex . class ). addKey ( name , Mapping . PREFIX_TREE . asParameter (), Parameter . of ( \"index-geo-max-levels\" , 18 ), Parameter . of ( \"index-geo-dist-error-pct\" , 0.0125 )). buildMixedIndex ( \"search\" ) mgmt . commit () Note that some indexing backends (e.g. Solr) may require additional external schema configuration to support and tune indexing non-point properties. Info Mapping.BKD mapping which is preferred for ElasticSearch doesn't support Circle shape. Instead of indexing a Circle as it was with Mapping.PREFIX_TREE there are Circle processors available which transform Circle into other shapes and index those shapes instead. Depending on the precision configuration used, search for such circles may behave differently. The higher the precision the more points is used during indexing which may affect storage size and index time. See index.[X].bkd-circle-processor configuration properties for more information about BKD circle processors.","title":"Index Parameters and Full-Text Search"},{"location":"index-backend/text-search/#index-parameters-and-full-text-search","text":"When defining a mixed index, a list of parameters can be optionally specified for each property key added to the index. These parameters control how the particular key is to be indexed. JanusGraph recognizes the following index parameters. Whether these are supported depends on the configured index backend. A particular index backend might also support custom parameters in addition to the ones listed here.","title":"Index Parameters and Full-Text Search"},{"location":"index-backend/text-search/#full-text-search","text":"When indexing string values, that is property keys with String.class data type, one has the choice to either index those as text or character strings which is controlled by the mapping parameter type. When the value is indexed as text, the string is tokenized into a bag of words which allows the user to efficiently query for all matches that contain one or multiple words. This is commonly referred to as full-text search . When the value is indexed as a character string, the string is index \"as-is\" without any further analysis or tokenization. This facilitates queries looking for an exact character sequence match. This is commonly referred to as string search .","title":"Full-Text Search"},{"location":"index-backend/text-search/#full-text-search_1","text":"By default, strings are indexed as text. To make this indexing option explicit, one can define a mapping when indexing a property key as text. mgmt = graph . openManagement () summary = mgmt . makePropertyKey ( 'booksummary' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( summary , Mapping . TEXT . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () This is identical to a standard mixed index definition with the only addition of an extra parameter that specifies the mapping in the index - in this case Mapping.TEXT . When a string property is indexed as text, the string value is tokenized into a bag of tokens. The exact tokenization depends on the indexing backend and its configuration. JanusGraph\u2019s default tokenization splits the string on non-alphanumeric characters and removes any tokens with less than 2 characters. The tokenization used by an indexing backend may differ (e.g. stop words are removed) which can lead to minor differences in how full-text search queries are handled for modifications inside a transaction and committed data in the indexing backend. When a string property is indexed as text, only full-text search predicates are supported in graph queries by the indexing backend. Full-text search is case-insensitive. textContains : is true if (at least) one word inside the text string matches the query string textContainsPrefix : is true if (at least) one word inside the text string begins with the query string textContainsRegex : is true if (at least) one word inside the text string matches the given regular expression textContainsFuzzy : is true if (at least) one word inside the text string is similar to the query String (based on Levenshtein edit distance) import static org . janusgraph . core . attribute . Text .* g . V (). has ( 'booksummary' , textContains ( 'unicorns' )) g . V (). has ( 'booksummary' , textContainsPrefix ( 'uni' )) g . V (). has ( 'booksummary' , textContainsRegex ( '.*corn.*' )) g . V (). has ( 'booksummary' , textContainsFuzzy ( 'unicorn' )) The Elasticsearch backend extends this functionality and includes support for negations of the above predicates, as well as phrase matching: textNotContains : is true if no words inside the text string match the query string textNotContainsPrefix : is true if no words inside the text string begin with the query string textNotContainsRegex : is true if no words inside the text string match the given regular expression textNotContainsFuzzy : is true if no words inside the text string are similar to the query string (based on Levenshtein edit distance) textNotContainsPhrase : is true if the text string does not contain the sequence of words in the query string String search predicates (see below) may be used in queries, but those require filtering in memory which can be very costly.","title":"Full-Text Search"},{"location":"index-backend/text-search/#string-search","text":"To index string properties as character sequences without any analysis or tokenization, specify the mapping as Mapping.STRING : mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'bookname' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( name , Mapping . STRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () When a string mapping is configured, the string value is indexed and can be queried \"as-is\" - including stop words and non-letter characters. However, in this case the query must match the entire string value. Hence, the string mapping is useful when indexing short character sequences that are considered to be one token. When a string property is indexed as string, only the following predicates are supported in graph queries by the indexing backend. String search is case-sensitive. eq : if the string is identical to the query string neq : if the string is different than the query string textPrefix : if the string value starts with the given query string textRegex : if the string value matches the given regular expression in its entirety textFuzzy : if the string value is similar to the given query string (based on Levenshtein edit distance) import static org . apache . tinkerpop . gremlin . process . traversal . P .* import static org . janusgraph . core . attribute . Text .* g . V (). has ( 'bookname' , eq ( 'unicorns' )) g . V (). has ( 'bookname' , neq ( 'unicorns' )) g . V (). has ( 'bookname' , textPrefix ( 'uni' )) g . V (). has ( 'bookname' , textRegex ( '.*corn.*' )) g . V (). has ( 'bookname' , textFuzzy ( 'unicorn' )) The Elasticsearch backend extends this functionality and includes support for negations of the above text predicates: textNotPrefix : if the string value does not start with the given query string textNotRegex : if the string value does not match the given regular expression in its entirety textNotFuzzy : if the string value is not similar to the given query string (based on Levenshtein edit distance) Full-text search predicates may be used in queries, but those require filtering in memory which can be very costly.","title":"String Search"},{"location":"index-backend/text-search/#full-text-and-string-search","text":"If you are using Elasticsearch it is possible to index properties as both text and string allowing you to use all of the predicates for exact and fuzzy matching. mgmt = graph . openManagement () summary = mgmt . makePropertyKey ( 'booksummary' ). dataType ( String . class ). make () mgmt . buildIndex ( 'booksBySummary' , Vertex . class ). addKey ( summary , Mapping . TEXTSTRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () Note that the data will be stored in the index twice, once for exact matching and once for fuzzy matching.","title":"Full text and string search"},{"location":"index-backend/text-search/#tinkerpop-text-predicates","text":"It is also possible to use the TinkerPop text predicates with JanusGraph, but these predicates do not make use of indices which means that they require filtering in memory which can be very costly. import static org . apache . tinkerpop . gremlin . process . traversal . TextP .*; g . V (). has ( 'bookname' , startingWith ( 'uni' )) g . V (). has ( 'bookname' , endingWith ( 'corn' )) g . V (). has ( 'bookname' , containing ( 'nico' ))","title":"TinkerPop Text Predicates"},{"location":"index-backend/text-search/#geo-mapping","text":"By default, JanusGraph supports indexing geo properties with point type and querying geo properties by circle or box. To index a non-point geo property with support for querying by any geoshape type, specify the mapping as Mapping.BKD (preferred for ElasticSearch) or Mapping.PREFIX_TREE (Supported in Solr, Lucene, and ElasticSearch 6. Deprecated in ElasticSearch 7. Support was removed in ElasticSearch 8): mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'border' ). dataType ( Geoshape . class ). make () mgmt . buildIndex ( 'borderIndex' , Vertex . class ). addKey ( name , Mapping . BKD . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () When Mapping.PREFIX_TREE is used it is possible to specify additional parameters to tune the configuration of the underlying prefix tree mapping. These optional parameters include the number of levels used in the prefix tree as well as the associated precision. mgmt = graph . openManagement () name = mgmt . makePropertyKey ( 'border' ). dataType ( Geoshape . class ). make () mgmt . buildIndex ( 'borderIndex' , Vertex . class ). addKey ( name , Mapping . PREFIX_TREE . asParameter (), Parameter . of ( \"index-geo-max-levels\" , 18 ), Parameter . of ( \"index-geo-dist-error-pct\" , 0.0125 )). buildMixedIndex ( \"search\" ) mgmt . commit () Note that some indexing backends (e.g. Solr) may require additional external schema configuration to support and tune indexing non-point properties. Info Mapping.BKD mapping which is preferred for ElasticSearch doesn't support Circle shape. Instead of indexing a Circle as it was with Mapping.PREFIX_TREE there are Circle processors available which transform Circle into other shapes and index those shapes instead. Depending on the precision configuration used, search for such circles may behave differently. The higher the precision the more points is used during indexing which may affect storage size and index time. See index.[X].bkd-circle-processor configuration properties for more information about BKD circle processors.","title":"Geo Mapping"},{"location":"interactions/search-predicates/","text":"Search Predicates and Mixed Index Data Types This page lists all of the comparison predicates that JanusGraph supports in global graph search and local traversals. Note Some parts of this section require mixed index, see mixed index backend . Compare Predicate The Compare enum specifies the following comparison predicates used for index query construction and used in the examples above: eq (equal) neq (not equal) gt (greater than) gte (greater than or equal) lt (less than) lte (less than or equal) All comparison predicates are supported by String, numeric, Date and Instant data types. Boolean and UUID data types support the eq and neq comparison predicates. eq and neq can be used on Boolean and UUID. Text Predicate The Text enum specifies the Text Search used to query for matching text or string values. We differentiate between two types of predicates: Text search predicates which match against the individual words inside a text string after it has been tokenized. These predicates are not case sensitive. textContains : is true if (at least) one word inside the text string matches the query string textNotContains : is true if no words inside the text string match the query string textContainsPrefix : is true if (at least) one word inside the text string begins with the query string textNotContainsPrefix : is true if no words inside the text string begin with the query string textContainsRegex : is true if (at least) one word inside the text string matches the given regular expression textNotContainsRegex : is true if no words inside the text string match the given regular expression textContainsFuzzy : is true if (at least) one word inside the text string is similar to the query string (based on Levenshtein edit distance) textNotContainsFuzzy : is true if no words inside the text string are similar to the query string (based on Levenshtein edit distance) textContainsPhrase : is true if the text string contains the exact sequence of words in the query string textNotContainsPhrase : is true if the text string does not contain the sequence of words in the query string String search predicates which match against the entire string value textPrefix : if the string value starts with the given query string textNotPrefix : if the string value does not start with the given query string textRegex : if the string value matches the given regular expression in its entirety textNotRegex : if the string value does not match the given regular expression in its entirety textFuzzy : if the string value is similar to the given query string (based on Levenshtein edit distance) textNotFuzzy : if the string value is not similar to the given query string (based on Levenshtein edit distance) See Text Search for more information about full-text and string search. Geo Predicate The Geo enum specifies geo-location predicates. geoIntersect which holds true if the two geometric objects have at least one point in common (opposite of geoDisjoint ). geoWithin which holds true if one geometric object contains the other. geoDisjoint which holds true if the two geometric objects have no points in common (opposite of geoIntersect ). geoContains which holds true if one geometric object is contained by the other. See Geo Mapping for more information about geo search. Query Examples The following query examples demonstrate some of the predicates on the tutorial graph. // 1) Find vertices with the name \"hercules\" g . V (). has ( \"name\" , \"hercules\" ) // 2) Find all vertices with an age greater than 50 g . V (). has ( \"age\" , gt ( 50 )) // or find all vertices between 1000 (inclusive) and 5000 (exclusive) years of age and order by ascending age g . V (). has ( \"age\" , inside ( 1000 , 5000 )). order (). by ( \"age\" , asc ) // which returns the same result set as the following query but in reverse order g . V (). has ( \"age\" , inside ( 1000 , 5000 )). order (). by ( \"age\" , desc ) // 3) Find all edges where the place is at most 50 kilometers from the given latitude-longitude pair g . E (). has ( \"place\" , geoWithin ( Geoshape . circle ( 37.97 , 23.72 , 50 ))) // 4) Find all edges where reason contains the word \"loves\" g . E (). has ( \"reason\" , textContains ( \"loves\" )) // or all edges which contain two words (need to chunk into individual words) g . E (). has ( \"reason\" , textContains ( \"loves\" )). has ( \"reason\" , textContains ( \"breezes\" )) // or all edges which contain words that start with \"lov\" g . E (). has ( \"reason\" , textContainsPrefix ( \"lov\" )) // or all edges which contain words that match the regular expression \"br[ez]*s\" in their entirety g . E (). has ( \"reason\" , textContainsRegex ( \"br[ez]*s\" )) // or all edges which contain words similar to \"love\" g . E (). has ( \"reason\" , textContainsFuzzy ( \"love\" )) // 5) Find all vertices older than a thousand years and named \"saturn\" g . V (). has ( \"age\" , gt ( 1000 )). has ( \"name\" , \"saturn\" ) Data Type Support While JanusGraph's composite indexes support any data type that can be stored in JanusGraph, the mixed indexes are limited to the following data types. Byte Short Integer Long Float Double String Geoshape Date Instant UUID Additional data types will be supported in the future. Geoshape Data Type The Geoshape data type supports representing a point, circle, box, line, polygon, multi-point, multi-line and multi-polygon. Index backends currently support indexing points, circles, boxes, lines, polygons, multi-point, multi-line, multi-polygon and geometry collection. Geospatial index lookups are only supported via mixed indexes. To construct a Geoshape use the following methods: //lat, lng Geoshape . point ( 37.97 , 23.72 ) //lat, lng, radius in km Geoshape . circle ( 37.97 , 23.72 , 50 ) //SW lat, SW lng, NE lat, NE lng Geoshape . box ( 37.97 , 23.72 , 38.97 , 24.72 ) //WKT Geoshape . fromWkt ( \"POLYGON ((35.4 48.9, 35.6 48.9, 35.6 49.1, 35.4 49.1, 35.4 48.9))\" ) //MultiPoint Geoshape . geoshape ( Geoshape . getShapeFactory (). multiPoint (). pointXY ( 60.0 , 60.0 ). pointXY ( 120.0 , 60.0 ) . build ()) //MultiLine Geoshape . geoshape ( Geoshape . getShapeFactory (). multiLineString () . add ( Geoshape . getShapeFactory (). lineString (). pointXY ( 59.0 , 60.0 ). pointXY ( 61.0 , 60.0 )) . add ( Geoshape . getShapeFactory (). lineString (). pointXY ( 119.0 , 60.0 ). pointXY ( 121.0 , 60.0 )). build ()) //MultiPolygon Geoshape . geoshape ( Geoshape . getShapeFactory (). multiPolygon () . add ( Geoshape . getShapeFactory (). polygon (). pointXY ( 59.0 , 59.0 ). pointXY ( 61.0 , 59.0 ) . pointXY ( 61.0 , 61.0 ). pointXY ( 59.0 , 61.0 ). pointXY ( 59.0 , 59.0 )) . add ( Geoshape . getShapeFactory (). polygon (). pointXY ( 119.0 , 59.0 ). pointXY ( 121.0 , 59.0 ) . pointXY ( 121.0 , 61.0 ). pointXY ( 119.0 , 61.0 ). pointXY ( 119.0 , 59.0 )). build ()) //GeometryCollection Geoshape . geoshape ( Geoshape . getGeometryCollectionBuilder () . add ( Geoshape . getShapeFactory (). pointXY ( 60.0 , 60.0 )) . add ( Geoshape . getShapeFactory (). lineString (). pointXY ( 119.0 , 60.0 ). pointXY ( 121.0 , 60.0 ). build ()) . add ( Geoshape . getShapeFactory (). polygon (). pointXY ( 119.0 , 59.0 ). pointXY ( 121.0 , 59.0 ) . pointXY ( 121.0 , 61.0 ). pointXY ( 119.0 , 61.0 ). pointXY ( 119.0 , 59.0 ). build ()). build ()) In addition, when importing a graph via GraphSON the geometry may be represented by GeoJSON: string \"37.97, 23.72\" list [ 37.97 , 23.72 ] GeoJSON feature { \"type\" : \"Feature\" , \"geometry\" : { \"type\" : \"Point\" , \"coordinates\" : [ 125.6 , 10.1 ] }, \"properties\" : { \"name\" : \"Dinagat Islands\" } } GeoJSON geometry { \"type\" : \"Point\" , \"coordinates\" : [ 125.6 , 10.1 ] } GeoJSON may be specified as Point, Circle, LineString or Polygon. Polygons must be closed. Note that unlike the JanusGraph API GeoJSON specifies coordinates as lng lat. Collections If you are using Elasticsearch then you can index properties with SET and LIST cardinality. For instance: mgmt = graph . openManagement () nameProperty = mgmt . makePropertyKey ( \"names\" ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () mgmt . buildIndex ( \"search\" , Vertex . class ). addKey ( nameProperty , Mapping . STRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () //Insert a vertex person = graph . addVertex () person . property ( \"names\" , \"Robert\" ) person . property ( \"names\" , \"Bob\" ) graph . tx (). commit () //Now query it g . V (). has ( \"names\" , \"Bob\" ). count (). next () //1 g . V (). has ( \"names\" , \"Robert\" ). count (). next () //1","title":"Search Predicates and Mixed Index Data Types"},{"location":"interactions/search-predicates/#search-predicates-and-mixed-index-data-types","text":"This page lists all of the comparison predicates that JanusGraph supports in global graph search and local traversals. Note Some parts of this section require mixed index, see mixed index backend .","title":"Search Predicates and Mixed Index Data Types"},{"location":"interactions/search-predicates/#compare-predicate","text":"The Compare enum specifies the following comparison predicates used for index query construction and used in the examples above: eq (equal) neq (not equal) gt (greater than) gte (greater than or equal) lt (less than) lte (less than or equal) All comparison predicates are supported by String, numeric, Date and Instant data types. Boolean and UUID data types support the eq and neq comparison predicates. eq and neq can be used on Boolean and UUID.","title":"Compare Predicate"},{"location":"interactions/search-predicates/#text-predicate","text":"The Text enum specifies the Text Search used to query for matching text or string values. We differentiate between two types of predicates: Text search predicates which match against the individual words inside a text string after it has been tokenized. These predicates are not case sensitive. textContains : is true if (at least) one word inside the text string matches the query string textNotContains : is true if no words inside the text string match the query string textContainsPrefix : is true if (at least) one word inside the text string begins with the query string textNotContainsPrefix : is true if no words inside the text string begin with the query string textContainsRegex : is true if (at least) one word inside the text string matches the given regular expression textNotContainsRegex : is true if no words inside the text string match the given regular expression textContainsFuzzy : is true if (at least) one word inside the text string is similar to the query string (based on Levenshtein edit distance) textNotContainsFuzzy : is true if no words inside the text string are similar to the query string (based on Levenshtein edit distance) textContainsPhrase : is true if the text string contains the exact sequence of words in the query string textNotContainsPhrase : is true if the text string does not contain the sequence of words in the query string String search predicates which match against the entire string value textPrefix : if the string value starts with the given query string textNotPrefix : if the string value does not start with the given query string textRegex : if the string value matches the given regular expression in its entirety textNotRegex : if the string value does not match the given regular expression in its entirety textFuzzy : if the string value is similar to the given query string (based on Levenshtein edit distance) textNotFuzzy : if the string value is not similar to the given query string (based on Levenshtein edit distance) See Text Search for more information about full-text and string search.","title":"Text Predicate"},{"location":"interactions/search-predicates/#geo-predicate","text":"The Geo enum specifies geo-location predicates. geoIntersect which holds true if the two geometric objects have at least one point in common (opposite of geoDisjoint ). geoWithin which holds true if one geometric object contains the other. geoDisjoint which holds true if the two geometric objects have no points in common (opposite of geoIntersect ). geoContains which holds true if one geometric object is contained by the other. See Geo Mapping for more information about geo search.","title":"Geo Predicate"},{"location":"interactions/search-predicates/#query-examples","text":"The following query examples demonstrate some of the predicates on the tutorial graph. // 1) Find vertices with the name \"hercules\" g . V (). has ( \"name\" , \"hercules\" ) // 2) Find all vertices with an age greater than 50 g . V (). has ( \"age\" , gt ( 50 )) // or find all vertices between 1000 (inclusive) and 5000 (exclusive) years of age and order by ascending age g . V (). has ( \"age\" , inside ( 1000 , 5000 )). order (). by ( \"age\" , asc ) // which returns the same result set as the following query but in reverse order g . V (). has ( \"age\" , inside ( 1000 , 5000 )). order (). by ( \"age\" , desc ) // 3) Find all edges where the place is at most 50 kilometers from the given latitude-longitude pair g . E (). has ( \"place\" , geoWithin ( Geoshape . circle ( 37.97 , 23.72 , 50 ))) // 4) Find all edges where reason contains the word \"loves\" g . E (). has ( \"reason\" , textContains ( \"loves\" )) // or all edges which contain two words (need to chunk into individual words) g . E (). has ( \"reason\" , textContains ( \"loves\" )). has ( \"reason\" , textContains ( \"breezes\" )) // or all edges which contain words that start with \"lov\" g . E (). has ( \"reason\" , textContainsPrefix ( \"lov\" )) // or all edges which contain words that match the regular expression \"br[ez]*s\" in their entirety g . E (). has ( \"reason\" , textContainsRegex ( \"br[ez]*s\" )) // or all edges which contain words similar to \"love\" g . E (). has ( \"reason\" , textContainsFuzzy ( \"love\" )) // 5) Find all vertices older than a thousand years and named \"saturn\" g . V (). has ( \"age\" , gt ( 1000 )). has ( \"name\" , \"saturn\" )","title":"Query Examples"},{"location":"interactions/search-predicates/#data-type-support","text":"While JanusGraph's composite indexes support any data type that can be stored in JanusGraph, the mixed indexes are limited to the following data types. Byte Short Integer Long Float Double String Geoshape Date Instant UUID Additional data types will be supported in the future.","title":"Data Type Support"},{"location":"interactions/search-predicates/#geoshape-data-type","text":"The Geoshape data type supports representing a point, circle, box, line, polygon, multi-point, multi-line and multi-polygon. Index backends currently support indexing points, circles, boxes, lines, polygons, multi-point, multi-line, multi-polygon and geometry collection. Geospatial index lookups are only supported via mixed indexes. To construct a Geoshape use the following methods: //lat, lng Geoshape . point ( 37.97 , 23.72 ) //lat, lng, radius in km Geoshape . circle ( 37.97 , 23.72 , 50 ) //SW lat, SW lng, NE lat, NE lng Geoshape . box ( 37.97 , 23.72 , 38.97 , 24.72 ) //WKT Geoshape . fromWkt ( \"POLYGON ((35.4 48.9, 35.6 48.9, 35.6 49.1, 35.4 49.1, 35.4 48.9))\" ) //MultiPoint Geoshape . geoshape ( Geoshape . getShapeFactory (). multiPoint (). pointXY ( 60.0 , 60.0 ). pointXY ( 120.0 , 60.0 ) . build ()) //MultiLine Geoshape . geoshape ( Geoshape . getShapeFactory (). multiLineString () . add ( Geoshape . getShapeFactory (). lineString (). pointXY ( 59.0 , 60.0 ). pointXY ( 61.0 , 60.0 )) . add ( Geoshape . getShapeFactory (). lineString (). pointXY ( 119.0 , 60.0 ). pointXY ( 121.0 , 60.0 )). build ()) //MultiPolygon Geoshape . geoshape ( Geoshape . getShapeFactory (). multiPolygon () . add ( Geoshape . getShapeFactory (). polygon (). pointXY ( 59.0 , 59.0 ). pointXY ( 61.0 , 59.0 ) . pointXY ( 61.0 , 61.0 ). pointXY ( 59.0 , 61.0 ). pointXY ( 59.0 , 59.0 )) . add ( Geoshape . getShapeFactory (). polygon (). pointXY ( 119.0 , 59.0 ). pointXY ( 121.0 , 59.0 ) . pointXY ( 121.0 , 61.0 ). pointXY ( 119.0 , 61.0 ). pointXY ( 119.0 , 59.0 )). build ()) //GeometryCollection Geoshape . geoshape ( Geoshape . getGeometryCollectionBuilder () . add ( Geoshape . getShapeFactory (). pointXY ( 60.0 , 60.0 )) . add ( Geoshape . getShapeFactory (). lineString (). pointXY ( 119.0 , 60.0 ). pointXY ( 121.0 , 60.0 ). build ()) . add ( Geoshape . getShapeFactory (). polygon (). pointXY ( 119.0 , 59.0 ). pointXY ( 121.0 , 59.0 ) . pointXY ( 121.0 , 61.0 ). pointXY ( 119.0 , 61.0 ). pointXY ( 119.0 , 59.0 ). build ()). build ()) In addition, when importing a graph via GraphSON the geometry may be represented by GeoJSON: string \"37.97, 23.72\" list [ 37.97 , 23.72 ] GeoJSON feature { \"type\" : \"Feature\" , \"geometry\" : { \"type\" : \"Point\" , \"coordinates\" : [ 125.6 , 10.1 ] }, \"properties\" : { \"name\" : \"Dinagat Islands\" } } GeoJSON geometry { \"type\" : \"Point\" , \"coordinates\" : [ 125.6 , 10.1 ] } GeoJSON may be specified as Point, Circle, LineString or Polygon. Polygons must be closed. Note that unlike the JanusGraph API GeoJSON specifies coordinates as lng lat.","title":"Geoshape Data Type"},{"location":"interactions/search-predicates/#collections","text":"If you are using Elasticsearch then you can index properties with SET and LIST cardinality. For instance: mgmt = graph . openManagement () nameProperty = mgmt . makePropertyKey ( \"names\" ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () mgmt . buildIndex ( \"search\" , Vertex . class ). addKey ( nameProperty , Mapping . STRING . asParameter ()). buildMixedIndex ( \"search\" ) mgmt . commit () //Insert a vertex person = graph . addVertex () person . property ( \"names\" , \"Robert\" ) person . property ( \"names\" , \"Bob\" ) graph . tx (). commit () //Now query it g . V (). has ( \"names\" , \"Bob\" ). count (). next () //1 g . V (). has ( \"names\" , \"Robert\" ). count (). next () //1","title":"Collections"},{"location":"interactions/transactions/","text":"Transactions Almost all interaction with JanusGraph is associated with a transaction. JanusGraph transactions are safe for concurrent use by multiple threads. Methods on a JanusGraph instance like graph.V(...) and graph.tx().commit() perform a ThreadLocal lookup to retrieve or create a transaction associated with the calling thread. Callers can alternatively forego ThreadLocal transaction management in favor of calling graph.tx().createThreadedTx() , which returns a reference to a transaction object with methods to read/write graph data and commit or rollback. JanusGraph transactions are not necessarily ACID. They can be so configured on BerkeleyDB, but they are not generally so on Cassandra or HBase, where the underlying storage system does not provide serializable isolation or multi-row atomic writes and the cost of simulating those properties would be substantial. This section describes JanusGraph\u2019s transactional semantics and API. Transaction Handling Every graph operation in JanusGraph occurs within the context of a transaction. According to the TinkerPop\u2019s transactional specification, each thread opens its own transaction against the graph database with the first operation (i.e. retrieval or mutation) on the graph: graph = JanusGraphFactory . open ( \"berkeleyje:/tmp/janusgraph\" ) juno = graph . addVertex () //Automatically opens a new transaction juno . property ( \"name\" , \"juno\" ) graph . tx (). commit () //Commits transaction In this example, a local JanusGraph graph database is opened. Adding the vertex \"juno\" is the first operation (in this thread) which automatically opens a new transaction. All subsequent operations occur in the context of that same transaction until the transaction is explicitly stopped or the graph database is closed. If transactions are still open when close() is called, then the behavior of the outstanding transactions is technically undefined. In practice, any non-thread-bound transactions will usually be effectively rolled back, but the thread-bound transaction belonging to the thread that invoked shutdown will first be committed. Note, that both read and write operations occur within the context of a transaction. Transactional Scope All graph elements (vertices, edges, and types) are associated with the transactional scope in which they were retrieved or created. Under TinkerPop\u2019s default transactional semantics, transactions are automatically created with the first operation on the graph and closed explicitly using commit() or rollback() . Once the transaction is closed, all graph elements associated with that transaction become stale and unavailable. However, JanusGraph will automatically transition vertices and types into the new transactional scope as shown in this example: graph = JanusGraphFactory . open ( \"berkeleyje:/tmp/janusgraph\" ) juno = graph . addVertex () //Automatically opens a new transaction graph . tx (). commit () //Ends transaction juno . property ( \"name\" , \"juno\" ) //Vertex is automatically transitioned Edges, on the other hand, are not automatically transitioned and cannot be accessed outside their original transaction. They must be explicitly transitioned: e = juno . addEdge ( \"knows\" , graph . addVertex ()) graph . tx (). commit () //Ends transaction e = g . E ( e ). next () //Need to refresh edge e . property ( \"time\" , 99 ) Transaction Failures When committing a transaction, JanusGraph will attempt to persist all changes to the storage backend. This might not always be successful due to IO exceptions, network errors, machine crashes or resource unavailability. Hence, transactions can fail. In fact, transactions will eventually fail in sufficiently large systems. Therefore, we highly recommend that your code expects and accommodates such failures: try { if ( g . V (). has ( \"name\" , name ). iterator (). hasNext ()) throw new IllegalArgumentException ( \"Username already taken: \" + name ) user = graph . addVertex () user . property ( \"name\" , name ) graph . tx (). commit () } catch ( Exception e ) { //Recover, retry, or return error message println ( e . getMessage ()) } The example above demonstrates a simplified user signup implementation where name is the name of the user who wishes to register. First, it is checked whether a user with that name already exists. If not, a new user vertex is created and the name assigned. Finally, the transaction is committed. If the transaction fails, a JanusGraphException is thrown. There are a variety of reasons why a transaction may fail. JanusGraph differentiates between potentially temporary and permanent failures. Potentially temporary failures are those related to resource unavailability and IO hiccups (e.g. network timeouts). JanusGraph automatically tries to recover from temporary failures by retrying to persist the transactional state after some delay. The number of retry attempts and the retry delay are configurable (see Configuration Reference ). Permanent failures can be caused by complete connection loss, hardware failure or lock contention. To understand the cause of lock contention, consider the signup example above and suppose a user tries to signup with username \"juno\". That username may still be available at the beginning of the transaction but by the time the transaction is committed, another user might have concurrently registered with \"juno\" as well and that transaction holds the lock on the username therefore causing the other transaction to fail. Depending on the transaction semantics one can recover from a lock contention failure by re-running the entire transaction. Permanent exceptions that can fail a transaction include: PermanentLockingException( Local lock contention ): Another local thread has already been granted a conflicting lock. PermanentLockingException( Expected value mismatch for X: expected=Y vs actual=Z ): The verification that the value read in this transaction is the same as the one in the datastore after applying for the lock failed. In other words, another transaction modified the value after it had been read and modified. Multi-Threaded Transactions JanusGraph supports multi-threaded transactions through TinkerPop\u2019s threaded transactions. Hence, to speed up transaction processing and utilize multi-core architectures multiple threads can run concurrently in a single transaction. With TinkerPop\u2019s default transaction handling, each thread automatically opens its own transaction against the graph database. To open a thread-independent transaction, use the createThreadedTx() method. threadedGraph = graph . tx (). createThreadedTx (); threads = new Thread [ 10 ]; for ( int i = 0 ; i < threads . length ; i ++) { threads [ i ]= new Thread ({ println ( \"Do something with 'threadedGraph'\" ); }); threads [ i ]. start (); } for ( int i = 0 ; i < threads . length ; i ++) threads [ i ]. join (); threadedGraph . tx (). commit (); The createThreadedTx() method returns a new Graph object that represents this newly opened transaction. The graph object tx supports all of the methods that the original graph did, but does so without opening new transactions for each thread. This allows us to start multiple threads which all work concurrently in the same transaction and one of which finally commits the transaction when all threads have completed their work. JanusGraph relies on optimized concurrent data structures to support hundreds of concurrent threads running efficiently in a single transaction. Concurrent Algorithms Thread independent transactions started through createThreadedTx() are particularly useful when implementing concurrent graph algorithms. Most traversal or message-passing (ego-centric) like graph algorithms are embarrassingly parallel which means they can be parallelized and executed through multiple threads with little effort. Each of these threads can operate on a single Graph object returned by createThreadedTx() without blocking each other. Nested Transactions Another use case for thread independent transactions is nested transactions that ought to be independent from the surrounding transaction. For instance, assume a long running transactional job that has to create a new vertex with a unique name. Since enforcing unique names requires the acquisition of a lock (see Eventually-Consistent Storage Backends for more detail) and since the transaction is running for a long time, lock congestion and expensive transactional failures are likely. v1 = graph . addVertex () //Do many other things v2 = graph . addVertex () v2 . property ( \"uniqueName\" , \"foo\" ) v1 . addEdge ( \"related\" , v2 ) //Do many other things graph . tx (). commit () // This long-running tx might fail due to contention on its uniqueName lock One way around this is to create the vertex in a short, nested thread-independent transaction as demonstrated by the following pseudo code v1 = graph . addVertex () //Do many other things tx = graph . tx (). createThreadedTx () v2 = tx . addVertex () v2 . property ( \"uniqueName\" , \"foo\" ) tx . commit () // Any lock contention will be detected here v1 . addEdge ( \"related\" , g . V ( v2 ). next ()) // Need to load v2 into outer transaction //Do many other things graph . tx (). commit () // Can't fail due to uniqueName write lock contention involving v2 Common Transaction Handling Problems Transactions are started automatically with the first operation executed against the graph. One does NOT have to start a transaction manually. The method newTransaction is used to start multi-threaded transactions only. Transactions are automatically started under the TinkerPop semantics but not automatically terminated. Transactions must be terminated manually with commit() or rollback() . If a commit() transactions fails, it should be terminated manually with rollback() after catching the failure. Manual termination of transactions is necessary because only the user knows the transactional boundary. A transaction will attempt to maintain its state from the beginning of the transaction. This might lead to unexpected behavior in multi-threaded applications as illustrated in the following artificial example v = g . V ( 4 ). next () // Retrieve vertex, first action automatically starts transaction g . V ( v ). bothE () >> returns nothing , v has no edges //thread is idle for a few seconds, another thread adds edges to v g . V ( v ). bothE () >> still returns nothing because the transactional state from the beginning is maintained Such unexpected behavior is likely to occur in client-server applications where the server maintains multiple threads to answer client requests. It is therefore important to terminate the transaction after a unit of work (e.g. code snippet, query, etc). So, the example above should be: v = g . V ( 4 ). next () // Retrieve vertex, first action automatically starts transaction g . V ( v ). bothE () graph . tx (). commit () //thread is idle for a few seconds, another thread adds edges to v g . V ( v ). bothE () >> returns the newly added edge graph . tx (). commit () When using multi-threaded transactions via newTransaction all vertices and edges retrieved or created in the scope of that transaction are not available outside the scope of that transaction. Accessing such elements after the transaction has been closed will result in an exception. As demonstrated in the example above, such elements have to be explicitly refreshed in the new transaction using g.V(existingVertex) or g.E(existingEdge) . Transaction Configuration JanusGraph\u2019s JanusGraph.buildTransaction() method gives the user the ability to configure and start a new multi-threaded transaction against a JanusGraph . Hence, it is identical to JanusGraph.newTransaction() with additional configuration options. buildTransaction() returns a TransactionBuilder which allows the following aspects of a transaction to be configured: readOnly() - makes the transaction read-only and any attempt to modify the graph will result in an exception. enableBatchLoading() - enables batch-loading for an individual transaction. This setting results in similar efficiencies as the graph-wide setting storage.batch-loading due to the disabling of consistency checks and other optimizations. Unlike storage.batch-loading this option will not change the behavior of the storage backend. Similarly, you could call disableBatchLoading() to disable batch-loading for an individual transaction. propertyPrefetching(boolean) - enables or disables property prefetching, i.e. query.fast-property , for an individual transaction. If enabled, all properties of a particular vertex will be pre-fetched on the first vertex property access, which eliminates backend calls on subsequent property access for the same vertex. multiQuery(boolean) - enables or disables query batching, i.e. query.batch , for an individual transaction. If enabled, queries for a single traversal will be batched when executed against the storage backend. setTimestamp(long) - Sets the timestamp for this transaction as communicated to the storage backend for persistence. Depending on the storage backend, this setting may be ignored. For eventually consistent backends, this is the timestamp used to resolve write conflicts. If this setting is not explicitly specified, JanusGraph uses the current time. setVertexCacheSize(long size) - The number of vertices this transaction caches in memory. The larger this number, the more memory a transaction can potentially consume. If this number is too small, a transaction might have to re-fetch data which causes delays in particular for long running transactions. checkExternalVertexExistence(boolean) - Whether this transaction should verify the existence of vertices for user provided vertex ids. Such checks requires access to the database which takes time. The existence check should only be disabled if the user is absolutely sure that the vertex must exist - otherwise data corruption can ensue. checkInternalVertexExistence(boolean) - Whether this transaction should double-check the existence of vertices during query execution. This can be useful to avoid phantom vertices on eventually consistent storage backends. Disabled by default. Enabling this setting can slow down query processing. consistencyChecks(boolean) - Whether JanusGraph should enforce schema level consistency constraints (e.g. multiplicity constraints). Disabling consistency checks leads to better performance but requires that the user ensures consistency confirmation at the application level to avoid inconsistencies. USE WITH GREAT CARE! skipDBCacheRead() - Disables JanusGraph database level cache access during read operations. Doesn't have any effect if database level cache was disabled via config cache.db-cache . Once, the desired configuration options have been specified, the new transaction is started via start() which returns a JanusGraphTransaction .","title":"Transactions"},{"location":"interactions/transactions/#transactions","text":"Almost all interaction with JanusGraph is associated with a transaction. JanusGraph transactions are safe for concurrent use by multiple threads. Methods on a JanusGraph instance like graph.V(...) and graph.tx().commit() perform a ThreadLocal lookup to retrieve or create a transaction associated with the calling thread. Callers can alternatively forego ThreadLocal transaction management in favor of calling graph.tx().createThreadedTx() , which returns a reference to a transaction object with methods to read/write graph data and commit or rollback. JanusGraph transactions are not necessarily ACID. They can be so configured on BerkeleyDB, but they are not generally so on Cassandra or HBase, where the underlying storage system does not provide serializable isolation or multi-row atomic writes and the cost of simulating those properties would be substantial. This section describes JanusGraph\u2019s transactional semantics and API.","title":"Transactions"},{"location":"interactions/transactions/#transaction-handling","text":"Every graph operation in JanusGraph occurs within the context of a transaction. According to the TinkerPop\u2019s transactional specification, each thread opens its own transaction against the graph database with the first operation (i.e. retrieval or mutation) on the graph: graph = JanusGraphFactory . open ( \"berkeleyje:/tmp/janusgraph\" ) juno = graph . addVertex () //Automatically opens a new transaction juno . property ( \"name\" , \"juno\" ) graph . tx (). commit () //Commits transaction In this example, a local JanusGraph graph database is opened. Adding the vertex \"juno\" is the first operation (in this thread) which automatically opens a new transaction. All subsequent operations occur in the context of that same transaction until the transaction is explicitly stopped or the graph database is closed. If transactions are still open when close() is called, then the behavior of the outstanding transactions is technically undefined. In practice, any non-thread-bound transactions will usually be effectively rolled back, but the thread-bound transaction belonging to the thread that invoked shutdown will first be committed. Note, that both read and write operations occur within the context of a transaction.","title":"Transaction Handling"},{"location":"interactions/transactions/#transactional-scope","text":"All graph elements (vertices, edges, and types) are associated with the transactional scope in which they were retrieved or created. Under TinkerPop\u2019s default transactional semantics, transactions are automatically created with the first operation on the graph and closed explicitly using commit() or rollback() . Once the transaction is closed, all graph elements associated with that transaction become stale and unavailable. However, JanusGraph will automatically transition vertices and types into the new transactional scope as shown in this example: graph = JanusGraphFactory . open ( \"berkeleyje:/tmp/janusgraph\" ) juno = graph . addVertex () //Automatically opens a new transaction graph . tx (). commit () //Ends transaction juno . property ( \"name\" , \"juno\" ) //Vertex is automatically transitioned Edges, on the other hand, are not automatically transitioned and cannot be accessed outside their original transaction. They must be explicitly transitioned: e = juno . addEdge ( \"knows\" , graph . addVertex ()) graph . tx (). commit () //Ends transaction e = g . E ( e ). next () //Need to refresh edge e . property ( \"time\" , 99 )","title":"Transactional Scope"},{"location":"interactions/transactions/#transaction-failures","text":"When committing a transaction, JanusGraph will attempt to persist all changes to the storage backend. This might not always be successful due to IO exceptions, network errors, machine crashes or resource unavailability. Hence, transactions can fail. In fact, transactions will eventually fail in sufficiently large systems. Therefore, we highly recommend that your code expects and accommodates such failures: try { if ( g . V (). has ( \"name\" , name ). iterator (). hasNext ()) throw new IllegalArgumentException ( \"Username already taken: \" + name ) user = graph . addVertex () user . property ( \"name\" , name ) graph . tx (). commit () } catch ( Exception e ) { //Recover, retry, or return error message println ( e . getMessage ()) } The example above demonstrates a simplified user signup implementation where name is the name of the user who wishes to register. First, it is checked whether a user with that name already exists. If not, a new user vertex is created and the name assigned. Finally, the transaction is committed. If the transaction fails, a JanusGraphException is thrown. There are a variety of reasons why a transaction may fail. JanusGraph differentiates between potentially temporary and permanent failures. Potentially temporary failures are those related to resource unavailability and IO hiccups (e.g. network timeouts). JanusGraph automatically tries to recover from temporary failures by retrying to persist the transactional state after some delay. The number of retry attempts and the retry delay are configurable (see Configuration Reference ). Permanent failures can be caused by complete connection loss, hardware failure or lock contention. To understand the cause of lock contention, consider the signup example above and suppose a user tries to signup with username \"juno\". That username may still be available at the beginning of the transaction but by the time the transaction is committed, another user might have concurrently registered with \"juno\" as well and that transaction holds the lock on the username therefore causing the other transaction to fail. Depending on the transaction semantics one can recover from a lock contention failure by re-running the entire transaction. Permanent exceptions that can fail a transaction include: PermanentLockingException( Local lock contention ): Another local thread has already been granted a conflicting lock. PermanentLockingException( Expected value mismatch for X: expected=Y vs actual=Z ): The verification that the value read in this transaction is the same as the one in the datastore after applying for the lock failed. In other words, another transaction modified the value after it had been read and modified.","title":"Transaction Failures"},{"location":"interactions/transactions/#multi-threaded-transactions","text":"JanusGraph supports multi-threaded transactions through TinkerPop\u2019s threaded transactions. Hence, to speed up transaction processing and utilize multi-core architectures multiple threads can run concurrently in a single transaction. With TinkerPop\u2019s default transaction handling, each thread automatically opens its own transaction against the graph database. To open a thread-independent transaction, use the createThreadedTx() method. threadedGraph = graph . tx (). createThreadedTx (); threads = new Thread [ 10 ]; for ( int i = 0 ; i < threads . length ; i ++) { threads [ i ]= new Thread ({ println ( \"Do something with 'threadedGraph'\" ); }); threads [ i ]. start (); } for ( int i = 0 ; i < threads . length ; i ++) threads [ i ]. join (); threadedGraph . tx (). commit (); The createThreadedTx() method returns a new Graph object that represents this newly opened transaction. The graph object tx supports all of the methods that the original graph did, but does so without opening new transactions for each thread. This allows us to start multiple threads which all work concurrently in the same transaction and one of which finally commits the transaction when all threads have completed their work. JanusGraph relies on optimized concurrent data structures to support hundreds of concurrent threads running efficiently in a single transaction.","title":"Multi-Threaded Transactions"},{"location":"interactions/transactions/#concurrent-algorithms","text":"Thread independent transactions started through createThreadedTx() are particularly useful when implementing concurrent graph algorithms. Most traversal or message-passing (ego-centric) like graph algorithms are embarrassingly parallel which means they can be parallelized and executed through multiple threads with little effort. Each of these threads can operate on a single Graph object returned by createThreadedTx() without blocking each other.","title":"Concurrent Algorithms"},{"location":"interactions/transactions/#nested-transactions","text":"Another use case for thread independent transactions is nested transactions that ought to be independent from the surrounding transaction. For instance, assume a long running transactional job that has to create a new vertex with a unique name. Since enforcing unique names requires the acquisition of a lock (see Eventually-Consistent Storage Backends for more detail) and since the transaction is running for a long time, lock congestion and expensive transactional failures are likely. v1 = graph . addVertex () //Do many other things v2 = graph . addVertex () v2 . property ( \"uniqueName\" , \"foo\" ) v1 . addEdge ( \"related\" , v2 ) //Do many other things graph . tx (). commit () // This long-running tx might fail due to contention on its uniqueName lock One way around this is to create the vertex in a short, nested thread-independent transaction as demonstrated by the following pseudo code v1 = graph . addVertex () //Do many other things tx = graph . tx (). createThreadedTx () v2 = tx . addVertex () v2 . property ( \"uniqueName\" , \"foo\" ) tx . commit () // Any lock contention will be detected here v1 . addEdge ( \"related\" , g . V ( v2 ). next ()) // Need to load v2 into outer transaction //Do many other things graph . tx (). commit () // Can't fail due to uniqueName write lock contention involving v2","title":"Nested Transactions"},{"location":"interactions/transactions/#common-transaction-handling-problems","text":"Transactions are started automatically with the first operation executed against the graph. One does NOT have to start a transaction manually. The method newTransaction is used to start multi-threaded transactions only. Transactions are automatically started under the TinkerPop semantics but not automatically terminated. Transactions must be terminated manually with commit() or rollback() . If a commit() transactions fails, it should be terminated manually with rollback() after catching the failure. Manual termination of transactions is necessary because only the user knows the transactional boundary. A transaction will attempt to maintain its state from the beginning of the transaction. This might lead to unexpected behavior in multi-threaded applications as illustrated in the following artificial example v = g . V ( 4 ). next () // Retrieve vertex, first action automatically starts transaction g . V ( v ). bothE () >> returns nothing , v has no edges //thread is idle for a few seconds, another thread adds edges to v g . V ( v ). bothE () >> still returns nothing because the transactional state from the beginning is maintained Such unexpected behavior is likely to occur in client-server applications where the server maintains multiple threads to answer client requests. It is therefore important to terminate the transaction after a unit of work (e.g. code snippet, query, etc). So, the example above should be: v = g . V ( 4 ). next () // Retrieve vertex, first action automatically starts transaction g . V ( v ). bothE () graph . tx (). commit () //thread is idle for a few seconds, another thread adds edges to v g . V ( v ). bothE () >> returns the newly added edge graph . tx (). commit () When using multi-threaded transactions via newTransaction all vertices and edges retrieved or created in the scope of that transaction are not available outside the scope of that transaction. Accessing such elements after the transaction has been closed will result in an exception. As demonstrated in the example above, such elements have to be explicitly refreshed in the new transaction using g.V(existingVertex) or g.E(existingEdge) .","title":"Common Transaction Handling Problems"},{"location":"interactions/transactions/#transaction-configuration","text":"JanusGraph\u2019s JanusGraph.buildTransaction() method gives the user the ability to configure and start a new multi-threaded transaction against a JanusGraph . Hence, it is identical to JanusGraph.newTransaction() with additional configuration options. buildTransaction() returns a TransactionBuilder which allows the following aspects of a transaction to be configured: readOnly() - makes the transaction read-only and any attempt to modify the graph will result in an exception. enableBatchLoading() - enables batch-loading for an individual transaction. This setting results in similar efficiencies as the graph-wide setting storage.batch-loading due to the disabling of consistency checks and other optimizations. Unlike storage.batch-loading this option will not change the behavior of the storage backend. Similarly, you could call disableBatchLoading() to disable batch-loading for an individual transaction. propertyPrefetching(boolean) - enables or disables property prefetching, i.e. query.fast-property , for an individual transaction. If enabled, all properties of a particular vertex will be pre-fetched on the first vertex property access, which eliminates backend calls on subsequent property access for the same vertex. multiQuery(boolean) - enables or disables query batching, i.e. query.batch , for an individual transaction. If enabled, queries for a single traversal will be batched when executed against the storage backend. setTimestamp(long) - Sets the timestamp for this transaction as communicated to the storage backend for persistence. Depending on the storage backend, this setting may be ignored. For eventually consistent backends, this is the timestamp used to resolve write conflicts. If this setting is not explicitly specified, JanusGraph uses the current time. setVertexCacheSize(long size) - The number of vertices this transaction caches in memory. The larger this number, the more memory a transaction can potentially consume. If this number is too small, a transaction might have to re-fetch data which causes delays in particular for long running transactions. checkExternalVertexExistence(boolean) - Whether this transaction should verify the existence of vertices for user provided vertex ids. Such checks requires access to the database which takes time. The existence check should only be disabled if the user is absolutely sure that the vertex must exist - otherwise data corruption can ensue. checkInternalVertexExistence(boolean) - Whether this transaction should double-check the existence of vertices during query execution. This can be useful to avoid phantom vertices on eventually consistent storage backends. Disabled by default. Enabling this setting can slow down query processing. consistencyChecks(boolean) - Whether JanusGraph should enforce schema level consistency constraints (e.g. multiplicity constraints). Disabling consistency checks leads to better performance but requires that the user ensures consistency confirmation at the application level to avoid inconsistencies. USE WITH GREAT CARE! skipDBCacheRead() - Disables JanusGraph database level cache access during read operations. Doesn't have any effect if database level cache was disabled via config cache.db-cache . Once, the desired configuration options have been specified, the new transaction is started via start() which returns a JanusGraphTransaction .","title":"Transaction Configuration"},{"location":"interactions/connecting/","text":"JanusGraph can be queried from all languages for which a TinkerPop driver exists. Drivers allow sending of Gremlin traversals to a Gremlin Server like the JanusGraph Server . A list of TinkerPop drivers is available on TinkerPop\u2019s homepage . In addition to drivers, there exist query languages for TinkerPop that make it easier to use Gremlin in different programming languages like Java, Python, or C#. Some of these languages even construct Gremlin traversals from completely different query languages like Cypher or SPARQL. Since JanusGraph implements TinkerPop, all of these languages can be used together with JanusGraph.","title":"Introduction"},{"location":"interactions/connecting/dotnet/","text":"Connecting from .NET Gremlin traversals can be constructed with Gremlin.Net just like in Gremlin-Java or Gremlin-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. The main syntactical difference for Gremlin.Net is that it follows .NET naming conventions, e.g., method names use PascalCase instead of camelCase. Getting Started with JanusGraph and Gremlin.Net To get started with Gremlin.Net: Create a console application: dotnet new console -o GremlinExample Add Gremlin.Net: dotnet add package Gremlin.Net -v 3 .6.2 Create a GraphTraversalSource which is the basis for all Gremlin traversals: using static Gremlin . Net . Process . Traversal . AnonymousTraversalSource ; var client = new GremlinClient ( new GremlinServer ( \"localhost\" , 8182 )); // The client should be disposed on shut down to release resources // and to close open connections with client.Dispose() var g = Traversal (). WithRemote ( new DriverRemoteConnection ( client )); // Reuse 'g' across the application Execute a simple traversal: var herculesAge = g . V (). Has ( \"name\" , \"hercules\" ). Values < int >( \"age\" ). Next (); Console . WriteLine ( $ \"Hercules is {herculesAge} years old.\" ); The traversal can also be executed asynchronously by using Promise() which is the recommended way as the underlying driver in Gremlin.Net also works asynchronously: var herculesAge = await g . V (). Has ( \"name\" , \"hercules\" ). Values < int >( \"age\" ). Promise ( t => t . Next ()); JanusGraph.Net for JanusGraph Specific Types and Predicates JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin.Net. JanusGraph.Net is a .NET library that adds support for these types and predicates to Gremlin.Net. After installing the library, a message serializer needs to be configured for JanusGraph which can be done like this for GraphSON 3: var client = new GremlinClient(new GremlinServer(\"localhost\", 8182), new JanusGraphGraphSONMessageSerializer()); or like this for GraphBinary: var client = new GremlinClient(new GremlinServer(\"localhost\", 8182), new GraphBinaryMessageSerializer(JanusGraphTypeSerializerRegistry.Instance)); Refer to the documentation of JanusGraph.Net for more information about the library, including its compatibility with different JanusGraph versions and differences in support between the different serialization formats .","title":"Using .NET"},{"location":"interactions/connecting/dotnet/#connecting-from-net","text":"Gremlin traversals can be constructed with Gremlin.Net just like in Gremlin-Java or Gremlin-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. The main syntactical difference for Gremlin.Net is that it follows .NET naming conventions, e.g., method names use PascalCase instead of camelCase.","title":"Connecting from .NET"},{"location":"interactions/connecting/dotnet/#getting-started-with-janusgraph-and-gremlinnet","text":"To get started with Gremlin.Net: Create a console application: dotnet new console -o GremlinExample Add Gremlin.Net: dotnet add package Gremlin.Net -v 3 .6.2 Create a GraphTraversalSource which is the basis for all Gremlin traversals: using static Gremlin . Net . Process . Traversal . AnonymousTraversalSource ; var client = new GremlinClient ( new GremlinServer ( \"localhost\" , 8182 )); // The client should be disposed on shut down to release resources // and to close open connections with client.Dispose() var g = Traversal (). WithRemote ( new DriverRemoteConnection ( client )); // Reuse 'g' across the application Execute a simple traversal: var herculesAge = g . V (). Has ( \"name\" , \"hercules\" ). Values < int >( \"age\" ). Next (); Console . WriteLine ( $ \"Hercules is {herculesAge} years old.\" ); The traversal can also be executed asynchronously by using Promise() which is the recommended way as the underlying driver in Gremlin.Net also works asynchronously: var herculesAge = await g . V (). Has ( \"name\" , \"hercules\" ). Values < int >( \"age\" ). Promise ( t => t . Next ());","title":"Getting Started with JanusGraph and Gremlin.Net"},{"location":"interactions/connecting/dotnet/#janusgraphnet-for-janusgraph-specific-types-and-predicates","text":"JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin.Net. JanusGraph.Net is a .NET library that adds support for these types and predicates to Gremlin.Net. After installing the library, a message serializer needs to be configured for JanusGraph which can be done like this for GraphSON 3: var client = new GremlinClient(new GremlinServer(\"localhost\", 8182), new JanusGraphGraphSONMessageSerializer()); or like this for GraphBinary: var client = new GremlinClient(new GremlinServer(\"localhost\", 8182), new GraphBinaryMessageSerializer(JanusGraphTypeSerializerRegistry.Instance)); Refer to the documentation of JanusGraph.Net for more information about the library, including its compatibility with different JanusGraph versions and differences in support between the different serialization formats .","title":"JanusGraph.Net for JanusGraph Specific Types and Predicates"},{"location":"interactions/connecting/java/","text":"Connecting from Java While it is possible to embed JanusGraph as a library inside a Java application and then directly connect to the backend, this section assumes that the application connects to JanusGraph Server. For information on how to embed JanusGraph, see the JanusGraph Examples projects . This section only covers how applications can connect to JanusGraph Server using the GraphBinary serialization. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. Getting Started with JanusGraph and Gremlin-Java To get started with JanusGraph in Java: Create an application with Maven: mvn archetype:generate -DgroupId = com.mycompany.project -DartifactId = gremlin-example -DarchetypeArtifactId = maven-archetype-quickstart -DinteractiveMode = false Add dependencies on janusgraph-driver and gremlin-driver to the dependency manager: Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-driver </artifactId> <version> 1.0.0-rc2 </version> </dependency> <dependency> <groupId> org.apache.tinkerpop </groupId> <artifactId> gremlin-driver </artifactId> <version> 3.6.2 </version> </dependency> Gradle implementation \"org.janusgraph:janusgraph-driver:1.0.0-rc2\" implementation \"org.apache.tinkerpop:gremlin-driver:3.6.2\" Add two configuration files, conf/remote-graph.properties and conf/remote-objects.yaml : conf/remote-graph.properties gremlin.remote.remoteConnectionClass=org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection gremlin.remote.driver.clusterFile=conf/remote-objects.yaml gremlin.remote.driver.sourceName=g conf/remote-objects.yaml hosts : [ localhost ] port : 8182 serializer : { className : org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1 , config : { ioRegistries : [ org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry ] }} Create a GraphTraversalSource which is the basis for all Gremlin traversals: import static org.apache.tinkerpop.gremlin.process.traversal.AnonymousTraversalSource.traversal ; GraphTraversalSource g = traversal (). withRemote ( \"conf/remote-graph.properties\" ); // Reuse 'g' across the application // and close it on shut-down to close open connections with g.close() Execute a simple traversal: Object herculesAge = g . V (). has ( \"name\" , \"hercules\" ). values ( \"age\" ). next (); System . out . println ( \"Hercules is \" + herculesAge + \" years old.\" ); next() is a terminal step that submits the traversal to the Gremlin Server and returns a single result. JanusGraph Specific Types and Predicates JanusGraph specific types and predicates can be used directly from a Java application through the dependency janusgraph-driver . Consideration for Accessing the Management API Note We are working to replace the Management API with a language acoustic solution, see Management System The described connection uses GraphBinary and the janusgraph-driver which doesn't allow accessing the internal JanusGraph components such as ManagementSystem . To access the ManagementSystem , you have to submit java-based scripts, see Submitting Scripts , or directly accessing JanusGraph by local opening a JanusGraph instance.","title":"Using Java"},{"location":"interactions/connecting/java/#connecting-from-java","text":"While it is possible to embed JanusGraph as a library inside a Java application and then directly connect to the backend, this section assumes that the application connects to JanusGraph Server. For information on how to embed JanusGraph, see the JanusGraph Examples projects . This section only covers how applications can connect to JanusGraph Server using the GraphBinary serialization. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources.","title":"Connecting from Java"},{"location":"interactions/connecting/java/#getting-started-with-janusgraph-and-gremlin-java","text":"To get started with JanusGraph in Java: Create an application with Maven: mvn archetype:generate -DgroupId = com.mycompany.project -DartifactId = gremlin-example -DarchetypeArtifactId = maven-archetype-quickstart -DinteractiveMode = false Add dependencies on janusgraph-driver and gremlin-driver to the dependency manager: Maven <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-driver </artifactId> <version> 1.0.0-rc2 </version> </dependency> <dependency> <groupId> org.apache.tinkerpop </groupId> <artifactId> gremlin-driver </artifactId> <version> 3.6.2 </version> </dependency> Gradle implementation \"org.janusgraph:janusgraph-driver:1.0.0-rc2\" implementation \"org.apache.tinkerpop:gremlin-driver:3.6.2\" Add two configuration files, conf/remote-graph.properties and conf/remote-objects.yaml : conf/remote-graph.properties gremlin.remote.remoteConnectionClass=org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection gremlin.remote.driver.clusterFile=conf/remote-objects.yaml gremlin.remote.driver.sourceName=g conf/remote-objects.yaml hosts : [ localhost ] port : 8182 serializer : { className : org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1 , config : { ioRegistries : [ org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry ] }} Create a GraphTraversalSource which is the basis for all Gremlin traversals: import static org.apache.tinkerpop.gremlin.process.traversal.AnonymousTraversalSource.traversal ; GraphTraversalSource g = traversal (). withRemote ( \"conf/remote-graph.properties\" ); // Reuse 'g' across the application // and close it on shut-down to close open connections with g.close() Execute a simple traversal: Object herculesAge = g . V (). has ( \"name\" , \"hercules\" ). values ( \"age\" ). next (); System . out . println ( \"Hercules is \" + herculesAge + \" years old.\" ); next() is a terminal step that submits the traversal to the Gremlin Server and returns a single result.","title":"Getting Started with JanusGraph and Gremlin-Java"},{"location":"interactions/connecting/java/#janusgraph-specific-types-and-predicates","text":"JanusGraph specific types and predicates can be used directly from a Java application through the dependency janusgraph-driver .","title":"JanusGraph Specific Types and Predicates"},{"location":"interactions/connecting/java/#consideration-for-accessing-the-management-api","text":"Note We are working to replace the Management API with a language acoustic solution, see Management System The described connection uses GraphBinary and the janusgraph-driver which doesn't allow accessing the internal JanusGraph components such as ManagementSystem . To access the ManagementSystem , you have to submit java-based scripts, see Submitting Scripts , or directly accessing JanusGraph by local opening a JanusGraph instance.","title":"Consideration for Accessing the Management API"},{"location":"interactions/connecting/python/","text":"Connecting from Python Gremlin traversals can be constructed with Gremlin-Python just like in Gremlin-Java or Gremlin-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. Important Some Gremlin step and predicate names are reserved words in Python. Those names are simply postfixed with _ in Gremlin-Python, e.g., in() becomes in_() , not() becomes not_() , and so on. The other names affected by this are: all , and , as , from , global , is , list , or , and set . Getting Started with JanusGraph and Gremlin-Python To get started with Gremlin-Python: Install Gremlin-Python: pip install gremlinpython == 3 .6.2 Create a text file gremlinexample.py and add the following imports to it: from gremlin_python import statics from gremlin_python.structure.graph import Graph from gremlin_python.process.graph_traversal import __ from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection Create a GraphTraversalSource which is the basis for all Gremlin traversals: from gremlin_python.process.anonymous_traversal import traversal connection = DriverRemoteConnection ( 'ws://localhost:8182/gremlin' , 'g' ) # The connection should be closed on shut down to close open connections with connection.close() g = traversal () . withRemote ( connection ) # Reuse 'g' across the application Execute a simple traversal: hercules_age = g . V () . has ( 'name' , 'hercules' ) . values ( 'age' ) . next () print ( f 'Hercules is { hercules_age } years old.' ) next() is a terminal step that submits the traversal to the Gremlin Server and returns a single result. JanusGraph Specific Types and Predicates JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin-Python.","title":"Using Python"},{"location":"interactions/connecting/python/#connecting-from-python","text":"Gremlin traversals can be constructed with Gremlin-Python just like in Gremlin-Java or Gremlin-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. Important Some Gremlin step and predicate names are reserved words in Python. Those names are simply postfixed with _ in Gremlin-Python, e.g., in() becomes in_() , not() becomes not_() , and so on. The other names affected by this are: all , and , as , from , global , is , list , or , and set .","title":"Connecting from Python"},{"location":"interactions/connecting/python/#getting-started-with-janusgraph-and-gremlin-python","text":"To get started with Gremlin-Python: Install Gremlin-Python: pip install gremlinpython == 3 .6.2 Create a text file gremlinexample.py and add the following imports to it: from gremlin_python import statics from gremlin_python.structure.graph import Graph from gremlin_python.process.graph_traversal import __ from gremlin_python.driver.driver_remote_connection import DriverRemoteConnection Create a GraphTraversalSource which is the basis for all Gremlin traversals: from gremlin_python.process.anonymous_traversal import traversal connection = DriverRemoteConnection ( 'ws://localhost:8182/gremlin' , 'g' ) # The connection should be closed on shut down to close open connections with connection.close() g = traversal () . withRemote ( connection ) # Reuse 'g' across the application Execute a simple traversal: hercules_age = g . V () . has ( 'name' , 'hercules' ) . values ( 'age' ) . next () print ( f 'Hercules is { hercules_age } years old.' ) next() is a terminal step that submits the traversal to the Gremlin Server and returns a single result.","title":"Getting Started with JanusGraph and Gremlin-Python"},{"location":"interactions/connecting/python/#janusgraph-specific-types-and-predicates","text":"JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin-Python.","title":"JanusGraph Specific Types and Predicates"},{"location":"operations/batch-processing/","text":"Batch Processing In order to answer queries, JanusGraph has to perform queries against the storage backend. In general, there are two ways of doing this: Once data from the backend is needed, execute a backend query and continue with the result. Maintain a list of what data is needed. Once the list reaches a certain size, execute a batched backend query to fetch all of it at once. The first option tends to be more responsive and consume less memory because the query can emit the first results very early without waiting for larger batches of queries to complete. It however sends many small queries to the storage backend for traversals that traverse a high number of vertices which leads to poor performance. That is why JanusGraph uses batch processing by default. Both of these options are described in greater detail below, including information about configuring batch processing. Note The default setting was changed in version 1.0.0. Older versions of JanusGraph used no batch processing (first option) by default. No Batch Processing In terms of graph traversals, the execution of queries is loosely coupled to the principle of Depth-First-Search. Use this configuration in use cases where for example ... ... each query only accesses few vertices of the graph. ... your application does not need the full result set immediately but rather requires a low latency for the first results to arrive. Possible limitations Traversing large neighborhoods can make the query slow. Steps to explicitly configure this option: Ensure query.batch is set to false Unrestricted Batch Processing Using this configuration, each step which traverses the Graph starting from a vertex (so e.g. in() , outE() and values() but not inV() or otherV() and also not valueMap() , see #2444 ) becomes a blocking operator which means that it produces no results until all the results of the previous step are known. Only then, a single backend query is executed and the results are passed to the next step. Manual barrier() steps do not affect this in any meaningful way. This way of execution can be thought of as a Breadth-First-Search. Use this configuration in use cases where for example ... ... your queries are likely to access multiple vertices in each step. ... there is a significant network latency between JanusGraph and the storage backend. Possible limitations Increased memory consumption If limit steps occur late in the query, there might be an unnecessary overhead produced by the steps before the limit step. Performing very large backend queries could stress the storage backend. Steps to explicitly configure this option: Ensure query.batch is set to true Ensure query.limit-batch-size is set to false Limited Batch Processing Using this configuration, each step which traverses the Graph starting from a vertex (so e.g. in() , outE() and values() but not inV() or otherV() ) aggregates a number of vertices first, before executing a batched backend query. This aggregation phase and backend query phase will repeat until all vertices are processed. In contrast to unrestricted batch processing where one batch corresponds to one step in the query, this approach can construct multiple batches per step. This is the default configuration of JanusGraph since version 1.0.0. Configuring the batch size Although batch size does not necessarily need to be configured, it can provide an additional tuning parameter to improve the performance of a query. By default, the batch size of TinkerPop's barrier step will be used, which is currently at 2500. The batch size of each vertex step can be individually configured by prepending a barrier(<size>) step. For example, in the query below, the first out() step would use the default batch size of 2500 and the second out() step would use a manually configured batch size of 1234: g . V ( list_of_vertices ). out (). barrier ( 1234 ). out () Using the same mechanism, the limit can also be increased or even effectively disabled by configuring an arbitrarily high value. For local traversals which start with a vertex step, the limit is best configured outside the local traversal, as seen below: g . V ( list_of_vertices ). out (). barrier ( 1234 ). where ( __ . out ()) The reason this is necessary is that traversers enter local traversals one by one. As part of the local traversal, the barrier(1234) step would not be allowed to aggregate multiple traversers. A special case applies to repeat() steps. Because the local traversal of a repeat() step has two inputs (first, the step before the repeat() step and second, the last step of the repeated traversal, which feeds the result back to the beginning), two limits can be configured here. g . V ( list_of_vertices ). barrier ( 1234 ). repeat ( __ . barrier ( 2345 ). out ()). times ( 5 ) Because the local traversal's output is also the input for the next iteration, the barrier(1234) step in front of the local traversal can only aggregate traversers once they enter the repeat step for the first time. For each iteration, the inner barrier(2345) is used to aggregate traversers from the previous iteration. Use this configuration in use cases where for example ... ... you have a mixture of traversals that traverse a high number of vertices and traversals that only access few vertices of the graph. Possible limitations Increased memory consumption (compared to no batch processing) The performance of queries depends on the configured batch size. If you use this configuration, make sure that the latency and throughput of your queries meet your requirements and if not, tweak the batch size accordingly. Steps to explicitly configure this option: Ensure query.batch is set to true Ensure query.limit-batch-size is set to true","title":"Batch Processing"},{"location":"operations/batch-processing/#batch-processing","text":"In order to answer queries, JanusGraph has to perform queries against the storage backend. In general, there are two ways of doing this: Once data from the backend is needed, execute a backend query and continue with the result. Maintain a list of what data is needed. Once the list reaches a certain size, execute a batched backend query to fetch all of it at once. The first option tends to be more responsive and consume less memory because the query can emit the first results very early without waiting for larger batches of queries to complete. It however sends many small queries to the storage backend for traversals that traverse a high number of vertices which leads to poor performance. That is why JanusGraph uses batch processing by default. Both of these options are described in greater detail below, including information about configuring batch processing. Note The default setting was changed in version 1.0.0. Older versions of JanusGraph used no batch processing (first option) by default.","title":"Batch Processing"},{"location":"operations/batch-processing/#no-batch-processing","text":"In terms of graph traversals, the execution of queries is loosely coupled to the principle of Depth-First-Search.","title":"No Batch Processing"},{"location":"operations/batch-processing/#use-this-configuration-in-use-cases-where-for-example","text":"... each query only accesses few vertices of the graph. ... your application does not need the full result set immediately but rather requires a low latency for the first results to arrive.","title":"Use this configuration in use cases where for example ..."},{"location":"operations/batch-processing/#possible-limitations","text":"Traversing large neighborhoods can make the query slow.","title":"Possible limitations"},{"location":"operations/batch-processing/#steps-to-explicitly-configure-this-option","text":"Ensure query.batch is set to false","title":"Steps to explicitly configure this option:"},{"location":"operations/batch-processing/#unrestricted-batch-processing","text":"Using this configuration, each step which traverses the Graph starting from a vertex (so e.g. in() , outE() and values() but not inV() or otherV() and also not valueMap() , see #2444 ) becomes a blocking operator which means that it produces no results until all the results of the previous step are known. Only then, a single backend query is executed and the results are passed to the next step. Manual barrier() steps do not affect this in any meaningful way. This way of execution can be thought of as a Breadth-First-Search.","title":"Unrestricted Batch Processing"},{"location":"operations/batch-processing/#use-this-configuration-in-use-cases-where-for-example_1","text":"... your queries are likely to access multiple vertices in each step. ... there is a significant network latency between JanusGraph and the storage backend.","title":"Use this configuration in use cases where for example ..."},{"location":"operations/batch-processing/#possible-limitations_1","text":"Increased memory consumption If limit steps occur late in the query, there might be an unnecessary overhead produced by the steps before the limit step. Performing very large backend queries could stress the storage backend.","title":"Possible limitations"},{"location":"operations/batch-processing/#steps-to-explicitly-configure-this-option_1","text":"Ensure query.batch is set to true Ensure query.limit-batch-size is set to false","title":"Steps to explicitly configure this option:"},{"location":"operations/batch-processing/#limited-batch-processing","text":"Using this configuration, each step which traverses the Graph starting from a vertex (so e.g. in() , outE() and values() but not inV() or otherV() ) aggregates a number of vertices first, before executing a batched backend query. This aggregation phase and backend query phase will repeat until all vertices are processed. In contrast to unrestricted batch processing where one batch corresponds to one step in the query, this approach can construct multiple batches per step. This is the default configuration of JanusGraph since version 1.0.0.","title":"Limited Batch Processing"},{"location":"operations/batch-processing/#configuring-the-batch-size","text":"Although batch size does not necessarily need to be configured, it can provide an additional tuning parameter to improve the performance of a query. By default, the batch size of TinkerPop's barrier step will be used, which is currently at 2500. The batch size of each vertex step can be individually configured by prepending a barrier(<size>) step. For example, in the query below, the first out() step would use the default batch size of 2500 and the second out() step would use a manually configured batch size of 1234: g . V ( list_of_vertices ). out (). barrier ( 1234 ). out () Using the same mechanism, the limit can also be increased or even effectively disabled by configuring an arbitrarily high value. For local traversals which start with a vertex step, the limit is best configured outside the local traversal, as seen below: g . V ( list_of_vertices ). out (). barrier ( 1234 ). where ( __ . out ()) The reason this is necessary is that traversers enter local traversals one by one. As part of the local traversal, the barrier(1234) step would not be allowed to aggregate multiple traversers. A special case applies to repeat() steps. Because the local traversal of a repeat() step has two inputs (first, the step before the repeat() step and second, the last step of the repeated traversal, which feeds the result back to the beginning), two limits can be configured here. g . V ( list_of_vertices ). barrier ( 1234 ). repeat ( __ . barrier ( 2345 ). out ()). times ( 5 ) Because the local traversal's output is also the input for the next iteration, the barrier(1234) step in front of the local traversal can only aggregate traversers once they enter the repeat step for the first time. For each iteration, the inner barrier(2345) is used to aggregate traversers from the previous iteration.","title":"Configuring the batch size"},{"location":"operations/batch-processing/#use-this-configuration-in-use-cases-where-for-example_2","text":"... you have a mixture of traversals that traverse a high number of vertices and traversals that only access few vertices of the graph.","title":"Use this configuration in use cases where for example ..."},{"location":"operations/batch-processing/#possible-limitations_2","text":"Increased memory consumption (compared to no batch processing) The performance of queries depends on the configured batch size. If you use this configuration, make sure that the latency and throughput of your queries meet your requirements and if not, tweak the batch size accordingly.","title":"Possible limitations"},{"location":"operations/batch-processing/#steps-to-explicitly-configure-this-option_2","text":"Ensure query.batch is set to true Ensure query.limit-batch-size is set to true","title":"Steps to explicitly configure this option:"},{"location":"operations/bulk-loading/","text":"Bulk Loading There are a number of configuration options and tools that make ingesting large amounts of graph data into JanusGraph more efficient. Such ingestion is referred to as bulk loading in contrast to the default transactional loading where small amounts of data are added through individual transactions. There are a number of use cases for bulk loading data into JanusGraph, including: Introducing JanusGraph into an existing environment with existing data and migrating or duplicating this data into a new JanusGraph cluster. Using JanusGraph as an end point of an ETL process. Adding an existing or external graph datasets (e.g. publicly available RDF datasets ) to a running JanusGraph cluster. Updating a JanusGraph graph with results from a graph analytics job. This page describes configuration options and tools that make bulk loading more efficient in JanusGraph. Please observe the limitations and assumptions for each option carefully before proceeding to avoid data loss or data corruption. This documentation focuses on JanusGraph specific optimization. In addition, consider improving the chosen storage backend and (optional) index backend for high write performance. Please refer to the documentation of the respective backend for more information. Configuration Options Batch Loading Enabling the storage.batch-loading configuration option will have the biggest positive impact on bulk loading times for most applications. Enabling batch loading disables JanusGraph internal consistency checks in a number of places. Most importantly, it disables locking. In other words, JanusGraph assumes that the data to be loaded into JanusGraph is consistent with the graph and hence disables its own checks in the interest of performance. In many bulk loading scenarios it is significantly cheaper to ensure data consistency prior to loading the data than ensuring data consistency while loading it into the database. The storage.batch-loading configuration option exists because of this observation. For example, consider the use case of bulk loading existing user profiles into JanusGraph. Furthermore, assume that the username property key has a unique composite index defined on it, i.e. usernames must be unique across the entire graph. If the user profiles are imported from another database, username uniqueness might already guaranteed. If not, it is simple to sort the profiles by name and filter out duplicates or writing a Hadoop job that does such filtering. Now, we can enable storage.batch-loading which significantly reduces the bulk loading time because JanusGraph does not have to check for every added user whether the name already exists in the database. Important : Enabling storage.batch-loading requires the user to ensure that the loaded data is internally consistent and consistent with any data already in the graph. In particular, concurrent type creation can lead to severe data integrity issues when batch loading is enabled. Hence, we strongly encourage disabling automatic type creation by setting schema.default = none in the graph configuration. Optimizing ID Allocation ID Block Size Each newly added vertex or edge is assigned a unique id. JanusGraph\u2019s id pool manager acquires ids in blocks for a particular JanusGraph instance. The id block acquisition process is expensive because it needs to guarantee globally unique assignment of blocks. Increasing ids.block-size reduces the number of acquisitions but potentially leaves many ids unassigned and hence wasted. For transactional workloads the default block size is reasonable, but during bulk loading vertices and edges are added much more frequently and in rapid succession. Hence, it is generally advisable to increase the block size by a factor of 10 or more depending on the number of vertices to be added per machine. Rule of thumb : Set ids.block-size to the number of vertices you expect to add per JanusGraph instance per hour. Important: All JanusGraph instances MUST be configured with the same value for ids.block-size to ensure proper id allocation. Hence, be careful to shut down all JanusGraph instances prior to changing this value. ID Acquisition Process When id blocks are frequently allocated by many JanusGraph instances in parallel, allocation conflicts between instances will inevitably arise and slow down the allocation process. In addition, the increased write load due to bulk loading may further slow down the process to the point where JanusGraph considers it failed and throws an exception. There are three configuration options that can be tuned to avoid this. 1) ids.authority.wait-time configures the time in milliseconds the id pool manager waits for an id block application to be acknowledged by the storage backend. The shorter this time, the more likely it is that an application will fail on a congested storage cluster. Rule of thumb : Set this to the sum of the 95th percentile read and write times measured on the storage backend cluster under load. Important : This value should be the same across all JanusGraph instances. 2) ids.renew-timeout configures the number of milliseconds JanusGraph\u2019s id pool manager will wait in total while attempting to acquire a new id block before failing. Rule of thumb : Set this value to be as large feasible to not have to wait too long for unrecoverable failures. The only downside of increasing it is that JanusGraph will try for a long time on an unavailable storage backend cluster. Optimizing Writes and Reads Buffer Size JanusGraph buffers writes and executes them in small batches to reduce the number of requests against the storage backend. The size of these batches is controlled by storage.buffer-size . When executing a lot of writes in a short period of time, it is possible that the storage backend can become overloaded with write requests. In that case, increasing storage.buffer-size can avoid failure by increasing the number of writes per request and thereby lowering the number of requests. However, increasing the buffer size increases the latency of the write request and its likelihood of failure. Hence, it is not advisable to increase this setting for transactional loads and one should carefully experiment with this setting during bulk loading. Read and Write Robustness During bulk loading, the load on the cluster typically increases making it more likely for read and write operations to fail (in particular if the buffer size is increased as described above). storage.read-attempts and storage.write-attempts configure how many times JanusGraph will attempt to execute a read or write operation against the storage backend before giving up. If it is expected that there is a high load on the backend during bulk loading, it is generally advisable to increase these configuration options. storage.attempt-wait specifies the number of milliseconds that JanusGraph will wait before re-attempting a failed backend operation. A higher value can ensure that operation re-tries do not further increase the load on the backend. Strategies Parallelizing the Load By parallelizing the bulk loading across multiple machines, the load time can be greatly reduced if JanusGraph\u2019s storage backend cluster is large enough to serve the additional requests. This is essentially the approach JanusGraph with TinkerPop\u2019s Hadoop-Gremlin takes to bulk loading data into JanusGraph using MapReduce. If Hadoop cannot be used for parallelizing the bulk loading process, here are some high level guidelines for effectively parallelizing the loading process: In some cases, the graph data can be decomposed into multiple disconnected subgraphs. Those subgraphs can be loaded independently in parallel across multiple machines (for instance, using BatchGraph as described above). If the graph cannot be decomposed, it is often beneficial to load in multiple steps where the last two steps can be parallelized across multiple machines: Make sure the vertex and edge data sets are de-duplicated and consistent. Set batch-loading=true . Possibly optimize additional configuration settings described above. Add all the vertices with their properties to the graph (but no edges). Maintain a (distributed) map from vertex id (as defined by the loaded data) to JanusGraph\u2019s internal vertex id (i.e. vertex.getId() ) which is a 64 bit long id. Add all the edges using the map to look-up JanusGraph\u2019s vertex id and retrieving the vertices using that id. Q&A What should I do to avoid the following exception during batch-loading: java.io.IOException: ID renewal thread on partition [X] did not complete in time. ? This exception is mostly likely caused by repeated time-outs during the id allocation phase due to highly stressed storage backend. Refer to the section on ID Allocation Optimization above.","title":"Bulk Loading"},{"location":"operations/bulk-loading/#bulk-loading","text":"There are a number of configuration options and tools that make ingesting large amounts of graph data into JanusGraph more efficient. Such ingestion is referred to as bulk loading in contrast to the default transactional loading where small amounts of data are added through individual transactions. There are a number of use cases for bulk loading data into JanusGraph, including: Introducing JanusGraph into an existing environment with existing data and migrating or duplicating this data into a new JanusGraph cluster. Using JanusGraph as an end point of an ETL process. Adding an existing or external graph datasets (e.g. publicly available RDF datasets ) to a running JanusGraph cluster. Updating a JanusGraph graph with results from a graph analytics job. This page describes configuration options and tools that make bulk loading more efficient in JanusGraph. Please observe the limitations and assumptions for each option carefully before proceeding to avoid data loss or data corruption. This documentation focuses on JanusGraph specific optimization. In addition, consider improving the chosen storage backend and (optional) index backend for high write performance. Please refer to the documentation of the respective backend for more information.","title":"Bulk Loading"},{"location":"operations/bulk-loading/#configuration-options","text":"","title":"Configuration Options"},{"location":"operations/bulk-loading/#batch-loading","text":"Enabling the storage.batch-loading configuration option will have the biggest positive impact on bulk loading times for most applications. Enabling batch loading disables JanusGraph internal consistency checks in a number of places. Most importantly, it disables locking. In other words, JanusGraph assumes that the data to be loaded into JanusGraph is consistent with the graph and hence disables its own checks in the interest of performance. In many bulk loading scenarios it is significantly cheaper to ensure data consistency prior to loading the data than ensuring data consistency while loading it into the database. The storage.batch-loading configuration option exists because of this observation. For example, consider the use case of bulk loading existing user profiles into JanusGraph. Furthermore, assume that the username property key has a unique composite index defined on it, i.e. usernames must be unique across the entire graph. If the user profiles are imported from another database, username uniqueness might already guaranteed. If not, it is simple to sort the profiles by name and filter out duplicates or writing a Hadoop job that does such filtering. Now, we can enable storage.batch-loading which significantly reduces the bulk loading time because JanusGraph does not have to check for every added user whether the name already exists in the database. Important : Enabling storage.batch-loading requires the user to ensure that the loaded data is internally consistent and consistent with any data already in the graph. In particular, concurrent type creation can lead to severe data integrity issues when batch loading is enabled. Hence, we strongly encourage disabling automatic type creation by setting schema.default = none in the graph configuration.","title":"Batch Loading"},{"location":"operations/bulk-loading/#optimizing-id-allocation","text":"","title":"Optimizing ID Allocation"},{"location":"operations/bulk-loading/#id-block-size","text":"Each newly added vertex or edge is assigned a unique id. JanusGraph\u2019s id pool manager acquires ids in blocks for a particular JanusGraph instance. The id block acquisition process is expensive because it needs to guarantee globally unique assignment of blocks. Increasing ids.block-size reduces the number of acquisitions but potentially leaves many ids unassigned and hence wasted. For transactional workloads the default block size is reasonable, but during bulk loading vertices and edges are added much more frequently and in rapid succession. Hence, it is generally advisable to increase the block size by a factor of 10 or more depending on the number of vertices to be added per machine. Rule of thumb : Set ids.block-size to the number of vertices you expect to add per JanusGraph instance per hour. Important: All JanusGraph instances MUST be configured with the same value for ids.block-size to ensure proper id allocation. Hence, be careful to shut down all JanusGraph instances prior to changing this value.","title":"ID Block Size"},{"location":"operations/bulk-loading/#id-acquisition-process","text":"When id blocks are frequently allocated by many JanusGraph instances in parallel, allocation conflicts between instances will inevitably arise and slow down the allocation process. In addition, the increased write load due to bulk loading may further slow down the process to the point where JanusGraph considers it failed and throws an exception. There are three configuration options that can be tuned to avoid this. 1) ids.authority.wait-time configures the time in milliseconds the id pool manager waits for an id block application to be acknowledged by the storage backend. The shorter this time, the more likely it is that an application will fail on a congested storage cluster. Rule of thumb : Set this to the sum of the 95th percentile read and write times measured on the storage backend cluster under load. Important : This value should be the same across all JanusGraph instances. 2) ids.renew-timeout configures the number of milliseconds JanusGraph\u2019s id pool manager will wait in total while attempting to acquire a new id block before failing. Rule of thumb : Set this value to be as large feasible to not have to wait too long for unrecoverable failures. The only downside of increasing it is that JanusGraph will try for a long time on an unavailable storage backend cluster.","title":"ID Acquisition Process"},{"location":"operations/bulk-loading/#optimizing-writes-and-reads","text":"","title":"Optimizing Writes and Reads"},{"location":"operations/bulk-loading/#buffer-size","text":"JanusGraph buffers writes and executes them in small batches to reduce the number of requests against the storage backend. The size of these batches is controlled by storage.buffer-size . When executing a lot of writes in a short period of time, it is possible that the storage backend can become overloaded with write requests. In that case, increasing storage.buffer-size can avoid failure by increasing the number of writes per request and thereby lowering the number of requests. However, increasing the buffer size increases the latency of the write request and its likelihood of failure. Hence, it is not advisable to increase this setting for transactional loads and one should carefully experiment with this setting during bulk loading.","title":"Buffer Size"},{"location":"operations/bulk-loading/#read-and-write-robustness","text":"During bulk loading, the load on the cluster typically increases making it more likely for read and write operations to fail (in particular if the buffer size is increased as described above). storage.read-attempts and storage.write-attempts configure how many times JanusGraph will attempt to execute a read or write operation against the storage backend before giving up. If it is expected that there is a high load on the backend during bulk loading, it is generally advisable to increase these configuration options. storage.attempt-wait specifies the number of milliseconds that JanusGraph will wait before re-attempting a failed backend operation. A higher value can ensure that operation re-tries do not further increase the load on the backend.","title":"Read and Write Robustness"},{"location":"operations/bulk-loading/#strategies","text":"","title":"Strategies"},{"location":"operations/bulk-loading/#parallelizing-the-load","text":"By parallelizing the bulk loading across multiple machines, the load time can be greatly reduced if JanusGraph\u2019s storage backend cluster is large enough to serve the additional requests. This is essentially the approach JanusGraph with TinkerPop\u2019s Hadoop-Gremlin takes to bulk loading data into JanusGraph using MapReduce. If Hadoop cannot be used for parallelizing the bulk loading process, here are some high level guidelines for effectively parallelizing the loading process: In some cases, the graph data can be decomposed into multiple disconnected subgraphs. Those subgraphs can be loaded independently in parallel across multiple machines (for instance, using BatchGraph as described above). If the graph cannot be decomposed, it is often beneficial to load in multiple steps where the last two steps can be parallelized across multiple machines: Make sure the vertex and edge data sets are de-duplicated and consistent. Set batch-loading=true . Possibly optimize additional configuration settings described above. Add all the vertices with their properties to the graph (but no edges). Maintain a (distributed) map from vertex id (as defined by the loaded data) to JanusGraph\u2019s internal vertex id (i.e. vertex.getId() ) which is a 64 bit long id. Add all the edges using the map to look-up JanusGraph\u2019s vertex id and retrieving the vertices using that id.","title":"Parallelizing the Load"},{"location":"operations/bulk-loading/#qa","text":"What should I do to avoid the following exception during batch-loading: java.io.IOException: ID renewal thread on partition [X] did not complete in time. ? This exception is mostly likely caused by repeated time-outs during the id allocation phase due to highly stressed storage backend. Refer to the section on ID Allocation Optimization above.","title":"Q&amp;A"},{"location":"operations/cache/","text":"JanusGraph Cache Caching JanusGraph employs multiple layers of data caching to facilitate fast graph traversals. The caching layers are listed here in the order they are accessed from within a JanusGraph transaction. The closer the cache is to the transaction, the faster the cache access and the higher the memory footprint and maintenance overhead. Transaction-Level Caching Within an open transaction, JanusGraph maintains two caches: Vertex Cache: Caches accessed vertices and their adjacency list (or subsets thereof) so that subsequent access is significantly faster within the same transaction. Hence, this cache speeds up iterative traversals. Index Cache: Caches the results for index queries so that subsequent index calls can be served from memory instead of calling the index backend and (usually) waiting for one or more network round trips. The size of both of those is determined by the transaction cache size . The transaction cache size can be configured via cache.tx-cache-size or on a per transaction basis by opening a transaction via the transaction builder graph.buildTransaction() and using the setVertexCacheSize(int) method. Vertex Cache The vertex cache contains vertices and the subset of their adjacency list (properties and edges) that has been retrieved in a particular transaction. The maximum number of vertices maintained in this cache is equal to the transaction cache size. If the transaction workload is an iterative traversal, the vertex cache will significantly speed it up. If the same vertex is not accessed again in the transaction, the transaction level cache will make no difference. Note, that the size of the vertex cache on heap is not only determined by the number of vertices it may hold but also by the size of their adjacency list. In other words, vertices with large adjacency lists (i.e. many incident edges) will consume more space in this cache than those with smaller lists. Furthermore note, that modified vertices are pinned in the cache, which means they cannot be evicted since that would entail loosing their changes. Therefore, transaction which contain a lot of modifications may end up with a larger than configured vertex cache. Assuming your vertex is not evicted from cache, or it is evicted from cache but your program context still holds the reference to the vertex, then its properties and edges are cached together with the vertex. This means once a property is queried, any subsequent reads will hit the cache. In case you want to force JanusGraph to read from the data storage again (provided you have disabled database-level cache), or you simply want to save memory, you could clear the cache of that vertex manually. Note this operation is not gremlin compliant, so you need to cast your vertex into CacheVertex type to do the refresh: // first read automatically caches the property together with v v . property ( \"prop\" ). value (); // force refresh to clear the cache (( CacheVertex ) v ). refresh (); // now a subsequent read will look up in JanusGraph's database-level // cache, and then backend storage read in case of cache miss v . property ( \"prop\" ). value (); Note that refresh operation cannot guarantee your current transaction reads the latest data from an eventual consistent backend. You should not attempt to achieve Compare-And-Set (CAS) via refresh operation, even though it might be helpful to detect conflicts among transactions in some cases. Index Cache The index cache contains the results of index queries executed in the context of this transaction. Subsequent identical index calls will be served from this cache and are therefore significantly cheaper. If the same index call never occurs twice in the same transaction, the index cache makes no difference. Each entry in the index cache is given a weight equal to 2 + result set size and the total weight of the cache will not exceed half of the transaction cache size. Database Level Caching The database level cache contains vertices and the subset of their adjacency list (properties and edges) across multiple transactions and beyond the duration of a single transaction. The database level cache is shared by all transactions across a database. It is more space efficient than the transaction level caches but also slightly slower to access. In contrast to the transaction level caches, the database level caches do not expire immediately after closing a transaction. Hence, the database level cache significantly speeds up graph traversals for read heavy workloads across transactions. A read looks up in transaction-level cache first, and then database-level cache. Configuration Reference lists all of the configuration options that pertain to JanusGraph\u2019s database level cache. This page attempts to explain their usage. Most importantly, the database level cache is disabled by default in the current release version of JanusGraph. To enable it, set cache.db-cache=true . Cache Expiration Time The most important setting for performance and query behavior is the cache expiration time which is configured via cache.db-cache-time . The cache will hold graph elements for at most that many milliseconds. If an element expires, the data will be re-read from the storage backend on the next access. If there is only one JanusGraph instance accessing the storage backend or if this instance is the only one modifying the graph, the cache expiration can be set to 0 which disables cache expiration. This allows the cache to hold elements indefinitely (unless they are evicted due to space constraints or on update) which provides the best cache performance. Since no other JanusGraph instance is modifying the graph, there is no danger of holding on to stale data. If there are multiple JanusGraph instances accessing the storage backend, the time should be set to the maximum time that can be allowed between another JanusGraph instance modifying the graph and this JanusGraph instance seeing the data. If any change should be immediately visible to all JanusGraph instances, the database level cache should be disabled in a distributed setup. However, for most applications it is acceptable that a particular JanusGraph instance sees remote modifications with some delay. The larger the maximally allowed delay, the better the cache performance. Note, that a given JanusGraph instance will always immediately see its own modifications to the graph irrespective of the configured cache expiration time. Cache Size The configuration option cache.db-cache-size controls how much heap space JanusGraph\u2019s database level cache is allowed to consume. The larger the cache, the more effective it will be. However, large cache sizes can lead to excessive GC and poor performance. The cache size can be configured as a percentage (expressed as a decimal between 0 and 1) of the total heap space available to the JVM running JanusGraph or as an absolute number of bytes. Note, that the cache size refers to the amount of heap space that is exclusively occupied by the cache. JanusGraph\u2019s other data structures and each open transaction will occupy additional heap space. If additional software layers are running in the same JVM, those may occupy a significant amount of heap space as well (e.g. Gremlin Server, etc). Be conservative in your heap memory estimation. Configuring a cache that is too large can lead to out-of-memory exceptions and excessive GC. In practice, you might observe JanusGraph uses more memory than configured for database level cache. This is a known limitation due to difficulty of estimating size of deserialized objects. Clean Up Wait Time When a vertex is locally modified (e.g. an edge is added) all of the vertex\u2019s related database level cache entries are marked as expired and eventually evicted. This will cause JanusGraph to refresh the vertex\u2019s data from the storage backend on the next access and re-populate the cache. However, when the storage backend is eventually consistent, the modifications that triggered the eviction may not yet be visible. By configuring cache.db-cache-clean-wait , the cache will wait for at least this many milliseconds before repopulating the cache with the entry retrieved from the storage backend. If JanusGraph runs locally or against a storage backend that guarantees immediate visibility of modifications, this value can be set to 0. Storage Backend Caching Each storage backend maintains its own data caching layer. These caches benefit from compression, data compactness, coordinated expiration and are often maintained off heap which means that large caches can be used without running into garbage collection issues. While these caches can be significantly larger than the database level cache, they are also slower to access. The exact type of caching and its properties depends on the particular storage backend . Please refer to the respective documentation for more information about the caching infrastructure and how to optimize it.","title":"JanusGraph Cache"},{"location":"operations/cache/#janusgraph-cache","text":"","title":"JanusGraph Cache"},{"location":"operations/cache/#caching","text":"JanusGraph employs multiple layers of data caching to facilitate fast graph traversals. The caching layers are listed here in the order they are accessed from within a JanusGraph transaction. The closer the cache is to the transaction, the faster the cache access and the higher the memory footprint and maintenance overhead.","title":"Caching"},{"location":"operations/cache/#transaction-level-caching","text":"Within an open transaction, JanusGraph maintains two caches: Vertex Cache: Caches accessed vertices and their adjacency list (or subsets thereof) so that subsequent access is significantly faster within the same transaction. Hence, this cache speeds up iterative traversals. Index Cache: Caches the results for index queries so that subsequent index calls can be served from memory instead of calling the index backend and (usually) waiting for one or more network round trips. The size of both of those is determined by the transaction cache size . The transaction cache size can be configured via cache.tx-cache-size or on a per transaction basis by opening a transaction via the transaction builder graph.buildTransaction() and using the setVertexCacheSize(int) method.","title":"Transaction-Level Caching"},{"location":"operations/cache/#vertex-cache","text":"The vertex cache contains vertices and the subset of their adjacency list (properties and edges) that has been retrieved in a particular transaction. The maximum number of vertices maintained in this cache is equal to the transaction cache size. If the transaction workload is an iterative traversal, the vertex cache will significantly speed it up. If the same vertex is not accessed again in the transaction, the transaction level cache will make no difference. Note, that the size of the vertex cache on heap is not only determined by the number of vertices it may hold but also by the size of their adjacency list. In other words, vertices with large adjacency lists (i.e. many incident edges) will consume more space in this cache than those with smaller lists. Furthermore note, that modified vertices are pinned in the cache, which means they cannot be evicted since that would entail loosing their changes. Therefore, transaction which contain a lot of modifications may end up with a larger than configured vertex cache. Assuming your vertex is not evicted from cache, or it is evicted from cache but your program context still holds the reference to the vertex, then its properties and edges are cached together with the vertex. This means once a property is queried, any subsequent reads will hit the cache. In case you want to force JanusGraph to read from the data storage again (provided you have disabled database-level cache), or you simply want to save memory, you could clear the cache of that vertex manually. Note this operation is not gremlin compliant, so you need to cast your vertex into CacheVertex type to do the refresh: // first read automatically caches the property together with v v . property ( \"prop\" ). value (); // force refresh to clear the cache (( CacheVertex ) v ). refresh (); // now a subsequent read will look up in JanusGraph's database-level // cache, and then backend storage read in case of cache miss v . property ( \"prop\" ). value (); Note that refresh operation cannot guarantee your current transaction reads the latest data from an eventual consistent backend. You should not attempt to achieve Compare-And-Set (CAS) via refresh operation, even though it might be helpful to detect conflicts among transactions in some cases.","title":"Vertex Cache"},{"location":"operations/cache/#index-cache","text":"The index cache contains the results of index queries executed in the context of this transaction. Subsequent identical index calls will be served from this cache and are therefore significantly cheaper. If the same index call never occurs twice in the same transaction, the index cache makes no difference. Each entry in the index cache is given a weight equal to 2 + result set size and the total weight of the cache will not exceed half of the transaction cache size.","title":"Index Cache"},{"location":"operations/cache/#database-level-caching","text":"The database level cache contains vertices and the subset of their adjacency list (properties and edges) across multiple transactions and beyond the duration of a single transaction. The database level cache is shared by all transactions across a database. It is more space efficient than the transaction level caches but also slightly slower to access. In contrast to the transaction level caches, the database level caches do not expire immediately after closing a transaction. Hence, the database level cache significantly speeds up graph traversals for read heavy workloads across transactions. A read looks up in transaction-level cache first, and then database-level cache. Configuration Reference lists all of the configuration options that pertain to JanusGraph\u2019s database level cache. This page attempts to explain their usage. Most importantly, the database level cache is disabled by default in the current release version of JanusGraph. To enable it, set cache.db-cache=true .","title":"Database Level Caching"},{"location":"operations/cache/#cache-expiration-time","text":"The most important setting for performance and query behavior is the cache expiration time which is configured via cache.db-cache-time . The cache will hold graph elements for at most that many milliseconds. If an element expires, the data will be re-read from the storage backend on the next access. If there is only one JanusGraph instance accessing the storage backend or if this instance is the only one modifying the graph, the cache expiration can be set to 0 which disables cache expiration. This allows the cache to hold elements indefinitely (unless they are evicted due to space constraints or on update) which provides the best cache performance. Since no other JanusGraph instance is modifying the graph, there is no danger of holding on to stale data. If there are multiple JanusGraph instances accessing the storage backend, the time should be set to the maximum time that can be allowed between another JanusGraph instance modifying the graph and this JanusGraph instance seeing the data. If any change should be immediately visible to all JanusGraph instances, the database level cache should be disabled in a distributed setup. However, for most applications it is acceptable that a particular JanusGraph instance sees remote modifications with some delay. The larger the maximally allowed delay, the better the cache performance. Note, that a given JanusGraph instance will always immediately see its own modifications to the graph irrespective of the configured cache expiration time.","title":"Cache Expiration Time"},{"location":"operations/cache/#cache-size","text":"The configuration option cache.db-cache-size controls how much heap space JanusGraph\u2019s database level cache is allowed to consume. The larger the cache, the more effective it will be. However, large cache sizes can lead to excessive GC and poor performance. The cache size can be configured as a percentage (expressed as a decimal between 0 and 1) of the total heap space available to the JVM running JanusGraph or as an absolute number of bytes. Note, that the cache size refers to the amount of heap space that is exclusively occupied by the cache. JanusGraph\u2019s other data structures and each open transaction will occupy additional heap space. If additional software layers are running in the same JVM, those may occupy a significant amount of heap space as well (e.g. Gremlin Server, etc). Be conservative in your heap memory estimation. Configuring a cache that is too large can lead to out-of-memory exceptions and excessive GC. In practice, you might observe JanusGraph uses more memory than configured for database level cache. This is a known limitation due to difficulty of estimating size of deserialized objects.","title":"Cache Size"},{"location":"operations/cache/#clean-up-wait-time","text":"When a vertex is locally modified (e.g. an edge is added) all of the vertex\u2019s related database level cache entries are marked as expired and eventually evicted. This will cause JanusGraph to refresh the vertex\u2019s data from the storage backend on the next access and re-populate the cache. However, when the storage backend is eventually consistent, the modifications that triggered the eviction may not yet be visible. By configuring cache.db-cache-clean-wait , the cache will wait for at least this many milliseconds before repopulating the cache with the entry retrieved from the storage backend. If JanusGraph runs locally or against a storage backend that guarantees immediate visibility of modifications, this value can be set to 0.","title":"Clean Up Wait Time"},{"location":"operations/cache/#storage-backend-caching","text":"Each storage backend maintains its own data caching layer. These caches benefit from compression, data compactness, coordinated expiration and are often maintained off heap which means that large caches can be used without running into garbage collection issues. While these caches can be significantly larger than the database level cache, they are also slower to access. The exact type of caching and its properties depends on the particular storage backend . Please refer to the respective documentation for more information about the caching infrastructure and how to optimize it.","title":"Storage Backend Caching"},{"location":"operations/configured-graph-factory/","text":"ConfiguredGraphFactory The JanusGraph Server can be configured to use the ConfiguredGraphFactory . The ConfiguredGraphFactory is an access point to your graphs, similar to the JanusGraphFactory . These graph factories provide methods for dynamically managing the graphs hosted on the server. Overview JanusGraphFactory is a class that provides an access point to your graphs by providing a Configuration object each time you access the graph. ConfiguredGraphFactory provides an access point to your graphs for which you have previously created configurations using the ConfigurationManagementGraph . It also offers an access point to manage graph configurations. ConfigurationManagementGraph allows you to manage graph configurations. JanusGraphManager is an internal server component that tracks graph references, provided your graphs are configured to use it. ConfiguredGraphFactory versus JanusGraphFactory However, there is an important distinction between these two graph factories: The ConfiguredGraphFactory can only be used if you have configured your server to use the ConfigurationManagementGraph APIs at server start. The benefits of using the ConfiguredGraphFactory are that: You only need to supply a String to access your graphs, as opposed to the JanusGraphFactory -- which requires you to specify information about the backend you wish to use when accessing a graph-- every time you open a graph. If your ConfigurationManagementGraph is configured with a distributed storage backend then your graph configurations are available to all JanusGraph nodes in your cluster. How Does the ConfiguredGraphFactory Work? The ConfiguredGraphFactory provides an access point to graphs under two scenarios: You have already created a configuration for your specific graph object using the ConfigurationManagementGraph#createConfiguration . In this scenario, your graph is opened using the previously created configuration for this graph. You have already created a template configuration using the ConfigurationManagementGraph#createTemplateConfiguration . In this scenario, we create a configuration for the graph you are creating by copying over all attributes stored in your template configuration and appending the relevant graphName attribute, and we then open the graph according to that specific configuration. Accessing the Graphs You can either use ConfiguredGraphFactory.create(\"graphName\") or ConfiguredGraphFactory.open(\"graphName\") . Learn more about the difference between these two options by reading the section below about the ConfigurationManagementGraph . You can also access your graphs by using the bindings. Read more about this in the Graph and Traversal Bindings section. Listing the Graphs ConfiguredGraphFactory.getGraphNames() will return a set of graph names for which you have created configurations using the ConfigurationManagementGraph APIs. JanusGraphFactory.getGraphNames() on the other hand returns a set of graph names for which you have instantiated and the references are stored inside the JanusGraphManager . Dropping a Graph ConfiguredGraphFactory.drop(\"graphName\") will drop the graph database, deleting all data in storage and indexing backends. The graph can be open or closed (will be closed as part of the drop operation). Furthermore, this will also remove any existing graph configuration in the ConfigurationManagementGraph . Important This is an irreversible operation that will delete all graph and index data. Important To ensure all graph representations are consistent across all JanusGraph nodes in your cluster, this removes the graph from the JanusGraphManager graph cache on every node in the cluster, assuming each node has been properly configured to use the JanusGraphManager . Learn more about this feature and how to configure your server to use said feature here . Configuring JanusGraph Server for ConfiguredGraphFactory Note Starting with JanusGraph v0.6.0, every JanusGraph Server is started with the JanusGraphManager, if ConfigurationManagementGraph is configured in the graphs section and the default GraphManager is not overwritten. To be able to use the ConfiguredGraphFactory , you must configure your server to use the ConfigurationManagementGraph APIs. To do this, you have to inject a graph variable named \"ConfigurationManagementGraph\" in your server\u2019s YAML\u2019s graphs map. For example: graphManager : org.janusgraph.graphdb.management.JanusGraphManager graphs : { ConfigurationManagementGraph : conf/JanusGraph-configurationmanagement.properties } In this example, our ConfigurationManagementGraph graph will be configured using the properties stored inside conf/JanusGraph-configurationmanagement.properties , which for example, look like: gremlin.graph = org.janusgraph.core.ConfiguredGraphFactory storage.backend = cql graph.graphname = ConfigurationManagementGraph storage.hostname = 127.0.0.1 Assuming the GremlinServer started successfully and the ConfigurationManagementGraph was successfully instantiated, then all the APIs available on the ConfigurationManagementGraph Singleton will also act upon said graph. Furthermore, this is the graph that will be used to access the configurations used to create/open graphs using the ConfiguredGraphFactory . Important The pom.xml included in the JanusGraph distribution lists this dependency as optional, but the ConfiguredGraphFactory makes use of the JanusGraphManager , which requires a declared dependency on the org.apache.tinkerpop:gremlin-server . So if you run into NoClassDefFoundError errors, then be sure to update according to this message. ConfigurationManagementGraph The ConfigurationManagementGraph is a Singleton that allows you to create/update/remove configurations that you can use to access your graphs using the ConfiguredGraphFactory . See above on configuring your server to enable use of these APIs. Important The ConfiguredGraphFactory offers an access point to manage your graph configurations managed by the ConfigurationManagementGraph , so instead of acting upon the Singleton itself, you may act upon the corresponding ConfiguredGraphFactory static methods. For example, you may use ConfiguredGraphFactory.removeTemplateConfiguration() instead of ConfiguredGraphFactory.getInstance().removeTemplateConfiguration() . Graph Configurations The ConfigurationManagementGraph singleton allows you to create configurations used to open specific graphs, referenced by the graph.graphname property. For example: map = new HashMap < String , Object >(); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); map . put ( \"graph.graphname\" , \"graph1\" ); ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); Then you could access this graph on any JanusGraph node using: ConfiguredGraphFactory . open ( \"graph1\" ); Template Configuration The ConfigurationManagementGraph also allows you to create one template configuration, which you can use to create many graphs using the same configuration template. For example: map = new HashMap < String , Object >(); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); After doing this, you can create graphs using the template configuration: ConfiguredGraphFactory . create ( \"graph2\" ); This method will first create a new configuration for \"graph2\" by copying over all the properties associated with the template configuration and storing it on a configuration for this specific graph. This means that this graph can be accessed in, on any JanusGraph node, in the future by doing: ConfiguredGraphFactory . open ( \"graph2\" ); Updating Configurations All interactions with both the JanusGraphFactory and the ConfiguredGraphFactory that interact with configurations that define the property graph.graphname go through the JanusGraphManager which keeps track of graph references created on the given JVM. Think of it as a graph cache. For this reason: Important Any updates to a graph configuration results in the eviction of the relevant graph from the graph cache on every node in the JanusGraph cluster, assuming each node has been configured properly to use the JanusGraphManager . Learn more about this feature and how to configure your server to use said feature here . Since graphs created using the template configuration first create a configuration for that graph in question using a copy and create method, this means that: Important Any updates to a specific graph created using the template configuration are not guaranteed to take effect on the specific graph until: 1. The relevant configuration is removed: ConfiguredGraphFactory.removeConfiguration(\"graph2\"); 2. The graph is recreated using the template configuration: ConfiguredGraphFactory.create(\"graph2\"); Update Examples 1) We migrated our Cassandra data to a new server with a new IP address: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); map . put ( \"graph.graphname\" , \"graph1\" ); ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . open ( \"graph1\" ); // Update configuration map = new HashMap (); map . put ( \"storage.hostname\" , \"10.0.0.1\" ); ConfiguredGraphFactory . updateConfiguration ( \"graph1\" , new MapConfiguration ( map )); // We can verify the configuration was updated by // retrieving the configuration for this graph ConfiguredGraphFactory . getConfiguration ( \"graph1\" ); // We are now guaranteed to use the updated configuration g1 = ConfiguredGraphFactory . open ( \"graph1\" ); 2) We added an Elasticsearch node to our setup: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); map . put ( \"graph.graphname\" , \"graph1\" ); ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . open ( \"graph1\" ); // Update configuration map = new HashMap (); map . put ( \"index.search.backend\" , \"elasticsearch\" ); map . put ( \"index.search.hostname\" , \"127.0.0.1\" ); map . put ( \"index.search.elasticsearch.transport-scheme\" , \"http\" ); ConfiguredGraphFactory . updateConfiguration ( \"graph1\" , new MapConfiguration ( map )); // We can verify the configuration was updated by // retrieving the configuration for this graph ConfiguredGraphFactory . getConfiguration ( \"graph1\" ); // We are now guaranteed to use the updated configuration g1 = ConfiguredGraphFactory . open ( \"graph1\" ); 3) Update a graph configuration that was created using a template configuration that has been updated: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . create ( \"graph1\" ); // Update template configuration map = new HashMap (); map . put ( \"index.search.backend\" , \"elasticsearch\" ); map . put ( \"index.search.hostname\" , \"127.0.0.1\" ); map . put ( \"index.search.elasticsearch.transport-scheme\" , \"http\" ); ConfiguredGraphFactory . updateTemplateConfiguration ( new MapConfiguration ( map )); // Remove Configuration ConfiguredGraphFactory . removeConfiguration ( \"graph1\" ); // Recreate ConfiguredGraphFactory . create ( \"graph1\" ); // Now this graph's configuration is guaranteed to be updated. // We can verify it by retrieving the configuration for this graph ConfiguredGraphFactory . getConfiguration ( \"graph1\" ); JanusGraphManager The JanusGraphManager is a Singleton adhering to the TinkerPop graphManager specifications. In particular, the JanusGraphManager provides: a coordinated mechanism by which to instantiate graph references on a given JanusGraph node a graph reference tracker (or cache) Any graph you create using the graph.graphname property will go through the JanusGraphManager and thus be instantiated in a coordinated fashion. The graph reference will also be placed in the graph cache on the JVM in question. Thus, any graph you open using the graph.graphname property that has already been instantiated on the JVM in question will be retrieved from the graph cache. This is why updates to your configurations require a few steps to guarantee correctness. How To Use The JanusGraphManager This is a new configuration option you can use when defining a property in your configuration that defines how to access a graph. All configurations that include this property will result in the graph instantiation happening through the JanusGraphManager (process explained above). For backwards compatibility, any graphs that do not supply this parameter but supplied at server start in your graphs object in your .yaml file, these graphs will be bound through the JanusGraphManager denoted by their key supplied for that graph. For example, if your .yaml graphs object looks like: graphManager : org.janusgraph.graphdb.management.JanusGraphManager graphs { graph1 : conf/graph1.properties, graph2 : conf/graph2.properties } but conf/graph1.properties and conf/graph2.properties do not include the property graph.graphname , then these graphs will be stored in the JanusGraphManager and thus bound in your gremlin script executions as graph1 and graph2 , respectively. Automatic Separation of Graphs in Backends For convenience, if your configuration used to open a graph specifies graph.graphname , but does not specify the backend\u2019s storage directory, table name, keyspace name, or index name, then the relevant parameter will automatically be set to the value of graph.graphname . However, if you supply one of those parameters, that value will always take precedence. And if you supply neither, they default to the configuration option\u2019s default value. One special case is storage.root configuration option. This is a new configuration option used to specify the base of the directory that will be used for any backend requiring local storage directory access. If you supply this parameter, you must also supply the graph.graphname property, and the absolute storage directory will be equal to the value of the graph.graphname property appended to the value of the storage.root property. Below are some example use cases: 1) Create a template configuration for my Cassandra backend such that each graph created using this configuration gets a unique keyspace equivalent to the String <graphName> provided to the factory: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . create ( \"graph1\" ); //keyspace === graph1 g2 = ConfiguredGraphFactory . create ( \"graph2\" ); //keyspace === graph2 g3 = ConfiguredGraphFactory . create ( \"graph3\" ); //keyspace === graph3 2) Create a template configuration for my BerkeleyJE backend such that each graph created using this configuration gets a unique storage directory equivalent to the \"<storage.root>/<graph.graphname>\": map = new HashMap (); map . put ( \"storage.backend\" , \"berkeleyje\" ); map . put ( \"storage.root\" , \"/data/graphs\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . create ( \"graph1\" ); //storage directory === /data/graphs/graph1 g2 = ConfiguredGraphFactory . create ( \"graph2\" ); //storage directory === /data/graphs/graph2 g3 = ConfiguredGraphFactory . create ( \"graph3\" ); //storage directory === /data/graphs/graph3 Graph and Traversal Bindings Graphs created using the ConfiguredGraphFactory are bound to the executor context on the Gremlin Server by the \"graph.graphname\" property, and the graph's traversal reference is bound to the context by \" <graphname>_traversal \". This means, on subsequent connections to the server after the first time you create/open a graph, you can access the graph and traversal references by the \" <graphname> \" and \" <graphname>_traversal \" properties. Learn more about this feature and how to configure your server to use said feature here . Important If you are connected to a remote Gremlin Server using the Gremlin Console and a sessioned connection, then you will have to reconnect to the server to bind the variables. This is also true for any sessioned WebSocket connection. Important The JanusGraphManager rebinds every graph stored on the ConfigurationManagementGraph (or those for which you have created configurations) every 20 seconds. This means your graph and traversal bindings for graphs created using the ConfiguredGraphFactory will be available on all JanusGraph nodes with a maximum of a 20 second lag. It also means that a binding will still be available on a node after a server restart. Binding Example gremlin > : remote connect tinkerpop . server conf /remote.yaml ==>Configured localhost/ 127.0 . 0.1 : 8182 gremlin > : remote console ==> All scripts will now be sent to Gremlin Server - [ localhost / 127.0 . 0.1 : 8182 ] - type ':remote console' to return to local mode gremlin > ConfiguredGraphFactory . open ( \"graph1\" ) ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > graph1 ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > graph1_traversal ==> graphtraversalsource [ standardjanusgraph [ cql: [ 127.0 . 0.1 ]], standard ] Examples It is recommended to use a sessioned connection when creating a Configured Graph Factory template. If a sessioned connection is not used the Configured Graph Factory Template creation must be sent to the server as a single line using semi-colons. See details on sessions can be found in Connecting to Gremlin Server . gremlin > : remote connect tinkerpop . server conf /remote.yaml session ==>Configured localhost/ 127.0 . 0.1 : 8182 gremlin > : remote console ==> All scripts will now be sent to Gremlin Server - [ localhost: 8182 ]-[ 5206 cdde - b231 - 41 fa - 9 e6c - 69 feac0fe2b2 ] - type ':remote console' to return to local mode gremlin > ConfiguredGraphFactory . open ( \"graph\" ); Please create configuration for this graph using the ConfigurationManagementGraph API . gremlin > ConfiguredGraphFactory . create ( \"graph\" ); Please create a template Configuration using the ConfigurationManagementGraph API . gremlin > map = new HashMap (); gremlin > map . put ( \"storage.backend\" , \"cql\" ); gremlin > map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > map . put ( \"GraphName\" , \"graph1\" ); gremlin > ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); Please include in your configuration the property \"graph.graphname\" . gremlin > map = new HashMap (); gremlin > map . put ( \"storage.backend\" , \"cql\" ); gremlin > map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > map . put ( \"graph.graphname\" , \"graph1\" ); gremlin > ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); ==> null gremlin > ConfiguredGraphFactory . open ( \"graph1\" ). vertices (); gremlin > map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > map . put ( \"graph.graphname\" , \"graph1\" ); gremlin > ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); Your template configuration may not contain the property \"graph.graphname\" . gremlin > map = new HashMap (); gremlin > map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); ==> null // Each graph is now acting in unique keyspaces equivalent to the graphnames . gremlin > g1 = ConfiguredGraphFactory . open ( \"graph1\" ); gremlin > g2 = ConfiguredGraphFactory . create ( \"graph2\" ); gremlin > g3 = ConfiguredGraphFactory . create ( \"graph3\" ); gremlin > g2 . addVertex (); gremlin > l = []; gremlin > l << g1 . vertices (). size (); ==> 0 gremlin > l << g2 . vertices (). size (); ==> 1 gremlin > l << g3 . vertices (). size (); ==> 0 // After a graph is created, you must access it using .open() gremlin > g2 = ConfiguredGraphFactory . create ( \"graph2\" ); g2 . vertices (). size (); Configuration for graph \"graph2\" already exists . gremlin > g2 = ConfiguredGraphFactory . open ( \"graph2\" ); g2 . vertices (). size (); ==> 1","title":"ConfiguredGraphFactory"},{"location":"operations/configured-graph-factory/#configuredgraphfactory","text":"The JanusGraph Server can be configured to use the ConfiguredGraphFactory . The ConfiguredGraphFactory is an access point to your graphs, similar to the JanusGraphFactory . These graph factories provide methods for dynamically managing the graphs hosted on the server.","title":"ConfiguredGraphFactory"},{"location":"operations/configured-graph-factory/#overview","text":"JanusGraphFactory is a class that provides an access point to your graphs by providing a Configuration object each time you access the graph. ConfiguredGraphFactory provides an access point to your graphs for which you have previously created configurations using the ConfigurationManagementGraph . It also offers an access point to manage graph configurations. ConfigurationManagementGraph allows you to manage graph configurations. JanusGraphManager is an internal server component that tracks graph references, provided your graphs are configured to use it.","title":"Overview"},{"location":"operations/configured-graph-factory/#configuredgraphfactory-versus-janusgraphfactory","text":"However, there is an important distinction between these two graph factories: The ConfiguredGraphFactory can only be used if you have configured your server to use the ConfigurationManagementGraph APIs at server start. The benefits of using the ConfiguredGraphFactory are that: You only need to supply a String to access your graphs, as opposed to the JanusGraphFactory -- which requires you to specify information about the backend you wish to use when accessing a graph-- every time you open a graph. If your ConfigurationManagementGraph is configured with a distributed storage backend then your graph configurations are available to all JanusGraph nodes in your cluster.","title":"ConfiguredGraphFactory versus JanusGraphFactory"},{"location":"operations/configured-graph-factory/#how-does-the-configuredgraphfactory-work","text":"The ConfiguredGraphFactory provides an access point to graphs under two scenarios: You have already created a configuration for your specific graph object using the ConfigurationManagementGraph#createConfiguration . In this scenario, your graph is opened using the previously created configuration for this graph. You have already created a template configuration using the ConfigurationManagementGraph#createTemplateConfiguration . In this scenario, we create a configuration for the graph you are creating by copying over all attributes stored in your template configuration and appending the relevant graphName attribute, and we then open the graph according to that specific configuration.","title":"How Does the ConfiguredGraphFactory Work?"},{"location":"operations/configured-graph-factory/#accessing-the-graphs","text":"You can either use ConfiguredGraphFactory.create(\"graphName\") or ConfiguredGraphFactory.open(\"graphName\") . Learn more about the difference between these two options by reading the section below about the ConfigurationManagementGraph . You can also access your graphs by using the bindings. Read more about this in the Graph and Traversal Bindings section. Listing the Graphs ConfiguredGraphFactory.getGraphNames() will return a set of graph names for which you have created configurations using the ConfigurationManagementGraph APIs. JanusGraphFactory.getGraphNames() on the other hand returns a set of graph names for which you have instantiated and the references are stored inside the JanusGraphManager .","title":"Accessing the Graphs"},{"location":"operations/configured-graph-factory/#dropping-a-graph","text":"ConfiguredGraphFactory.drop(\"graphName\") will drop the graph database, deleting all data in storage and indexing backends. The graph can be open or closed (will be closed as part of the drop operation). Furthermore, this will also remove any existing graph configuration in the ConfigurationManagementGraph . Important This is an irreversible operation that will delete all graph and index data. Important To ensure all graph representations are consistent across all JanusGraph nodes in your cluster, this removes the graph from the JanusGraphManager graph cache on every node in the cluster, assuming each node has been properly configured to use the JanusGraphManager . Learn more about this feature and how to configure your server to use said feature here .","title":"Dropping a Graph"},{"location":"operations/configured-graph-factory/#configuring-janusgraph-server-for-configuredgraphfactory","text":"Note Starting with JanusGraph v0.6.0, every JanusGraph Server is started with the JanusGraphManager, if ConfigurationManagementGraph is configured in the graphs section and the default GraphManager is not overwritten. To be able to use the ConfiguredGraphFactory , you must configure your server to use the ConfigurationManagementGraph APIs. To do this, you have to inject a graph variable named \"ConfigurationManagementGraph\" in your server\u2019s YAML\u2019s graphs map. For example: graphManager : org.janusgraph.graphdb.management.JanusGraphManager graphs : { ConfigurationManagementGraph : conf/JanusGraph-configurationmanagement.properties } In this example, our ConfigurationManagementGraph graph will be configured using the properties stored inside conf/JanusGraph-configurationmanagement.properties , which for example, look like: gremlin.graph = org.janusgraph.core.ConfiguredGraphFactory storage.backend = cql graph.graphname = ConfigurationManagementGraph storage.hostname = 127.0.0.1 Assuming the GremlinServer started successfully and the ConfigurationManagementGraph was successfully instantiated, then all the APIs available on the ConfigurationManagementGraph Singleton will also act upon said graph. Furthermore, this is the graph that will be used to access the configurations used to create/open graphs using the ConfiguredGraphFactory . Important The pom.xml included in the JanusGraph distribution lists this dependency as optional, but the ConfiguredGraphFactory makes use of the JanusGraphManager , which requires a declared dependency on the org.apache.tinkerpop:gremlin-server . So if you run into NoClassDefFoundError errors, then be sure to update according to this message.","title":"Configuring JanusGraph Server for ConfiguredGraphFactory"},{"location":"operations/configured-graph-factory/#configurationmanagementgraph","text":"The ConfigurationManagementGraph is a Singleton that allows you to create/update/remove configurations that you can use to access your graphs using the ConfiguredGraphFactory . See above on configuring your server to enable use of these APIs. Important The ConfiguredGraphFactory offers an access point to manage your graph configurations managed by the ConfigurationManagementGraph , so instead of acting upon the Singleton itself, you may act upon the corresponding ConfiguredGraphFactory static methods. For example, you may use ConfiguredGraphFactory.removeTemplateConfiguration() instead of ConfiguredGraphFactory.getInstance().removeTemplateConfiguration() .","title":"ConfigurationManagementGraph"},{"location":"operations/configured-graph-factory/#graph-configurations","text":"The ConfigurationManagementGraph singleton allows you to create configurations used to open specific graphs, referenced by the graph.graphname property. For example: map = new HashMap < String , Object >(); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); map . put ( \"graph.graphname\" , \"graph1\" ); ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); Then you could access this graph on any JanusGraph node using: ConfiguredGraphFactory . open ( \"graph1\" );","title":"Graph Configurations"},{"location":"operations/configured-graph-factory/#template-configuration","text":"The ConfigurationManagementGraph also allows you to create one template configuration, which you can use to create many graphs using the same configuration template. For example: map = new HashMap < String , Object >(); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); After doing this, you can create graphs using the template configuration: ConfiguredGraphFactory . create ( \"graph2\" ); This method will first create a new configuration for \"graph2\" by copying over all the properties associated with the template configuration and storing it on a configuration for this specific graph. This means that this graph can be accessed in, on any JanusGraph node, in the future by doing: ConfiguredGraphFactory . open ( \"graph2\" );","title":"Template Configuration"},{"location":"operations/configured-graph-factory/#updating-configurations","text":"All interactions with both the JanusGraphFactory and the ConfiguredGraphFactory that interact with configurations that define the property graph.graphname go through the JanusGraphManager which keeps track of graph references created on the given JVM. Think of it as a graph cache. For this reason: Important Any updates to a graph configuration results in the eviction of the relevant graph from the graph cache on every node in the JanusGraph cluster, assuming each node has been configured properly to use the JanusGraphManager . Learn more about this feature and how to configure your server to use said feature here . Since graphs created using the template configuration first create a configuration for that graph in question using a copy and create method, this means that: Important Any updates to a specific graph created using the template configuration are not guaranteed to take effect on the specific graph until: 1. The relevant configuration is removed: ConfiguredGraphFactory.removeConfiguration(\"graph2\"); 2. The graph is recreated using the template configuration: ConfiguredGraphFactory.create(\"graph2\");","title":"Updating Configurations"},{"location":"operations/configured-graph-factory/#update-examples","text":"1) We migrated our Cassandra data to a new server with a new IP address: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); map . put ( \"graph.graphname\" , \"graph1\" ); ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . open ( \"graph1\" ); // Update configuration map = new HashMap (); map . put ( \"storage.hostname\" , \"10.0.0.1\" ); ConfiguredGraphFactory . updateConfiguration ( \"graph1\" , new MapConfiguration ( map )); // We can verify the configuration was updated by // retrieving the configuration for this graph ConfiguredGraphFactory . getConfiguration ( \"graph1\" ); // We are now guaranteed to use the updated configuration g1 = ConfiguredGraphFactory . open ( \"graph1\" ); 2) We added an Elasticsearch node to our setup: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); map . put ( \"graph.graphname\" , \"graph1\" ); ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . open ( \"graph1\" ); // Update configuration map = new HashMap (); map . put ( \"index.search.backend\" , \"elasticsearch\" ); map . put ( \"index.search.hostname\" , \"127.0.0.1\" ); map . put ( \"index.search.elasticsearch.transport-scheme\" , \"http\" ); ConfiguredGraphFactory . updateConfiguration ( \"graph1\" , new MapConfiguration ( map )); // We can verify the configuration was updated by // retrieving the configuration for this graph ConfiguredGraphFactory . getConfiguration ( \"graph1\" ); // We are now guaranteed to use the updated configuration g1 = ConfiguredGraphFactory . open ( \"graph1\" ); 3) Update a graph configuration that was created using a template configuration that has been updated: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . create ( \"graph1\" ); // Update template configuration map = new HashMap (); map . put ( \"index.search.backend\" , \"elasticsearch\" ); map . put ( \"index.search.hostname\" , \"127.0.0.1\" ); map . put ( \"index.search.elasticsearch.transport-scheme\" , \"http\" ); ConfiguredGraphFactory . updateTemplateConfiguration ( new MapConfiguration ( map )); // Remove Configuration ConfiguredGraphFactory . removeConfiguration ( \"graph1\" ); // Recreate ConfiguredGraphFactory . create ( \"graph1\" ); // Now this graph's configuration is guaranteed to be updated. // We can verify it by retrieving the configuration for this graph ConfiguredGraphFactory . getConfiguration ( \"graph1\" );","title":"Update Examples"},{"location":"operations/configured-graph-factory/#janusgraphmanager","text":"The JanusGraphManager is a Singleton adhering to the TinkerPop graphManager specifications. In particular, the JanusGraphManager provides: a coordinated mechanism by which to instantiate graph references on a given JanusGraph node a graph reference tracker (or cache) Any graph you create using the graph.graphname property will go through the JanusGraphManager and thus be instantiated in a coordinated fashion. The graph reference will also be placed in the graph cache on the JVM in question. Thus, any graph you open using the graph.graphname property that has already been instantiated on the JVM in question will be retrieved from the graph cache. This is why updates to your configurations require a few steps to guarantee correctness.","title":"JanusGraphManager"},{"location":"operations/configured-graph-factory/#how-to-use-the-janusgraphmanager","text":"This is a new configuration option you can use when defining a property in your configuration that defines how to access a graph. All configurations that include this property will result in the graph instantiation happening through the JanusGraphManager (process explained above). For backwards compatibility, any graphs that do not supply this parameter but supplied at server start in your graphs object in your .yaml file, these graphs will be bound through the JanusGraphManager denoted by their key supplied for that graph. For example, if your .yaml graphs object looks like: graphManager : org.janusgraph.graphdb.management.JanusGraphManager graphs { graph1 : conf/graph1.properties, graph2 : conf/graph2.properties } but conf/graph1.properties and conf/graph2.properties do not include the property graph.graphname , then these graphs will be stored in the JanusGraphManager and thus bound in your gremlin script executions as graph1 and graph2 , respectively.","title":"How To Use The JanusGraphManager"},{"location":"operations/configured-graph-factory/#automatic-separation-of-graphs-in-backends","text":"For convenience, if your configuration used to open a graph specifies graph.graphname , but does not specify the backend\u2019s storage directory, table name, keyspace name, or index name, then the relevant parameter will automatically be set to the value of graph.graphname . However, if you supply one of those parameters, that value will always take precedence. And if you supply neither, they default to the configuration option\u2019s default value. One special case is storage.root configuration option. This is a new configuration option used to specify the base of the directory that will be used for any backend requiring local storage directory access. If you supply this parameter, you must also supply the graph.graphname property, and the absolute storage directory will be equal to the value of the graph.graphname property appended to the value of the storage.root property. Below are some example use cases: 1) Create a template configuration for my Cassandra backend such that each graph created using this configuration gets a unique keyspace equivalent to the String <graphName> provided to the factory: map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . create ( \"graph1\" ); //keyspace === graph1 g2 = ConfiguredGraphFactory . create ( \"graph2\" ); //keyspace === graph2 g3 = ConfiguredGraphFactory . create ( \"graph3\" ); //keyspace === graph3 2) Create a template configuration for my BerkeleyJE backend such that each graph created using this configuration gets a unique storage directory equivalent to the \"<storage.root>/<graph.graphname>\": map = new HashMap (); map . put ( \"storage.backend\" , \"berkeleyje\" ); map . put ( \"storage.root\" , \"/data/graphs\" ); ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); g1 = ConfiguredGraphFactory . create ( \"graph1\" ); //storage directory === /data/graphs/graph1 g2 = ConfiguredGraphFactory . create ( \"graph2\" ); //storage directory === /data/graphs/graph2 g3 = ConfiguredGraphFactory . create ( \"graph3\" ); //storage directory === /data/graphs/graph3","title":"Automatic Separation of Graphs in Backends"},{"location":"operations/configured-graph-factory/#graph-and-traversal-bindings","text":"Graphs created using the ConfiguredGraphFactory are bound to the executor context on the Gremlin Server by the \"graph.graphname\" property, and the graph's traversal reference is bound to the context by \" <graphname>_traversal \". This means, on subsequent connections to the server after the first time you create/open a graph, you can access the graph and traversal references by the \" <graphname> \" and \" <graphname>_traversal \" properties. Learn more about this feature and how to configure your server to use said feature here . Important If you are connected to a remote Gremlin Server using the Gremlin Console and a sessioned connection, then you will have to reconnect to the server to bind the variables. This is also true for any sessioned WebSocket connection. Important The JanusGraphManager rebinds every graph stored on the ConfigurationManagementGraph (or those for which you have created configurations) every 20 seconds. This means your graph and traversal bindings for graphs created using the ConfiguredGraphFactory will be available on all JanusGraph nodes with a maximum of a 20 second lag. It also means that a binding will still be available on a node after a server restart.","title":"Graph and Traversal Bindings"},{"location":"operations/configured-graph-factory/#binding-example","text":"gremlin > : remote connect tinkerpop . server conf /remote.yaml ==>Configured localhost/ 127.0 . 0.1 : 8182 gremlin > : remote console ==> All scripts will now be sent to Gremlin Server - [ localhost / 127.0 . 0.1 : 8182 ] - type ':remote console' to return to local mode gremlin > ConfiguredGraphFactory . open ( \"graph1\" ) ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > graph1 ==> standardjanusgraph [ cql: [ 127.0 . 0.1 ]] gremlin > graph1_traversal ==> graphtraversalsource [ standardjanusgraph [ cql: [ 127.0 . 0.1 ]], standard ]","title":"Binding Example"},{"location":"operations/configured-graph-factory/#examples","text":"It is recommended to use a sessioned connection when creating a Configured Graph Factory template. If a sessioned connection is not used the Configured Graph Factory Template creation must be sent to the server as a single line using semi-colons. See details on sessions can be found in Connecting to Gremlin Server . gremlin > : remote connect tinkerpop . server conf /remote.yaml session ==>Configured localhost/ 127.0 . 0.1 : 8182 gremlin > : remote console ==> All scripts will now be sent to Gremlin Server - [ localhost: 8182 ]-[ 5206 cdde - b231 - 41 fa - 9 e6c - 69 feac0fe2b2 ] - type ':remote console' to return to local mode gremlin > ConfiguredGraphFactory . open ( \"graph\" ); Please create configuration for this graph using the ConfigurationManagementGraph API . gremlin > ConfiguredGraphFactory . create ( \"graph\" ); Please create a template Configuration using the ConfigurationManagementGraph API . gremlin > map = new HashMap (); gremlin > map . put ( \"storage.backend\" , \"cql\" ); gremlin > map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > map . put ( \"GraphName\" , \"graph1\" ); gremlin > ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); Please include in your configuration the property \"graph.graphname\" . gremlin > map = new HashMap (); gremlin > map . put ( \"storage.backend\" , \"cql\" ); gremlin > map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > map . put ( \"graph.graphname\" , \"graph1\" ); gremlin > ConfiguredGraphFactory . createConfiguration ( new MapConfiguration ( map )); ==> null gremlin > ConfiguredGraphFactory . open ( \"graph1\" ). vertices (); gremlin > map = new HashMap (); map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > map . put ( \"graph.graphname\" , \"graph1\" ); gremlin > ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); Your template configuration may not contain the property \"graph.graphname\" . gremlin > map = new HashMap (); gremlin > map . put ( \"storage.backend\" , \"cql\" ); map . put ( \"storage.hostname\" , \"127.0.0.1\" ); gremlin > ConfiguredGraphFactory . createTemplateConfiguration ( new MapConfiguration ( map )); ==> null // Each graph is now acting in unique keyspaces equivalent to the graphnames . gremlin > g1 = ConfiguredGraphFactory . open ( \"graph1\" ); gremlin > g2 = ConfiguredGraphFactory . create ( \"graph2\" ); gremlin > g3 = ConfiguredGraphFactory . create ( \"graph3\" ); gremlin > g2 . addVertex (); gremlin > l = []; gremlin > l << g1 . vertices (). size (); ==> 0 gremlin > l << g2 . vertices (). size (); ==> 1 gremlin > l << g3 . vertices (). size (); ==> 0 // After a graph is created, you must access it using .open() gremlin > g2 = ConfiguredGraphFactory . create ( \"graph2\" ); g2 . vertices (). size (); Configuration for graph \"graph2\" already exists . gremlin > g2 = ConfiguredGraphFactory . open ( \"graph2\" ); g2 . vertices (). size (); ==> 1","title":"Examples"},{"location":"operations/deployment/","text":"Deployment Scenarios JanusGraph offers a wide choice of storage and index backends which results in great flexibility of how it can be deployed. This chapter presents a few possible deployment scenarios to help with the complexity that comes with this flexibility. Before discussing the different deployment scenarios, it is important to understand the roles of JanusGraph itself and that of the backends. First of all, applications only communicate directly with JanusGraph, mostly by sending Gremlin traversals for execution. JanusGraph then communicates with the configured backends to execute the received traversal. When JanusGraph is used in the form of JanusGraph Server, then there is nothing like a master JanusGraph Server. Applications can therefore connect to any JanusGraph Server instance. They can also use a load-balancer to schedule requests to the different instances. The JanusGraph Server instances themselves don\u2019t communicate to each other directly which makes it easy to scale them when the need arises to process more traversals. Note The scenarios presented in this chapter are only examples of how JanusGraph can be deployed. Each deployment needs to take into account the concrete use cases and production needs. Getting Started Scenario This scenario is the scenario most users probably want to choose when they are just getting started with JanusGraph. It offers scalability and fault tolerance with a minimum number of servers required. JanusGraph Server runs together with an instance of the storage backend and optionally also an instance of the index backend on every server. A setup like this can be extended by simply adding more servers of the same kind or by moving one of the components onto dedicated servers. The latter describes a growth path to transform the deployment into the Advanced Scenario . Any of the scalable storage backends can be used with this scenario. Note however that for Scylla some configuration is required when it is hosted co-located with other services like in this scenario. When an index backend should be used in this scenario then it also needs to be one that is scalable. Advanced Scenario The advanced scenario is an evolution of the Getting Started Scenario . Instead of hosting the JanusGraph Server instances together with the storage backend and optionally also the index backend, they are now separated on different servers. The advantage of hosting the different components (JanusGraph Server, storage/index backend) on different servers is that they can be scaled and managed independently of each other. This offers a higher flexibility at the cost of having to maintain more servers. Since this scenario offers independent scalability of the different components, it of course makes most sense to also use scalable backends. Minimalist Scenario It is also possible to host JanusGraph Server together with the backend(s) on just one server. This is especially attractive for testing purposes or for example when JanusGraph just supports a single application which can then also run on the same server. Opposed to the previous scenarios, it makes most sense to use backends for this scenario that are not scalable. The in-memory backend can be used for testing purposes or Berkeley DB for production and Lucene as the optional index backend. Embedded JanusGraph Instead of connecting to the JanusGraph Server from an application it is also possible to embed JanusGraph as a library inside a JVM based application. While this reduces the administrative overhead, it makes it impossible to scale JanusGraph independently of the application. Embedded JanusGraph can be deployed as a variation of any of the other scenarios. JanusGraph just moves from the server(s) directly into the application as its now just used as a library instead of an independent service.","title":"Deployment Scenarios"},{"location":"operations/deployment/#deployment-scenarios","text":"JanusGraph offers a wide choice of storage and index backends which results in great flexibility of how it can be deployed. This chapter presents a few possible deployment scenarios to help with the complexity that comes with this flexibility. Before discussing the different deployment scenarios, it is important to understand the roles of JanusGraph itself and that of the backends. First of all, applications only communicate directly with JanusGraph, mostly by sending Gremlin traversals for execution. JanusGraph then communicates with the configured backends to execute the received traversal. When JanusGraph is used in the form of JanusGraph Server, then there is nothing like a master JanusGraph Server. Applications can therefore connect to any JanusGraph Server instance. They can also use a load-balancer to schedule requests to the different instances. The JanusGraph Server instances themselves don\u2019t communicate to each other directly which makes it easy to scale them when the need arises to process more traversals. Note The scenarios presented in this chapter are only examples of how JanusGraph can be deployed. Each deployment needs to take into account the concrete use cases and production needs.","title":"Deployment Scenarios"},{"location":"operations/deployment/#getting-started-scenario","text":"This scenario is the scenario most users probably want to choose when they are just getting started with JanusGraph. It offers scalability and fault tolerance with a minimum number of servers required. JanusGraph Server runs together with an instance of the storage backend and optionally also an instance of the index backend on every server. A setup like this can be extended by simply adding more servers of the same kind or by moving one of the components onto dedicated servers. The latter describes a growth path to transform the deployment into the Advanced Scenario . Any of the scalable storage backends can be used with this scenario. Note however that for Scylla some configuration is required when it is hosted co-located with other services like in this scenario. When an index backend should be used in this scenario then it also needs to be one that is scalable.","title":"Getting Started Scenario"},{"location":"operations/deployment/#advanced-scenario","text":"The advanced scenario is an evolution of the Getting Started Scenario . Instead of hosting the JanusGraph Server instances together with the storage backend and optionally also the index backend, they are now separated on different servers. The advantage of hosting the different components (JanusGraph Server, storage/index backend) on different servers is that they can be scaled and managed independently of each other. This offers a higher flexibility at the cost of having to maintain more servers. Since this scenario offers independent scalability of the different components, it of course makes most sense to also use scalable backends.","title":"Advanced Scenario"},{"location":"operations/deployment/#minimalist-scenario","text":"It is also possible to host JanusGraph Server together with the backend(s) on just one server. This is especially attractive for testing purposes or for example when JanusGraph just supports a single application which can then also run on the same server. Opposed to the previous scenarios, it makes most sense to use backends for this scenario that are not scalable. The in-memory backend can be used for testing purposes or Berkeley DB for production and Lucene as the optional index backend.","title":"Minimalist Scenario"},{"location":"operations/deployment/#embedded-janusgraph","text":"Instead of connecting to the JanusGraph Server from an application it is also possible to embed JanusGraph as a library inside a JVM based application. While this reduces the administrative overhead, it makes it impossible to scale JanusGraph independently of the application. Embedded JanusGraph can be deployed as a variation of any of the other scenarios. JanusGraph just moves from the server(s) directly into the application as its now just used as a library instead of an independent service.","title":"Embedded JanusGraph"},{"location":"operations/dynamic-graphs/","text":"Dynamic Graphs JanusGraph supports dynamically creating graphs . This is deviation from the way in which standard Gremlin Server implementations allow one to access a graph. Traditionally, users create bindings to graphs at server-start, by configuring the gremlin-server.yaml file accordingly. For example, if the graphs section of your yaml file looks like this: graphs { graph1 : conf/graph1.properties, graph2 : conf/graph2.properties } then you will access your graphs on the Gremlin Server using the fact that the String graph1 will be bound to the graph opened on the server as per its supplied properties file, and the same holds true for graph2 . However, if we use the ConfiguredGraphFactory to dynamically create graphs, then those graphs are managed by the JanusGraphManager and the graph configurations are managed by the ConfigurationManagementGraph . This is especially useful because it 1. allows you to define graph configurations post-server-start and 2. allows the graph configurations to be managed in a persisted and distributed nature across your JanusGraph cluster. To properly use the ConfiguredGraphFactory , you must configure every Gremlin Server in your cluster to use the JanusGraphManager and the ConfigurationManagementGraph . This procedure is explained in detail here . Graph Reference Consistency If you configure all your JanusGraph servers to use the ConfiguredGraphFactory , JanusGraph will ensure all graph representations are-up-to-date across all JanusGraph nodes in your cluster. For example, if you update or delete the configuration to a graph on one JanusGraph node, then we must evict that graph from the cache on every JanusGraph node in the cluster . Otherwise, we may have inconsistent graph representations across your cluster. JanusGraph automatically handles this eviction using a messaging log queue through the backend system that the graph in question is configured to use. If one of your servers is configured incorrectly, then it may not be able to successfully remove the graph from the cache. Important Any updates to your TemplateConfiguration will not result in the updating of graphs/graph configurations previously created using said template configuration. If you want to update the individual graph configurations, you must do so using the available update APIs . These update APIs will then result in the graph cache eviction across all JanusGraph nodes in your cluster. Dynamic Graph and Traversal Bindings JanusGraph has the ability to bind dynamically created graphs and their traversal references to <graph.graphname> and <graph.graphname>_traversal , respectively, across all JanusGraph nodes in your cluster, with a maximum of a 20s lag for the binding to take effect on any node in the cluster. Read more about this here . JanusGraph accomplishes this by having each node in your cluster poll the ConfigurationManagementGraph for all graphs for which you have created configurations. The JanusGraphManager will then open said graph with its persisted configuration, store it in its graph cache, and bind the <graph.graphname> to the graph reference on the GremlinExecutor as well as bind <graph.graphname>_traversal to the graph\u2019s traversal reference on the GremlinExecutor . This allows you to access a dynamically created graph and its traversal reference by their string bindings, on every node in your JanusGraph cluster. This is particularly important to be able to work with Gremlin Server clients and use TinkerPops\u2019s withRemote functionality . Note To set up your cluster to bind dynamically created graphs and their traversal references, you must configure each node to use the ConfiguredGraphFactory . Using TinkerPop\u2019s withRemote Functionality Since traversal references are bound on the JanusGraph servers, we can make use of TinkerPop\u2019s withRemote functionality . This will allow one to run gremlin queries locally, against a remote graph reference. Traditionally, one runs queries against remote Gremlin Servers by sending String script representations, which are processed on the remote server and the response serialized and sent back. However, TinkerPop also allows for the use of remoteGraph , which could be useful if you are building a TinkerPop compliant graph infrastructure that is easily transferable to multiple implementations. To use this functionality in JanusGraph, we must first ensure we have created a graph on the remote JanusGraph cluster: ConfiguredGraphFactory.create(\"graph1\"); Next, we must wait 20 seconds to ensure the traversal reference is bound on every JanusGraph node in the remote cluster. Finally, we can locally make use of the withRemote method to access a local reference to a remote graph: gremlin > cluster = Cluster . open ( 'conf/remote-objects.yaml' ) ==> localhost / 127.0 . 0.1 : 8182 gremlin > g = traversal (). withRemote ( DriverRemoteConnection . using ( cluster , \"graph1_traversal\" )) ==> graphtraversalsource [ emptygraph [ empty ], standard ] For completion, the above conf/remote-objects.yaml should tell the Cluster API how to access the remote JanusGraph servers; for example, it may look like: hosts : [ remoteaddress1.com , remoteaddress2.com ] port : 8182 username : admin password : password connectionPool : { enableSsl : true } serializer : className : org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1 config : ioRegistries : [ org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry ]","title":"Dynamic Graphs"},{"location":"operations/dynamic-graphs/#dynamic-graphs","text":"JanusGraph supports dynamically creating graphs . This is deviation from the way in which standard Gremlin Server implementations allow one to access a graph. Traditionally, users create bindings to graphs at server-start, by configuring the gremlin-server.yaml file accordingly. For example, if the graphs section of your yaml file looks like this: graphs { graph1 : conf/graph1.properties, graph2 : conf/graph2.properties } then you will access your graphs on the Gremlin Server using the fact that the String graph1 will be bound to the graph opened on the server as per its supplied properties file, and the same holds true for graph2 . However, if we use the ConfiguredGraphFactory to dynamically create graphs, then those graphs are managed by the JanusGraphManager and the graph configurations are managed by the ConfigurationManagementGraph . This is especially useful because it 1. allows you to define graph configurations post-server-start and 2. allows the graph configurations to be managed in a persisted and distributed nature across your JanusGraph cluster. To properly use the ConfiguredGraphFactory , you must configure every Gremlin Server in your cluster to use the JanusGraphManager and the ConfigurationManagementGraph . This procedure is explained in detail here .","title":"Dynamic Graphs"},{"location":"operations/dynamic-graphs/#graph-reference-consistency","text":"If you configure all your JanusGraph servers to use the ConfiguredGraphFactory , JanusGraph will ensure all graph representations are-up-to-date across all JanusGraph nodes in your cluster. For example, if you update or delete the configuration to a graph on one JanusGraph node, then we must evict that graph from the cache on every JanusGraph node in the cluster . Otherwise, we may have inconsistent graph representations across your cluster. JanusGraph automatically handles this eviction using a messaging log queue through the backend system that the graph in question is configured to use. If one of your servers is configured incorrectly, then it may not be able to successfully remove the graph from the cache. Important Any updates to your TemplateConfiguration will not result in the updating of graphs/graph configurations previously created using said template configuration. If you want to update the individual graph configurations, you must do so using the available update APIs . These update APIs will then result in the graph cache eviction across all JanusGraph nodes in your cluster.","title":"Graph Reference Consistency"},{"location":"operations/dynamic-graphs/#dynamic-graph-and-traversal-bindings","text":"JanusGraph has the ability to bind dynamically created graphs and their traversal references to <graph.graphname> and <graph.graphname>_traversal , respectively, across all JanusGraph nodes in your cluster, with a maximum of a 20s lag for the binding to take effect on any node in the cluster. Read more about this here . JanusGraph accomplishes this by having each node in your cluster poll the ConfigurationManagementGraph for all graphs for which you have created configurations. The JanusGraphManager will then open said graph with its persisted configuration, store it in its graph cache, and bind the <graph.graphname> to the graph reference on the GremlinExecutor as well as bind <graph.graphname>_traversal to the graph\u2019s traversal reference on the GremlinExecutor . This allows you to access a dynamically created graph and its traversal reference by their string bindings, on every node in your JanusGraph cluster. This is particularly important to be able to work with Gremlin Server clients and use TinkerPops\u2019s withRemote functionality . Note To set up your cluster to bind dynamically created graphs and their traversal references, you must configure each node to use the ConfiguredGraphFactory .","title":"Dynamic Graph and Traversal Bindings"},{"location":"operations/dynamic-graphs/#using-tinkerpops-withremote-functionality","text":"Since traversal references are bound on the JanusGraph servers, we can make use of TinkerPop\u2019s withRemote functionality . This will allow one to run gremlin queries locally, against a remote graph reference. Traditionally, one runs queries against remote Gremlin Servers by sending String script representations, which are processed on the remote server and the response serialized and sent back. However, TinkerPop also allows for the use of remoteGraph , which could be useful if you are building a TinkerPop compliant graph infrastructure that is easily transferable to multiple implementations. To use this functionality in JanusGraph, we must first ensure we have created a graph on the remote JanusGraph cluster: ConfiguredGraphFactory.create(\"graph1\"); Next, we must wait 20 seconds to ensure the traversal reference is bound on every JanusGraph node in the remote cluster. Finally, we can locally make use of the withRemote method to access a local reference to a remote graph: gremlin > cluster = Cluster . open ( 'conf/remote-objects.yaml' ) ==> localhost / 127.0 . 0.1 : 8182 gremlin > g = traversal (). withRemote ( DriverRemoteConnection . using ( cluster , \"graph1_traversal\" )) ==> graphtraversalsource [ emptygraph [ empty ], standard ] For completion, the above conf/remote-objects.yaml should tell the Cluster API how to access the remote JanusGraph servers; for example, it may look like: hosts : [ remoteaddress1.com , remoteaddress2.com ] port : 8182 username : admin password : password connectionPool : { enableSsl : true } serializer : className : org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1 config : ioRegistries : [ org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry ]","title":"Using TinkerPop\u2019s withRemote Functionality"},{"location":"operations/management/","text":"Management System JanusGraph Management System provides methods to define, update, and inspect the schema of a JanusGraph graph, and more. Checkout the JanusGraph Management API documentation for all core APIs available. JanusGraph Management System behaves like a transaction in that it opens a transactional scope. As such, it needs to be closed via its commit or rollback methods, unless otherwise specified. mgmt = graph . openManagement () // do something mgmt . commit () Note We strongly encourage all users of JanusGraph not to use JanusGraph\u2019s APIs outside of the management system, and always use standard Gremlin query language for any queries. Schema Management Management System allows you to view, update, and create vertex labels, edge labels, and property keys. See schema management for details. Index Management Management System allows you to manage both vertex-centric indexes and graph indexes. See index management for details. Consistency Management Management System allows you to set the consistency level of individual schema elements. See Eventual Consistency for details. Ghost Vertex Removal Management System allows you to purge ghost vertices in the graph. It uses a local thread pool to initiate multiple threads to scan your entire graph, detecting and purging ghost vertices as well as their incident edges, leveraging GhostVertexRemover By default, the concurrency level is the number of available processors on your machine. You can also configure the number of threads as shown in the example below. If your graph is huge, you could consider running GhostVertexRemover on a MapReduce cluster. mgmt = graph . openManagement () // by default, concurrency level = the number of available processors mgmt . removeGhostVertices (). get () // alternatively, you could also configure the concurrency mgmt . removeGhostVertices ( 4 ). get () // it is not necessary to commit here, since GhostVertexRemover commits // periodically and automatically, but it is a good habit to do so // calling rollback() won't really rollback the ghost vertex removal process mgmt . commit ()","title":"Management System"},{"location":"operations/management/#management-system","text":"JanusGraph Management System provides methods to define, update, and inspect the schema of a JanusGraph graph, and more. Checkout the JanusGraph Management API documentation for all core APIs available. JanusGraph Management System behaves like a transaction in that it opens a transactional scope. As such, it needs to be closed via its commit or rollback methods, unless otherwise specified. mgmt = graph . openManagement () // do something mgmt . commit () Note We strongly encourage all users of JanusGraph not to use JanusGraph\u2019s APIs outside of the management system, and always use standard Gremlin query language for any queries.","title":"Management System"},{"location":"operations/management/#schema-management","text":"Management System allows you to view, update, and create vertex labels, edge labels, and property keys. See schema management for details.","title":"Schema Management"},{"location":"operations/management/#index-management","text":"Management System allows you to manage both vertex-centric indexes and graph indexes. See index management for details.","title":"Index Management"},{"location":"operations/management/#consistency-management","text":"Management System allows you to set the consistency level of individual schema elements. See Eventual Consistency for details.","title":"Consistency Management"},{"location":"operations/management/#ghost-vertex-removal","text":"Management System allows you to purge ghost vertices in the graph. It uses a local thread pool to initiate multiple threads to scan your entire graph, detecting and purging ghost vertices as well as their incident edges, leveraging GhostVertexRemover By default, the concurrency level is the number of available processors on your machine. You can also configure the number of threads as shown in the example below. If your graph is huge, you could consider running GhostVertexRemover on a MapReduce cluster. mgmt = graph . openManagement () // by default, concurrency level = the number of available processors mgmt . removeGhostVertices (). get () // alternatively, you could also configure the concurrency mgmt . removeGhostVertices ( 4 ). get () // it is not necessary to commit here, since GhostVertexRemover commits // periodically and automatically, but it is a good habit to do so // calling rollback() won't really rollback the ghost vertex removal process mgmt . commit ()","title":"Ghost Vertex Removal"},{"location":"operations/migrating-thrift/","text":"Migrating from Thirft This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by a JanusGraph Thrift backend to a JanusGraph CQL Backend. Note The following backends are no longer supported: cassandrathrift , cassandra , astyanax , and embeddedcassandra Configuration Replacements Old configuration New CQL configuration .backend=cassandrathrift .backend=cql .backend=cassandra .backend=cql .backend=astyanax .backend=cql .backend=embeddedcassandra .backend=cql .port=9160 .port=9042 .cassandra.atomic-batch-mutate .cql.atomic-batch-mutate .cassandra.compaction-strategy-class .cql.compaction-strategy-class .cassandra.compaction-strategy-options .cql.compaction-strategy-options .cassandra.compression .cql.compression Embedded Mode JanusGraph does no longer support the embedded mode of Cassandra. Therefore, users have to deploy Cassandra as a external instance, see Cassandra . The JanusGraph distribution already comes with a version of Cassandra included in the archive janusgraph-full-1.0.0-rc2.zip that is used by the janusgraph.sh script.","title":"Migrating from Thrift"},{"location":"operations/migrating-thrift/#migrating-from-thirft","text":"This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by a JanusGraph Thrift backend to a JanusGraph CQL Backend. Note The following backends are no longer supported: cassandrathrift , cassandra , astyanax , and embeddedcassandra","title":"Migrating from Thirft"},{"location":"operations/migrating-thrift/#configuration-replacements","text":"Old configuration New CQL configuration .backend=cassandrathrift .backend=cql .backend=cassandra .backend=cql .backend=astyanax .backend=cql .backend=embeddedcassandra .backend=cql .port=9160 .port=9042 .cassandra.atomic-batch-mutate .cql.atomic-batch-mutate .cassandra.compaction-strategy-class .cql.compaction-strategy-class .cassandra.compaction-strategy-options .cql.compaction-strategy-options .cassandra.compression .cql.compression","title":"Configuration Replacements"},{"location":"operations/migrating-thrift/#embedded-mode","text":"JanusGraph does no longer support the embedded mode of Cassandra. Therefore, users have to deploy Cassandra as a external instance, see Cassandra . The JanusGraph distribution already comes with a version of Cassandra included in the archive janusgraph-full-1.0.0-rc2.zip that is used by the janusgraph.sh script.","title":"Embedded Mode"},{"location":"operations/migrating-titan/","text":"Migrating from Titan This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by Titan. Please note after migrating to version 0.3.0, or later, of JanusGraph you will not be able to connect to a graph using a Titan client. Configuration When connecting to an existing Titan data store the graph.titan-version property should already be set in the global configuration to Titan version 1.0.0 . The ID store name in JanusGraph is configurable via the ids.store-name property whereas in Titan it was a constant. If the graph.titan-version has been set in the existing global configuration, then you do not need to explicitly set the ID store as it will default to titan_ids . Cassandra The default keyspace used by Titan was titan and in order to reuse that existing keyspace the storage.cql.keyspace property needs to be set accordingly. storage.cql.keyspace = titan These configuration options allow JanusGraph to read data from a Cassandra database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in Cassandra before attempting to use it with the JanusGraph release. HBase The name of the table used by Titan was titan and in order to reuse that existing table the storage.hbase.table property needs to be set accordingly. storage.hbase.table = titan These configuration options allow JanusGraph to read data from an HBase database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in HBase before attempting to use it with the JanusGraph release. BerkeleyDB The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk. This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with Titan can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release.","title":"Migrating from Titan"},{"location":"operations/migrating-titan/#migrating-from-titan","text":"This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by Titan. Please note after migrating to version 0.3.0, or later, of JanusGraph you will not be able to connect to a graph using a Titan client.","title":"Migrating from Titan"},{"location":"operations/migrating-titan/#configuration","text":"When connecting to an existing Titan data store the graph.titan-version property should already be set in the global configuration to Titan version 1.0.0 . The ID store name in JanusGraph is configurable via the ids.store-name property whereas in Titan it was a constant. If the graph.titan-version has been set in the existing global configuration, then you do not need to explicitly set the ID store as it will default to titan_ids .","title":"Configuration"},{"location":"operations/migrating-titan/#cassandra","text":"The default keyspace used by Titan was titan and in order to reuse that existing keyspace the storage.cql.keyspace property needs to be set accordingly. storage.cql.keyspace = titan These configuration options allow JanusGraph to read data from a Cassandra database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in Cassandra before attempting to use it with the JanusGraph release.","title":"Cassandra"},{"location":"operations/migrating-titan/#hbase","text":"The name of the table used by Titan was titan and in order to reuse that existing table the storage.hbase.table property needs to be set accordingly. storage.hbase.table = titan These configuration options allow JanusGraph to read data from an HBase database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in HBase before attempting to use it with the JanusGraph release.","title":"HBase"},{"location":"operations/migrating-titan/#berkeleydb","text":"The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk. This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with Titan can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release.","title":"BerkeleyDB"},{"location":"operations/monitoring/","text":"Monitoring JanusGraph Metrics in JanusGraph JanusGraph supports Metrics . JanusGraph can measure the following: The number of transactions begun, committed, and rolled back The number of attempts and failures of each storage backend operation type The response time distribution of each storage backend operation type Configuring Metrics Collection To enable Metrics collection, set the following in JanusGraph\u2019s properties file: # Required to enable Metrics in JanusGraph metrics.enabled = true This setting makes JanusGraph record measurements at runtime using Metrics classes like Timer, Counter, Histogram, etc. To access these measurements, one or more Metrics reporters must be configured as described in the section Configuring Metrics Reporting . Customizing the Default Metric Names JanusGraph prefixes all metric names with \"org.janusgraph\" by default. This prefix can be set through the metrics.prefix configuration property. For example, to shorten the default \"org.janusgraph\" prefix to just \"janusgraph\": # Optional metrics.prefix = janusgraph Transaction-Specific Metrics Names Each JanusGraph transaction may optionally specify its own Metrics name prefix, overriding both the default Metrics name prefix and the metrics.prefix configuration property. For example, the prefix could be changed to the name of the frontend application that opened the JanusGraph transaction. Note that Metrics maintains a ConcurrentHashMap of metric names and their associated objects in memory, so it\u2019s probably a good idea to keep the number of distinct metric prefixes small. To do this, call TransactionBuilder.setMetricsPrefix(String) : JanusGraph graph = ...; TransactionBuilder tbuilder = graph . buildTransaction (); JanusGraphTransaction tx = tbuilder . groupName ( \"foobar\" ). start (); Separating Metrics by Backend Store JanusGraph combines the Metrics for its various internal storage backend handles by default. All Metrics for storage backend interactions follow the pattern \"<prefix>.stores.<opname>\", regardless of whether they come from the ID store, edge store, etc. When metrics.merge-basic-metrics = false is set in JanusGraph\u2019s properties file, the \"stores\" string in metric names is replaced by \"idStore\", \"edgeStore\", \"vertexIndexStore\", or \"edgeIndexStore\". Index Provider Metrics JanusGraph collects basic metrics for mixed index operations. These metrics can be found here <prefix>.indexProvider.<INDEX-NAME>.<opname> . Configuring Metrics Reporting JanusGraph supports the following Metrics reporters: Console CSV Ganglia Graphite JMX Slf4j User-provided/Custom Each reporter type is independent and can coexist with other reporter types. For example, it\u2019s possible to configure JMX and Slf4j Metrics reporters to operate simultaneously. Just set all their respective configuration keys in janusgraph.properties (and enable metrics as directed above). Console Reporter Metrics Console Reporter Configuration Options Config Key Required? Value Default metrics.console.interval yes Milliseconds to wait between dumping metrics to the console null Example janusgraph.properties snippet that prints metrics to the console once a minute: metrics.enabled = true # Required; specify logging interval in milliseconds metrics.console.interval = 60000 CSV File Reporter Metrics CSV Reporter Configuration Options Config Key Required? Value Default metrics.csv.interval yes Milliseconds to wait between writing CSV lines null metrics.csv.directory yes Directory in which CSV files are written (will be created if it does not exist) null Example janusgraph.properties snippet that writes CSV files once a minute to the directory ./foo/bar/ (relative to the process\u2019s working directory): metrics.enabled = true # Required; specify logging interval in milliseconds metrics.csv.interval = 60000 metrics.csv.directory = foo/bar Graphite Reporter Metrics Graphite Reporter Configuration Options Config Key Required? Value Default metrics.graphite.hostname yes IP address or hostname to which Graphite plaintext protocol data are sent null metrics.graphite.interval yes Milliseconds to wait between pushing data to Graphite null metrics.graphite.port no Port to which Graphite plaintext protocol reports are sent 2003 metrics.graphite.prefix no Arbitrary string prepended to all metric names sent to Graphite null Example janusgraph.properties snippet that sends metrics to a Graphite server on 192.168.0.1 every minute: metrics.enabled = true # Required; IP or hostname string metrics.graphite.hostname = 192.168.0.1 # Required; specify logging interval in milliseconds metrics.graphite.interval = 60000 JMX Reporter Metrics JMX Reporter Configuration Options Config Key Required? Value Default metrics.jmx.enabled yes Boolean false metrics.jmx.domain no Metrics will appear in this JMX domain Metrics\u2019s own default metrics.jmx.agentid no Metrics will be reported with this JMX agent ID Metrics\u2019s own default Example janusgraph.properties snippet: metrics.enabled = true # Required metrics.jmx.enabled = true # Optional; if omitted, then Metrics uses its default values metrics.jmx.domain = foo metrics.jmx.agentid = baz Slf4j Reporter Metrics Slf4j Reporter Configuration Options Config Key Required? Value Default metrics.slf4j.interval yes Milliseconds to wait between dumping metrics to the logger null metrics.slf4j.logger no Slf4j logger name to use \"metrics\" Example janusgraph.properties snippet that logs metrics once a minute to the logger named foo : metrics.enabled = true # Required; specify logging interval in milliseconds metrics.slf4j.interval = 60000 # Optional; uses Metrics default when unset metrics.slf4j.logger = foo User-Provided/Custom Reporter In case the Metrics reporter configuration options listed above are insufficient, JanusGraph provides a utility method to access the single MetricRegistry instance which holds all of its measurements. com . codahale . metrics . MetricRegistry janusgraphRegistry = org . janusgraph . util . stats . MetricManager . INSTANCE . getRegistry (); Code that accesses janusgraphRegistry this way can then attach non-standard reporter types or standard reporter types with exotic configurations to janusgraphRegistry . This approach is also useful if the surrounding application already has a framework for Metrics reporter configuration, or if the application needs multiple differently-configured instances of one of JanusGraph\u2019s supported reporter types. For instance, one could use this approach to setup multiple unicast Graphite reporters whereas JanusGraph\u2019s properties configuration is limited to just one Graphite reporter.","title":"Monitoring"},{"location":"operations/monitoring/#monitoring-janusgraph","text":"","title":"Monitoring JanusGraph"},{"location":"operations/monitoring/#metrics-in-janusgraph","text":"JanusGraph supports Metrics . JanusGraph can measure the following: The number of transactions begun, committed, and rolled back The number of attempts and failures of each storage backend operation type The response time distribution of each storage backend operation type","title":"Metrics in JanusGraph"},{"location":"operations/monitoring/#configuring-metrics-collection","text":"To enable Metrics collection, set the following in JanusGraph\u2019s properties file: # Required to enable Metrics in JanusGraph metrics.enabled = true This setting makes JanusGraph record measurements at runtime using Metrics classes like Timer, Counter, Histogram, etc. To access these measurements, one or more Metrics reporters must be configured as described in the section Configuring Metrics Reporting .","title":"Configuring Metrics Collection"},{"location":"operations/monitoring/#customizing-the-default-metric-names","text":"JanusGraph prefixes all metric names with \"org.janusgraph\" by default. This prefix can be set through the metrics.prefix configuration property. For example, to shorten the default \"org.janusgraph\" prefix to just \"janusgraph\": # Optional metrics.prefix = janusgraph","title":"Customizing the Default Metric Names"},{"location":"operations/monitoring/#transaction-specific-metrics-names","text":"Each JanusGraph transaction may optionally specify its own Metrics name prefix, overriding both the default Metrics name prefix and the metrics.prefix configuration property. For example, the prefix could be changed to the name of the frontend application that opened the JanusGraph transaction. Note that Metrics maintains a ConcurrentHashMap of metric names and their associated objects in memory, so it\u2019s probably a good idea to keep the number of distinct metric prefixes small. To do this, call TransactionBuilder.setMetricsPrefix(String) : JanusGraph graph = ...; TransactionBuilder tbuilder = graph . buildTransaction (); JanusGraphTransaction tx = tbuilder . groupName ( \"foobar\" ). start ();","title":"Transaction-Specific Metrics Names"},{"location":"operations/monitoring/#separating-metrics-by-backend-store","text":"JanusGraph combines the Metrics for its various internal storage backend handles by default. All Metrics for storage backend interactions follow the pattern \"<prefix>.stores.<opname>\", regardless of whether they come from the ID store, edge store, etc. When metrics.merge-basic-metrics = false is set in JanusGraph\u2019s properties file, the \"stores\" string in metric names is replaced by \"idStore\", \"edgeStore\", \"vertexIndexStore\", or \"edgeIndexStore\".","title":"Separating Metrics by Backend Store"},{"location":"operations/monitoring/#index-provider-metrics","text":"JanusGraph collects basic metrics for mixed index operations. These metrics can be found here <prefix>.indexProvider.<INDEX-NAME>.<opname> .","title":"Index Provider Metrics"},{"location":"operations/monitoring/#configuring-metrics-reporting","text":"JanusGraph supports the following Metrics reporters: Console CSV Ganglia Graphite JMX Slf4j User-provided/Custom Each reporter type is independent and can coexist with other reporter types. For example, it\u2019s possible to configure JMX and Slf4j Metrics reporters to operate simultaneously. Just set all their respective configuration keys in janusgraph.properties (and enable metrics as directed above).","title":"Configuring Metrics Reporting"},{"location":"operations/monitoring/#console-reporter","text":"Metrics Console Reporter Configuration Options Config Key Required? Value Default metrics.console.interval yes Milliseconds to wait between dumping metrics to the console null Example janusgraph.properties snippet that prints metrics to the console once a minute: metrics.enabled = true # Required; specify logging interval in milliseconds metrics.console.interval = 60000","title":"Console Reporter"},{"location":"operations/monitoring/#csv-file-reporter","text":"Metrics CSV Reporter Configuration Options Config Key Required? Value Default metrics.csv.interval yes Milliseconds to wait between writing CSV lines null metrics.csv.directory yes Directory in which CSV files are written (will be created if it does not exist) null Example janusgraph.properties snippet that writes CSV files once a minute to the directory ./foo/bar/ (relative to the process\u2019s working directory): metrics.enabled = true # Required; specify logging interval in milliseconds metrics.csv.interval = 60000 metrics.csv.directory = foo/bar","title":"CSV File Reporter"},{"location":"operations/monitoring/#graphite-reporter","text":"Metrics Graphite Reporter Configuration Options Config Key Required? Value Default metrics.graphite.hostname yes IP address or hostname to which Graphite plaintext protocol data are sent null metrics.graphite.interval yes Milliseconds to wait between pushing data to Graphite null metrics.graphite.port no Port to which Graphite plaintext protocol reports are sent 2003 metrics.graphite.prefix no Arbitrary string prepended to all metric names sent to Graphite null Example janusgraph.properties snippet that sends metrics to a Graphite server on 192.168.0.1 every minute: metrics.enabled = true # Required; IP or hostname string metrics.graphite.hostname = 192.168.0.1 # Required; specify logging interval in milliseconds metrics.graphite.interval = 60000","title":"Graphite Reporter"},{"location":"operations/monitoring/#jmx-reporter","text":"Metrics JMX Reporter Configuration Options Config Key Required? Value Default metrics.jmx.enabled yes Boolean false metrics.jmx.domain no Metrics will appear in this JMX domain Metrics\u2019s own default metrics.jmx.agentid no Metrics will be reported with this JMX agent ID Metrics\u2019s own default Example janusgraph.properties snippet: metrics.enabled = true # Required metrics.jmx.enabled = true # Optional; if omitted, then Metrics uses its default values metrics.jmx.domain = foo metrics.jmx.agentid = baz","title":"JMX Reporter"},{"location":"operations/monitoring/#slf4j-reporter","text":"Metrics Slf4j Reporter Configuration Options Config Key Required? Value Default metrics.slf4j.interval yes Milliseconds to wait between dumping metrics to the logger null metrics.slf4j.logger no Slf4j logger name to use \"metrics\" Example janusgraph.properties snippet that logs metrics once a minute to the logger named foo : metrics.enabled = true # Required; specify logging interval in milliseconds metrics.slf4j.interval = 60000 # Optional; uses Metrics default when unset metrics.slf4j.logger = foo","title":"Slf4j Reporter"},{"location":"operations/monitoring/#user-providedcustom-reporter","text":"In case the Metrics reporter configuration options listed above are insufficient, JanusGraph provides a utility method to access the single MetricRegistry instance which holds all of its measurements. com . codahale . metrics . MetricRegistry janusgraphRegistry = org . janusgraph . util . stats . MetricManager . INSTANCE . getRegistry (); Code that accesses janusgraphRegistry this way can then attach non-standard reporter types or standard reporter types with exotic configurations to janusgraphRegistry . This approach is also useful if the surrounding application already has a framework for Metrics reporter configuration, or if the application needs multiple differently-configured instances of one of JanusGraph\u2019s supported reporter types. For instance, one could use this approach to setup multiple unicast Graphite reporters whereas JanusGraph\u2019s properties configuration is limited to just one Graphite reporter.","title":"User-Provided/Custom Reporter"},{"location":"operations/recovery/","text":"Failure & Recovery JanusGraph is a highly available and robust graph database. In large scale JanusGraph deployments failure is inevitable. This page describes some failure situations and how JanusGraph can handle them. Transaction Failure Transactions can fail for a number of reasons. If the transaction fails before the commit the changes will be discarded and the application can retry the transaction in coherence with the business logic. Likewise, locking or other consistency failures will cause an exception prior to persistence and hence can be retried. The persistence stage of a transaction is when JanusGraph starts persisting data to the various backend systems. JanusGraph first persists all graph mutations to the storage backend. This persistence is executed as one batch mutation to ensure that the mutation is committed atomically for those backends supporting atomicity. If the batch mutation fails due to an exception in the storage backend, the entire transaction is failed. If the primary persistence into the storage backend succeeds but secondary persistence into the indexing backends or the logging system fail, the transaction is still considered to be successful because the storage backend is the authoritative source of the graph. However, this can create inconsistencies with the indexes and logs. To automatically repair such inconsistencies, JanusGraph can maintain a transaction write-ahead log which is enabled through the configuration. tx.log-tx = true tx.max-commit-time = 10000 The max-commit-time property is used to determine when a transaction has failed. If the persistence stage of the transaction takes longer than this time, JanusGraph will attempt to recover it if necessary. Hence, this time out should be configured as a generous upper bound on the maximum duration of persistence. Note, that this does not include the time spent before commit. In addition, a separate process must be setup that reads the log to identify partially failed transaction and repair any inconsistencies caused. It is suggested to run the transaction repair process on a separate machine connected to the cluster to isolate failures. Configure a separately controlled process to run the following where the start time specifies the time since epoch where the recovery process should start reading from the write-ahead log. recovery = JanusGraphFactory . startTransactionRecovery ( graph , startTime , TimeUnit . MILLISECONDS ); Enabling the transaction write-ahead log causes an additional write operation for mutating transactions which increases the latency. Also note, that additional space is required to store the log. The transaction write-ahead log has a configurable time-to-live of 2 days which means that log entries expire after that time to keep the storage overhead small. Refer to Configuration Reference for a complete list of all log related configuration options to fine tune logging behavior. JanusGraph Instance Failure JanusGraph is robust against individual instance failure in that other instances of the JanusGraph cluster are not impacted by such failure and can continue processing transactions without loss of performance while the failed instance is restarted. However, some schema related operations - such as installing indexes - require the coordination of all JanusGraph instances. For this reason, JanusGraph maintains a record of all running instances. If an instance fails, i.e. is not properly shut down, JanusGraph considers it to be active and expects its participation in cluster-wide operations which subsequently fail because this instances did not participate in or did not acknowledge the operation. In this case, the user must manually remove the failed instance record from the cluster and then retry the operation. To remove the failed instance, open a management transaction against any of the running JanusGraph instances, inspect the list of running instances to identify the failed one, and finally remove it. mgmt = graph . openManagement () mgmt . getOpenInstances () //all open instances ==> 7 f0001016161 - dunwich1 ( current ) ==> 7 f0001016161 - atlantis1 mgmt . forceCloseInstance ( '7f0001016161-atlantis1' ) //remove an instance mgmt . commit () The unique identifier of the current JanusGraph instance is marked with the suffix (current) so that it can be easily identified. This instance cannot be closed via the forceCloseInstance method and instead should be closed via g.close() It must be ensured that the manually removed instance is indeed no longer active. Removing an active JanusGraph instance from a cluster can cause data inconsistencies. Hence, use this method with great care in particular when JanusGraph is operated in an environment where instances are automatically restarted.","title":"Failure & Recovery"},{"location":"operations/recovery/#failure-recovery","text":"JanusGraph is a highly available and robust graph database. In large scale JanusGraph deployments failure is inevitable. This page describes some failure situations and how JanusGraph can handle them.","title":"Failure &amp; Recovery"},{"location":"operations/recovery/#transaction-failure","text":"Transactions can fail for a number of reasons. If the transaction fails before the commit the changes will be discarded and the application can retry the transaction in coherence with the business logic. Likewise, locking or other consistency failures will cause an exception prior to persistence and hence can be retried. The persistence stage of a transaction is when JanusGraph starts persisting data to the various backend systems. JanusGraph first persists all graph mutations to the storage backend. This persistence is executed as one batch mutation to ensure that the mutation is committed atomically for those backends supporting atomicity. If the batch mutation fails due to an exception in the storage backend, the entire transaction is failed. If the primary persistence into the storage backend succeeds but secondary persistence into the indexing backends or the logging system fail, the transaction is still considered to be successful because the storage backend is the authoritative source of the graph. However, this can create inconsistencies with the indexes and logs. To automatically repair such inconsistencies, JanusGraph can maintain a transaction write-ahead log which is enabled through the configuration. tx.log-tx = true tx.max-commit-time = 10000 The max-commit-time property is used to determine when a transaction has failed. If the persistence stage of the transaction takes longer than this time, JanusGraph will attempt to recover it if necessary. Hence, this time out should be configured as a generous upper bound on the maximum duration of persistence. Note, that this does not include the time spent before commit. In addition, a separate process must be setup that reads the log to identify partially failed transaction and repair any inconsistencies caused. It is suggested to run the transaction repair process on a separate machine connected to the cluster to isolate failures. Configure a separately controlled process to run the following where the start time specifies the time since epoch where the recovery process should start reading from the write-ahead log. recovery = JanusGraphFactory . startTransactionRecovery ( graph , startTime , TimeUnit . MILLISECONDS ); Enabling the transaction write-ahead log causes an additional write operation for mutating transactions which increases the latency. Also note, that additional space is required to store the log. The transaction write-ahead log has a configurable time-to-live of 2 days which means that log entries expire after that time to keep the storage overhead small. Refer to Configuration Reference for a complete list of all log related configuration options to fine tune logging behavior.","title":"Transaction Failure"},{"location":"operations/recovery/#janusgraph-instance-failure","text":"JanusGraph is robust against individual instance failure in that other instances of the JanusGraph cluster are not impacted by such failure and can continue processing transactions without loss of performance while the failed instance is restarted. However, some schema related operations - such as installing indexes - require the coordination of all JanusGraph instances. For this reason, JanusGraph maintains a record of all running instances. If an instance fails, i.e. is not properly shut down, JanusGraph considers it to be active and expects its participation in cluster-wide operations which subsequently fail because this instances did not participate in or did not acknowledge the operation. In this case, the user must manually remove the failed instance record from the cluster and then retry the operation. To remove the failed instance, open a management transaction against any of the running JanusGraph instances, inspect the list of running instances to identify the failed one, and finally remove it. mgmt = graph . openManagement () mgmt . getOpenInstances () //all open instances ==> 7 f0001016161 - dunwich1 ( current ) ==> 7 f0001016161 - atlantis1 mgmt . forceCloseInstance ( '7f0001016161-atlantis1' ) //remove an instance mgmt . commit () The unique identifier of the current JanusGraph instance is marked with the suffix (current) so that it can be easily identified. This instance cannot be closed via the forceCloseInstance method and instead should be closed via g.close() It must be ensured that the manually removed instance is indeed no longer active. Removing an active JanusGraph instance from a cluster can cause data inconsistencies. Hence, use this method with great care in particular when JanusGraph is operated in an environment where instances are automatically restarted.","title":"JanusGraph Instance Failure"},{"location":"operations/server/","text":"JanusGraph Server JanusGraph uses the Gremlin Server engine as the server component to process and answer client queries and extends it with convenience features for JanusGraph. From now on, we will call this JanusGraph Server. JanusGraph Server must be started manually in order to use it. JanusGraph Server provides a way to remotely execute Gremlin traversals against one or more JanusGraph instances hosted within it. This section will describe how to use the WebSocket configuration, as well as describe how to configure JanusGraph Server to handle HTTP endpoint interactions. For information about how to connect to a JanusGraph Server from different languages refer to Connecting to JanusGraph . Starting a JanusGraph Server JanusGraph Server comes packaged with a script called bin/janusgraph-server.sh to get it started: $ ./bin/janusgraph-server.sh console SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/var/lib/janusgraph/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/var/lib/janusgraph/lib/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 0 [main] INFO org.janusgraph.graphdb.server.JanusGraphServer - mmm mmm # # mmm m mm m m mmm m\" \" m mm mmm mmmm # mm # \" # #\" # # # # \" # mm #\" \" \" # #\" \"# #\" # # m\"\"\"# # # # # \"\"\"m # # # m\"\"\"# # # # # \"mmm\" \"mm\"# # # \"mm\"# \"mmm\" \"mmm\" # \"mm\"# ##m#\" # # # \" [...] 2240 [gremlin-server-boss-1] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Channel started at port 8182. JanusGraph Server is configured by the provided YAML file conf/gremlin-server/gremlin-server.yaml . That file tells JanusGraph Server many things and is based on the Gremlin Server config, see Gremlin Server . Usage of janusgraph-server.sh The JanusGraph Server can be started in the foreground with stdout logging or detached. $ ./bin/janusgraph-server.sh Usage: ./bin/janusgraph-server.sh {start [conf file]|stop|restart [conf file]|status|console|usage <group> <artifact> <version>|<conf file>} start Start the server in the background. Configuration file can be specified as a second argument or as JANUSGRAPH_YAML environment variable. If configuration file is not specified or has invalid path than JanusGraph server will try to use the default configuration file at relative location conf/gremlin-server/gremlin-server.yaml stop Stop the server restart Stop and start the server. To use previously used configuration it should be specified again as described in \"start\" command status Check if the server is running console Start the server in the foreground. Same rules are applied for configurations as described in \"start\" command usage Print out this help message In case command is not specified and the configuration is specified as the first argument, JanusGraph Server will be started in the foreground using the specified configuration (same as with \"console\" command). Env variables Variable Description Default Value $JANUSGRAPH_HOME Root directory of a default janusgraph installation. (default directory below janusgraph-server.sh) $JANUSGRAPH_CONF Config directory containing all kinds of server and graph configs. $JANUSGRAPH_HOME/conf $LOG_DIR Log directory \"$JANUSGRAPH_HOME/logs\" $LOG_FILE Default log file \"$LOG_DIR/janusgraph.log\" $PID_DIR \"$JANUSGRAPH_HOME/run\" $PID_FILE \"$PID_DIR/janusgraph.pid\" $JANUSGRAPH_YAML JanusGraph Server config path \"$JANUSGRAPH_CONF/gremlin-server/gremlin-server.yaml\" $JANUSGRAPH_LIB JanusGraph library directory \"$JANUSGRAPH_HOME/lib\" $JAVA_HOME If not set java home fallback to java . NOT_SET $JAVA_OPTIONS_FILE \"$JANUSGRAPH_CONF/jvm.options\" $JAVA_OPTIONS NOT_SET $CP Can be used to override the classpath's. (expert mode) NOT_SET $DEBUG If you enable debug by creating this env, bash debug will be enabled. NOT_SET Configure jvm.options JanusGraph runs on the JVM which is configurable for special use cases. Therefore, JanusGraph provides a jvm.options file with some default options. # Copyright 2020 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################# # HEAP SETTINGS # ################# -Xms4096m -Xmx4096m ######################## # GENERAL JVM SETTINGS # ######################## # enable thread priorities, primarily so we can give periodic tasks # a lower priority to avoid interfering with client workload -XX:+UseThreadPriorities # allows lowering thread priority without being root on linux - probably # not necessary on Windows but doesn't harm anything. # see http://tech.stolsvik.com/2010/01/linux-java-thread-priorities-workar -XX:ThreadPriorityPolicy = 42 # Enable heap-dump if there's an OOM -XX:+HeapDumpOnOutOfMemoryError # Per-thread stack size. -Xss256k # Make sure all memory is faulted and zeroed on startup. # This helps prevent soft faults in containers and makes # transparent hugepage allocation more effective. -XX:+AlwaysPreTouch # Enable thread-local allocation blocks and allow the JVM to automatically # resize them at runtime. -XX:+UseTLAB -XX:+ResizeTLAB -XX:+UseNUMA #################### # GREMLIN SETTINGS # #################### -Dgremlin.io.kryoShimService = org.janusgraph.hadoop.serialize.JanusGraphKryoShimService ################# # GC SETTINGS # ################# ### CMS Settings -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio = 8 -XX:MaxTenuringThreshold = 1 -XX:CMSInitiatingOccupancyFraction = 75 -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSWaitDuration = 10000 -XX:+CMSParallelInitialMarkEnabled -XX:+CMSEdenChunksRecordAlways -XX:+CMSClassUnloadingEnabled JanusGraph Server as a WebSocket Endpoint The default configuration described in Getting Started is already a WebSocket configuration. If you want to alter the default configuration to work with your own Cassandra or HBase environment rather than use the quick start environment, follow these steps: To Configure JanusGraph Server For WebSocket Test a local connection to a JanusGraph database first. This step applies whether using the Gremlin Console to test the connection, or whether connecting from a program. Make appropriate changes in a properties file in the ./conf directory for your environment. For example, edit ./conf/janusgraph-hbase.properties and make sure the storage.backend, storage.hostname and storage.hbase.table parameters are specified correctly. For more information on configuring JanusGraph for various storage backends, see Storage Backends . Make sure the properties file contains the following line: gremlin.graph = org.janusgraph.core.JanusGraphFactory Once a local configuration is tested and you have a working properties file, copy the properties file from the ./conf directory to the ./conf/gremlin-server directory. cp conf/janusgraph-hbase.properties conf/gremlin-server/socket-janusgraph-hbase-server.properties Copy ./conf/gremlin-server/gremlin-server.yaml to a new file called socket-gremlin-server.yaml . Do this in case you need to refer to the original version of the file cp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/socket-gremlin-server.yaml Edit the socket-gremlin-server.yaml file and make the following updates: If you are planning to connect to JanusGraph Server from something other than localhost, update the IP address for host: host: 10.10.10.100 Update the graphs section to point to your new properties file so the JanusGraph Server can find and connect to your JanusGraph instance: graphs : { graph : conf/gremlin-server/socket-janusgraph-hbase-server.properties } Start the JanusGraph Server, specifying the yaml file you just configured: bin/janusgraph-server.sh console ./conf/gremlin-server/socket-gremlin-server.yaml The JanusGraph Server should now be running in WebSocket mode and can be tested by following the instructions in Connecting to Gremlin Server Important Do not use bin/janusgraph.sh . That starts the default configuration, which starts a separate Cassandra/Elasticsearch environment. JanusGraph Server as a HTTP Endpoint The default configuration described in Getting Started is a WebSocket configuration. If you want to alter the default configuration in order to use JanusGraph Server as an HTTP endpoint for your JanusGraph database, follow these steps: Test a local connection to a JanusGraph database first. This step applies whether using the Gremlin Console to test the connection, or whether connecting from a program. Make appropriate changes in a properties file in the ./conf directory for your environment. For example, edit ./conf/janusgraph-hbase.properties and make sure the storage.backend, storage.hostname and storage.hbase.table parameters are specified correctly. For more information on configuring JanusGraph for various storage backends, see Storage Backends . Make sure the properties file contains the following line: gremlin.graph = org.janusgraph.core.JanusGraphFactory Once a local configuration is tested and you have a working properties file, copy the properties file from the ./conf directory to the ./conf/gremlin-server directory. cp conf/janusgraph-hbase.properties conf/gremlin-server/http-janusgraph-hbase-server.properties Copy ./conf/gremlin-server/gremlin-server.yaml to a new file called http-gremlin-server.yaml . Do this in case you need to refer to the original version of the file cp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/http-gremlin-server.yaml Edit the http-gremlin-server.yaml file and make the following updates: If you are planning to connect to JanusGraph Server from something other than localhost, update the IP address for host: host: 10.10.10.100 Update the channelizer setting to specify the HttpChannelizer: channelizer : org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer Update the graphs section to point to your new properties file so the JanusGraph Server can find and connect to your JanusGraph instance: graphs : { graph : conf/gremlin-server/http-janusgraph-hbase-server.properties } Start the JanusGraph Server, specifying the yaml file you just configured: bin/janusgraph-server.sh console ./conf/gremlin-server/http-gremlin-server.yaml The JanusGraph Server should now be running in HTTP mode and available for testing. curl can be used to verify the server is working: curl -XPOST -Hcontent-type:application/json -d * { \"gremlin\" : \"g.V().count()\" } * [ IP for JanusGraph server host ]( http:// ) :8182 JanusGraph Server as Both a WebSocket and HTTP Endpoint As of JanusGraph 0.2.0, you can configure your gremlin-server.yaml to accept both WebSocket and HTTP connections over the same port. This can be achieved by changing the channelizer in any of the previous examples as follows. channelizer : org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer Advanced JanusGraph Server Configurations Authentication over HTTP Important In the following example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords. HTTP Basic authentication To enable Basic authentication in JanusGraph Server include the following configuration in your gremlin-server.yaml . authentication : { authenticator : org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.JanusGraphSimpleAuthenticator , authenticationHandler : org.apache.tinkerpop.gremlin.server.handler.HttpBasicAuthenticationHandler , config : { defaultUsername : user , defaultPassword : password , credentialsDb : conf/janusgraph-credentials-server.properties } } Verify that basic authentication is configured correctly. For example curl -v -XPOST http://localhost:8182 -d '{\"gremlin\": \"g.V().count()\"}' should return a 401 if the authentication is configured correctly and curl -v -XPOST http://localhost:8182 -d '{\"gremlin\": \"g.V().count()\"}' -u user:password should return a 200 and the result of 4 if authentication is configured correctly. Authentication over WebSocket Authentication over WebSocket occurs through a Simple Authentication and Security Layer (https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer[SASL]) mechanism. To enable SASL authentication include the following configuration in the gremlin-server.yaml authentication : { authenticator : org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.JanusGraphSimpleAuthenticator , authenticationHandler : org.apache.tinkerpop.gremlin.server.handler.SaslAuthenticationHandler , config : { defaultUsername : user , defaultPassword : password , credentialsDb : conf/janusgraph-credentials-server.properties } } Important In the preceding example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords. If you are connecting through the gremlin console, your remote yaml file should amend the username and password properties with the appropriate values. username : user password : password Authentication over HTTP and WebSocket If you are using the combined channelizer for both HTTP and WebSocket you can use the SaslAndHMACAuthenticator to authorize through either WebSocket through SASL, HTTP through basic auth, and HTTP through hash-based message authentication code (https://en.wikipedia.org/wiki/Hash-based_message_authentication_code[HMAC]) Auth. HMAC is a token based authentication designed to be used over HTTP. You first acquire a token via the /session endpoint and then use that to authenticate. It is used to amortize the time spent encrypting the password using basic auth. The gremlin-server.yaml should include the following configurations authentication : { authenticator : org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.SaslAndHMACAuthenticator , authenticationHandler : org.janusgraph.graphdb.tinkerpop.gremlin.server.handler.SaslAndHMACAuthenticationHandler , config : { defaultUsername : user , defaultPassword : password , hmacSecret : secret , credentialsDb : conf/janusgraph-credentials-server.properties } } Important In the preceding example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords. Important Note the hmacSecret here. This should be the same across all running JanusGraph servers if you want to be able to use the same HMAC token on each server. For HMAC authentication over HTTP, this creates a /session endpoint that provides a token that expires after an hour by default. This timeout for the token can be configured through the tokenTimeout configuration option in the authentication.config map. This value is a Long value and in milliseconds. You can obtain the token using curl by issuing a get request to the /session endpoint. For example curl http://localhost:8182/session -XGET -u user:password { \"token\" : \"dXNlcjoxNTA5NTQ2NjI0NDUzOkhrclhYaGhRVG9KTnVSRXJ5U2VpdndhalJRcVBtWEpSMzh5WldqRTM4MW89\" } You can then use that token for authentication by using the \"Authorization: Token\" header. For example curl -v http://localhost:8182/session -XPOST -d '{\"gremlin\": \"g.V().count()\"}' -H \"Authorization: Token dXNlcjoxNTA5NTQ2NjI0NDUzOkhrclhYaGhRVG9KTnVSRXJ5U2VpdndhalJRcVBtWEpSMzh5WldqRTM4MW89\" Extending JanusGraph Server Note We currently are refactoring JanusGraph Server. If you like to get information or want to give input, see issue #2119 . It is possible to extend Gremlin Server with other means of communication by implementing the interfaces that it provides and leverage this with JanusGraph. See more details in the appropriate TinkerPop documentation.","title":"JanusGraph Server"},{"location":"operations/server/#janusgraph-server","text":"JanusGraph uses the Gremlin Server engine as the server component to process and answer client queries and extends it with convenience features for JanusGraph. From now on, we will call this JanusGraph Server. JanusGraph Server must be started manually in order to use it. JanusGraph Server provides a way to remotely execute Gremlin traversals against one or more JanusGraph instances hosted within it. This section will describe how to use the WebSocket configuration, as well as describe how to configure JanusGraph Server to handle HTTP endpoint interactions. For information about how to connect to a JanusGraph Server from different languages refer to Connecting to JanusGraph .","title":"JanusGraph Server"},{"location":"operations/server/#starting-a-janusgraph-server","text":"JanusGraph Server comes packaged with a script called bin/janusgraph-server.sh to get it started: $ ./bin/janusgraph-server.sh console SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/var/lib/janusgraph/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/var/lib/janusgraph/lib/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 0 [main] INFO org.janusgraph.graphdb.server.JanusGraphServer - mmm mmm # # mmm m mm m m mmm m\" \" m mm mmm mmmm # mm # \" # #\" # # # # \" # mm #\" \" \" # #\" \"# #\" # # m\"\"\"# # # # # \"\"\"m # # # m\"\"\"# # # # # \"mmm\" \"mm\"# # # \"mm\"# \"mmm\" \"mmm\" # \"mm\"# ##m#\" # # # \" [...] 2240 [gremlin-server-boss-1] INFO org.apache.tinkerpop.gremlin.server.GremlinServer - Channel started at port 8182. JanusGraph Server is configured by the provided YAML file conf/gremlin-server/gremlin-server.yaml . That file tells JanusGraph Server many things and is based on the Gremlin Server config, see Gremlin Server .","title":"Starting a JanusGraph Server"},{"location":"operations/server/#usage-of-janusgraph-serversh","text":"The JanusGraph Server can be started in the foreground with stdout logging or detached. $ ./bin/janusgraph-server.sh Usage: ./bin/janusgraph-server.sh {start [conf file]|stop|restart [conf file]|status|console|usage <group> <artifact> <version>|<conf file>} start Start the server in the background. Configuration file can be specified as a second argument or as JANUSGRAPH_YAML environment variable. If configuration file is not specified or has invalid path than JanusGraph server will try to use the default configuration file at relative location conf/gremlin-server/gremlin-server.yaml stop Stop the server restart Stop and start the server. To use previously used configuration it should be specified again as described in \"start\" command status Check if the server is running console Start the server in the foreground. Same rules are applied for configurations as described in \"start\" command usage Print out this help message In case command is not specified and the configuration is specified as the first argument, JanusGraph Server will be started in the foreground using the specified configuration (same as with \"console\" command).","title":"Usage of janusgraph-server.sh"},{"location":"operations/server/#env-variables","text":"Variable Description Default Value $JANUSGRAPH_HOME Root directory of a default janusgraph installation. (default directory below janusgraph-server.sh) $JANUSGRAPH_CONF Config directory containing all kinds of server and graph configs. $JANUSGRAPH_HOME/conf $LOG_DIR Log directory \"$JANUSGRAPH_HOME/logs\" $LOG_FILE Default log file \"$LOG_DIR/janusgraph.log\" $PID_DIR \"$JANUSGRAPH_HOME/run\" $PID_FILE \"$PID_DIR/janusgraph.pid\" $JANUSGRAPH_YAML JanusGraph Server config path \"$JANUSGRAPH_CONF/gremlin-server/gremlin-server.yaml\" $JANUSGRAPH_LIB JanusGraph library directory \"$JANUSGRAPH_HOME/lib\" $JAVA_HOME If not set java home fallback to java . NOT_SET $JAVA_OPTIONS_FILE \"$JANUSGRAPH_CONF/jvm.options\" $JAVA_OPTIONS NOT_SET $CP Can be used to override the classpath's. (expert mode) NOT_SET $DEBUG If you enable debug by creating this env, bash debug will be enabled. NOT_SET","title":"Env variables"},{"location":"operations/server/#configure-jvmoptions","text":"JanusGraph runs on the JVM which is configurable for special use cases. Therefore, JanusGraph provides a jvm.options file with some default options. # Copyright 2020 JanusGraph Authors # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. ################# # HEAP SETTINGS # ################# -Xms4096m -Xmx4096m ######################## # GENERAL JVM SETTINGS # ######################## # enable thread priorities, primarily so we can give periodic tasks # a lower priority to avoid interfering with client workload -XX:+UseThreadPriorities # allows lowering thread priority without being root on linux - probably # not necessary on Windows but doesn't harm anything. # see http://tech.stolsvik.com/2010/01/linux-java-thread-priorities-workar -XX:ThreadPriorityPolicy = 42 # Enable heap-dump if there's an OOM -XX:+HeapDumpOnOutOfMemoryError # Per-thread stack size. -Xss256k # Make sure all memory is faulted and zeroed on startup. # This helps prevent soft faults in containers and makes # transparent hugepage allocation more effective. -XX:+AlwaysPreTouch # Enable thread-local allocation blocks and allow the JVM to automatically # resize them at runtime. -XX:+UseTLAB -XX:+ResizeTLAB -XX:+UseNUMA #################### # GREMLIN SETTINGS # #################### -Dgremlin.io.kryoShimService = org.janusgraph.hadoop.serialize.JanusGraphKryoShimService ################# # GC SETTINGS # ################# ### CMS Settings -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:SurvivorRatio = 8 -XX:MaxTenuringThreshold = 1 -XX:CMSInitiatingOccupancyFraction = 75 -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSWaitDuration = 10000 -XX:+CMSParallelInitialMarkEnabled -XX:+CMSEdenChunksRecordAlways -XX:+CMSClassUnloadingEnabled","title":"Configure jvm.options"},{"location":"operations/server/#janusgraph-server-as-a-websocket-endpoint","text":"The default configuration described in Getting Started is already a WebSocket configuration. If you want to alter the default configuration to work with your own Cassandra or HBase environment rather than use the quick start environment, follow these steps: To Configure JanusGraph Server For WebSocket Test a local connection to a JanusGraph database first. This step applies whether using the Gremlin Console to test the connection, or whether connecting from a program. Make appropriate changes in a properties file in the ./conf directory for your environment. For example, edit ./conf/janusgraph-hbase.properties and make sure the storage.backend, storage.hostname and storage.hbase.table parameters are specified correctly. For more information on configuring JanusGraph for various storage backends, see Storage Backends . Make sure the properties file contains the following line: gremlin.graph = org.janusgraph.core.JanusGraphFactory Once a local configuration is tested and you have a working properties file, copy the properties file from the ./conf directory to the ./conf/gremlin-server directory. cp conf/janusgraph-hbase.properties conf/gremlin-server/socket-janusgraph-hbase-server.properties Copy ./conf/gremlin-server/gremlin-server.yaml to a new file called socket-gremlin-server.yaml . Do this in case you need to refer to the original version of the file cp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/socket-gremlin-server.yaml Edit the socket-gremlin-server.yaml file and make the following updates: If you are planning to connect to JanusGraph Server from something other than localhost, update the IP address for host: host: 10.10.10.100 Update the graphs section to point to your new properties file so the JanusGraph Server can find and connect to your JanusGraph instance: graphs : { graph : conf/gremlin-server/socket-janusgraph-hbase-server.properties } Start the JanusGraph Server, specifying the yaml file you just configured: bin/janusgraph-server.sh console ./conf/gremlin-server/socket-gremlin-server.yaml The JanusGraph Server should now be running in WebSocket mode and can be tested by following the instructions in Connecting to Gremlin Server Important Do not use bin/janusgraph.sh . That starts the default configuration, which starts a separate Cassandra/Elasticsearch environment.","title":"JanusGraph Server as a WebSocket Endpoint"},{"location":"operations/server/#janusgraph-server-as-a-http-endpoint","text":"The default configuration described in Getting Started is a WebSocket configuration. If you want to alter the default configuration in order to use JanusGraph Server as an HTTP endpoint for your JanusGraph database, follow these steps: Test a local connection to a JanusGraph database first. This step applies whether using the Gremlin Console to test the connection, or whether connecting from a program. Make appropriate changes in a properties file in the ./conf directory for your environment. For example, edit ./conf/janusgraph-hbase.properties and make sure the storage.backend, storage.hostname and storage.hbase.table parameters are specified correctly. For more information on configuring JanusGraph for various storage backends, see Storage Backends . Make sure the properties file contains the following line: gremlin.graph = org.janusgraph.core.JanusGraphFactory Once a local configuration is tested and you have a working properties file, copy the properties file from the ./conf directory to the ./conf/gremlin-server directory. cp conf/janusgraph-hbase.properties conf/gremlin-server/http-janusgraph-hbase-server.properties Copy ./conf/gremlin-server/gremlin-server.yaml to a new file called http-gremlin-server.yaml . Do this in case you need to refer to the original version of the file cp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/http-gremlin-server.yaml Edit the http-gremlin-server.yaml file and make the following updates: If you are planning to connect to JanusGraph Server from something other than localhost, update the IP address for host: host: 10.10.10.100 Update the channelizer setting to specify the HttpChannelizer: channelizer : org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer Update the graphs section to point to your new properties file so the JanusGraph Server can find and connect to your JanusGraph instance: graphs : { graph : conf/gremlin-server/http-janusgraph-hbase-server.properties } Start the JanusGraph Server, specifying the yaml file you just configured: bin/janusgraph-server.sh console ./conf/gremlin-server/http-gremlin-server.yaml The JanusGraph Server should now be running in HTTP mode and available for testing. curl can be used to verify the server is working: curl -XPOST -Hcontent-type:application/json -d * { \"gremlin\" : \"g.V().count()\" } * [ IP for JanusGraph server host ]( http:// ) :8182","title":"JanusGraph Server as a HTTP Endpoint"},{"location":"operations/server/#janusgraph-server-as-both-a-websocket-and-http-endpoint","text":"As of JanusGraph 0.2.0, you can configure your gremlin-server.yaml to accept both WebSocket and HTTP connections over the same port. This can be achieved by changing the channelizer in any of the previous examples as follows. channelizer : org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer","title":"JanusGraph Server as Both a WebSocket and HTTP Endpoint"},{"location":"operations/server/#advanced-janusgraph-server-configurations","text":"","title":"Advanced JanusGraph Server Configurations"},{"location":"operations/server/#authentication-over-http","text":"Important In the following example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords.","title":"Authentication over HTTP"},{"location":"operations/server/#http-basic-authentication","text":"To enable Basic authentication in JanusGraph Server include the following configuration in your gremlin-server.yaml . authentication : { authenticator : org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.JanusGraphSimpleAuthenticator , authenticationHandler : org.apache.tinkerpop.gremlin.server.handler.HttpBasicAuthenticationHandler , config : { defaultUsername : user , defaultPassword : password , credentialsDb : conf/janusgraph-credentials-server.properties } } Verify that basic authentication is configured correctly. For example curl -v -XPOST http://localhost:8182 -d '{\"gremlin\": \"g.V().count()\"}' should return a 401 if the authentication is configured correctly and curl -v -XPOST http://localhost:8182 -d '{\"gremlin\": \"g.V().count()\"}' -u user:password should return a 200 and the result of 4 if authentication is configured correctly.","title":"HTTP Basic authentication"},{"location":"operations/server/#authentication-over-websocket","text":"Authentication over WebSocket occurs through a Simple Authentication and Security Layer (https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer[SASL]) mechanism. To enable SASL authentication include the following configuration in the gremlin-server.yaml authentication : { authenticator : org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.JanusGraphSimpleAuthenticator , authenticationHandler : org.apache.tinkerpop.gremlin.server.handler.SaslAuthenticationHandler , config : { defaultUsername : user , defaultPassword : password , credentialsDb : conf/janusgraph-credentials-server.properties } } Important In the preceding example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords. If you are connecting through the gremlin console, your remote yaml file should amend the username and password properties with the appropriate values. username : user password : password","title":"Authentication over WebSocket"},{"location":"operations/server/#authentication-over-http-and-websocket","text":"If you are using the combined channelizer for both HTTP and WebSocket you can use the SaslAndHMACAuthenticator to authorize through either WebSocket through SASL, HTTP through basic auth, and HTTP through hash-based message authentication code (https://en.wikipedia.org/wiki/Hash-based_message_authentication_code[HMAC]) Auth. HMAC is a token based authentication designed to be used over HTTP. You first acquire a token via the /session endpoint and then use that to authenticate. It is used to amortize the time spent encrypting the password using basic auth. The gremlin-server.yaml should include the following configurations authentication : { authenticator : org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.SaslAndHMACAuthenticator , authenticationHandler : org.janusgraph.graphdb.tinkerpop.gremlin.server.handler.SaslAndHMACAuthenticationHandler , config : { defaultUsername : user , defaultPassword : password , hmacSecret : secret , credentialsDb : conf/janusgraph-credentials-server.properties } } Important In the preceding example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords. Important Note the hmacSecret here. This should be the same across all running JanusGraph servers if you want to be able to use the same HMAC token on each server. For HMAC authentication over HTTP, this creates a /session endpoint that provides a token that expires after an hour by default. This timeout for the token can be configured through the tokenTimeout configuration option in the authentication.config map. This value is a Long value and in milliseconds. You can obtain the token using curl by issuing a get request to the /session endpoint. For example curl http://localhost:8182/session -XGET -u user:password { \"token\" : \"dXNlcjoxNTA5NTQ2NjI0NDUzOkhrclhYaGhRVG9KTnVSRXJ5U2VpdndhalJRcVBtWEpSMzh5WldqRTM4MW89\" } You can then use that token for authentication by using the \"Authorization: Token\" header. For example curl -v http://localhost:8182/session -XPOST -d '{\"gremlin\": \"g.V().count()\"}' -H \"Authorization: Token dXNlcjoxNTA5NTQ2NjI0NDUzOkhrclhYaGhRVG9KTnVSRXJ5U2VpdndhalJRcVBtWEpSMzh5WldqRTM4MW89\"","title":"Authentication over HTTP and WebSocket"},{"location":"operations/server/#extending-janusgraph-server","text":"Note We currently are refactoring JanusGraph Server. If you like to get information or want to give input, see issue #2119 . It is possible to extend Gremlin Server with other means of communication by implementing the interfaces that it provides and leverage this with JanusGraph. See more details in the appropriate TinkerPop documentation.","title":"Extending JanusGraph Server"},{"location":"schema/","text":"Schema and Data Modeling Each JanusGraph graph has a schema comprised of the edge labels, property keys, and vertex labels used therein. A JanusGraph schema can either be explicitly or implicitly defined. Users are encouraged to explicitly define the graph schema during application development. An explicitly defined schema is an important component of a robust graph application and greatly improves collaborative software development. Note, that a JanusGraph schema can be evolved over time without any interruption of normal database operations. Extending the schema does not slow down query answering and does not require database downtime. The schema type - i.e. edge label, property key, or vertex label - is assigned to elements in the graph - i.e. edge, properties or vertices respectively - when they are first created. The assigned schema type cannot be changed for a particular element. This ensures a stable type system that is easy to reason about. Beyond the schema definition options explained in this section, schema types provide performance tuning options that are discussed in Advanced Schema . Displaying Schema Information There are methods to view specific elements of the graph schema within the management API. These methods are mgmt.printIndexes() , mgmt.printPropertyKeys() , mgmt.printVertexLabels() , and mgmt.printEdgeLabels() . There is also a method that displays all the combined output named printSchema() . mgmt = graph . openManagement () mgmt . printSchema () Defining Edge Labels Each edge connecting two vertices has a label which defines the semantics of the relationship. For instance, an edge labeled friend between vertices A and B encodes a friendship between the two individuals. To define an edge label, call makeEdgeLabel(String) on an open graph or management transaction and provide the name of the edge label as the argument. Edge label names must be unique in the graph. This method returns a builder for edge labels that allows to define its multiplicity. The multiplicity of an edge label defines a multiplicity constraint on all edges of this label, that is, a maximum number of edges between pairs of vertices. JanusGraph recognizes the following multiplicity settings. Edge Label Multiplicity MULTI : Allows multiple edges of the same label between any pair of vertices. In other words, the graph is a multi graph with respect to such edge label. There is no constraint on edge multiplicity. SIMPLE : Allows at most one edge of such label between any pair of vertices. In other words, the graph is a simple graph with respect to the label. Ensures that edges are unique for a given label and pairs of vertices. MANY2ONE : Allows at most one outgoing edge of such label on any vertex in the graph but places no constraint on incoming edges. The edge label mother is an example with MANY2ONE multiplicity since each person has at most one mother but mothers can have multiple children. ONE2MANY : Allows at most one incoming edge of such label on any vertex in the graph but places no constraint on outgoing edges. The edge label winnerOf is an example with ONE2MANY multiplicity since each contest is won by at most one person but a person can win multiple contests. ONE2ONE : Allows at most one incoming and one outgoing edge of such label on any vertex in the graph. The edge label marriedTo is an example with ONE2ONE multiplicity since a person is married to exactly one other person. The default multiplicity is MULTI. The definition of an edge label is completed by calling the make() method on the builder which returns the defined edge label as shown in the following example. mgmt = graph . openManagement () follow = mgmt . makeEdgeLabel ( 'follow' ). multiplicity ( MULTI ). make () mother = mgmt . makeEdgeLabel ( 'mother' ). multiplicity ( MANY2ONE ). make () mgmt . commit () Defining Property Keys Properties on vertices and edges are key-value pairs. For instance, the property name='Daniel' has the key name and the value 'Daniel' . Property keys are part of the JanusGraph schema and can constrain the allowed data types and cardinality of values. To define a property key, call makePropertyKey(String) on an open graph or management transaction and provide the name of the property key as the argument. Property key names must be unique in the graph, and it is recommended to avoid spaces or special characters in property names. This method returns a builder for the property keys. Note During property key creation, consider creating also graph indices for better performance, see Index Performance . Property Key Data Type Use dataType(Class) to define the data type of a property key. JanusGraph will enforce that all values associated with the key have the configured data type and thereby ensures that data added to the graph is valid. For instance, one can define that the name key has a String data type. Note that primitive types are not supported. Use the corresponding wrapper class, e.g. Integer instead of int . Define the data type as Object.class in order to allow any (serializable) value to be associated with a key. However, it is encouraged to use concrete data types whenever possible. Configured data types must be concrete classes and not interfaces or abstract classes. JanusGraph enforces class equality, so adding a sub-class of a configured data type is not allowed. JanusGraph natively supports the following data types. Native JanusGraph Data Types Name Description String Character sequence Character Individual character Boolean true or false Byte byte value Short short value Integer integer value Long long value Float 4 byte floating point number Double 8 byte floating point number Date Specific instant in time ( java.util.Date ) Geoshape Geographic shape like point, circle or box UUID Universally unique identifier ( java.util.UUID ) Property Key Cardinality Use cardinality(Cardinality) to define the allowed cardinality of the values associated with the key on any given vertex. SINGLE : Allows at most one value per element for such key. In other words, the key\u2192value mapping is unique for all elements in the graph. The property key birthDate is an example with SINGLE cardinality since each person has exactly one birth date. LIST : Allows an arbitrary number of values per element for such key. In other words, the key is associated with a list of values allowing duplicate values. Assuming we model sensors as vertices in a graph, the property key sensorReading is an example with LIST cardinality to allow lots of (potentially duplicate) sensor readings to be recorded. SET : Allows multiple values but no duplicate values per element for such key. In other words, the key is associated with a set of values. The property key name has SET cardinality if we want to capture all names of an individual (including nick name, maiden name, etc). The default cardinality setting is SINGLE. Note, that property keys used on edges and properties have cardinality SINGLE. Attaching multiple values for a single key on an edge or property is not supported. mgmt = graph . openManagement () birthDate = mgmt . makePropertyKey ( ' birthDate ' ). dataType ( Long . class ). cardinality ( Cardinality . SINGLE ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () sensorReading = mgmt . makePropertyKey ( ' sensorReading ' ). dataType ( Double . class ). cardinality ( Cardinality . LIST ). make () mgmt . commit () Relation Types Edge labels and property keys are jointly referred to as relation types . Names of relation types must be unique in the graph which means that property keys and edge labels cannot have the same name. There are methods in the JanusGraph API to query for the existence or retrieve relation types which encompasses both property keys and edge labels. mgmt = graph . openManagement () if ( mgmt . containsRelationType ( ' name ' )) name = mgmt . getPropertyKey ( ' name ' ) mgmt . getRelationTypes ( EdgeLabel . class ) mgmt . commit () Defining Vertex Labels Like edges, vertices have labels. Unlike edge labels, vertex labels are optional. Vertex labels are useful to distinguish different types of vertices, e.g. user vertices and product vertices. Although labels are optional at the conceptual and data model level, JanusGraph assigns all vertices a label as an internal implementation detail. Vertices created by the addVertex methods use JanusGraph\u2019s default label. To create a label, call makeVertexLabel(String).make() on an open graph or management transaction and provide the name of the vertex label as the argument. Vertex label names must be unique in the graph. mgmt = graph . openManagement () person = mgmt . makeVertexLabel ( ' person ' ). make () mgmt . commit () // Create a labeled vertex person = graph . addVertex ( label , ' person ' ) // Create an unlabeled vertex v = graph . addVertex () graph . tx (). commit () Automatic Schema Maker If an edge label, property key, or vertex label has not been defined explicitly, it will be defined implicitly when it is first used during the addition of an edge, vertex or the setting of a property. The DefaultSchemaMaker configured for the JanusGraph graph defines such types. By default, implicitly created edge labels have multiplicity MULTI and implicitly created property keys have cardinality SINGLE. Data types of implicitly created property keys are inferred as long as they are natively supported by JanusGraph. Object.class is used if and only if the given value is not any of the Native JanusGraph Data Types . Users can control automatic schema element creation by implementing and registering their own DefaultSchemaMaker . When defining a cardinality for a vertex property which differs from SINGLE, the cardinality should be used for all values of the vertex property in the first query (i.e. the query which defines a new vertex property key). It is strongly encouraged to explicitly define all schema elements and to disable automatic schema creation by setting schema.default=none in the JanusGraph graph configuration. Changing Schema Elements The definition of an edge label, property key, or vertex label cannot be changed once its committed into the graph. However, the names of schema elements can be changed via JanusGraphManagement.changeName(JanusGraphSchemaElement, String) as shown in the following example where the property key place is renamed to location . mgmt = graph . openManagement () place = mgmt . getPropertyKey ( ' place ' ) mgmt . changeName ( place , ' location ' ) mgmt . commit () Note, that schema name changes may not be immediately visible in currently running transactions and other JanusGraph graph instances in the cluster. While schema name changes are announced to all JanusGraph instances through the storage backend, it may take a while for the schema changes to take effect and it may require a instance restart in the event of certain failure conditions - like network partitions - if they coincide with the rename. Hence, the user must ensure that either of the following holds: The renamed label or key is not currently in active use (i.e. written or read) and will not be in use until all JanusGraph instances are aware of the name change. Running transactions actively accommodate the brief intermediate period where either the old or new name is valid based on the specific JanusGraph instance and status of the name-change announcement. For instance, that could mean transactions query for both names simultaneously. Should the need arise to re-define an existing schema type, it is recommended to change the name of this type to a name that is not currently (and will never be) in use. After that, a new label or key can be defined with the original name, thereby effectively replacing the old one. However, note that this would not affect vertices, edges, or properties previously written with the existing type. Redefining existing graph elements is not supported online and must be accomplished through a batch graph transformation. Be careful that if you change a property name that is part of some mixed index, you shall not reuse that property name in the same index. For example, the following code will fail. name = mgmt . makePropertyKey ( \"name\" ). dataType ( String . class ). make () mgmt . buildIndex ( \"nameIndex\" , Vertex . class ). addKey ( name ). buildMixedIndex ( \"search\" ) mgmt . commit () mgmt = graph . openManagement () mgmt . changeName ( mgmt . getPropertyKey ( \"name\" ), \"oldName\" ); name = mgmt . makePropertyKey ( \"name\" ). dataType ( String . class ). make () mgmt . addIndexKey ( mgmt . getGraphIndex ( \"nameIndex\" ), name ) mgmt . commit () // the following query will throw an exception graph . traversal (). V (). has ( \"name\" , textContains ( \"value\" )). hasNext () The reason is, JanusGraph does not attempt to alter the field name stored in the mixed index backend when you call mgmt.changeName . Instead, JanusGraph maintains a dual mapping between property name and index field name. If you create a new index using the old name and add it to the same index, conflicts will occur because JanusGraph cannot figure out which property should the index field map to. Schema Constraints The definition of the schema allows users to configure explicit property and connection constraints. Properties can be bound to specific vertex label and/or edge labels. Moreover, connection constraints allow users to explicitly define which two vertex labels can be connected by an edge label. These constraints can be used to ensure that a graph matches a given domain model. For example for the graph of the gods, a god can be a brother of another god , but not of a monster and a god can have a property age , but location can not have a property age . These constraints are disabled by default. Enable these schema constraints by setting schema.constraints=true . This setting depends on the setting schema.default . If config schema.default is set to none , then an IllegalArgumentException is thrown for schema constraint violations. If schema.default is not set none , schema constraints are automatically created, but no exception is thrown. Activating schema constraints has no impact on the existing data, because these schema constraints are only applied during the insertion process. So reading of data is not affected at all by those constraints. Multiple properties can be bound to a vertex using JanusGraphManagement.addProperties(VertexLabel, PropertyKey...) , for example: mgmt = graph . openManagement () person = mgmt . makeVertexLabel ( ' person ' ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () birthDate = mgmt . makePropertyKey ( ' birthDate ' ). dataType ( Long . class ). cardinality ( Cardinality . SINGLE ). make () mgmt . addProperties ( person , name , birthDate ) mgmt . commit () Multiple properties can be bound to an edge using JanusGraphManagement.addProperties(EdgeLabel, PropertyKey...) , for example: mgmt = graph . openManagement () follow = mgmt . makeEdgeLabel ( ' follow ' ). multiplicity ( MULTI ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () mgmt . addProperties ( follow , name ) mgmt . commit () Connections can be defined using JanusGraphManagement.addConnection(EdgeLabel, VertexLabel out, VertexLabel in) between an outgoing, an incoming and an edge, for example: mgmt = graph . openManagement () person = mgmt . makeVertexLabel ( ' person ' ). make () company = mgmt . makeVertexLabel ( ' company ' ). make () works = mgmt . makeEdgeLabel ( ' works ' ). multiplicity ( MULTI ). make () mgmt . addConnection ( works , person , company ) mgmt . commit ()","title":"Schema and Data Modeling"},{"location":"schema/#schema-and-data-modeling","text":"Each JanusGraph graph has a schema comprised of the edge labels, property keys, and vertex labels used therein. A JanusGraph schema can either be explicitly or implicitly defined. Users are encouraged to explicitly define the graph schema during application development. An explicitly defined schema is an important component of a robust graph application and greatly improves collaborative software development. Note, that a JanusGraph schema can be evolved over time without any interruption of normal database operations. Extending the schema does not slow down query answering and does not require database downtime. The schema type - i.e. edge label, property key, or vertex label - is assigned to elements in the graph - i.e. edge, properties or vertices respectively - when they are first created. The assigned schema type cannot be changed for a particular element. This ensures a stable type system that is easy to reason about. Beyond the schema definition options explained in this section, schema types provide performance tuning options that are discussed in Advanced Schema .","title":"Schema and Data Modeling"},{"location":"schema/#displaying-schema-information","text":"There are methods to view specific elements of the graph schema within the management API. These methods are mgmt.printIndexes() , mgmt.printPropertyKeys() , mgmt.printVertexLabels() , and mgmt.printEdgeLabels() . There is also a method that displays all the combined output named printSchema() . mgmt = graph . openManagement () mgmt . printSchema ()","title":"Displaying Schema Information"},{"location":"schema/#defining-edge-labels","text":"Each edge connecting two vertices has a label which defines the semantics of the relationship. For instance, an edge labeled friend between vertices A and B encodes a friendship between the two individuals. To define an edge label, call makeEdgeLabel(String) on an open graph or management transaction and provide the name of the edge label as the argument. Edge label names must be unique in the graph. This method returns a builder for edge labels that allows to define its multiplicity. The multiplicity of an edge label defines a multiplicity constraint on all edges of this label, that is, a maximum number of edges between pairs of vertices. JanusGraph recognizes the following multiplicity settings.","title":"Defining Edge Labels"},{"location":"schema/#edge-label-multiplicity","text":"MULTI : Allows multiple edges of the same label between any pair of vertices. In other words, the graph is a multi graph with respect to such edge label. There is no constraint on edge multiplicity. SIMPLE : Allows at most one edge of such label between any pair of vertices. In other words, the graph is a simple graph with respect to the label. Ensures that edges are unique for a given label and pairs of vertices. MANY2ONE : Allows at most one outgoing edge of such label on any vertex in the graph but places no constraint on incoming edges. The edge label mother is an example with MANY2ONE multiplicity since each person has at most one mother but mothers can have multiple children. ONE2MANY : Allows at most one incoming edge of such label on any vertex in the graph but places no constraint on outgoing edges. The edge label winnerOf is an example with ONE2MANY multiplicity since each contest is won by at most one person but a person can win multiple contests. ONE2ONE : Allows at most one incoming and one outgoing edge of such label on any vertex in the graph. The edge label marriedTo is an example with ONE2ONE multiplicity since a person is married to exactly one other person. The default multiplicity is MULTI. The definition of an edge label is completed by calling the make() method on the builder which returns the defined edge label as shown in the following example. mgmt = graph . openManagement () follow = mgmt . makeEdgeLabel ( 'follow' ). multiplicity ( MULTI ). make () mother = mgmt . makeEdgeLabel ( 'mother' ). multiplicity ( MANY2ONE ). make () mgmt . commit ()","title":"Edge Label Multiplicity"},{"location":"schema/#defining-property-keys","text":"Properties on vertices and edges are key-value pairs. For instance, the property name='Daniel' has the key name and the value 'Daniel' . Property keys are part of the JanusGraph schema and can constrain the allowed data types and cardinality of values. To define a property key, call makePropertyKey(String) on an open graph or management transaction and provide the name of the property key as the argument. Property key names must be unique in the graph, and it is recommended to avoid spaces or special characters in property names. This method returns a builder for the property keys. Note During property key creation, consider creating also graph indices for better performance, see Index Performance .","title":"Defining Property Keys"},{"location":"schema/#property-key-data-type","text":"Use dataType(Class) to define the data type of a property key. JanusGraph will enforce that all values associated with the key have the configured data type and thereby ensures that data added to the graph is valid. For instance, one can define that the name key has a String data type. Note that primitive types are not supported. Use the corresponding wrapper class, e.g. Integer instead of int . Define the data type as Object.class in order to allow any (serializable) value to be associated with a key. However, it is encouraged to use concrete data types whenever possible. Configured data types must be concrete classes and not interfaces or abstract classes. JanusGraph enforces class equality, so adding a sub-class of a configured data type is not allowed. JanusGraph natively supports the following data types. Native JanusGraph Data Types Name Description String Character sequence Character Individual character Boolean true or false Byte byte value Short short value Integer integer value Long long value Float 4 byte floating point number Double 8 byte floating point number Date Specific instant in time ( java.util.Date ) Geoshape Geographic shape like point, circle or box UUID Universally unique identifier ( java.util.UUID )","title":"Property Key Data Type"},{"location":"schema/#property-key-cardinality","text":"Use cardinality(Cardinality) to define the allowed cardinality of the values associated with the key on any given vertex. SINGLE : Allows at most one value per element for such key. In other words, the key\u2192value mapping is unique for all elements in the graph. The property key birthDate is an example with SINGLE cardinality since each person has exactly one birth date. LIST : Allows an arbitrary number of values per element for such key. In other words, the key is associated with a list of values allowing duplicate values. Assuming we model sensors as vertices in a graph, the property key sensorReading is an example with LIST cardinality to allow lots of (potentially duplicate) sensor readings to be recorded. SET : Allows multiple values but no duplicate values per element for such key. In other words, the key is associated with a set of values. The property key name has SET cardinality if we want to capture all names of an individual (including nick name, maiden name, etc). The default cardinality setting is SINGLE. Note, that property keys used on edges and properties have cardinality SINGLE. Attaching multiple values for a single key on an edge or property is not supported. mgmt = graph . openManagement () birthDate = mgmt . makePropertyKey ( ' birthDate ' ). dataType ( Long . class ). cardinality ( Cardinality . SINGLE ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () sensorReading = mgmt . makePropertyKey ( ' sensorReading ' ). dataType ( Double . class ). cardinality ( Cardinality . LIST ). make () mgmt . commit ()","title":"Property Key Cardinality"},{"location":"schema/#relation-types","text":"Edge labels and property keys are jointly referred to as relation types . Names of relation types must be unique in the graph which means that property keys and edge labels cannot have the same name. There are methods in the JanusGraph API to query for the existence or retrieve relation types which encompasses both property keys and edge labels. mgmt = graph . openManagement () if ( mgmt . containsRelationType ( ' name ' )) name = mgmt . getPropertyKey ( ' name ' ) mgmt . getRelationTypes ( EdgeLabel . class ) mgmt . commit ()","title":"Relation Types"},{"location":"schema/#defining-vertex-labels","text":"Like edges, vertices have labels. Unlike edge labels, vertex labels are optional. Vertex labels are useful to distinguish different types of vertices, e.g. user vertices and product vertices. Although labels are optional at the conceptual and data model level, JanusGraph assigns all vertices a label as an internal implementation detail. Vertices created by the addVertex methods use JanusGraph\u2019s default label. To create a label, call makeVertexLabel(String).make() on an open graph or management transaction and provide the name of the vertex label as the argument. Vertex label names must be unique in the graph. mgmt = graph . openManagement () person = mgmt . makeVertexLabel ( ' person ' ). make () mgmt . commit () // Create a labeled vertex person = graph . addVertex ( label , ' person ' ) // Create an unlabeled vertex v = graph . addVertex () graph . tx (). commit ()","title":"Defining Vertex Labels"},{"location":"schema/#automatic-schema-maker","text":"If an edge label, property key, or vertex label has not been defined explicitly, it will be defined implicitly when it is first used during the addition of an edge, vertex or the setting of a property. The DefaultSchemaMaker configured for the JanusGraph graph defines such types. By default, implicitly created edge labels have multiplicity MULTI and implicitly created property keys have cardinality SINGLE. Data types of implicitly created property keys are inferred as long as they are natively supported by JanusGraph. Object.class is used if and only if the given value is not any of the Native JanusGraph Data Types . Users can control automatic schema element creation by implementing and registering their own DefaultSchemaMaker . When defining a cardinality for a vertex property which differs from SINGLE, the cardinality should be used for all values of the vertex property in the first query (i.e. the query which defines a new vertex property key). It is strongly encouraged to explicitly define all schema elements and to disable automatic schema creation by setting schema.default=none in the JanusGraph graph configuration.","title":"Automatic Schema Maker"},{"location":"schema/#changing-schema-elements","text":"The definition of an edge label, property key, or vertex label cannot be changed once its committed into the graph. However, the names of schema elements can be changed via JanusGraphManagement.changeName(JanusGraphSchemaElement, String) as shown in the following example where the property key place is renamed to location . mgmt = graph . openManagement () place = mgmt . getPropertyKey ( ' place ' ) mgmt . changeName ( place , ' location ' ) mgmt . commit () Note, that schema name changes may not be immediately visible in currently running transactions and other JanusGraph graph instances in the cluster. While schema name changes are announced to all JanusGraph instances through the storage backend, it may take a while for the schema changes to take effect and it may require a instance restart in the event of certain failure conditions - like network partitions - if they coincide with the rename. Hence, the user must ensure that either of the following holds: The renamed label or key is not currently in active use (i.e. written or read) and will not be in use until all JanusGraph instances are aware of the name change. Running transactions actively accommodate the brief intermediate period where either the old or new name is valid based on the specific JanusGraph instance and status of the name-change announcement. For instance, that could mean transactions query for both names simultaneously. Should the need arise to re-define an existing schema type, it is recommended to change the name of this type to a name that is not currently (and will never be) in use. After that, a new label or key can be defined with the original name, thereby effectively replacing the old one. However, note that this would not affect vertices, edges, or properties previously written with the existing type. Redefining existing graph elements is not supported online and must be accomplished through a batch graph transformation. Be careful that if you change a property name that is part of some mixed index, you shall not reuse that property name in the same index. For example, the following code will fail. name = mgmt . makePropertyKey ( \"name\" ). dataType ( String . class ). make () mgmt . buildIndex ( \"nameIndex\" , Vertex . class ). addKey ( name ). buildMixedIndex ( \"search\" ) mgmt . commit () mgmt = graph . openManagement () mgmt . changeName ( mgmt . getPropertyKey ( \"name\" ), \"oldName\" ); name = mgmt . makePropertyKey ( \"name\" ). dataType ( String . class ). make () mgmt . addIndexKey ( mgmt . getGraphIndex ( \"nameIndex\" ), name ) mgmt . commit () // the following query will throw an exception graph . traversal (). V (). has ( \"name\" , textContains ( \"value\" )). hasNext () The reason is, JanusGraph does not attempt to alter the field name stored in the mixed index backend when you call mgmt.changeName . Instead, JanusGraph maintains a dual mapping between property name and index field name. If you create a new index using the old name and add it to the same index, conflicts will occur because JanusGraph cannot figure out which property should the index field map to.","title":"Changing Schema Elements"},{"location":"schema/#schema-constraints","text":"The definition of the schema allows users to configure explicit property and connection constraints. Properties can be bound to specific vertex label and/or edge labels. Moreover, connection constraints allow users to explicitly define which two vertex labels can be connected by an edge label. These constraints can be used to ensure that a graph matches a given domain model. For example for the graph of the gods, a god can be a brother of another god , but not of a monster and a god can have a property age , but location can not have a property age . These constraints are disabled by default. Enable these schema constraints by setting schema.constraints=true . This setting depends on the setting schema.default . If config schema.default is set to none , then an IllegalArgumentException is thrown for schema constraint violations. If schema.default is not set none , schema constraints are automatically created, but no exception is thrown. Activating schema constraints has no impact on the existing data, because these schema constraints are only applied during the insertion process. So reading of data is not affected at all by those constraints. Multiple properties can be bound to a vertex using JanusGraphManagement.addProperties(VertexLabel, PropertyKey...) , for example: mgmt = graph . openManagement () person = mgmt . makeVertexLabel ( ' person ' ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () birthDate = mgmt . makePropertyKey ( ' birthDate ' ). dataType ( Long . class ). cardinality ( Cardinality . SINGLE ). make () mgmt . addProperties ( person , name , birthDate ) mgmt . commit () Multiple properties can be bound to an edge using JanusGraphManagement.addProperties(EdgeLabel, PropertyKey...) , for example: mgmt = graph . openManagement () follow = mgmt . makeEdgeLabel ( ' follow ' ). multiplicity ( MULTI ). make () name = mgmt . makePropertyKey ( ' name ' ). dataType ( String . class ). cardinality ( Cardinality . SET ). make () mgmt . addProperties ( follow , name ) mgmt . commit () Connections can be defined using JanusGraphManagement.addConnection(EdgeLabel, VertexLabel out, VertexLabel in) between an outgoing, an incoming and an edge, for example: mgmt = graph . openManagement () person = mgmt . makeVertexLabel ( ' person ' ). make () company = mgmt . makeVertexLabel ( ' company ' ). make () works = mgmt . makeEdgeLabel ( ' works ' ). multiplicity ( MULTI ). make () mgmt . addConnection ( works , person , company ) mgmt . commit ()","title":"Schema Constraints"},{"location":"schema/advschema/","text":"Advanced Schema This page describes some of the advanced schema definition options that JanusGraph provides. For general information on JanusGraph\u2019s schema and how to define it, refer to Schema and Data Modeling . Static Vertices Vertex labels can be defined as static which means that vertices with that label cannot be modified outside the transaction in which they were created. mgmt = graph . openManagement () tweet = mgmt . makeVertexLabel ( 'tweet' ). setStatic (). make () mgmt . commit () Static vertex labels are a method of controlling the data lifecycle and useful when loading data into the graph that should not be modified after its creation. Edge and Vertex TTL Edge and vertex labels can be configured with a time-to-live (TTL) . Edges and vertices with such labels will automatically be removed from the graph when the configured TTL has passed after their initial creation. TTL configuration is useful when loading a large amount of data into the graph that is only of temporary use. Defining a TTL removes the need for manual clean up and handles the removal very efficiently. For example, it would make sense to TTL event edges such as user-page visits when those are summarized after a certain period of time or simply no longer needed for analytics or operational query processing. The following storage backends support edge and vertex TTL. CQL compatible storage backends HBase BerkeleyDB - supports only hour-discrete TTL, thus the minimal TTL is one hour. Edge TTL Edge TTL is defined on a per-edge label basis, meaning that all edges of that label have the same time-to-live. Note that the backend must support cell level TTL. Currently only CQL, HBase and BerkeleyDB support this. mgmt = graph . openManagement () visits = mgmt . makeEdgeLabel ( 'visits' ). make () mgmt . setTTL ( visits , Duration . ofDays ( 7 )) mgmt . commit () Note, that modifying an edge resets the TTL for that edge. Also note, that the TTL of an edge label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label. Property TTL Property TTL is very similar to edge TTL and defined on a per-property key basis, meaning that all properties of that key have the same time-to-live. Note that the backend must support cell level TTL. Currently only CQL, HBase and BerkeleyDB support this. mgmt = graph . openManagement () sensor = mgmt . makePropertyKey ( 'sensor' ). cardinality ( Cardinality . LIST ). dataType ( Double . class ). make () mgmt . setTTL ( sensor , Duration . ofDays ( 21 )) mgmt . commit () As with edge TTL, modifying an existing property resets the TTL for that property and modifying the TTL for a property key might not immediately take effect. Vertex TTL Vertex TTL is defined on a per-vertex label basis, meaning that all vertices of that label have the same time-to-live. The configured TTL applies to the vertex, its properties, and all incident edges to ensure that the entire vertex is removed from the graph. For this reason, a vertex label must be defined as static before a TTL can be set to rule out any modifications that would invalidate the vertex TTL. Vertex TTL only applies to static vertex labels. Note that the backend must support store level TTL. Currently only CQL, HBase and BerkeleyDB support this. mgmt = graph . openManagement () tweet = mgmt . makeVertexLabel ( 'tweet' ). setStatic (). make () mgmt . setTTL ( tweet , Duration . ofHours ( 36 )) mgmt . commit () Note, that the TTL of a vertex label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label. Multi-Properties As discussed in Schema and Data Modeling , JanusGraph supports property keys with SET and LIST cardinality. Hence, JanusGraph supports multiple properties with the same key on a single vertex. Furthermore, JanusGraph treats properties similarly to edges in that single-valued property annotations are allowed on properties as shown in the following example. mgmt = graph . openManagement () mgmt . makePropertyKey ( 'name' ). dataType ( String . class ). cardinality ( Cardinality . LIST ). make () mgmt . commit () v = graph . addVertex () p1 = v . property ( 'name' , 'Dan LaRocque' ) p1 . property ( 'source' , 'web' ) p2 = v . property ( 'name' , 'dalaro' ) p2 . property ( 'source' , 'github' ) graph . tx (). commit () v . properties ( 'name' ) ==> Iterable over all name properties These features are useful in a number of applications such as those where attaching provenance information (e.g. who added a property, when and from where?) to properties is necessary. Support for higher cardinality properties and property annotations on properties is also useful in high-concurrency, scale-out design patterns as described in Eventually-Consistent Storage Backends . Vertex-centric indexes and global graph indexes are supported for properties in the same manner as they are supported for edges. Refer to Indexing for Better Performance for information on defining these indexes for edges and use the corresponding API methods to define the same indexes for properties. Unidirected Edges Unidirected edges are edges that can only be traversed in the out-going direction. Unidirected edges have a lower storage footprint but are limited in the types of traversals they support. Unidirected edges are conceptually similar to hyperlinks in the world-wide-web in the sense that the out-vertex can traverse through the edge, but the in-vertex is unaware of its existence. mgmt = graph . openManagement () mgmt . makeEdgeLabel ( 'author' ). unidirected (). make () mgmt . commit () Note, that unidirected edges do not get automatically deleted when their in-vertices are deleted. The user must ensure that such inconsistencies do not arise or resolve them at query time by explicitly checking vertex existence in a transaction. See the discussion in Ghost Vertices for more information.","title":"Advanced Schema"},{"location":"schema/advschema/#advanced-schema","text":"This page describes some of the advanced schema definition options that JanusGraph provides. For general information on JanusGraph\u2019s schema and how to define it, refer to Schema and Data Modeling .","title":"Advanced Schema"},{"location":"schema/advschema/#static-vertices","text":"Vertex labels can be defined as static which means that vertices with that label cannot be modified outside the transaction in which they were created. mgmt = graph . openManagement () tweet = mgmt . makeVertexLabel ( 'tweet' ). setStatic (). make () mgmt . commit () Static vertex labels are a method of controlling the data lifecycle and useful when loading data into the graph that should not be modified after its creation.","title":"Static Vertices"},{"location":"schema/advschema/#edge-and-vertex-ttl","text":"Edge and vertex labels can be configured with a time-to-live (TTL) . Edges and vertices with such labels will automatically be removed from the graph when the configured TTL has passed after their initial creation. TTL configuration is useful when loading a large amount of data into the graph that is only of temporary use. Defining a TTL removes the need for manual clean up and handles the removal very efficiently. For example, it would make sense to TTL event edges such as user-page visits when those are summarized after a certain period of time or simply no longer needed for analytics or operational query processing. The following storage backends support edge and vertex TTL. CQL compatible storage backends HBase BerkeleyDB - supports only hour-discrete TTL, thus the minimal TTL is one hour.","title":"Edge and Vertex TTL"},{"location":"schema/advschema/#edge-ttl","text":"Edge TTL is defined on a per-edge label basis, meaning that all edges of that label have the same time-to-live. Note that the backend must support cell level TTL. Currently only CQL, HBase and BerkeleyDB support this. mgmt = graph . openManagement () visits = mgmt . makeEdgeLabel ( 'visits' ). make () mgmt . setTTL ( visits , Duration . ofDays ( 7 )) mgmt . commit () Note, that modifying an edge resets the TTL for that edge. Also note, that the TTL of an edge label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label.","title":"Edge TTL"},{"location":"schema/advschema/#property-ttl","text":"Property TTL is very similar to edge TTL and defined on a per-property key basis, meaning that all properties of that key have the same time-to-live. Note that the backend must support cell level TTL. Currently only CQL, HBase and BerkeleyDB support this. mgmt = graph . openManagement () sensor = mgmt . makePropertyKey ( 'sensor' ). cardinality ( Cardinality . LIST ). dataType ( Double . class ). make () mgmt . setTTL ( sensor , Duration . ofDays ( 21 )) mgmt . commit () As with edge TTL, modifying an existing property resets the TTL for that property and modifying the TTL for a property key might not immediately take effect.","title":"Property TTL"},{"location":"schema/advschema/#vertex-ttl","text":"Vertex TTL is defined on a per-vertex label basis, meaning that all vertices of that label have the same time-to-live. The configured TTL applies to the vertex, its properties, and all incident edges to ensure that the entire vertex is removed from the graph. For this reason, a vertex label must be defined as static before a TTL can be set to rule out any modifications that would invalidate the vertex TTL. Vertex TTL only applies to static vertex labels. Note that the backend must support store level TTL. Currently only CQL, HBase and BerkeleyDB support this. mgmt = graph . openManagement () tweet = mgmt . makeVertexLabel ( 'tweet' ). setStatic (). make () mgmt . setTTL ( tweet , Duration . ofHours ( 36 )) mgmt . commit () Note, that the TTL of a vertex label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label.","title":"Vertex TTL"},{"location":"schema/advschema/#multi-properties","text":"As discussed in Schema and Data Modeling , JanusGraph supports property keys with SET and LIST cardinality. Hence, JanusGraph supports multiple properties with the same key on a single vertex. Furthermore, JanusGraph treats properties similarly to edges in that single-valued property annotations are allowed on properties as shown in the following example. mgmt = graph . openManagement () mgmt . makePropertyKey ( 'name' ). dataType ( String . class ). cardinality ( Cardinality . LIST ). make () mgmt . commit () v = graph . addVertex () p1 = v . property ( 'name' , 'Dan LaRocque' ) p1 . property ( 'source' , 'web' ) p2 = v . property ( 'name' , 'dalaro' ) p2 . property ( 'source' , 'github' ) graph . tx (). commit () v . properties ( 'name' ) ==> Iterable over all name properties These features are useful in a number of applications such as those where attaching provenance information (e.g. who added a property, when and from where?) to properties is necessary. Support for higher cardinality properties and property annotations on properties is also useful in high-concurrency, scale-out design patterns as described in Eventually-Consistent Storage Backends . Vertex-centric indexes and global graph indexes are supported for properties in the same manner as they are supported for edges. Refer to Indexing for Better Performance for information on defining these indexes for edges and use the corresponding API methods to define the same indexes for properties.","title":"Multi-Properties"},{"location":"schema/advschema/#unidirected-edges","text":"Unidirected edges are edges that can only be traversed in the out-going direction. Unidirected edges have a lower storage footprint but are limited in the types of traversals they support. Unidirected edges are conceptually similar to hyperlinks in the world-wide-web in the sense that the out-vertex can traverse through the edge, but the in-vertex is unaware of its existence. mgmt = graph . openManagement () mgmt . makeEdgeLabel ( 'author' ). unidirected (). make () mgmt . commit () Note, that unidirected edges do not get automatically deleted when their in-vertices are deleted. The user must ensure that such inconsistencies do not arise or resolve them at query time by explicitly checking vertex existence in a transaction. See the discussion in Ghost Vertices for more information.","title":"Unidirected Edges"},{"location":"schema/index-management/index-lifecycle/","text":"Index Lifecycle JanusGraph uses only indexes which have status ENABLED . When the index is created it will not be used by JanusGraph until it is enabled. After the index is build you should wait until it is registered (i.e. available) by JanusGraph: //Wait for the index to become available (i.e. wait for status REGISTERED) ManagementSystem . awaitGraphIndexStatus ( graph , \"myIndex\" ). call (); After the index is registered we should either enable the index (if we are sure that the current data should not be indexed by the newly created index) or we should reindex current data so that it would be available in the newly created index. Reindex the existing data and automatically enable the index example: mgmt = graph . openManagement (); mgmt . updateIndex ( mgmt . getGraphIndex ( \"myIndex\" ), SchemaAction . REINDEX ). get (); mgmt . commit (); Enable the index without reindexing existing data example: mgmt = graph . openManagement (); mgmt . updateIndex ( mgmt . getGraphIndex ( \"myAnotherIndex\" ), SchemaAction . ENABLE_INDEX ). get (); mgmt . commit (); Index states and transitions The diagram below shows the possible states of an index in JanusGraph. Transitions which should be used with caution are shown in grey. These are either not recommended to be used in production scenarios or have more complex implications to be aware of. States (SchemaStatus) An index can be in one of the following states: INSTALLED The index has been created on the current JanusGraph instance, but it is not yet known to all instances in the cluster. This state is not shown in the diagram above because it is not stable in terms of synchronization between instances. REGISTERED The index is known to all instances in the cluster. However, it is not in use yet. In this state, the index contains no data and will not be used in queries. ENABLED The index is enabled and can be used to answer queries. Furthermore, this is the only state in which insertions and deletions in the graph are forwarded to the index. DISABLED The index is disabled and will not be used to answer queries. It also does no longer receive graph updates. Therefore, it is recommended to use REINDEX to re-enable the index. DISCARDED The index has been decommissioned and its contents have been deleted. The only remaining artifact is the schema vertex representing the index. Actions (SchemaAction) The following actions can be performed on an index to change its state via mgmt.updateIndex() : REGISTER_INDEX Registers the index with all instances in the cluster. After an index is installed, it must be registered with all JanusGraph instances. REINDEX Re-builds the index from the ground up using the data stored in the graph. Depending on the size of the graph, this action can take a long time to complete. Reindexing is possible in all stable states and automatically enables the index once finished. ENABLE_INDEX Enables the index so that it can be used by the query processing engine. An index must be registered before it can be enabled. If enabling the index manually instead of performing a reindex, be aware that past modifications of the graph are not represented in the index. Enabling a previously disabled index without REINDEX may cause index queries to return ghost vertices, which have been deleted from the graph while the index was disabled. DISABLE_INDEX Disables the index temporarily so that it is no longer used to answer queries. The index also stops receiving updates in the graph. DISCARD_INDEX Removes all data from the index, preparing it for removal. This operation is supported for all composite indices and for some mixed index backends. For more information on index removal see Removal . MARK_DISCARDED All indices which can not be discarded using DISCARD_INDEX have to be discarded manually in the backend. As this is not recognized by JanusGraph automatically, the state can be set to DISCARDED manually via this action. DROP_INDEX Removes the index from the schema and communicates the change to other instances in the cluster. After an index has been dropped, a new index is allowed to use the same name again.","title":"Index Lifecycle"},{"location":"schema/index-management/index-lifecycle/#index-lifecycle","text":"JanusGraph uses only indexes which have status ENABLED . When the index is created it will not be used by JanusGraph until it is enabled. After the index is build you should wait until it is registered (i.e. available) by JanusGraph: //Wait for the index to become available (i.e. wait for status REGISTERED) ManagementSystem . awaitGraphIndexStatus ( graph , \"myIndex\" ). call (); After the index is registered we should either enable the index (if we are sure that the current data should not be indexed by the newly created index) or we should reindex current data so that it would be available in the newly created index. Reindex the existing data and automatically enable the index example: mgmt = graph . openManagement (); mgmt . updateIndex ( mgmt . getGraphIndex ( \"myIndex\" ), SchemaAction . REINDEX ). get (); mgmt . commit (); Enable the index without reindexing existing data example: mgmt = graph . openManagement (); mgmt . updateIndex ( mgmt . getGraphIndex ( \"myAnotherIndex\" ), SchemaAction . ENABLE_INDEX ). get (); mgmt . commit ();","title":"Index Lifecycle"},{"location":"schema/index-management/index-lifecycle/#index-states-and-transitions","text":"The diagram below shows the possible states of an index in JanusGraph. Transitions which should be used with caution are shown in grey. These are either not recommended to be used in production scenarios or have more complex implications to be aware of.","title":"Index states and transitions"},{"location":"schema/index-management/index-lifecycle/#states-schemastatus","text":"An index can be in one of the following states: INSTALLED The index has been created on the current JanusGraph instance, but it is not yet known to all instances in the cluster. This state is not shown in the diagram above because it is not stable in terms of synchronization between instances. REGISTERED The index is known to all instances in the cluster. However, it is not in use yet. In this state, the index contains no data and will not be used in queries. ENABLED The index is enabled and can be used to answer queries. Furthermore, this is the only state in which insertions and deletions in the graph are forwarded to the index. DISABLED The index is disabled and will not be used to answer queries. It also does no longer receive graph updates. Therefore, it is recommended to use REINDEX to re-enable the index. DISCARDED The index has been decommissioned and its contents have been deleted. The only remaining artifact is the schema vertex representing the index.","title":"States (SchemaStatus)"},{"location":"schema/index-management/index-lifecycle/#actions-schemaaction","text":"The following actions can be performed on an index to change its state via mgmt.updateIndex() : REGISTER_INDEX Registers the index with all instances in the cluster. After an index is installed, it must be registered with all JanusGraph instances. REINDEX Re-builds the index from the ground up using the data stored in the graph. Depending on the size of the graph, this action can take a long time to complete. Reindexing is possible in all stable states and automatically enables the index once finished. ENABLE_INDEX Enables the index so that it can be used by the query processing engine. An index must be registered before it can be enabled. If enabling the index manually instead of performing a reindex, be aware that past modifications of the graph are not represented in the index. Enabling a previously disabled index without REINDEX may cause index queries to return ghost vertices, which have been deleted from the graph while the index was disabled. DISABLE_INDEX Disables the index temporarily so that it is no longer used to answer queries. The index also stops receiving updates in the graph. DISCARD_INDEX Removes all data from the index, preparing it for removal. This operation is supported for all composite indices and for some mixed index backends. For more information on index removal see Removal . MARK_DISCARDED All indices which can not be discarded using DISCARD_INDEX have to be discarded manually in the backend. As this is not recognized by JanusGraph automatically, the state can be set to DISCARDED manually via this action. DROP_INDEX Removes the index from the schema and communicates the change to other instances in the cluster. After an index has been dropped, a new index is allowed to use the same name again.","title":"Actions (SchemaAction)"},{"location":"schema/index-management/index-performance/","text":"Indexing for Better Performance JanusGraph supports two different kinds of indexing to speed up query processing: graph indexes and vertex-centric indexes (a.k.a. relation indexes) . Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Graph indexes make these global retrieval operations efficient on large graphs. Vertex-centric indexes speed up the actual traversal through the graph, in particular when traversing through vertices with many incident edges. Graph Index Graph indexes are global index structures over the entire graph which allow efficient retrieval of vertices or edges by their properties for sufficiently selective conditions. For instance, consider the following queries g . V (). has ( ' name ' , ' hercules ' ) g . E (). has ( ' reason ' , textContains ( ' loves ' )) The first query asks for all vertices with the name hercules . The second asks for all edges where the property reason contains the word loves . Without a graph index answering those queries would require a full scan over all vertices or edges in the graph to find those that match the given condition which is very inefficient and infeasible for huge graphs. JanusGraph distinguishes between two types of graph indexes: composite and mixed indexes. Composite indexes are very fast and efficient but limited to equality lookups for a particular, previously-defined combination of property keys. Mixed indexes can be used for lookups on any combination of indexed keys and support multiple condition predicates in addition to equality depending on the backing index store. Both types of indexes are created through the JanusGraph management system and the index builder returned by JanusGraphManagement.buildIndex(String, Class) where the first argument defines the name of the index and the second argument specifies the type of element to be indexed (e.g. Vertex.class ). The name of a graph index must be unique. Graph indexes built against newly defined property keys, i.e. property keys that are defined in the same management transaction as the index, are immediately available. The same applies to graph indexes that are constrained to a label that is created in the same management transaction as the index. Graph indexes built against property keys that are already in use without being constrained to a newly created label require the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the index will not be available. It is encouraged to define graph indexes in the same transaction as the initial schema. Note In the absence of an index, JanusGraph will default to a full graph scan in order to retrieve the desired list of vertices. While this produces the correct result set, the graph scan can be very inefficient and lead to poor overall system performance in a production environment. Enable the force-index configuration option in production deployments of JanusGraph to prohibit graph scans. Info See index lifecycle documentation for more information about index states. Composite Index Composite indexes are stored in a separate store called graphIndex . For example, if your storage backend is Cassandra, you will see a table named graphIndex under your namespace. Composite indexes retrieve vertices or edges by one or a (fixed) composition of multiple keys. Consider the following composite index definitions. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( ' name ' ) age = mgmt . getPropertyKey ( ' age ' ) mgmt . buildIndex ( ' byNameComposite ' , Vertex . class ). addKey ( name ). buildCompositeIndex () mgmt . buildIndex ( ' byNameAndAgeComposite ' , Vertex . class ). addKey ( name ). addKey ( age ). buildCompositeIndex () mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , ' byNameComposite ' ). call () ManagementSystem . awaitGraphIndexStatus ( graph , ' byNameAndAgeComposite ' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameComposite\" ), SchemaAction . REINDEX ). get () mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameAndAgeComposite\" ), SchemaAction . REINDEX ). get () mgmt . commit () First, two property keys name and age are already defined. Next, a simple composite index on just the name property key is built. JanusGraph will use this index to answer the following query. g . V (). has ( 'name' , 'hercules' ) The second composite graph index includes both keys. JanusGraph will use this index to answer the following query. g . V (). has ( 'age' , 30 ). has ( 'name' , 'hercules' ) Note, that all keys of a composite graph index must be found in the query\u2019s equality conditions for this index to be used. For example, the following query cannot be answered with either of the indexes because it only contains a constraint on age but not name . g . V (). has ( 'age' , 30 ) Also note, that composite graph indexes can only be used for equality constraints like those in the queries above. The following query would be answered with just the simple composite index defined on the name key because the age constraint is not an equality constraint. g . V (). has ( ' name ' , ' hercules ' ). has ( ' age ' , inside ( 20 , 50 )) Composite indexes do not require configuration of an external indexing backend and are supported through the primary storage backend. Hence, composite index modifications are persisted through the same transaction as graph modifications which means that those changes are atomic and/or consistent if the underlying storage backend supports atomicity and/or consistency. Note A composite index may comprise just one or multiple keys. A composite index with just one key is sometimes referred to as a key-index. Index Uniqueness Composite indexes can also be used to enforce property uniqueness in the graph. If a composite graph index is defined as unique() there can be at most one vertex or edge for any given concatenation of property values associated with the keys of that index. For instance, to enforce that names are unique across the entire graph the following composite graph index would be defined. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( 'name' ) mgmt . buildIndex ( 'byNameUnique' , Vertex . class ). addKey ( name ). unique (). buildCompositeIndex () mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , 'byNameUnique' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameUnique\" ), SchemaAction . REINDEX ). get () mgmt . commit () Note To enforce uniqueness against an eventually consistent storage backend, the consistency of the index must be explicitly set to enabling locking. Mixed Index Mixed indexes retrieve vertices or edges by any combination of previously added property keys. Mixed indexes provide more flexibility than composite indexes and support additional condition predicates beyond equality. On the other hand, mixed indexes are slower for most equality queries than composite indexes. Unlike composite indexes, mixed indexes require the configuration of an indexing backend and use that indexing backend to execute lookup operations. JanusGraph can support multiple indexing backends in a single installation. Each indexing backend must be uniquely identified by name in the JanusGraph configuration which is called the indexing backend name . graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( 'name' ) age = mgmt . getPropertyKey ( 'age' ) mgmt . buildIndex ( 'nameAndAge' , Vertex . class ). addKey ( name ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , 'nameAndAge' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"nameAndAge\" ), SchemaAction . REINDEX ). get () mgmt . commit () The example above defines a mixed index containing the property keys name and age . The definition refers to the indexing backend name search so that JanusGraph knows which configured indexing backend it should use for this particular index. The search parameter specified in the buildMixedIndex call must match the second clause in the JanusGraph configuration definition like this: index. search .backend If the index was named solrsearch then the configuration definition would appear like this: index. solrsearch .backend. The mgmt.buildIndex example specified above uses text search as its default behavior. An index statement that explicitly defines the index as a text index can be written as follows: mgmt . buildIndex ( 'nameAndAge' , Vertex . class ). addKey ( name , Mapping . TEXT . asParameter ()). addKey ( age , Mapping . TEXT . asParameter ()). buildMixedIndex ( \"search\" ) See Index Parameters and Full-Text Search for more information on text and string search options, and see the documentation section specific to the indexing backend in use for more details on how each backend handles text versus string searches. While the index definition example looks similar to the composite index above, it provides greater query support and can answer any of the following queries. g . V (). has ( 'name' , textContains ( 'hercules' )). has ( 'age' , inside ( 20 , 50 )) g . V (). has ( 'name' , textContains ( 'hercules' )) g . V (). has ( 'age' , lt ( 50 )) g . V (). has ( 'age' , outside ( 20 , 50 )) g . V (). has ( 'age' , lt ( 50 ). or ( gte ( 60 ))) g . V (). or ( __ . has ( 'name' , textContains ( 'hercules' )), __ . has ( 'age' , inside ( 20 , 50 ))) Mixed indexes support full-text search, range search, geo search and others. Refer to Search Predicates and Data Types for a list of predicates supported by a particular indexing backend. Note Unlike composite indexes, mixed indexes do not support uniqueness. Adding Property Keys Property keys can be added to an existing mixed index which allows subsequent queries to include this key in the query condition. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () location = mgmt . makePropertyKey ( 'location' ). dataType ( Geoshape . class ). make () nameAndAge = mgmt . getGraphIndex ( 'nameAndAge' ) mgmt . addIndexKey ( nameAndAge , location ) mgmt . commit () //Previously created property keys already have the status ENABLED, but //our newly created property key \"location\" needs to REGISTER so we wait for both statuses ManagementSystem . awaitGraphIndexStatus ( graph , 'nameAndAge' ). status ( SchemaStatus . REGISTERED , SchemaStatus . ENABLED ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"nameAndAge\" ), SchemaAction . REINDEX ). get () mgmt . commit () To add a newly defined key, we first retrieve the existing index from the management transaction by its name and then invoke the addIndexKey method to add the key to this index. If the added key is defined in the same management transaction, it will be immediately available for querying. If the property key has already been in use, adding the key requires the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the key will not be available in the mixed index. Mapping Parameters When adding a property key to a mixed index - either through the index builder or the addIndexKey method - a list of parameters can be optionally specified to adjust how the property value is mapped into the indexing backend. Refer to the mapping parameters overview for a complete list of parameter types supported by each indexing backend. Ordering The order in which the results of a graph query are returned can be defined using the order().by() directive. The order().by() method expects two parameters: The name of the property key by which to order the results. The results will be ordered by the value of the vertices or edges for this property key. The sort order: either ascending asc or descending desc For example, the query g.V().has('name', textContains('hercules')).order().by('age', desc).limit(10) retrieves the ten oldest individuals with hercules in their name. When using order().by() it is important to note that: Composite graph indexes do not natively support ordering search results. All results will be retrieved and then sorted in-memory. For large result sets, this can be very expensive. Mixed indexes support ordering natively and efficiently. However, the property key used in the order().by() method must have been previously added to the mixed indexed for native result ordering support. This is important in cases where the the order().by() key is different from the query keys. If the property key is not part of the index, then sorting requires loading all results into memory. Label Constraint In many cases it is desirable to only index vertices or edges with a particular label. For instance, one may want to index only gods by their name and not every single vertex that has a name property. When defining an index it is possible to restrict the index to a particular vertex or edge label using the indexOnly method of the index builder. The following creates a composite index for the property key name that indexes only vertices labeled god . graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( 'name' ) god = mgmt . getVertexLabel ( 'god' ) mgmt . buildIndex ( 'byNameAndLabel' , Vertex . class ). addKey ( name ). indexOnly ( god ). buildCompositeIndex () mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , 'byNameAndLabel' ). call () //You can check the indexOnly constraint built just now mgmt = graph . openManagement () mgmt . getIndexOnlyConstraint ( \"byNameAndLabel\" ) //Reindex the existing data mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameAndLabel\" ), SchemaAction . REINDEX ). get () mgmt . commit () Label restrictions similarly apply to mixed indexes. When a composite index with label restriction is defined as unique, the uniqueness constraint only applies to properties on vertices or edges for the specified label. Composite versus Mixed Indexes Use a composite index for exact match index retrievals. Composite indexes do not require configuring or operating an external index system and are often significantly faster than mixed indexes. As an exception, use a mixed index for exact matches when the number of distinct values for query constraint is relatively small or if one value is expected to be associated with many elements in the graph (i.e. in case of low selectivity). Use a mixed indexes for numeric range, full-text or geo-spatial indexing. Also, using a mixed index can speed up the order().by() queries. A composite index requires all fields to be present, while a mixed index only needs at least one field to be present. For example, say you have a composite index with key1 and key2 , a mixed index with key1 and key3 . If you add a vertex with only property key1 , then JanusGraph will create a new mixed index entry but not a composite index entry. Vertex-centric Indexes Vertex-centric indexes, also known as Relation indexes, are local index structures built individually per vertex. They are stored together with edges and properties in edgeStore . There are two types of vertex-centric indexes, edge indexes and property indexes. Edge Indexes In large graphs vertices can have thousands of incident edges. Traversing through those vertices can be very slow because a large subset of the incident edges has to be retrieved and then filtered in memory to match the conditions of the traversal. Vertex-centric indexes can speed up such traversals by using localized index structures to retrieve only those edges that need to be traversed. Suppose that Hercules battled hundreds of monsters in addition to the three captured in the introductory Graph of the Gods . Without a vertex-centric index, a query asking for those monsters battled between time point 10 and 20 would require retrieving all battled edges even though there are only a handful of matching edges. h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). outE ( 'battled' ). has ( 'time' , inside ( 10 , 20 )). inV () Building a vertex-centric index by time speeds up such traversal queries. Note, this initial index example already exists in the Graph of the Gods as an index named edges . As a result, running the steps below will result in a uniqueness constraint error. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () time = mgmt . getPropertyKey ( 'time' ) battled = mgmt . getEdgeLabel ( 'battled' ) mgmt . buildEdgeIndex ( battled , 'battlesByTime' , Direction . BOTH , Order . desc , time ) mgmt . commit () //Wait for the index to become available ManagementSystem . awaitRelationIndexStatus ( graph , 'battlesByTime' , 'battled' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getRelationIndex ( battled , \"battlesByTime\" ), SchemaAction . REINDEX ). get () mgmt . commit () This example builds a vertex-centric index which indexes battled edges in both direction by time in descending order. A vertex-centric index is built against a particular edge label which is the first argument to the index construction method JanusGraphManagement.buildEdgeIndex() . The index only applies to edges of this label - battled in the example above. The second argument is a unique name for the index. The third argument is the edge direction in which the index is built. The index will only apply to traversals along edges in this direction. In this example, the vertex-centric index is built in both direction which means that time restricted traversals along battled edges can be served by this index in both the IN and OUT direction. JanusGraph will maintain a vertex-centric index on both the in- and out-vertex of battled edges. Alternatively, one could define the index to apply to the OUT direction only which would speed up traversals from Hercules to the monsters but not in the reverse direction. This would only require maintaining one index and hence half the index maintenance and storage cost. The last two arguments are the sort order of the index and a list of property keys to index by. The sort order is optional and defaults to ascending order (i.e. Order.ASC ). The list of property keys must be non-empty and defines the keys by which to index the edges of the given label. A vertex-centric index can be defined with multiple keys. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () time = mgmt . getPropertyKey ( 'time' ) rating = mgmt . makePropertyKey ( 'rating' ). dataType ( Double . class ). make () battled = mgmt . getEdgeLabel ( 'battled' ) mgmt . buildEdgeIndex ( battled , 'battlesByRatingAndTime' , Direction . OUT , Order . desc , rating , time ) mgmt . commit () //Wait for the index to become available ManagementSystem . awaitRelationIndexStatus ( graph , 'battlesByRatingAndTime' , 'battled' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getRelationIndex ( battled , 'battlesByRatingAndTime' ), SchemaAction . REINDEX ). get () mgmt . commit () This example extends the schema by a rating property on battled edges and builds a vertex-centric index which indexes battled edges in the out-going direction by rating and time in descending order. Note, that the order in which the property keys are specified is important because vertex-centric indexes are prefix indexes. This means, that battled edges are indexed by rating first and time second . h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). outE ( 'battled' ). property ( 'rating' , 5.0 ) //Add some rating properties g . V ( h ). outE ( 'battled' ). has ( 'rating' , gt ( 3.0 )). inV () g . V ( h ). outE ( 'battled' ). has ( 'rating' , 5.0 ). has ( 'time' , inside ( 10 , 50 )). inV () g . V ( h ). outE ( 'battled' ). has ( 'time' , inside ( 10 , 50 )). inV () Hence, the battlesByRatingAndTime index can speed up the first two but not the third query. Property Indexes Similar to edge indexes, property indexes can be built to traverse properties based on associated meta-properties efficiently. The following toy example illustrates the usage of property indexes. graph = JanusGraphFactory . open ( \"inmemory\" ) mgmt = graph . openManagement () timestamp = mgmt . makePropertyKey ( \"timestamp\" ). dataType ( Integer . class ). make () amount = mgmt . makePropertyKey ( \"amount\" ). dataType ( Integer . class ). cardinality ( Cardinality . LIST ). make () mgmt . buildPropertyIndex ( amount , 'amountByTime' , Order . desc , timestamp ) mgmt . commit () bob = graph . addVertex () bob . property ( \"amount\" , 100 , \"timestamp\" , 1600000000 ) bob . property ( \"amount\" , 200 , \"timestamp\" , 1500000000 ) bob . property ( \"amount\" , - 150 , \"timestamp\" , 1550000000 ) graph . tx (). commit () g = graph . traversal () g . V ( bob ). properties ( \"amount\" ). has ( \"timestamp\" , P . gt ( 1500000000 )) In the above example, we model a number of transactions Bob has made. amount is a vertex property that records the amount of money involved in each transaction, while timestamp is a meta property of amount which records the time that a particular record happens. By creating a property index, we can efficiently retrieve transaction amounts by timestamp. Alternatively, we can also model the transactions as edges, then both amount and timestamp will be edge properties, and we could use Edge indexes to speed up the query. Multiple vertex-centric indexes can be built for the same edge label in order to support different constraint traversals. JanusGraph\u2019s query optimizer attempts to pick the most efficient index for any given traversal. Vertex-centric indexes only support equality and range/interval constraints. Note The property keys used in a vertex-centric index must have an explicitly defined data type (i.e. not Object.class ) which supports a native sort order. This means not only that they must implement Comparable but that their serializer must implement OrderPreservingSerializer . The types that are currently supported are Boolean , UUID , Byte , Float , Long , String , Integer , Date , Double , Character , and Short If the vertex-centric index is built against either an edge label or at least one property key that is defined in the same management transaction, the index will be immediately available for querying. If both the edge label and all of the indexed property keys have already been in use, building a vertex-centric index against it requires the execution of a reindex procedure to ensure that the index contains all previously added edges. Until the reindex procedure has completed, the index will not be available. Note JanusGraph automatically builds vertex-centric indexes per edge label and property key. That means, even with thousands of incident battled edges, queries like g.V(h).out('mother') or g.V(h).values('age') are efficiently answered by the local index. Vertex-centric indexes cannot speed up unconstrained traversals which require traversing through all incident edges of a particular label. Those traversals will become slower as the number of incident edges increases. Often, such traversals can be rewritten as constrained traversals that can utilize a vertex-centric index to ensure acceptable performance at scale. Using vertex-centric indexes on adjacent vertex ids In some cases it is relevant to find an edge based on properties of the adjacent vertex. Let's say we want to find out whether or not Hercules has battled Cerberus. h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). out ( 'battled' ). has ( 'name' , 'cerberus' ). hasNext () A query like this can not use a vertex centric index because it filters on vertex properties rather than edge properties. But by restructuring the query, we can achieve exactly this. As both vertices are known, the vertex ids can be used to select the edge. h = g . V (). has ( 'name' , 'hercules' ). next () c = g . V (). has ( 'name' , 'cerberus' ). next () In contrast to the name \"Cebereus\", which is a property of the adjacent vertex, the id of this vertex is already saved within the connecting edge itself. Therefore, this query runs much faster if hercules has battled many opponents: g . V ( h ). outE ( 'battled' ). where ( inV (). is ( c )). hasNext () ... or even shorter: g . V ( h ). out ( 'battled' ). is ( c ). limit ( 1 ). hasNext () Assuming there is a global index on the name property, this improves the performance a lot, because it's not necessary to fetch every adjacent vertex anymore. In addition, a vertex-centric index on adjacent vertex ids does not need to be constructed and maintained explicitly. Due to the data model , efficient access to edges by their adjacent vertex id is already provided by the storage backend. Ordered Traversals The following queries specify an order in which the incident edges are to be traversed. Use the localLimit command to retrieve a subset of the edges (in a given order) for EACH vertex that is traversed. h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). local ( outE ( 'battled' ). order (). by ( 'time' , desc ). limit ( 10 )). inV (). values ( 'name' ) g . V ( h ). local ( outE ( 'battled' ). has ( 'rating' , 5.0 ). order (). by ( 'time' , desc ). limit ( 10 )). values ( 'place' ) The first query asks for the names of the 10 most recently battled monsters by Hercules. The second query asks for the places of the 10 most recent battles of Hercules that are rated 5 stars. In both cases, the query is constrained by an order on a property key with a limit on the number of elements to be returned. Such queries can also be efficiently answered by vertex-centric indexes if the order key matches the key of the index and the requested order (i.e. ascending or descending) is the same as the one defined for the index. The battlesByTime index would be used to answer the first query and battlesByRatingAndTime applies to the second. Note, that the battlesByRatingAndTime index cannot be used to answer the first query because an equality constraint on rating must be present for the second key in the index to be effective.","title":"Indexing for Better Performance"},{"location":"schema/index-management/index-performance/#indexing-for-better-performance","text":"JanusGraph supports two different kinds of indexing to speed up query processing: graph indexes and vertex-centric indexes (a.k.a. relation indexes) . Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Graph indexes make these global retrieval operations efficient on large graphs. Vertex-centric indexes speed up the actual traversal through the graph, in particular when traversing through vertices with many incident edges.","title":"Indexing for Better Performance"},{"location":"schema/index-management/index-performance/#graph-index","text":"Graph indexes are global index structures over the entire graph which allow efficient retrieval of vertices or edges by their properties for sufficiently selective conditions. For instance, consider the following queries g . V (). has ( ' name ' , ' hercules ' ) g . E (). has ( ' reason ' , textContains ( ' loves ' )) The first query asks for all vertices with the name hercules . The second asks for all edges where the property reason contains the word loves . Without a graph index answering those queries would require a full scan over all vertices or edges in the graph to find those that match the given condition which is very inefficient and infeasible for huge graphs. JanusGraph distinguishes between two types of graph indexes: composite and mixed indexes. Composite indexes are very fast and efficient but limited to equality lookups for a particular, previously-defined combination of property keys. Mixed indexes can be used for lookups on any combination of indexed keys and support multiple condition predicates in addition to equality depending on the backing index store. Both types of indexes are created through the JanusGraph management system and the index builder returned by JanusGraphManagement.buildIndex(String, Class) where the first argument defines the name of the index and the second argument specifies the type of element to be indexed (e.g. Vertex.class ). The name of a graph index must be unique. Graph indexes built against newly defined property keys, i.e. property keys that are defined in the same management transaction as the index, are immediately available. The same applies to graph indexes that are constrained to a label that is created in the same management transaction as the index. Graph indexes built against property keys that are already in use without being constrained to a newly created label require the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the index will not be available. It is encouraged to define graph indexes in the same transaction as the initial schema. Note In the absence of an index, JanusGraph will default to a full graph scan in order to retrieve the desired list of vertices. While this produces the correct result set, the graph scan can be very inefficient and lead to poor overall system performance in a production environment. Enable the force-index configuration option in production deployments of JanusGraph to prohibit graph scans. Info See index lifecycle documentation for more information about index states.","title":"Graph Index"},{"location":"schema/index-management/index-performance/#composite-index","text":"Composite indexes are stored in a separate store called graphIndex . For example, if your storage backend is Cassandra, you will see a table named graphIndex under your namespace. Composite indexes retrieve vertices or edges by one or a (fixed) composition of multiple keys. Consider the following composite index definitions. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( ' name ' ) age = mgmt . getPropertyKey ( ' age ' ) mgmt . buildIndex ( ' byNameComposite ' , Vertex . class ). addKey ( name ). buildCompositeIndex () mgmt . buildIndex ( ' byNameAndAgeComposite ' , Vertex . class ). addKey ( name ). addKey ( age ). buildCompositeIndex () mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , ' byNameComposite ' ). call () ManagementSystem . awaitGraphIndexStatus ( graph , ' byNameAndAgeComposite ' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameComposite\" ), SchemaAction . REINDEX ). get () mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameAndAgeComposite\" ), SchemaAction . REINDEX ). get () mgmt . commit () First, two property keys name and age are already defined. Next, a simple composite index on just the name property key is built. JanusGraph will use this index to answer the following query. g . V (). has ( 'name' , 'hercules' ) The second composite graph index includes both keys. JanusGraph will use this index to answer the following query. g . V (). has ( 'age' , 30 ). has ( 'name' , 'hercules' ) Note, that all keys of a composite graph index must be found in the query\u2019s equality conditions for this index to be used. For example, the following query cannot be answered with either of the indexes because it only contains a constraint on age but not name . g . V (). has ( 'age' , 30 ) Also note, that composite graph indexes can only be used for equality constraints like those in the queries above. The following query would be answered with just the simple composite index defined on the name key because the age constraint is not an equality constraint. g . V (). has ( ' name ' , ' hercules ' ). has ( ' age ' , inside ( 20 , 50 )) Composite indexes do not require configuration of an external indexing backend and are supported through the primary storage backend. Hence, composite index modifications are persisted through the same transaction as graph modifications which means that those changes are atomic and/or consistent if the underlying storage backend supports atomicity and/or consistency. Note A composite index may comprise just one or multiple keys. A composite index with just one key is sometimes referred to as a key-index.","title":"Composite Index"},{"location":"schema/index-management/index-performance/#index-uniqueness","text":"Composite indexes can also be used to enforce property uniqueness in the graph. If a composite graph index is defined as unique() there can be at most one vertex or edge for any given concatenation of property values associated with the keys of that index. For instance, to enforce that names are unique across the entire graph the following composite graph index would be defined. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( 'name' ) mgmt . buildIndex ( 'byNameUnique' , Vertex . class ). addKey ( name ). unique (). buildCompositeIndex () mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , 'byNameUnique' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameUnique\" ), SchemaAction . REINDEX ). get () mgmt . commit () Note To enforce uniqueness against an eventually consistent storage backend, the consistency of the index must be explicitly set to enabling locking.","title":"Index Uniqueness"},{"location":"schema/index-management/index-performance/#mixed-index","text":"Mixed indexes retrieve vertices or edges by any combination of previously added property keys. Mixed indexes provide more flexibility than composite indexes and support additional condition predicates beyond equality. On the other hand, mixed indexes are slower for most equality queries than composite indexes. Unlike composite indexes, mixed indexes require the configuration of an indexing backend and use that indexing backend to execute lookup operations. JanusGraph can support multiple indexing backends in a single installation. Each indexing backend must be uniquely identified by name in the JanusGraph configuration which is called the indexing backend name . graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( 'name' ) age = mgmt . getPropertyKey ( 'age' ) mgmt . buildIndex ( 'nameAndAge' , Vertex . class ). addKey ( name ). addKey ( age ). buildMixedIndex ( \"search\" ) mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , 'nameAndAge' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"nameAndAge\" ), SchemaAction . REINDEX ). get () mgmt . commit () The example above defines a mixed index containing the property keys name and age . The definition refers to the indexing backend name search so that JanusGraph knows which configured indexing backend it should use for this particular index. The search parameter specified in the buildMixedIndex call must match the second clause in the JanusGraph configuration definition like this: index. search .backend If the index was named solrsearch then the configuration definition would appear like this: index. solrsearch .backend. The mgmt.buildIndex example specified above uses text search as its default behavior. An index statement that explicitly defines the index as a text index can be written as follows: mgmt . buildIndex ( 'nameAndAge' , Vertex . class ). addKey ( name , Mapping . TEXT . asParameter ()). addKey ( age , Mapping . TEXT . asParameter ()). buildMixedIndex ( \"search\" ) See Index Parameters and Full-Text Search for more information on text and string search options, and see the documentation section specific to the indexing backend in use for more details on how each backend handles text versus string searches. While the index definition example looks similar to the composite index above, it provides greater query support and can answer any of the following queries. g . V (). has ( 'name' , textContains ( 'hercules' )). has ( 'age' , inside ( 20 , 50 )) g . V (). has ( 'name' , textContains ( 'hercules' )) g . V (). has ( 'age' , lt ( 50 )) g . V (). has ( 'age' , outside ( 20 , 50 )) g . V (). has ( 'age' , lt ( 50 ). or ( gte ( 60 ))) g . V (). or ( __ . has ( 'name' , textContains ( 'hercules' )), __ . has ( 'age' , inside ( 20 , 50 ))) Mixed indexes support full-text search, range search, geo search and others. Refer to Search Predicates and Data Types for a list of predicates supported by a particular indexing backend. Note Unlike composite indexes, mixed indexes do not support uniqueness.","title":"Mixed Index"},{"location":"schema/index-management/index-performance/#adding-property-keys","text":"Property keys can be added to an existing mixed index which allows subsequent queries to include this key in the query condition. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () location = mgmt . makePropertyKey ( 'location' ). dataType ( Geoshape . class ). make () nameAndAge = mgmt . getGraphIndex ( 'nameAndAge' ) mgmt . addIndexKey ( nameAndAge , location ) mgmt . commit () //Previously created property keys already have the status ENABLED, but //our newly created property key \"location\" needs to REGISTER so we wait for both statuses ManagementSystem . awaitGraphIndexStatus ( graph , 'nameAndAge' ). status ( SchemaStatus . REGISTERED , SchemaStatus . ENABLED ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"nameAndAge\" ), SchemaAction . REINDEX ). get () mgmt . commit () To add a newly defined key, we first retrieve the existing index from the management transaction by its name and then invoke the addIndexKey method to add the key to this index. If the added key is defined in the same management transaction, it will be immediately available for querying. If the property key has already been in use, adding the key requires the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the key will not be available in the mixed index.","title":"Adding Property Keys"},{"location":"schema/index-management/index-performance/#mapping-parameters","text":"When adding a property key to a mixed index - either through the index builder or the addIndexKey method - a list of parameters can be optionally specified to adjust how the property value is mapped into the indexing backend. Refer to the mapping parameters overview for a complete list of parameter types supported by each indexing backend.","title":"Mapping Parameters"},{"location":"schema/index-management/index-performance/#ordering","text":"The order in which the results of a graph query are returned can be defined using the order().by() directive. The order().by() method expects two parameters: The name of the property key by which to order the results. The results will be ordered by the value of the vertices or edges for this property key. The sort order: either ascending asc or descending desc For example, the query g.V().has('name', textContains('hercules')).order().by('age', desc).limit(10) retrieves the ten oldest individuals with hercules in their name. When using order().by() it is important to note that: Composite graph indexes do not natively support ordering search results. All results will be retrieved and then sorted in-memory. For large result sets, this can be very expensive. Mixed indexes support ordering natively and efficiently. However, the property key used in the order().by() method must have been previously added to the mixed indexed for native result ordering support. This is important in cases where the the order().by() key is different from the query keys. If the property key is not part of the index, then sorting requires loading all results into memory.","title":"Ordering"},{"location":"schema/index-management/index-performance/#label-constraint","text":"In many cases it is desirable to only index vertices or edges with a particular label. For instance, one may want to index only gods by their name and not every single vertex that has a name property. When defining an index it is possible to restrict the index to a particular vertex or edge label using the indexOnly method of the index builder. The following creates a composite index for the property key name that indexes only vertices labeled god . graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () name = mgmt . getPropertyKey ( 'name' ) god = mgmt . getVertexLabel ( 'god' ) mgmt . buildIndex ( 'byNameAndLabel' , Vertex . class ). addKey ( name ). indexOnly ( god ). buildCompositeIndex () mgmt . commit () //Wait for the index to become available ManagementSystem . awaitGraphIndexStatus ( graph , 'byNameAndLabel' ). call () //You can check the indexOnly constraint built just now mgmt = graph . openManagement () mgmt . getIndexOnlyConstraint ( \"byNameAndLabel\" ) //Reindex the existing data mgmt . updateIndex ( mgmt . getGraphIndex ( \"byNameAndLabel\" ), SchemaAction . REINDEX ). get () mgmt . commit () Label restrictions similarly apply to mixed indexes. When a composite index with label restriction is defined as unique, the uniqueness constraint only applies to properties on vertices or edges for the specified label.","title":"Label Constraint"},{"location":"schema/index-management/index-performance/#composite-versus-mixed-indexes","text":"Use a composite index for exact match index retrievals. Composite indexes do not require configuring or operating an external index system and are often significantly faster than mixed indexes. As an exception, use a mixed index for exact matches when the number of distinct values for query constraint is relatively small or if one value is expected to be associated with many elements in the graph (i.e. in case of low selectivity). Use a mixed indexes for numeric range, full-text or geo-spatial indexing. Also, using a mixed index can speed up the order().by() queries. A composite index requires all fields to be present, while a mixed index only needs at least one field to be present. For example, say you have a composite index with key1 and key2 , a mixed index with key1 and key3 . If you add a vertex with only property key1 , then JanusGraph will create a new mixed index entry but not a composite index entry.","title":"Composite versus Mixed Indexes"},{"location":"schema/index-management/index-performance/#vertex-centric-indexes","text":"Vertex-centric indexes, also known as Relation indexes, are local index structures built individually per vertex. They are stored together with edges and properties in edgeStore . There are two types of vertex-centric indexes, edge indexes and property indexes.","title":"Vertex-centric Indexes"},{"location":"schema/index-management/index-performance/#edge-indexes","text":"In large graphs vertices can have thousands of incident edges. Traversing through those vertices can be very slow because a large subset of the incident edges has to be retrieved and then filtered in memory to match the conditions of the traversal. Vertex-centric indexes can speed up such traversals by using localized index structures to retrieve only those edges that need to be traversed. Suppose that Hercules battled hundreds of monsters in addition to the three captured in the introductory Graph of the Gods . Without a vertex-centric index, a query asking for those monsters battled between time point 10 and 20 would require retrieving all battled edges even though there are only a handful of matching edges. h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). outE ( 'battled' ). has ( 'time' , inside ( 10 , 20 )). inV () Building a vertex-centric index by time speeds up such traversal queries. Note, this initial index example already exists in the Graph of the Gods as an index named edges . As a result, running the steps below will result in a uniqueness constraint error. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () time = mgmt . getPropertyKey ( 'time' ) battled = mgmt . getEdgeLabel ( 'battled' ) mgmt . buildEdgeIndex ( battled , 'battlesByTime' , Direction . BOTH , Order . desc , time ) mgmt . commit () //Wait for the index to become available ManagementSystem . awaitRelationIndexStatus ( graph , 'battlesByTime' , 'battled' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getRelationIndex ( battled , \"battlesByTime\" ), SchemaAction . REINDEX ). get () mgmt . commit () This example builds a vertex-centric index which indexes battled edges in both direction by time in descending order. A vertex-centric index is built against a particular edge label which is the first argument to the index construction method JanusGraphManagement.buildEdgeIndex() . The index only applies to edges of this label - battled in the example above. The second argument is a unique name for the index. The third argument is the edge direction in which the index is built. The index will only apply to traversals along edges in this direction. In this example, the vertex-centric index is built in both direction which means that time restricted traversals along battled edges can be served by this index in both the IN and OUT direction. JanusGraph will maintain a vertex-centric index on both the in- and out-vertex of battled edges. Alternatively, one could define the index to apply to the OUT direction only which would speed up traversals from Hercules to the monsters but not in the reverse direction. This would only require maintaining one index and hence half the index maintenance and storage cost. The last two arguments are the sort order of the index and a list of property keys to index by. The sort order is optional and defaults to ascending order (i.e. Order.ASC ). The list of property keys must be non-empty and defines the keys by which to index the edges of the given label. A vertex-centric index can be defined with multiple keys. graph . tx (). rollback () //Never create new indexes while a transaction is active mgmt = graph . openManagement () time = mgmt . getPropertyKey ( 'time' ) rating = mgmt . makePropertyKey ( 'rating' ). dataType ( Double . class ). make () battled = mgmt . getEdgeLabel ( 'battled' ) mgmt . buildEdgeIndex ( battled , 'battlesByRatingAndTime' , Direction . OUT , Order . desc , rating , time ) mgmt . commit () //Wait for the index to become available ManagementSystem . awaitRelationIndexStatus ( graph , 'battlesByRatingAndTime' , 'battled' ). call () //Reindex the existing data mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getRelationIndex ( battled , 'battlesByRatingAndTime' ), SchemaAction . REINDEX ). get () mgmt . commit () This example extends the schema by a rating property on battled edges and builds a vertex-centric index which indexes battled edges in the out-going direction by rating and time in descending order. Note, that the order in which the property keys are specified is important because vertex-centric indexes are prefix indexes. This means, that battled edges are indexed by rating first and time second . h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). outE ( 'battled' ). property ( 'rating' , 5.0 ) //Add some rating properties g . V ( h ). outE ( 'battled' ). has ( 'rating' , gt ( 3.0 )). inV () g . V ( h ). outE ( 'battled' ). has ( 'rating' , 5.0 ). has ( 'time' , inside ( 10 , 50 )). inV () g . V ( h ). outE ( 'battled' ). has ( 'time' , inside ( 10 , 50 )). inV () Hence, the battlesByRatingAndTime index can speed up the first two but not the third query.","title":"Edge Indexes"},{"location":"schema/index-management/index-performance/#property-indexes","text":"Similar to edge indexes, property indexes can be built to traverse properties based on associated meta-properties efficiently. The following toy example illustrates the usage of property indexes. graph = JanusGraphFactory . open ( \"inmemory\" ) mgmt = graph . openManagement () timestamp = mgmt . makePropertyKey ( \"timestamp\" ). dataType ( Integer . class ). make () amount = mgmt . makePropertyKey ( \"amount\" ). dataType ( Integer . class ). cardinality ( Cardinality . LIST ). make () mgmt . buildPropertyIndex ( amount , 'amountByTime' , Order . desc , timestamp ) mgmt . commit () bob = graph . addVertex () bob . property ( \"amount\" , 100 , \"timestamp\" , 1600000000 ) bob . property ( \"amount\" , 200 , \"timestamp\" , 1500000000 ) bob . property ( \"amount\" , - 150 , \"timestamp\" , 1550000000 ) graph . tx (). commit () g = graph . traversal () g . V ( bob ). properties ( \"amount\" ). has ( \"timestamp\" , P . gt ( 1500000000 )) In the above example, we model a number of transactions Bob has made. amount is a vertex property that records the amount of money involved in each transaction, while timestamp is a meta property of amount which records the time that a particular record happens. By creating a property index, we can efficiently retrieve transaction amounts by timestamp. Alternatively, we can also model the transactions as edges, then both amount and timestamp will be edge properties, and we could use Edge indexes to speed up the query. Multiple vertex-centric indexes can be built for the same edge label in order to support different constraint traversals. JanusGraph\u2019s query optimizer attempts to pick the most efficient index for any given traversal. Vertex-centric indexes only support equality and range/interval constraints. Note The property keys used in a vertex-centric index must have an explicitly defined data type (i.e. not Object.class ) which supports a native sort order. This means not only that they must implement Comparable but that their serializer must implement OrderPreservingSerializer . The types that are currently supported are Boolean , UUID , Byte , Float , Long , String , Integer , Date , Double , Character , and Short If the vertex-centric index is built against either an edge label or at least one property key that is defined in the same management transaction, the index will be immediately available for querying. If both the edge label and all of the indexed property keys have already been in use, building a vertex-centric index against it requires the execution of a reindex procedure to ensure that the index contains all previously added edges. Until the reindex procedure has completed, the index will not be available. Note JanusGraph automatically builds vertex-centric indexes per edge label and property key. That means, even with thousands of incident battled edges, queries like g.V(h).out('mother') or g.V(h).values('age') are efficiently answered by the local index. Vertex-centric indexes cannot speed up unconstrained traversals which require traversing through all incident edges of a particular label. Those traversals will become slower as the number of incident edges increases. Often, such traversals can be rewritten as constrained traversals that can utilize a vertex-centric index to ensure acceptable performance at scale.","title":"Property Indexes"},{"location":"schema/index-management/index-performance/#using-vertex-centric-indexes-on-adjacent-vertex-ids","text":"In some cases it is relevant to find an edge based on properties of the adjacent vertex. Let's say we want to find out whether or not Hercules has battled Cerberus. h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). out ( 'battled' ). has ( 'name' , 'cerberus' ). hasNext () A query like this can not use a vertex centric index because it filters on vertex properties rather than edge properties. But by restructuring the query, we can achieve exactly this. As both vertices are known, the vertex ids can be used to select the edge. h = g . V (). has ( 'name' , 'hercules' ). next () c = g . V (). has ( 'name' , 'cerberus' ). next () In contrast to the name \"Cebereus\", which is a property of the adjacent vertex, the id of this vertex is already saved within the connecting edge itself. Therefore, this query runs much faster if hercules has battled many opponents: g . V ( h ). outE ( 'battled' ). where ( inV (). is ( c )). hasNext () ... or even shorter: g . V ( h ). out ( 'battled' ). is ( c ). limit ( 1 ). hasNext () Assuming there is a global index on the name property, this improves the performance a lot, because it's not necessary to fetch every adjacent vertex anymore. In addition, a vertex-centric index on adjacent vertex ids does not need to be constructed and maintained explicitly. Due to the data model , efficient access to edges by their adjacent vertex id is already provided by the storage backend.","title":"Using vertex-centric indexes on adjacent vertex ids"},{"location":"schema/index-management/index-performance/#ordered-traversals","text":"The following queries specify an order in which the incident edges are to be traversed. Use the localLimit command to retrieve a subset of the edges (in a given order) for EACH vertex that is traversed. h = g . V (). has ( 'name' , 'hercules' ). next () g . V ( h ). local ( outE ( 'battled' ). order (). by ( 'time' , desc ). limit ( 10 )). inV (). values ( 'name' ) g . V ( h ). local ( outE ( 'battled' ). has ( 'rating' , 5.0 ). order (). by ( 'time' , desc ). limit ( 10 )). values ( 'place' ) The first query asks for the names of the 10 most recently battled monsters by Hercules. The second query asks for the places of the 10 most recent battles of Hercules that are rated 5 stars. In both cases, the query is constrained by an order on a property key with a limit on the number of elements to be returned. Such queries can also be efficiently answered by vertex-centric indexes if the order key matches the key of the index and the requested order (i.e. ascending or descending) is the same as the one defined for the index. The battlesByTime index would be used to answer the first query and battlesByRatingAndTime applies to the second. Note, that the battlesByRatingAndTime index cannot be used to answer the first query because an equality constraint on rating must be present for the second key in the index to be effective.","title":"Ordered Traversals"},{"location":"schema/index-management/index-reindexing/","text":"Reindexing Graph Index and Vertex-centric Indexes describe how to build graph-global and vertex-centric indexes to improve query performance. These indexes are immediately available if the indexed keys or labels have been newly defined in the same management transaction. In this case, there is no need to reindex the graph and this section can be skipped. If the indexed keys and labels already existed prior to index construction it is necessary to reindex the entire graph in order to ensure that the index contains previously added elements. This section describes the reindexing process. Warning Reindexing is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies. Overview JanusGraph can begin writing incremental index updates right after an index is defined. However, before the index is complete and usable, JanusGraph must also take a one-time read pass over all existing graph elements associated with the newly indexed schema type(s). Once this reindexing job has completed, the index is fully populated and ready to be used. The index must then be enabled to be used during query processing. Prior to Reindex The starting point of the reindexing process is the construction of an index. Refer to Indexing for Better Performance for a complete discussion of global graph and vertex-centric indexes. Note, that a global graph index is uniquely identified by its name. A vertex-centric index is uniquely identified by the combination of its name and the edge label or property key on which the index is defined - the name of the latter is referred to as the index type in this section and only applies to vertex-centric indexes. After building a new index against existing schema elements it is recommended to wait a few minutes for the index to be announced to the cluster. Note the index name (and the index type in case of a vertex-centric index) since this information is needed when reindexing. Preparing to Reindex There is a choice between two execution frameworks for reindex jobs: MapReduce JanusGraphManagement Reindex on MapReduce supports large, horizontally-distributed databases. Reindex on JanusGraphManagement spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine. Reindexing requires: The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when building a new index) The index type (a string\u2009\u2014\u2009the name of the edge label or property key on which the vertex-centric index is built). This applies only to vertex-centric indexes - leave blank for global graph indexes. Executing a Reindex Job on MapReduce The recommended way to generate and run a reindex job on MapReduce is through the MapReduceIndexManagement class. Here is a rough outline of the steps to run a reindex job using this class: Open a JanusGraph instance Pass the graph instance into MapReduceIndexManagement 's constructor Call updateIndex(<index>, SchemaAction.REINDEX) on the MapReduceIndexManagement instance If the index has not yet been enabled, enable it through JanusGraphManagement This class implements an updateIndex method that supports only the REINDEX and DISCARD_INDEX actions for its SchemaAction parameter. The class starts a Hadoop MapReduce job using the Hadoop configuration and jars on the classpath. Both Hadoop 1 and 2 are supported. This class gets metadata about the index and storage backend (e.g. the Cassandra partitioner) from the JanusGraph instance given to its constructor. graph = JanusGraphFactory . open (...) mgmt = graph . openManagement () mr = new MapReduceIndexManagement ( graph ) mr . updateIndex ( mgmt . getRelationIndex ( mgmt . getRelationType ( \"battled\" ), \"battlesByTime\" ), SchemaAction . REINDEX ). get () mgmt . commit () Reindex Example on MapReduce The following Gremlin snippet outlines all steps of the MapReduce reindex process in one self-contained example using minimal dummy data against the Cassandra storage backend. // Open a graph graph = JanusGraphFactory . open ( \"conf/janusgraph-cql-es.properties\" ) g = graph . traversal () // Define a property mgmt = graph . openManagement () desc = mgmt . makePropertyKey ( \"desc\" ). dataType ( String . class ). make () mgmt . commit () // Insert some data graph . addVertex ( \"desc\" , \"foo bar\" ) graph . addVertex ( \"desc\" , \"foo baz\" ) graph . tx (). commit () // Run a query -- note the planner warning recommending the use of an index g . V (). has ( \"desc\" , containsText ( \"baz\" )) // Create an index mgmt = graph . openManagement () desc = mgmt . getPropertyKey ( \"desc\" ) mixedIndex = mgmt . buildIndex ( \"mixedExample\" , Vertex . class ). addKey ( desc ). buildMixedIndex ( \"search\" ) mgmt . commit () // Rollback or commit transactions on the graph which predate the index definition graph . tx (). rollback () // Block until the SchemaStatus transitions from INSTALLED to REGISTERED report = ManagementSystem . awaitGraphIndexStatus ( graph , \"mixedExample\" ). call () // Run a JanusGraph-Hadoop job to reindex mgmt = graph . openManagement () mr = new MapReduceIndexManagement ( graph ) mr . updateIndex ( mgmt . getGraphIndex ( \"mixedExample\" ), SchemaAction . REINDEX ). get () // Enable the index mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"mixedExample\" ), SchemaAction . ENABLE_INDEX ). get () mgmt . commit () // Block until the SchemaStatus is ENABLED mgmt = graph . openManagement () report = ManagementSystem . awaitGraphIndexStatus ( graph , \"mixedExample\" ). status ( SchemaStatus . ENABLED ). call () mgmt . rollback () // Run a query -- JanusGraph will use the new index, no planner warning g . V (). has ( \"desc\" , containsText ( \"baz\" )) // Concerned that JanusGraph could have read cache in that last query, instead of relying on the index? // Start a new instance to rule out cache hits. Now we're definitely using the index. graph . close () graph = JanusGraphFactory . open ( \"conf/janusgraph-cql-es.properties\" ) g . V (). has ( \"desc\" , containsText ( \"baz\" )) Executing a Reindex job on ManagementSystem To run a reindex job on ManagementSystem, invoke ManagementSystem.updateIndex with the SchemaAction.REINDEX argument. For example: m = graph . openManagement () i = m . getGraphIndex ( 'indexName' ) m . updateIndex ( i , SchemaAction . REINDEX ). get () m . commit () ManagementSystem uses a local thread pool to run reindexing jobs concurrently. By default, the concurrency level equals the number of available processors. If you want to change the concurrency level, you can add a parameter like this: // only use one thread to run reindexing m . updateIndex ( i , SchemaAction . REINDEX , 1 ). get () Example for ManagementSystem The following loads some sample data into a BerkeleyDB-backed JanusGraph database, defines an index after the fact, reindexes using ManagementSystem, and finally enables and uses the index: import org.janusgraph.graphdb.database.management.ManagementSystem // Load some data from a file without any predefined schema graph = JanusGraphFactory . open ( 'conf/janusgraph-berkeleyje.properties' ) g = graph . traversal () m = graph . openManagement () m . makePropertyKey ( 'name' ). dataType ( String . class ). cardinality ( Cardinality . LIST ). make () m . makePropertyKey ( 'lang' ). dataType ( String . class ). cardinality ( Cardinality . LIST ). make () m . makePropertyKey ( 'age' ). dataType ( Integer . class ). cardinality ( Cardinality . LIST ). make () m . commit () graph . io ( IoCore . gryo ()). readGraph ( 'data/tinkerpop-modern.gio' ) graph . tx (). commit () // Run a query -- note the planner warning recommending the use of an index g . V (). has ( 'name' , 'lop' ) graph . tx (). rollback () // Create an index m = graph . openManagement () m . buildIndex ( 'names' , Vertex . class ). addKey ( m . getPropertyKey ( 'name' )). buildCompositeIndex () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from INSTALLED to REGISTERED ManagementSystem . awaitGraphIndexStatus ( graph , 'names' ). status ( SchemaStatus . REGISTERED ). call () // Reindex using JanusGraphManagement m = graph . openManagement () i = m . getGraphIndex ( 'names' ) m . updateIndex ( i , SchemaAction . REINDEX ) m . commit () // Wait for the index to be enabled ManagementSystem . awaitGraphIndexStatus ( graph , 'names' ). status ( SchemaStatus . ENABLED ). call () // Run a query -- JanusGraph will use the new index, no planner warning g . V (). has ( 'name' , 'lop' ) graph . tx (). rollback () // Concerned that JanusGraph could have read cache in that last query, instead of relying on the index? // Start a new instance to rule out cache hits. Now we're definitely using the index. graph . close () graph = JanusGraphFactory . open ( \"conf/janusgraph-berkeleyje.properties\" ) g = graph . traversal () g . V (). has ( 'name' , 'lop' ) Common problems IllegalArgumentException when starting job When a reindexing job is started shortly after a the index has been built, the job might fail with an exception like one of the following: The index mixedExample is in an invalid state and cannot be indexed . The following index keys have invalid status : desc has status INSTALLED ( status must be one of [ REGISTERED , ENABLED , DISABLED ]) The index mixedExample is in an invalid state and cannot be indexed . The index has status INSTALLED , but one of [ REGISTERED , ENABLED , DISABLED ] is required When an index is built, its existence is broadcast to all other JanusGraph instances in the cluster. Those must acknowledge the existence of the index before the reindexing process can be started. The acknowledgments can take a while to come in depending on the size of the cluster and the connection speed. Hence, one should wait a few minutes after building the index and before starting the reindex process. Note, that the acknowledgment might fail due to JanusGraph instance failure. In other words, the cluster might wait indefinitely on the acknowledgment of a failed instance. In this case, the user must manually remove the failed instance from the cluster registry as described in Failure & Recovery . After the cluster state has been restored, the acknowledgment process must be reinitiated by manually registering the index again in the management system. mgmt = graph . openManagement () rindex = mgmt . getRelationIndex ( mgmt . getRelationType ( \"battled\" ), \"battlesByTime\" ) mgmt . updateIndex ( rindex , SchemaAction . REGISTER_INDEX ). get () gindex = mgmt . getGraphIndex ( \"byName\" ) mgmt . updateIndex ( gindex , SchemaAction . REGISTER_INDEX ). get () mgmt . commit () After waiting a few minutes for the acknowledgment to arrive the reindex job should start successfully. Could not find index This exception in the reindexing job indicates that an index with the given name does not exist or that the name has not been specified correctly. When reindexing a global graph index, only the name of the index as defined when building the index should be specified. When reindexing a global graph index, the name of the index must be given in addition to the name of the edge label or property key on which the vertex-centric index is defined.","title":"Reindexing"},{"location":"schema/index-management/index-reindexing/#reindexing","text":"Graph Index and Vertex-centric Indexes describe how to build graph-global and vertex-centric indexes to improve query performance. These indexes are immediately available if the indexed keys or labels have been newly defined in the same management transaction. In this case, there is no need to reindex the graph and this section can be skipped. If the indexed keys and labels already existed prior to index construction it is necessary to reindex the entire graph in order to ensure that the index contains previously added elements. This section describes the reindexing process. Warning Reindexing is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.","title":"Reindexing"},{"location":"schema/index-management/index-reindexing/#overview","text":"JanusGraph can begin writing incremental index updates right after an index is defined. However, before the index is complete and usable, JanusGraph must also take a one-time read pass over all existing graph elements associated with the newly indexed schema type(s). Once this reindexing job has completed, the index is fully populated and ready to be used. The index must then be enabled to be used during query processing.","title":"Overview"},{"location":"schema/index-management/index-reindexing/#prior-to-reindex","text":"The starting point of the reindexing process is the construction of an index. Refer to Indexing for Better Performance for a complete discussion of global graph and vertex-centric indexes. Note, that a global graph index is uniquely identified by its name. A vertex-centric index is uniquely identified by the combination of its name and the edge label or property key on which the index is defined - the name of the latter is referred to as the index type in this section and only applies to vertex-centric indexes. After building a new index against existing schema elements it is recommended to wait a few minutes for the index to be announced to the cluster. Note the index name (and the index type in case of a vertex-centric index) since this information is needed when reindexing.","title":"Prior to Reindex"},{"location":"schema/index-management/index-reindexing/#preparing-to-reindex","text":"There is a choice between two execution frameworks for reindex jobs: MapReduce JanusGraphManagement Reindex on MapReduce supports large, horizontally-distributed databases. Reindex on JanusGraphManagement spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine. Reindexing requires: The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when building a new index) The index type (a string\u2009\u2014\u2009the name of the edge label or property key on which the vertex-centric index is built). This applies only to vertex-centric indexes - leave blank for global graph indexes.","title":"Preparing to Reindex"},{"location":"schema/index-management/index-reindexing/#executing-a-reindex-job-on-mapreduce","text":"The recommended way to generate and run a reindex job on MapReduce is through the MapReduceIndexManagement class. Here is a rough outline of the steps to run a reindex job using this class: Open a JanusGraph instance Pass the graph instance into MapReduceIndexManagement 's constructor Call updateIndex(<index>, SchemaAction.REINDEX) on the MapReduceIndexManagement instance If the index has not yet been enabled, enable it through JanusGraphManagement This class implements an updateIndex method that supports only the REINDEX and DISCARD_INDEX actions for its SchemaAction parameter. The class starts a Hadoop MapReduce job using the Hadoop configuration and jars on the classpath. Both Hadoop 1 and 2 are supported. This class gets metadata about the index and storage backend (e.g. the Cassandra partitioner) from the JanusGraph instance given to its constructor. graph = JanusGraphFactory . open (...) mgmt = graph . openManagement () mr = new MapReduceIndexManagement ( graph ) mr . updateIndex ( mgmt . getRelationIndex ( mgmt . getRelationType ( \"battled\" ), \"battlesByTime\" ), SchemaAction . REINDEX ). get () mgmt . commit ()","title":"Executing a Reindex Job on MapReduce"},{"location":"schema/index-management/index-reindexing/#reindex-example-on-mapreduce","text":"The following Gremlin snippet outlines all steps of the MapReduce reindex process in one self-contained example using minimal dummy data against the Cassandra storage backend. // Open a graph graph = JanusGraphFactory . open ( \"conf/janusgraph-cql-es.properties\" ) g = graph . traversal () // Define a property mgmt = graph . openManagement () desc = mgmt . makePropertyKey ( \"desc\" ). dataType ( String . class ). make () mgmt . commit () // Insert some data graph . addVertex ( \"desc\" , \"foo bar\" ) graph . addVertex ( \"desc\" , \"foo baz\" ) graph . tx (). commit () // Run a query -- note the planner warning recommending the use of an index g . V (). has ( \"desc\" , containsText ( \"baz\" )) // Create an index mgmt = graph . openManagement () desc = mgmt . getPropertyKey ( \"desc\" ) mixedIndex = mgmt . buildIndex ( \"mixedExample\" , Vertex . class ). addKey ( desc ). buildMixedIndex ( \"search\" ) mgmt . commit () // Rollback or commit transactions on the graph which predate the index definition graph . tx (). rollback () // Block until the SchemaStatus transitions from INSTALLED to REGISTERED report = ManagementSystem . awaitGraphIndexStatus ( graph , \"mixedExample\" ). call () // Run a JanusGraph-Hadoop job to reindex mgmt = graph . openManagement () mr = new MapReduceIndexManagement ( graph ) mr . updateIndex ( mgmt . getGraphIndex ( \"mixedExample\" ), SchemaAction . REINDEX ). get () // Enable the index mgmt = graph . openManagement () mgmt . updateIndex ( mgmt . getGraphIndex ( \"mixedExample\" ), SchemaAction . ENABLE_INDEX ). get () mgmt . commit () // Block until the SchemaStatus is ENABLED mgmt = graph . openManagement () report = ManagementSystem . awaitGraphIndexStatus ( graph , \"mixedExample\" ). status ( SchemaStatus . ENABLED ). call () mgmt . rollback () // Run a query -- JanusGraph will use the new index, no planner warning g . V (). has ( \"desc\" , containsText ( \"baz\" )) // Concerned that JanusGraph could have read cache in that last query, instead of relying on the index? // Start a new instance to rule out cache hits. Now we're definitely using the index. graph . close () graph = JanusGraphFactory . open ( \"conf/janusgraph-cql-es.properties\" ) g . V (). has ( \"desc\" , containsText ( \"baz\" ))","title":"Reindex Example on MapReduce"},{"location":"schema/index-management/index-reindexing/#executing-a-reindex-job-on-managementsystem","text":"To run a reindex job on ManagementSystem, invoke ManagementSystem.updateIndex with the SchemaAction.REINDEX argument. For example: m = graph . openManagement () i = m . getGraphIndex ( 'indexName' ) m . updateIndex ( i , SchemaAction . REINDEX ). get () m . commit () ManagementSystem uses a local thread pool to run reindexing jobs concurrently. By default, the concurrency level equals the number of available processors. If you want to change the concurrency level, you can add a parameter like this: // only use one thread to run reindexing m . updateIndex ( i , SchemaAction . REINDEX , 1 ). get ()","title":"Executing a Reindex job on ManagementSystem"},{"location":"schema/index-management/index-reindexing/#example-for-managementsystem","text":"The following loads some sample data into a BerkeleyDB-backed JanusGraph database, defines an index after the fact, reindexes using ManagementSystem, and finally enables and uses the index: import org.janusgraph.graphdb.database.management.ManagementSystem // Load some data from a file without any predefined schema graph = JanusGraphFactory . open ( 'conf/janusgraph-berkeleyje.properties' ) g = graph . traversal () m = graph . openManagement () m . makePropertyKey ( 'name' ). dataType ( String . class ). cardinality ( Cardinality . LIST ). make () m . makePropertyKey ( 'lang' ). dataType ( String . class ). cardinality ( Cardinality . LIST ). make () m . makePropertyKey ( 'age' ). dataType ( Integer . class ). cardinality ( Cardinality . LIST ). make () m . commit () graph . io ( IoCore . gryo ()). readGraph ( 'data/tinkerpop-modern.gio' ) graph . tx (). commit () // Run a query -- note the planner warning recommending the use of an index g . V (). has ( 'name' , 'lop' ) graph . tx (). rollback () // Create an index m = graph . openManagement () m . buildIndex ( 'names' , Vertex . class ). addKey ( m . getPropertyKey ( 'name' )). buildCompositeIndex () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from INSTALLED to REGISTERED ManagementSystem . awaitGraphIndexStatus ( graph , 'names' ). status ( SchemaStatus . REGISTERED ). call () // Reindex using JanusGraphManagement m = graph . openManagement () i = m . getGraphIndex ( 'names' ) m . updateIndex ( i , SchemaAction . REINDEX ) m . commit () // Wait for the index to be enabled ManagementSystem . awaitGraphIndexStatus ( graph , 'names' ). status ( SchemaStatus . ENABLED ). call () // Run a query -- JanusGraph will use the new index, no planner warning g . V (). has ( 'name' , 'lop' ) graph . tx (). rollback () // Concerned that JanusGraph could have read cache in that last query, instead of relying on the index? // Start a new instance to rule out cache hits. Now we're definitely using the index. graph . close () graph = JanusGraphFactory . open ( \"conf/janusgraph-berkeleyje.properties\" ) g = graph . traversal () g . V (). has ( 'name' , 'lop' )","title":"Example for ManagementSystem"},{"location":"schema/index-management/index-reindexing/#common-problems","text":"","title":"Common problems"},{"location":"schema/index-management/index-reindexing/#illegalargumentexception-when-starting-job","text":"When a reindexing job is started shortly after a the index has been built, the job might fail with an exception like one of the following: The index mixedExample is in an invalid state and cannot be indexed . The following index keys have invalid status : desc has status INSTALLED ( status must be one of [ REGISTERED , ENABLED , DISABLED ]) The index mixedExample is in an invalid state and cannot be indexed . The index has status INSTALLED , but one of [ REGISTERED , ENABLED , DISABLED ] is required When an index is built, its existence is broadcast to all other JanusGraph instances in the cluster. Those must acknowledge the existence of the index before the reindexing process can be started. The acknowledgments can take a while to come in depending on the size of the cluster and the connection speed. Hence, one should wait a few minutes after building the index and before starting the reindex process. Note, that the acknowledgment might fail due to JanusGraph instance failure. In other words, the cluster might wait indefinitely on the acknowledgment of a failed instance. In this case, the user must manually remove the failed instance from the cluster registry as described in Failure & Recovery . After the cluster state has been restored, the acknowledgment process must be reinitiated by manually registering the index again in the management system. mgmt = graph . openManagement () rindex = mgmt . getRelationIndex ( mgmt . getRelationType ( \"battled\" ), \"battlesByTime\" ) mgmt . updateIndex ( rindex , SchemaAction . REGISTER_INDEX ). get () gindex = mgmt . getGraphIndex ( \"byName\" ) mgmt . updateIndex ( gindex , SchemaAction . REGISTER_INDEX ). get () mgmt . commit () After waiting a few minutes for the acknowledgment to arrive the reindex job should start successfully.","title":"IllegalArgumentException when starting job"},{"location":"schema/index-management/index-reindexing/#could-not-find-index","text":"This exception in the reindexing job indicates that an index with the given name does not exist or that the name has not been specified correctly. When reindexing a global graph index, only the name of the index as defined when building the index should be specified. When reindexing a global graph index, the name of the index must be given in addition to the name of the edge label or property key on which the vertex-centric index is defined.","title":"Could not find index"},{"location":"schema/index-management/index-removal/","text":"Removal Warning Index removal is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies. Overview Index removal is a multi-stage process. In the first stage, one JanusGraph instance deactivates the index by setting its state to DISABLED . At that point, JanusGraph stops using the index to answer queries and stops incrementally updating the index. Index-related data in the storage backend remains present but ignored. The second stage depends on whether the index is mixed or composite. All composite indices can be deleted via JanusGraph. As with reindexing, removal can be done through either MapReduce or JanusGraphManagement. However, not all mixed index backends allow automatic removal. Index backends for which JanusGraph does not provide an automated deletion mechanism\u2009\u2014\u2009currently Lucene and Solr\u2009\u2014\u2009must be manually dropped in the index backend. For supported mixed index backends, the removal process is analogue to the composite index removal, with the exception that neither MapReduce nor multi threading in JanusGraphManagement is necessary. Discarding an index deletes everything associated with the index except its schema definition and its DISCARDED state. Removing this schema stub for the index is the third step. Preparing for Index Removal If the index is currently enabled, it should first be disabled. This is done through the ManagementSystem . mgmt = graph . openManagement () rindex = mgmt . getRelationIndex ( mgmt . getRelationType ( \"battled\" ), \"battlesByTime\" ) mgmt . updateIndex ( rindex , SchemaAction . DISABLE_INDEX ). get () gindex = mgmt . getGraphIndex ( \"byName\" ) mgmt . updateIndex ( gindex , SchemaAction . DISABLE_INDEX ). get () mgmt . commit () Once the status of all keys on the index changes to DISABLED , the index is ready to be removed. A utility in ManagementSystem can automate the wait-for- DISABLED step: ManagementSystem . awaitGraphIndexStatus ( graph , 'byName' ). status ( SchemaStatus . DISABLED ). call () After a composite index is DISABLED , there is a choice between two execution frameworks for its removal: MapReduce ManagementSystem Index removal on MapReduce supports large, horizontally-distributed databases. Index removal on ManagementSystem spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine. Index removal requires: The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when building a new index) The index type (a string\u2009\u2014\u2009the name of the edge label or property key on which the vertex-centric index is built). This applies only to vertex-centric indices - leave blank for global graph indices. Executing an index removal job on MapReduce Info This method applies to the following types of indices: All composite indices As with reindexing, the recommended way to generate and run an index removal job on MapReduce is through the MapReduceIndexManagement class. Here is a rough outline of the steps to run an index removal job using this class: Open a JanusGraph instance If the index has not yet been disabled, disable it through JanusGraphManagement Pass the graph instance into MapReduceIndexManagement 's constructor Call updateIndex(<index>, SchemaAction.DISCARD_INDEX) A commented code example follows in the next subsection. Example for MapReduce import org.janusgraph.graphdb.database.management.ManagementSystem // Load the \"Graph of the Gods\" sample data graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) g = graph . traversal () GraphOfTheGodsFactory . load ( graph ) g . V (). has ( 'name' , 'jupiter' ) // Disable the \"name\" composite index m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISABLE_INDEX ). get () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from ENABLED to DISABLED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISABLED ). call () // Delete the indexed data using MapReduceIndexJobs m = graph . openManagement () mr = new MapReduceIndexManagement ( graph ) future = mr . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISCARD_INDEX ) m . commit () graph . tx (). commit () future . get () // Block until the SchemaStatus transitions from DISABLED to DISCARDED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISCARDED ). call () // Index still shows up in management interface as DISCARDED -- it can now be dropped entirely m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DROP_INDEX ). get () m . commit () // JanusGraph should issue a warning about this query requiring a full scan g . V (). has ( 'name' , 'jupiter' ) Executing an index removal job on ManagementSystem Info This method applies to the following types of indices: All composite indices Mixed indices (Elasticsearch only) To run an index removal job on ManagementSystem, invoke ManagementSystem.updateIndex with the SchemaAction.DISCARD_INDEX argument. For example: m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'indexName' ), SchemaAction . DISCARD_INDEX ). get () m . commit () Similar to reindex, ManagementSystem uses a local thread pool to execute index removal job concurrently. The concurrency level is equal to the number of available processors. If you want to change the default concurrency level, you can add a parameter as follows: // Use only one thread to execute index removal job m . updateIndex ( m . getGraphIndex ( 'indexName' ), SchemaAction . DISCARD_INDEX , 1 ). get () Example for ManagementSystem The following loads some indexed sample data into a BerkeleyDB-backed JanusGraph database, then disables and removes the index through ManagementSystem: import org.janusgraph.graphdb.database.management.ManagementSystem // Load the \"Graph of the Gods\" sample data graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) g = graph . traversal () GraphOfTheGodsFactory . load ( graph ) g . V (). has ( 'name' , 'jupiter' ) // Disable the \"name\" composite index m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISABLE_INDEX ). get () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from ENABLED to DISABLED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISABLED ). call () // Delete the index using JanusGraphManagement m = graph . openManagement () future = m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISCARD_INDEX ) m . commit () graph . tx (). commit () future . get () // Block until the SchemaStatus transitions from DISABLED to DISCARDED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISCARDED ). call () // Index still shows up in management interface as DISCARDED -- it can now be dropped entirely m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DROP_INDEX ). get () m . commit () // JanusGraph should issue a warning about this query requiring a full scan g . V (). has ( 'name' , 'jupiter' ) Executing a manual index removal for mixed indices Info This method applies to the following types of indices: Mixed indices which can not be removed by Janusgraph automatically If an index backend does not support automatic removal, you will receive an exception when trying to execute DISCARD_INDEX . This currently applies to all mixed indices except for Elasticsearch. For those index backends, indices have to be removed by hand in the index backend without the help of JanusGraph. In order to inform JanusGraph that an index has been removed manually, the transition to the state DISCARDED can be manually triggered by executing MARK_DISCARDED . m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'indexName' ), SchemaAction . MARK_DISCARDED ). get () m . commit () Warning This action does not execute any operation except overwriting the state of the index. Any data stored in the index backend will still be present. It is recommended to execute this step before actually touching the backend and deleting the data. This way, it is guaranteed that no instance concurrently re-enables the index. Once the index has entered the state DISCARDED , all indexed data can be safely deleted. Example for manual mixed index removal import org.janusgraph.graphdb.database.management.ManagementSystem // Load the \"Graph of the Gods\" sample data graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) g = graph . traversal () GraphOfTheGodsFactory . load ( graph ) g . V (). has ( \"name\" , \"jupiter\" ) // Disable the \"name\" composite index m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( \"name\" ), SchemaAction . DISABLE_INDEX ). get () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from ENABLED to DISABLED ManagementSystem . awaitGraphIndexStatus ( graph , \"name\" ). status ( SchemaStatus . DISABLED ). call () // Since JanusGraph is not capable of removing the index, we have to set the state manually. m = graph . openManagement () future = m . updateIndex ( m . getGraphIndex ( \"name\" ), SchemaAction . MARK_DISCARDED ) m . commit () graph . tx (). commit () future . get () // Block until the SchemaStatus transitions from DISABLED to DISCARDED ManagementSystem . awaitGraphIndexStatus ( graph , \"name\" ). status ( SchemaStatus . DISCARDED ). call () /* * At this point, the index is marked as DISCARDED, * so it is ensured that no instance will ever be * able to use it again. You are now safe to manually * perform the necessary operations in your index * backend to remove the indexed data by hand. */ // Index still shows up in management interface as DISCARDED -- it can now be dropped entirely m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( \"name\" ), SchemaAction . DROP_INDEX ). get () m . commit () // JanusGraph should issue a warning about this query requiring a full scan g . V (). has ( \"name\" , \"jupiter\" )","title":"Removal"},{"location":"schema/index-management/index-removal/#removal","text":"Warning Index removal is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.","title":"Removal"},{"location":"schema/index-management/index-removal/#overview","text":"Index removal is a multi-stage process. In the first stage, one JanusGraph instance deactivates the index by setting its state to DISABLED . At that point, JanusGraph stops using the index to answer queries and stops incrementally updating the index. Index-related data in the storage backend remains present but ignored. The second stage depends on whether the index is mixed or composite. All composite indices can be deleted via JanusGraph. As with reindexing, removal can be done through either MapReduce or JanusGraphManagement. However, not all mixed index backends allow automatic removal. Index backends for which JanusGraph does not provide an automated deletion mechanism\u2009\u2014\u2009currently Lucene and Solr\u2009\u2014\u2009must be manually dropped in the index backend. For supported mixed index backends, the removal process is analogue to the composite index removal, with the exception that neither MapReduce nor multi threading in JanusGraphManagement is necessary. Discarding an index deletes everything associated with the index except its schema definition and its DISCARDED state. Removing this schema stub for the index is the third step.","title":"Overview"},{"location":"schema/index-management/index-removal/#preparing-for-index-removal","text":"If the index is currently enabled, it should first be disabled. This is done through the ManagementSystem . mgmt = graph . openManagement () rindex = mgmt . getRelationIndex ( mgmt . getRelationType ( \"battled\" ), \"battlesByTime\" ) mgmt . updateIndex ( rindex , SchemaAction . DISABLE_INDEX ). get () gindex = mgmt . getGraphIndex ( \"byName\" ) mgmt . updateIndex ( gindex , SchemaAction . DISABLE_INDEX ). get () mgmt . commit () Once the status of all keys on the index changes to DISABLED , the index is ready to be removed. A utility in ManagementSystem can automate the wait-for- DISABLED step: ManagementSystem . awaitGraphIndexStatus ( graph , 'byName' ). status ( SchemaStatus . DISABLED ). call () After a composite index is DISABLED , there is a choice between two execution frameworks for its removal: MapReduce ManagementSystem Index removal on MapReduce supports large, horizontally-distributed databases. Index removal on ManagementSystem spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine. Index removal requires: The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when building a new index) The index type (a string\u2009\u2014\u2009the name of the edge label or property key on which the vertex-centric index is built). This applies only to vertex-centric indices - leave blank for global graph indices.","title":"Preparing for Index Removal"},{"location":"schema/index-management/index-removal/#executing-an-index-removal-job-on-mapreduce","text":"Info This method applies to the following types of indices: All composite indices As with reindexing, the recommended way to generate and run an index removal job on MapReduce is through the MapReduceIndexManagement class. Here is a rough outline of the steps to run an index removal job using this class: Open a JanusGraph instance If the index has not yet been disabled, disable it through JanusGraphManagement Pass the graph instance into MapReduceIndexManagement 's constructor Call updateIndex(<index>, SchemaAction.DISCARD_INDEX) A commented code example follows in the next subsection.","title":"Executing an index removal job on MapReduce"},{"location":"schema/index-management/index-removal/#example-for-mapreduce","text":"import org.janusgraph.graphdb.database.management.ManagementSystem // Load the \"Graph of the Gods\" sample data graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) g = graph . traversal () GraphOfTheGodsFactory . load ( graph ) g . V (). has ( 'name' , 'jupiter' ) // Disable the \"name\" composite index m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISABLE_INDEX ). get () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from ENABLED to DISABLED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISABLED ). call () // Delete the indexed data using MapReduceIndexJobs m = graph . openManagement () mr = new MapReduceIndexManagement ( graph ) future = mr . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISCARD_INDEX ) m . commit () graph . tx (). commit () future . get () // Block until the SchemaStatus transitions from DISABLED to DISCARDED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISCARDED ). call () // Index still shows up in management interface as DISCARDED -- it can now be dropped entirely m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DROP_INDEX ). get () m . commit () // JanusGraph should issue a warning about this query requiring a full scan g . V (). has ( 'name' , 'jupiter' )","title":"Example for MapReduce"},{"location":"schema/index-management/index-removal/#executing-an-index-removal-job-on-managementsystem","text":"Info This method applies to the following types of indices: All composite indices Mixed indices (Elasticsearch only) To run an index removal job on ManagementSystem, invoke ManagementSystem.updateIndex with the SchemaAction.DISCARD_INDEX argument. For example: m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'indexName' ), SchemaAction . DISCARD_INDEX ). get () m . commit () Similar to reindex, ManagementSystem uses a local thread pool to execute index removal job concurrently. The concurrency level is equal to the number of available processors. If you want to change the default concurrency level, you can add a parameter as follows: // Use only one thread to execute index removal job m . updateIndex ( m . getGraphIndex ( 'indexName' ), SchemaAction . DISCARD_INDEX , 1 ). get ()","title":"Executing an index removal job on ManagementSystem"},{"location":"schema/index-management/index-removal/#example-for-managementsystem","text":"The following loads some indexed sample data into a BerkeleyDB-backed JanusGraph database, then disables and removes the index through ManagementSystem: import org.janusgraph.graphdb.database.management.ManagementSystem // Load the \"Graph of the Gods\" sample data graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) g = graph . traversal () GraphOfTheGodsFactory . load ( graph ) g . V (). has ( 'name' , 'jupiter' ) // Disable the \"name\" composite index m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISABLE_INDEX ). get () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from ENABLED to DISABLED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISABLED ). call () // Delete the index using JanusGraphManagement m = graph . openManagement () future = m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DISCARD_INDEX ) m . commit () graph . tx (). commit () future . get () // Block until the SchemaStatus transitions from DISABLED to DISCARDED ManagementSystem . awaitGraphIndexStatus ( graph , 'name' ). status ( SchemaStatus . DISCARDED ). call () // Index still shows up in management interface as DISCARDED -- it can now be dropped entirely m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'name' ), SchemaAction . DROP_INDEX ). get () m . commit () // JanusGraph should issue a warning about this query requiring a full scan g . V (). has ( 'name' , 'jupiter' )","title":"Example for ManagementSystem"},{"location":"schema/index-management/index-removal/#executing-a-manual-index-removal-for-mixed-indices","text":"Info This method applies to the following types of indices: Mixed indices which can not be removed by Janusgraph automatically If an index backend does not support automatic removal, you will receive an exception when trying to execute DISCARD_INDEX . This currently applies to all mixed indices except for Elasticsearch. For those index backends, indices have to be removed by hand in the index backend without the help of JanusGraph. In order to inform JanusGraph that an index has been removed manually, the transition to the state DISCARDED can be manually triggered by executing MARK_DISCARDED . m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( 'indexName' ), SchemaAction . MARK_DISCARDED ). get () m . commit () Warning This action does not execute any operation except overwriting the state of the index. Any data stored in the index backend will still be present. It is recommended to execute this step before actually touching the backend and deleting the data. This way, it is guaranteed that no instance concurrently re-enables the index. Once the index has entered the state DISCARDED , all indexed data can be safely deleted.","title":"Executing a manual index removal for mixed indices"},{"location":"schema/index-management/index-removal/#example-for-manual-mixed-index-removal","text":"import org.janusgraph.graphdb.database.management.ManagementSystem // Load the \"Graph of the Gods\" sample data graph = JanusGraphFactory . open ( 'conf/janusgraph-cql-es.properties' ) g = graph . traversal () GraphOfTheGodsFactory . load ( graph ) g . V (). has ( \"name\" , \"jupiter\" ) // Disable the \"name\" composite index m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( \"name\" ), SchemaAction . DISABLE_INDEX ). get () m . commit () graph . tx (). commit () // Block until the SchemaStatus transitions from ENABLED to DISABLED ManagementSystem . awaitGraphIndexStatus ( graph , \"name\" ). status ( SchemaStatus . DISABLED ). call () // Since JanusGraph is not capable of removing the index, we have to set the state manually. m = graph . openManagement () future = m . updateIndex ( m . getGraphIndex ( \"name\" ), SchemaAction . MARK_DISCARDED ) m . commit () graph . tx (). commit () future . get () // Block until the SchemaStatus transitions from DISABLED to DISCARDED ManagementSystem . awaitGraphIndexStatus ( graph , \"name\" ). status ( SchemaStatus . DISCARDED ). call () /* * At this point, the index is marked as DISCARDED, * so it is ensured that no instance will ever be * able to use it again. You are now safe to manually * perform the necessary operations in your index * backend to remove the indexed data by hand. */ // Index still shows up in management interface as DISCARDED -- it can now be dropped entirely m = graph . openManagement () m . updateIndex ( m . getGraphIndex ( \"name\" ), SchemaAction . DROP_INDEX ). get () m . commit () // JanusGraph should issue a warning about this query requiring a full scan g . V (). has ( \"name\" , \"jupiter\" )","title":"Example for manual mixed index removal"},{"location":"storage-backend/","text":"JanusGraph\u2019s data storage layer is pluggable. Implementations of the pluggable storage layer\u2009\u2014\u2009that is, the software component tells JanusGraph how to talk to its data store\u2009\u2014\u2009are called storage backends. This section describes configuration and administration of JanusGraph\u2019s standard storage backends. This section only addresses configuration of the stock storage backends that come with JanusGraph.","title":"Introduction"},{"location":"storage-backend/bdb/","text":"Oracle Berkeley DB Java Edition Oracle Berkeley DB Java Edition is an open source, embeddable, transactional storage engine written entirely in Java. It takes full advantage of the Java environment to simplify development and deployment. The architecture of Oracle Berkeley DB Java Edition supports very high performance and concurrency for both read-intensive and write-intensive workloads. \u2014 Oracle Berkeley DB Java Edition Homepage The Oracle Berkeley DB Java Edition storage backend runs in the same JVM as JanusGraph and provides local persistence on a single machine. Hence, the BerkeleyDB storage backend requires that all of the graph data fits on the local disk and all of the frequently accessed graph elements fit into main memory. This imposes a practical limitation of graphs with 10-100s million vertices on commodity hardware. However, for graphs of that size the BerkeleyDB storage backend exhibits high performance because all data can be accessed locally within the same JVM. BerkeleyDB JE Setup Since BerkeleyDB runs in the same JVM as JanusGraph, connecting the two only requires a simple configuration and no additional setup: JanusGraph g = JanusGraphFactory . build (). set ( \"storage.backend\" , \"berkeleyje\" ). set ( \"storage.directory\" , \"/data/graph\" ). open (); In the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration. BerkeleyDB Specific Configuration Refer to Configuration Reference for a complete listing of all BerkeleyDB specific configuration options in addition to the general JanusGraph configuration options. BerkeleyDB configured with SHARED_CACHE those multiple graphs will make better use of memory because the cache LRU algorithm is applied across all information in all graphs sharing the cache. When configuring BerkeleyDB it is recommended to consider the following BerkeleyDB specific configuration options: transactions : Enables transactions and detects conflicting database operations. CAUTION: While disabling transactions can lead to better performance it can cause to inconsistencies and even corrupt the database if multiple JanusGraph instances interact with the same instance of BerkeleyDB. cache-percentage : The percentage of JVM heap space (configured via -Xmx) to be allocated to BerkeleyDB for its cache. Try to give BerkeleyDB as much space as possible without causing memory problems for JanusGraph. For instance, if JanusGraph only runs short transactions, use a value of 80 or higher. Ideal Use Case The BerkeleyDB storage backend is best suited for small to medium size graphs with up to 100 million vertices on commodity hardware. For graphs of that size, it will likely deliver higher performance than the distributed storage backends. Note, that BerkeleyDB is also limited in the number of concurrent requests it can handle efficiently because it runs on a single machine. Hence, it is not well suited for applications with many concurrent users mutating the graph, even if that graph is small to medium size. Since BerkeleyDB runs in the same JVM as JanusGraph, this storage backend is ideally suited for unit testing of application code using JanusGraph. Global Graph Operations JanusGraph backed by BerkeleyDB supports global graph operations such as iterating over all vertices or edges. However, note that such operations need to scan the entire database which can require a significant amount of time for larger graphs. In order to not run out of memory, it is advised to disable transactions ( storage.transactions=false ) when iterating over large graphs. Having transactions enabled requires BerkeleyDB to acquire read locks on the data it is reading. When iterating over the entire graph, these read locks can easily require more memory than is available.","title":"Oracle Berkeley DB Java Edition"},{"location":"storage-backend/bdb/#oracle-berkeley-db-java-edition","text":"Oracle Berkeley DB Java Edition is an open source, embeddable, transactional storage engine written entirely in Java. It takes full advantage of the Java environment to simplify development and deployment. The architecture of Oracle Berkeley DB Java Edition supports very high performance and concurrency for both read-intensive and write-intensive workloads. \u2014 Oracle Berkeley DB Java Edition Homepage The Oracle Berkeley DB Java Edition storage backend runs in the same JVM as JanusGraph and provides local persistence on a single machine. Hence, the BerkeleyDB storage backend requires that all of the graph data fits on the local disk and all of the frequently accessed graph elements fit into main memory. This imposes a practical limitation of graphs with 10-100s million vertices on commodity hardware. However, for graphs of that size the BerkeleyDB storage backend exhibits high performance because all data can be accessed locally within the same JVM.","title":"Oracle Berkeley DB Java Edition"},{"location":"storage-backend/bdb/#berkeleydb-je-setup","text":"Since BerkeleyDB runs in the same JVM as JanusGraph, connecting the two only requires a simple configuration and no additional setup: JanusGraph g = JanusGraphFactory . build (). set ( \"storage.backend\" , \"berkeleyje\" ). set ( \"storage.directory\" , \"/data/graph\" ). open (); In the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration.","title":"BerkeleyDB JE Setup"},{"location":"storage-backend/bdb/#berkeleydb-specific-configuration","text":"Refer to Configuration Reference for a complete listing of all BerkeleyDB specific configuration options in addition to the general JanusGraph configuration options. BerkeleyDB configured with SHARED_CACHE those multiple graphs will make better use of memory because the cache LRU algorithm is applied across all information in all graphs sharing the cache. When configuring BerkeleyDB it is recommended to consider the following BerkeleyDB specific configuration options: transactions : Enables transactions and detects conflicting database operations. CAUTION: While disabling transactions can lead to better performance it can cause to inconsistencies and even corrupt the database if multiple JanusGraph instances interact with the same instance of BerkeleyDB. cache-percentage : The percentage of JVM heap space (configured via -Xmx) to be allocated to BerkeleyDB for its cache. Try to give BerkeleyDB as much space as possible without causing memory problems for JanusGraph. For instance, if JanusGraph only runs short transactions, use a value of 80 or higher.","title":"BerkeleyDB Specific Configuration"},{"location":"storage-backend/bdb/#ideal-use-case","text":"The BerkeleyDB storage backend is best suited for small to medium size graphs with up to 100 million vertices on commodity hardware. For graphs of that size, it will likely deliver higher performance than the distributed storage backends. Note, that BerkeleyDB is also limited in the number of concurrent requests it can handle efficiently because it runs on a single machine. Hence, it is not well suited for applications with many concurrent users mutating the graph, even if that graph is small to medium size. Since BerkeleyDB runs in the same JVM as JanusGraph, this storage backend is ideally suited for unit testing of application code using JanusGraph.","title":"Ideal Use Case"},{"location":"storage-backend/bdb/#global-graph-operations","text":"JanusGraph backed by BerkeleyDB supports global graph operations such as iterating over all vertices or edges. However, note that such operations need to scan the entire database which can require a significant amount of time for larger graphs. In order to not run out of memory, it is advised to disable transactions ( storage.transactions=false ) when iterating over large graphs. Having transactions enabled requires BerkeleyDB to acquire read locks on the data it is reading. When iterating over the entire graph, these read locks can easily require more memory than is available.","title":"Global Graph Operations"},{"location":"storage-backend/bigtable/","text":"Google Cloud Bigtable Cloud Bigtable is Google\u2019s NoSQL Big Data database service. It\u2019s the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it\u2019s a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis. \u2014 Google Cloud Bigtable Homepage Bigtable Setup Bigtable implements the HBase interface for all data access operations, and requires a few configuration options to connect. Connecting to Bigtable Configuring JanusGraph to connect to Bigtable is achieved by using the hbase backend, along with a custom connection implementation, the project id of the Google Cloud Platform project containing the Bigtable instance, and the Cloud Bigtable instance id you are connecting to. Example: storage.backend = hbase storage.hbase.ext.hbase.client.connection.impl = com.google.cloud.bigtable.hbase2_x.BigtableConnection storage.hbase.ext.google.bigtable.project.id = <Google Cloud Platform project id> storage.hbase.ext.google.bigtable.instance.id = <Bigtable instance id>","title":"Google Cloud Bigtable"},{"location":"storage-backend/bigtable/#google-cloud-bigtable","text":"Cloud Bigtable is Google\u2019s NoSQL Big Data database service. It\u2019s the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it\u2019s a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis. \u2014 Google Cloud Bigtable Homepage","title":"Google Cloud Bigtable"},{"location":"storage-backend/bigtable/#bigtable-setup","text":"Bigtable implements the HBase interface for all data access operations, and requires a few configuration options to connect.","title":"Bigtable Setup"},{"location":"storage-backend/bigtable/#connecting-to-bigtable","text":"Configuring JanusGraph to connect to Bigtable is achieved by using the hbase backend, along with a custom connection implementation, the project id of the Google Cloud Platform project containing the Bigtable instance, and the Cloud Bigtable instance id you are connecting to. Example: storage.backend = hbase storage.hbase.ext.hbase.client.connection.impl = com.google.cloud.bigtable.hbase2_x.BigtableConnection storage.hbase.ext.google.bigtable.project.id = <Google Cloud Platform project id> storage.hbase.ext.google.bigtable.instance.id = <Bigtable instance id>","title":"Connecting to Bigtable"},{"location":"storage-backend/cassandra/","text":"Apache Cassandra The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra\u2019s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages. The largest known Cassandra cluster has over 75,000 nodes storing over 10 PB of data. \u2014 Apache Cassandra Homepage The following sections outline the various ways in which JanusGraph can be used in concert with Apache Cassandra. Cassandra Storage Backend Cassandra has two protocols for clients to use: CQL and Thrift. With Cassandra 4.0, Thrift support will be removed in Cassandra. JanusGraph just supports the CQL storage backend. Note If security is enabled on Cassandra, the user must have CREATE permission on <all keyspaces> , otherwise the keyspace must be created ahead of time by an administrator including the required tables or the user must have CREATE permission on <the configured keyspace> . The create table file containing the required tables is located in conf/cassandra/cassandraTables.cql . Please define your keyspace before executing it. Local Server Mode Cassandra can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and Cassandra communicate with one another via a localhost socket. Running JanusGraph over Cassandra requires the following setup steps: Download Cassandra , unpack it, and set filesystem paths in conf/cassandra.yaml . Connecting Gremlin Server to Cassandra using the default configuration files provided in the pre-packaged distribution. Start Cassandra by invoking bin/cassandra -f on the command line in the directory where Cassandra was unpacked. Read output to check that Cassandra started successfully. Now, you can create a Cassandra JanusGraph as follows JanusGraph g = JanusGraphFactory . build (). set ( \"storage.backend\" , \"cql\" ). set ( \"storage.hostname\" , \"127.0.0.1\" ). open (); In the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration. Local Container Mode Cassandra does not have a native install for Windows or OSX. One of the easiest ways to run Cassandra on OSX, Windows, or Linux is to use a Docker Container. You can download and run Cassandra with a single Docker command. It is important to install a version that is supported by the version of JanusGraph you intend to use. The compatible versions can be found under the Tested Compatibility section of the specific release on the Releases page . The Cassandra Docker Hub page can be referenced for the available versions and useful commands. A description of the ports can be found here . Port 9160 is used for the Thrift client API. Port 9042 is for CQL native clients. Ports 7000, 7001 and 7099 are for inter-node communication. Version 3.11 of Cassandra was the latest compatible version for JanusGraph 0.2.0 and is specified in the reference command below. docker run --name jg-cassandra -d -e CASSANDRA_START_RPC = true -p 9160 :9160 \\ -p 9042 :9042 -p 7199 :7199 -p 7001 :7001 -p 7000 :7000 cassandra:3.11 Remote Server Mode When the graph needs to scale beyond the confines of a single machine, then Cassandra and JanusGraph are logically separated into different machines. In this model, the Cassandra cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the Cassandra cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph. For example, suppose we have a running Cassandra cluster where one of the machines has the IP address 77.77.77.77, then connecting JanusGraph with the cluster is accomplished as follows (comma separate IP addresses to reference more than one machine): JanusGraph graph = JanusGraphFactory . build (). set ( \"storage.backend\" , \"cql\" ). set ( \"storage.hostname\" , \"77.77.77.77\" ). open (); In the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration. Remote Server Mode with Gremlin Server Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph. Start Gremlin Server using bin/janusgraph-server.sh and then in an external Gremlin Console session using bin/gremlin.sh you can send Gremlin commands over the wire: : plugin use tinkerpop . server : remote connect tinkerpop . server conf / remote . yaml :> g . addV () In this case, each Gremlin Server would be configured to connect to the Cassandra cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server. ... graphs : { g : conf/janusgraph-cql.properties } scriptEngines : { gremlin-groovy : { plugins : { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin : {}, org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin : { classImports : [ java.lang.Math ], methodImports : [ java.lang.Math#* ]}, org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin : { files : [ scripts/empty-sample.groovy ]}}}} ... For more information about Gremlin Server see the Apache TinkerPop documentation CQL Specific Configuration Refer to Configuration Reference for a complete listing of all Cassandra specific configuration options in addition to the general JanusGraph configuration options. When configuring CQL it is recommended to consider the following CQL specific configuration options: read-consistency-level : Cassandra consistency level for read operations write-consistency-level : Cassandra consistency level for write operations replication-factor : The replication factor to use. The higher the replication factor, the more robust the graph database is to machine failure at the expense of data duplication. The default value should be overwritten for production system to ensure robustness. A value of 3 is recommended. This replication factor can only be set when the keyspace is initially created. On an existing keyspace, this value is ignored. keyspace : The name of the keyspace to store the JanusGraph graph in. Allows multiple JanusGraph graphs to co-exist in the same Cassandra cluster. More information on Cassandra consistency levels and acceptable values can be found here . In general, higher levels are more consistent and robust but have higher latency. Global Graph Operations JanusGraph over Cassandra supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause OutOfMemoryException . Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively. Deploying on DataStax Astra Astra DB simplifies cloud-native Cassandra application development. It reduces deployment time from weeks to minutes, and delivers an unprecedented combination of serverless, pay-as-you-go pricing with the freedom and agility of multi-cloud and open source. \u2014 DataStax Astra Download the secure-connect zipped bundle for your Astra database. While connecting to Astra DB from JanusGraph, it is preferred to make use of the secure bundle connection file as-is without extracting it. There are multiple ways in which a secure bundle connection file can be passed on to the JanusGraph configuration to connect to Astra DB using the DataStax driver. Internal string configuration Set the property storage.cql.internal.string-configuration to datastax-java-driver { basic.cloud.secure-connect-bundle=<path-to-secure-bundle-zip-file> } and set the username, password and keyspace details. For example: gremlin.graph = org.janusgraph.core.JanusGraphFactory storage.backend = cql storage.cql.keyspace = <keyspace name which was created in AstraDB> storage.username = <clientID> storage.password = <clientSecret> storage.cql.internal.string-configuration = datastax-java-driver { basic.cloud.secure-connect-bundle=<path-to-secure-bundle-zip-file> } Also, you can set a jvm argument to pass the secure bundle file as shown below and remove that property (storage.cql.internal.string-configuration) from the list above. -Ddatastax-java-driver.basic.cloud.secure-connect-bundle=<path-to-secure-bundle-zip-file> Internal file configuration Set the property storage.cql.internal.file-configuration to an external configuration file if you would like to externalize the astra connection related properties to a separate file and specify the secure bundle and credentials information on that file. For example: gremlin.graph = org.janusgraph.core.JanusGraphFactory storage.backend = cql storage.cql.keyspace = <keyspace-name> # Link to the external file that DataStax driver understands storage.cql.internal.file-configuration = <path-to-astra.conf> astra.conf (external file) datastax-java-driver { basic.cloud { secure-connect-bundle = \"<path-to-secure-bundle-zip-file>\" } advanced.auth-provider { class = PlainTextAuthProvider username = \"<clientID>\" password = \"<clientSecret>\" } } Note: Client id and Client secret need to be generated and copied from your Astra Account as per this doc . To learn more about different configuration options of DataStax driver, please refer to the DataStax driver configuration documentation . Deploying on Amazon Keyspaces Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra\u2013compatible database service. Amazon Keyspaces is serverless, so you pay for only the resources you use and the service can automatically scale tables up and down in response to application traffic. \u2014 Amazon Keyspaces Note The support for Amazon Keyspaces is experimental. We discourage usage in production systems unless you have thoroughly tested it against your use case. Follow these steps to set up a Amazon Keyspaces cluster and deploy JanusGraph over it. Prior to these instructions, make sure you have already followed this guide to sign up for AWS and set up your identity and access management. Creating Credentials You would need to generate service-specific credentials. See this guide for more details. After your service-specific credential is generated, you would get an output similar to the following: { \"ServiceSpecificCredential\" : { \"CreateDate\" : \"2019-10-09T16:12:04Z\" , \"ServiceName\" : \"cassandra.amazonaws.com\" , \"ServiceUserName\" : \"alice-at-111122223333\" , \"ServicePassword\" : \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" , \"ServiceSpecificCredentialId\" : \"ACCAYFI33SINPGJEBYESF\" , \"UserName\" : \"alice\" , \"Status\" : \"Active\" } } Please save ServiceUserName and ServicePassword in a secure location and you would need them in JanusGraph config later. Setting up SSL/TLS Download the Starfield digital certificate using the following command curl https://certs.secureserver.net/repository/sf-class2-root.crt -O Convert the Starfield digital certificate to a trustStore file openssl x509 -outform der -in sf-class2-root.crt -out temp_file.der keytool -import -alias cassandra -keystore cassandra_truststore.jks -file temp_file.der Now you would see a cassandra_truststore.jks file generated locally. You would need this file and your truststore password later. For more details, see this doc . Note that you don't need to follow every step of that doc, since it is written for users who connect to Amazon Keyspaces using a Java client directly. . Configurations Below is a complete sample of configuration. Unlike standard Apache Cassandra or ScyllaDB, Amazon Keyspaces only supports a subset of functionalities. Therefore, some specific configurations are needed. # Basic settings for CQL gremlin.graph = org.janusgraph.core.JanusGraphFactory storage.backend = cql storage.hostname = cassandra.<your-datacenter, e.g. ap-east-1>.amazonaws.com storage.port = 9142 storage.username = <your-service-username> storage.password = <your-service-password> storage.cql.keyspace = janusgraph storage.cql.local-datacenter = <your-datacenter, e.g. ap-east-1> # SSL related settings storage.cql.ssl.enabled = true storage.cql.ssl.truststore.location = <your-trust-store-location> storage.cql.ssl.truststore.password = <your-trust-store-password> # Amazon Keyspaces does not support user-generated timestamps # Thus, the below config must be turned off graph.assign-timestamp = false # We strongly recommend you to turn on this config. It will # prohibit all full-scan attempts. This is because Amazon keyspace # diverges from Apache Cassandra and might result in incomplete # results when a full-scan is executed. See issue #3390 for more # details query.force-index = true # Amazon Keyspaces only supports LOCAL QUORUM consistency storage.cql.only-use-local-consistency-for-system-operations = true storage.cql.read-consistency-level = LOCAL_QUORUM storage.cql.write-consistency-level = LOCAL_QUORUM log.janusgraph.key-consistent = true log.tx.key-consistent = true # Amazon Keyspaces does not have metadata available to clients # Thus, we need to tell JanusGraph that metadata are disabled, # and provide a hint of which partitioner AWS is using. Valid # partitioner-names are: Murmur3Partitioner, RandomPartitioner, # and DefaultPartitioner storage.cql.metadata-schema-enabled = false storage.cql.metadata-token-map-enabled = false storage.cql.partitioner-name = Murmur3Partitioner Now you should be able to open the graph via gremlin console or java code, using the above configuration file. Known Problems Amazon Keyspaces creates tables on-demand. If you are connecting to it the first time, you would likely see error message like unconfigured table janusgraph.system_properties At the same time, you should be able to see the same table getting created on Amazon Keyspaces console UI. The creation process typically takes a few seconds. Once the creation is done, you could open the graph again, and the error will be gone. Unfortunately, you would have to follow the same process for all tables. Alternatively, you could create the tables on AWS manually, if you are familiar with JanusGraph. Typically nine tables are needed: edgestore , edgestore_lock_ , graphindex , graphindex_lock_ , janusgraph_ids , system_properties , system_properties_lock_ , systemlog , and txlog . Deploying on Amazon EC2 Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. \u2014 Amazon EC2 Note The below documentation might be partially out-of-date. Follow these steps to setup a Cassandra cluster on EC2 and deploy JanusGraph over Cassandra. To follow these instructions, you need an Amazon AWS account with established authentication credentials and some basic knowledge of AWS and EC2. Setup Cassandra Cluster These instructions for configuring and launching the DataStax Cassandra Community Edition AMI are based on the DataStax AMI Docs and focus on aspects relevant for a JanusGraph deployment. Setting up Security Group Navigate to the EC2 Console Dashboard, then click on \"Security Groups\" under \"Network & Security\". Create a new security group. Click Inbound. Set the \"Create a new rule\" dropdown menu to \"Custom TCP rule\". Add a rule for port 22 from source 0.0.0.0/0. Add a rule for ports 1024-65535 from the security group members. If you don\u2019t want to open all unprivileged ports among security group members, then at least open 7000, 7199, and 9160 among security group members. Tip: the \"Source\" dropdown will autocomplete security group identifiers once \"sg\" is typed in the box, so you needn\u2019t have the exact value ready beforehand. Launch DataStax Cassandra AMI \"Launch the DataStax AMI in your desired zone On the Instance Details page of the Request Instances Wizard, set \"Number of Instances\" to your desired number of Cassandra nodes. Set \"Instance Type\" to at least m1.large. We recommend m1.large. On the Advanced Instance Options page of the Request Instances Wizard, set the \"as text\" radio button under \"User Data\", then fill this into the text box: --clustername [cassandra-cluster-name] --totalnodes [number-of-instances] --version community --opscenter no [number-of-instances] in this configuration must match the number of EC2 instances configured on the previous wizard page. [cassandra-cluster-name] can be any string used for identification. For example: --clustername janusgraph --totalnodes 4 --version community --opscenter no On the Tags page of the Request Instances Wizard you can apply any desired configurations. These tags exist only at the EC2 administrative level and have no effect on the Cassandra daemons' configuration or operation. On the Create Key Pair page of the Request Instances Wizard, either select an existing key pair or create a new one. The PEM file containing the private half of the selected key pair will be required to connect to these instances. On the Configure Firewall page of the Request Instances Wizard, select the security group created earlier. Review and launch instances on the final wizard page. Verify Successful Instance Launch SSH into any Cassandra instance node: ssh -i [your-private-key].pem ubuntu@[public-dns-name-of-any-cassandra-instance] Run the Cassandra nodetool nodetool -h 127.0.0.1 ring to inspect the state of the Cassandra token ring. You should see as many nodes in this command\u2019s output as instances launched in the previous steps. Note, that the AMI takes a few minutes to configure each instance. A shell prompt will appear upon successful configuration when you SSH into the instance. Launch JanusGraph Instances Launch additional EC2 instances to run JanusGraph which are either configured in Remote Server Mode or Remote Server Mode with Gremlin-Server as described above. You only need to note the IP address of one of the Cassandra cluster instances and configure it as the host name. The particular EC2 instance to run and the particular configuration depends on your use case. Example JanusGraph Instance on Amazon Linux AMI Launch the Amazon Linux AMI in the same zone of the Cassandra cluster. Choose your desired EC2 instance type depending on the amount of resources you need. Use the default configuration options and select the same Key Pair and Security Group as for the Cassandra cluster configured in the previous step. SSH into the newly created instance via ssh -i [your-private-key].pem ec2-user@[public-dns-name-of-the-instance] . You may have to wait a little for the instance to launch. Download the current JanusGraph distribution with wget and unpack the archive locally to the home directory. Start the Gremlin Console to verify that JanusGraph runs successfully. For more information on how to unpack JanusGraph and start the Gremlin Console, please refer to the Getting Started guide Create a configuration file with vi janusgraph.properties and add the following lines:: storage.backend = cql storage.hostname = [IP-address-of-one-Cassandra-EC2-instance] You may add additional configuration options found on this page or in Configuration Reference . Start the Gremlin Console again and type the following:: gremlin > graph = JanusGraphFactory . open ( 'janusgraph.properties' ) ==> janusgraph [ cql: [ IP - address - of - one - Cassandra - EC2 - instance ]] Note You have successfully connected this JanusGraph instance to the Cassandra cluster and can start to operate on the graph. Connect to Cassandra cluster in EC2 from outside EC2 Opening the usual Cassandra ports (9042, 7000, 7199) in the security group is not enough, because the Cassandra nodes by default broadcast their ec2-internal IPs, and not their public-facing IPs. The resulting behavior is that you can open a JanusGraph graph on the cluster by connecting to port 9042 on any Cassandra node, but all requests to that graph time out. This is because Cassandra is telling the client to connect to an unreachable IP. To fix this, set the \"broadcast-address\" property for each instance in /etc/cassandra/cassandra.yaml to its public-facing IP, and restart the instance. Do this for all nodes in the cluster. Once the cluster comes back, nodetool reports the correct public-facing IPs to which connections from the local machine are allowed. Changing the \"broadcast-address\" property allows you to connect to the cluster from outside ec2, but it might also mean that traffic originating within ec2 will have to round-trip to the internet and back before it gets to the cluster. So, this approach is only useful for development and testing.","title":"Apache Cassandra"},{"location":"storage-backend/cassandra/#apache-cassandra","text":"The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra\u2019s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages. The largest known Cassandra cluster has over 75,000 nodes storing over 10 PB of data. \u2014 Apache Cassandra Homepage The following sections outline the various ways in which JanusGraph can be used in concert with Apache Cassandra.","title":"Apache Cassandra"},{"location":"storage-backend/cassandra/#cassandra-storage-backend","text":"Cassandra has two protocols for clients to use: CQL and Thrift. With Cassandra 4.0, Thrift support will be removed in Cassandra. JanusGraph just supports the CQL storage backend. Note If security is enabled on Cassandra, the user must have CREATE permission on <all keyspaces> , otherwise the keyspace must be created ahead of time by an administrator including the required tables or the user must have CREATE permission on <the configured keyspace> . The create table file containing the required tables is located in conf/cassandra/cassandraTables.cql . Please define your keyspace before executing it.","title":"Cassandra Storage Backend"},{"location":"storage-backend/cassandra/#local-server-mode","text":"Cassandra can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and Cassandra communicate with one another via a localhost socket. Running JanusGraph over Cassandra requires the following setup steps: Download Cassandra , unpack it, and set filesystem paths in conf/cassandra.yaml . Connecting Gremlin Server to Cassandra using the default configuration files provided in the pre-packaged distribution. Start Cassandra by invoking bin/cassandra -f on the command line in the directory where Cassandra was unpacked. Read output to check that Cassandra started successfully. Now, you can create a Cassandra JanusGraph as follows JanusGraph g = JanusGraphFactory . build (). set ( \"storage.backend\" , \"cql\" ). set ( \"storage.hostname\" , \"127.0.0.1\" ). open (); In the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration.","title":"Local Server Mode"},{"location":"storage-backend/cassandra/#local-container-mode","text":"Cassandra does not have a native install for Windows or OSX. One of the easiest ways to run Cassandra on OSX, Windows, or Linux is to use a Docker Container. You can download and run Cassandra with a single Docker command. It is important to install a version that is supported by the version of JanusGraph you intend to use. The compatible versions can be found under the Tested Compatibility section of the specific release on the Releases page . The Cassandra Docker Hub page can be referenced for the available versions and useful commands. A description of the ports can be found here . Port 9160 is used for the Thrift client API. Port 9042 is for CQL native clients. Ports 7000, 7001 and 7099 are for inter-node communication. Version 3.11 of Cassandra was the latest compatible version for JanusGraph 0.2.0 and is specified in the reference command below. docker run --name jg-cassandra -d -e CASSANDRA_START_RPC = true -p 9160 :9160 \\ -p 9042 :9042 -p 7199 :7199 -p 7001 :7001 -p 7000 :7000 cassandra:3.11","title":"Local Container Mode"},{"location":"storage-backend/cassandra/#remote-server-mode","text":"When the graph needs to scale beyond the confines of a single machine, then Cassandra and JanusGraph are logically separated into different machines. In this model, the Cassandra cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the Cassandra cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph. For example, suppose we have a running Cassandra cluster where one of the machines has the IP address 77.77.77.77, then connecting JanusGraph with the cluster is accomplished as follows (comma separate IP addresses to reference more than one machine): JanusGraph graph = JanusGraphFactory . build (). set ( \"storage.backend\" , \"cql\" ). set ( \"storage.hostname\" , \"77.77.77.77\" ). open (); In the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration.","title":"Remote Server Mode"},{"location":"storage-backend/cassandra/#remote-server-mode-with-gremlin-server","text":"Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph. Start Gremlin Server using bin/janusgraph-server.sh and then in an external Gremlin Console session using bin/gremlin.sh you can send Gremlin commands over the wire: : plugin use tinkerpop . server : remote connect tinkerpop . server conf / remote . yaml :> g . addV () In this case, each Gremlin Server would be configured to connect to the Cassandra cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server. ... graphs : { g : conf/janusgraph-cql.properties } scriptEngines : { gremlin-groovy : { plugins : { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin : {}, org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin : { classImports : [ java.lang.Math ], methodImports : [ java.lang.Math#* ]}, org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin : { files : [ scripts/empty-sample.groovy ]}}}} ... For more information about Gremlin Server see the Apache TinkerPop documentation","title":"Remote Server Mode with Gremlin Server"},{"location":"storage-backend/cassandra/#cql-specific-configuration","text":"Refer to Configuration Reference for a complete listing of all Cassandra specific configuration options in addition to the general JanusGraph configuration options. When configuring CQL it is recommended to consider the following CQL specific configuration options: read-consistency-level : Cassandra consistency level for read operations write-consistency-level : Cassandra consistency level for write operations replication-factor : The replication factor to use. The higher the replication factor, the more robust the graph database is to machine failure at the expense of data duplication. The default value should be overwritten for production system to ensure robustness. A value of 3 is recommended. This replication factor can only be set when the keyspace is initially created. On an existing keyspace, this value is ignored. keyspace : The name of the keyspace to store the JanusGraph graph in. Allows multiple JanusGraph graphs to co-exist in the same Cassandra cluster. More information on Cassandra consistency levels and acceptable values can be found here . In general, higher levels are more consistent and robust but have higher latency.","title":"CQL Specific Configuration"},{"location":"storage-backend/cassandra/#global-graph-operations","text":"JanusGraph over Cassandra supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause OutOfMemoryException . Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively.","title":"Global Graph Operations"},{"location":"storage-backend/cassandra/#deploying-on-datastax-astra","text":"Astra DB simplifies cloud-native Cassandra application development. It reduces deployment time from weeks to minutes, and delivers an unprecedented combination of serverless, pay-as-you-go pricing with the freedom and agility of multi-cloud and open source. \u2014 DataStax Astra Download the secure-connect zipped bundle for your Astra database. While connecting to Astra DB from JanusGraph, it is preferred to make use of the secure bundle connection file as-is without extracting it. There are multiple ways in which a secure bundle connection file can be passed on to the JanusGraph configuration to connect to Astra DB using the DataStax driver.","title":"Deploying on DataStax Astra"},{"location":"storage-backend/cassandra/#internal-string-configuration","text":"Set the property storage.cql.internal.string-configuration to datastax-java-driver { basic.cloud.secure-connect-bundle=<path-to-secure-bundle-zip-file> } and set the username, password and keyspace details. For example: gremlin.graph = org.janusgraph.core.JanusGraphFactory storage.backend = cql storage.cql.keyspace = <keyspace name which was created in AstraDB> storage.username = <clientID> storage.password = <clientSecret> storage.cql.internal.string-configuration = datastax-java-driver { basic.cloud.secure-connect-bundle=<path-to-secure-bundle-zip-file> } Also, you can set a jvm argument to pass the secure bundle file as shown below and remove that property (storage.cql.internal.string-configuration) from the list above. -Ddatastax-java-driver.basic.cloud.secure-connect-bundle=<path-to-secure-bundle-zip-file>","title":"Internal string configuration"},{"location":"storage-backend/cassandra/#internal-file-configuration","text":"Set the property storage.cql.internal.file-configuration to an external configuration file if you would like to externalize the astra connection related properties to a separate file and specify the secure bundle and credentials information on that file. For example: gremlin.graph = org.janusgraph.core.JanusGraphFactory storage.backend = cql storage.cql.keyspace = <keyspace-name> # Link to the external file that DataStax driver understands storage.cql.internal.file-configuration = <path-to-astra.conf> astra.conf (external file) datastax-java-driver { basic.cloud { secure-connect-bundle = \"<path-to-secure-bundle-zip-file>\" } advanced.auth-provider { class = PlainTextAuthProvider username = \"<clientID>\" password = \"<clientSecret>\" } } Note: Client id and Client secret need to be generated and copied from your Astra Account as per this doc . To learn more about different configuration options of DataStax driver, please refer to the DataStax driver configuration documentation .","title":"Internal file configuration"},{"location":"storage-backend/cassandra/#deploying-on-amazon-keyspaces","text":"Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra\u2013compatible database service. Amazon Keyspaces is serverless, so you pay for only the resources you use and the service can automatically scale tables up and down in response to application traffic. \u2014 Amazon Keyspaces Note The support for Amazon Keyspaces is experimental. We discourage usage in production systems unless you have thoroughly tested it against your use case. Follow these steps to set up a Amazon Keyspaces cluster and deploy JanusGraph over it. Prior to these instructions, make sure you have already followed this guide to sign up for AWS and set up your identity and access management.","title":"Deploying on Amazon Keyspaces"},{"location":"storage-backend/cassandra/#creating-credentials","text":"You would need to generate service-specific credentials. See this guide for more details. After your service-specific credential is generated, you would get an output similar to the following: { \"ServiceSpecificCredential\" : { \"CreateDate\" : \"2019-10-09T16:12:04Z\" , \"ServiceName\" : \"cassandra.amazonaws.com\" , \"ServiceUserName\" : \"alice-at-111122223333\" , \"ServicePassword\" : \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" , \"ServiceSpecificCredentialId\" : \"ACCAYFI33SINPGJEBYESF\" , \"UserName\" : \"alice\" , \"Status\" : \"Active\" } } Please save ServiceUserName and ServicePassword in a secure location and you would need them in JanusGraph config later.","title":"Creating Credentials"},{"location":"storage-backend/cassandra/#setting-up-ssltls","text":"Download the Starfield digital certificate using the following command curl https://certs.secureserver.net/repository/sf-class2-root.crt -O Convert the Starfield digital certificate to a trustStore file openssl x509 -outform der -in sf-class2-root.crt -out temp_file.der keytool -import -alias cassandra -keystore cassandra_truststore.jks -file temp_file.der Now you would see a cassandra_truststore.jks file generated locally. You would need this file and your truststore password later. For more details, see this doc . Note that you don't need to follow every step of that doc, since it is written for users who connect to Amazon Keyspaces using a Java client directly. .","title":"Setting up SSL/TLS"},{"location":"storage-backend/cassandra/#configurations","text":"Below is a complete sample of configuration. Unlike standard Apache Cassandra or ScyllaDB, Amazon Keyspaces only supports a subset of functionalities. Therefore, some specific configurations are needed. # Basic settings for CQL gremlin.graph = org.janusgraph.core.JanusGraphFactory storage.backend = cql storage.hostname = cassandra.<your-datacenter, e.g. ap-east-1>.amazonaws.com storage.port = 9142 storage.username = <your-service-username> storage.password = <your-service-password> storage.cql.keyspace = janusgraph storage.cql.local-datacenter = <your-datacenter, e.g. ap-east-1> # SSL related settings storage.cql.ssl.enabled = true storage.cql.ssl.truststore.location = <your-trust-store-location> storage.cql.ssl.truststore.password = <your-trust-store-password> # Amazon Keyspaces does not support user-generated timestamps # Thus, the below config must be turned off graph.assign-timestamp = false # We strongly recommend you to turn on this config. It will # prohibit all full-scan attempts. This is because Amazon keyspace # diverges from Apache Cassandra and might result in incomplete # results when a full-scan is executed. See issue #3390 for more # details query.force-index = true # Amazon Keyspaces only supports LOCAL QUORUM consistency storage.cql.only-use-local-consistency-for-system-operations = true storage.cql.read-consistency-level = LOCAL_QUORUM storage.cql.write-consistency-level = LOCAL_QUORUM log.janusgraph.key-consistent = true log.tx.key-consistent = true # Amazon Keyspaces does not have metadata available to clients # Thus, we need to tell JanusGraph that metadata are disabled, # and provide a hint of which partitioner AWS is using. Valid # partitioner-names are: Murmur3Partitioner, RandomPartitioner, # and DefaultPartitioner storage.cql.metadata-schema-enabled = false storage.cql.metadata-token-map-enabled = false storage.cql.partitioner-name = Murmur3Partitioner Now you should be able to open the graph via gremlin console or java code, using the above configuration file.","title":"Configurations"},{"location":"storage-backend/cassandra/#known-problems","text":"Amazon Keyspaces creates tables on-demand. If you are connecting to it the first time, you would likely see error message like unconfigured table janusgraph.system_properties At the same time, you should be able to see the same table getting created on Amazon Keyspaces console UI. The creation process typically takes a few seconds. Once the creation is done, you could open the graph again, and the error will be gone. Unfortunately, you would have to follow the same process for all tables. Alternatively, you could create the tables on AWS manually, if you are familiar with JanusGraph. Typically nine tables are needed: edgestore , edgestore_lock_ , graphindex , graphindex_lock_ , janusgraph_ids , system_properties , system_properties_lock_ , systemlog , and txlog .","title":"Known Problems"},{"location":"storage-backend/cassandra/#deploying-on-amazon-ec2","text":"Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. \u2014 Amazon EC2 Note The below documentation might be partially out-of-date. Follow these steps to setup a Cassandra cluster on EC2 and deploy JanusGraph over Cassandra. To follow these instructions, you need an Amazon AWS account with established authentication credentials and some basic knowledge of AWS and EC2.","title":"Deploying on Amazon EC2"},{"location":"storage-backend/cassandra/#setup-cassandra-cluster","text":"These instructions for configuring and launching the DataStax Cassandra Community Edition AMI are based on the DataStax AMI Docs and focus on aspects relevant for a JanusGraph deployment.","title":"Setup Cassandra Cluster"},{"location":"storage-backend/cassandra/#setting-up-security-group","text":"Navigate to the EC2 Console Dashboard, then click on \"Security Groups\" under \"Network & Security\". Create a new security group. Click Inbound. Set the \"Create a new rule\" dropdown menu to \"Custom TCP rule\". Add a rule for port 22 from source 0.0.0.0/0. Add a rule for ports 1024-65535 from the security group members. If you don\u2019t want to open all unprivileged ports among security group members, then at least open 7000, 7199, and 9160 among security group members. Tip: the \"Source\" dropdown will autocomplete security group identifiers once \"sg\" is typed in the box, so you needn\u2019t have the exact value ready beforehand.","title":"Setting up Security Group"},{"location":"storage-backend/cassandra/#launch-datastax-cassandra-ami","text":"\"Launch the DataStax AMI in your desired zone On the Instance Details page of the Request Instances Wizard, set \"Number of Instances\" to your desired number of Cassandra nodes. Set \"Instance Type\" to at least m1.large. We recommend m1.large. On the Advanced Instance Options page of the Request Instances Wizard, set the \"as text\" radio button under \"User Data\", then fill this into the text box: --clustername [cassandra-cluster-name] --totalnodes [number-of-instances] --version community --opscenter no [number-of-instances] in this configuration must match the number of EC2 instances configured on the previous wizard page. [cassandra-cluster-name] can be any string used for identification. For example: --clustername janusgraph --totalnodes 4 --version community --opscenter no On the Tags page of the Request Instances Wizard you can apply any desired configurations. These tags exist only at the EC2 administrative level and have no effect on the Cassandra daemons' configuration or operation. On the Create Key Pair page of the Request Instances Wizard, either select an existing key pair or create a new one. The PEM file containing the private half of the selected key pair will be required to connect to these instances. On the Configure Firewall page of the Request Instances Wizard, select the security group created earlier. Review and launch instances on the final wizard page.","title":"Launch DataStax Cassandra AMI"},{"location":"storage-backend/cassandra/#verify-successful-instance-launch","text":"SSH into any Cassandra instance node: ssh -i [your-private-key].pem ubuntu@[public-dns-name-of-any-cassandra-instance] Run the Cassandra nodetool nodetool -h 127.0.0.1 ring to inspect the state of the Cassandra token ring. You should see as many nodes in this command\u2019s output as instances launched in the previous steps. Note, that the AMI takes a few minutes to configure each instance. A shell prompt will appear upon successful configuration when you SSH into the instance.","title":"Verify Successful Instance Launch"},{"location":"storage-backend/cassandra/#launch-janusgraph-instances","text":"Launch additional EC2 instances to run JanusGraph which are either configured in Remote Server Mode or Remote Server Mode with Gremlin-Server as described above. You only need to note the IP address of one of the Cassandra cluster instances and configure it as the host name. The particular EC2 instance to run and the particular configuration depends on your use case.","title":"Launch JanusGraph Instances"},{"location":"storage-backend/cassandra/#example-janusgraph-instance-on-amazon-linux-ami","text":"Launch the Amazon Linux AMI in the same zone of the Cassandra cluster. Choose your desired EC2 instance type depending on the amount of resources you need. Use the default configuration options and select the same Key Pair and Security Group as for the Cassandra cluster configured in the previous step. SSH into the newly created instance via ssh -i [your-private-key].pem ec2-user@[public-dns-name-of-the-instance] . You may have to wait a little for the instance to launch. Download the current JanusGraph distribution with wget and unpack the archive locally to the home directory. Start the Gremlin Console to verify that JanusGraph runs successfully. For more information on how to unpack JanusGraph and start the Gremlin Console, please refer to the Getting Started guide Create a configuration file with vi janusgraph.properties and add the following lines:: storage.backend = cql storage.hostname = [IP-address-of-one-Cassandra-EC2-instance] You may add additional configuration options found on this page or in Configuration Reference . Start the Gremlin Console again and type the following:: gremlin > graph = JanusGraphFactory . open ( 'janusgraph.properties' ) ==> janusgraph [ cql: [ IP - address - of - one - Cassandra - EC2 - instance ]] Note You have successfully connected this JanusGraph instance to the Cassandra cluster and can start to operate on the graph.","title":"Example JanusGraph Instance on Amazon Linux AMI"},{"location":"storage-backend/cassandra/#connect-to-cassandra-cluster-in-ec2-from-outside-ec2","text":"Opening the usual Cassandra ports (9042, 7000, 7199) in the security group is not enough, because the Cassandra nodes by default broadcast their ec2-internal IPs, and not their public-facing IPs. The resulting behavior is that you can open a JanusGraph graph on the cluster by connecting to port 9042 on any Cassandra node, but all requests to that graph time out. This is because Cassandra is telling the client to connect to an unreachable IP. To fix this, set the \"broadcast-address\" property for each instance in /etc/cassandra/cassandra.yaml to its public-facing IP, and restart the instance. Do this for all nodes in the cluster. Once the cluster comes back, nodetool reports the correct public-facing IPs to which connections from the local machine are allowed. Changing the \"broadcast-address\" property allows you to connect to the cluster from outside ec2, but it might also mean that traffic originating within ec2 will have to round-trip to the internet and back before it gets to the cluster. So, this approach is only useful for development and testing.","title":"Connect to Cassandra cluster in EC2 from outside EC2"},{"location":"storage-backend/hbase/","text":"Apache HBase Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google\u2019s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS. \u2014 Apache HBase Homepage HBase Setup The following sections outline the various ways in which JanusGraph can be used in concert with Apache HBase. Local Server Mode HBase can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and HBase communicate with one another via a localhost socket. Running JanusGraph over HBase requires the following setup steps: Download and extract a stable HBase from https://www.apache.org/dyn/closer.cgi/hbase/stable/ . Start HBase by invoking the start-hbase.sh script in the bin directory inside the extracted HBase directory. To stop HBase, use stop-hbase.sh . $ ./bin/start-hbase.sh starting master, logging to ../logs/hbase-master-machine-name.local.out Now, you can create an HBase JanusGraph as follows: JanusGraph graph = JanusGraphFactory . build () . set ( \"storage.backend\" , \"hbase\" ) . open (); Note, that you do not need to specify a hostname since a localhost connection is attempted by default. Also, in the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration. Remote Server Mode When the graph needs to scale beyond the confines of a single machine, then HBase and JanusGraph are logically separated into different machines. In this model, the HBase cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the HBase cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph. For example, suppose we have a running HBase cluster with a ZooKeeper quorum composed of three machines at IP address 77.77.77.77, 77.77.77.78, and 77.77.77.79, then connecting JanusGraph with the cluster is accomplished as follows: JanusGraph g = JanusGraphFactory . build () . set ( \"storage.backend\" , \"hbase\" ) . set ( \"storage.hostname\" , \"77.77.77.77, 77.77.77.78, 77.77.77.79\" ) . open (); storage.hostname accepts a comma separated list of IP addresses and hostname for any subset of machines in the HBase cluster JanusGraph should connect to. Also, in the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration. Remote Server Mode with Gremlin Server Finally, Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph. http://gremlin-server.janusgraph.machine1/mygraph/vertices/1 http://gremlin-server.janusgraph.machine2/mygraph/tp/gremlin?script=g.v(1).out('follows').out('created') In this case, each Gremlin Server would be configured to connect to the HBase cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server. ... graphs : { g : conf/janusgraph-hbase.properties } scriptEngines : { gremlin-groovy : { plugins : { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin : {}, org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin : { classImports : [ java.lang.Math ], methodImports : [ java.lang.Math#* ]}, org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin : { files : [ scripts/empty-sample.groovy ]}}}} ... HBase Specific Configuration Refer to Configuration Reference for a complete listing of all HBase specific configuration options in addition to the general JanusGraph configuration options. When configuring HBase it is recommended to consider the following HBase specific configuration options: storage.hbase.table : Name of the HBase table in which to store the JanusGraph graph. Allows multiple JanusGraph graphs to co-exist in the same HBase cluster. Please refer to the HBase configuration documentation for more HBase configuration options and their description. By prefixing the respective HBase configuration option with storage.hbase.ext in the JanusGraph configuration it will be passed on to HBase at initialization time. For example, to use the znode /hbase-secure for HBase, set the property: storage.hbase.ext.zookeeper.znode.parent=/hbase-secure . The prefix allows arbitrary HBase configuration options to be configured through JanusGraph. Important HBase backend uses millisecond for timestamps. In JanusGraph 0.2.0 and earlier, if the graph.timestamps property is not explicitly set, the default is MICRO . In this case, the graph.timestamps property must be explicitly set to MILLI . Do not set the graph.timestamps property to another value in any cases. Global Graph Operations JanusGraph over HBase supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause OutOfMemoryException . Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively. Tips and Tricks for Managing an HBase Cluster The HBase shell on the master server can be used to get an overall status check of the cluster. $HBASE_HOME /bin/hbase shell From the shell, the following commands are generally useful for understanding the status of the cluster. status 'janusgraph' status 'simple' status 'detailed' The above commands can identify if a region server has gone down. If so, it is possible to ssh into the failed region server machines and do the following: sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh stop regionserver sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh start regionserver The use of pssh can make this process easy as there is no need to log into each machine individually to run the commands. Put the IP addresses of the regionservers into a hosts.txt file and then execute the following. pssh -h host.txt sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh stop regionserver pssh -h host.txt sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh start regionserver Next, sometimes you need to restart the master server (e.g. connection refused exceptions). To do so, on the master execute the following: sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh stop master sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh start master Finally, if an HBase cluster has already been deployed and more memory is required of the master or region servers, simply edit the $HBASE_HOME/conf/hbase-env.sh files on the respective machines with requisite -Xmx -Xms parameters. Once edited, stop/start the master and/or region servers as described previous.","title":"Apache HBase"},{"location":"storage-backend/hbase/#apache-hbase","text":"Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google\u2019s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS. \u2014 Apache HBase Homepage","title":"Apache HBase"},{"location":"storage-backend/hbase/#hbase-setup","text":"The following sections outline the various ways in which JanusGraph can be used in concert with Apache HBase.","title":"HBase Setup"},{"location":"storage-backend/hbase/#local-server-mode","text":"HBase can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and HBase communicate with one another via a localhost socket. Running JanusGraph over HBase requires the following setup steps: Download and extract a stable HBase from https://www.apache.org/dyn/closer.cgi/hbase/stable/ . Start HBase by invoking the start-hbase.sh script in the bin directory inside the extracted HBase directory. To stop HBase, use stop-hbase.sh . $ ./bin/start-hbase.sh starting master, logging to ../logs/hbase-master-machine-name.local.out Now, you can create an HBase JanusGraph as follows: JanusGraph graph = JanusGraphFactory . build () . set ( \"storage.backend\" , \"hbase\" ) . open (); Note, that you do not need to specify a hostname since a localhost connection is attempted by default. Also, in the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration.","title":"Local Server Mode"},{"location":"storage-backend/hbase/#remote-server-mode","text":"When the graph needs to scale beyond the confines of a single machine, then HBase and JanusGraph are logically separated into different machines. In this model, the HBase cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the HBase cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph. For example, suppose we have a running HBase cluster with a ZooKeeper quorum composed of three machines at IP address 77.77.77.77, 77.77.77.78, and 77.77.77.79, then connecting JanusGraph with the cluster is accomplished as follows: JanusGraph g = JanusGraphFactory . build () . set ( \"storage.backend\" , \"hbase\" ) . set ( \"storage.hostname\" , \"77.77.77.77, 77.77.77.78, 77.77.77.79\" ) . open (); storage.hostname accepts a comma separated list of IP addresses and hostname for any subset of machines in the HBase cluster JanusGraph should connect to. Also, in the Gremlin Console, you can not define the type of the variables conf and g . Therefore, simply leave off the type declaration.","title":"Remote Server Mode"},{"location":"storage-backend/hbase/#remote-server-mode-with-gremlin-server","text":"Finally, Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph. http://gremlin-server.janusgraph.machine1/mygraph/vertices/1 http://gremlin-server.janusgraph.machine2/mygraph/tp/gremlin?script=g.v(1).out('follows').out('created') In this case, each Gremlin Server would be configured to connect to the HBase cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server. ... graphs : { g : conf/janusgraph-hbase.properties } scriptEngines : { gremlin-groovy : { plugins : { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin : {}, org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin : {}, org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin : { classImports : [ java.lang.Math ], methodImports : [ java.lang.Math#* ]}, org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin : { files : [ scripts/empty-sample.groovy ]}}}} ...","title":"Remote Server Mode with Gremlin Server"},{"location":"storage-backend/hbase/#hbase-specific-configuration","text":"Refer to Configuration Reference for a complete listing of all HBase specific configuration options in addition to the general JanusGraph configuration options. When configuring HBase it is recommended to consider the following HBase specific configuration options: storage.hbase.table : Name of the HBase table in which to store the JanusGraph graph. Allows multiple JanusGraph graphs to co-exist in the same HBase cluster. Please refer to the HBase configuration documentation for more HBase configuration options and their description. By prefixing the respective HBase configuration option with storage.hbase.ext in the JanusGraph configuration it will be passed on to HBase at initialization time. For example, to use the znode /hbase-secure for HBase, set the property: storage.hbase.ext.zookeeper.znode.parent=/hbase-secure . The prefix allows arbitrary HBase configuration options to be configured through JanusGraph. Important HBase backend uses millisecond for timestamps. In JanusGraph 0.2.0 and earlier, if the graph.timestamps property is not explicitly set, the default is MICRO . In this case, the graph.timestamps property must be explicitly set to MILLI . Do not set the graph.timestamps property to another value in any cases.","title":"HBase Specific Configuration"},{"location":"storage-backend/hbase/#global-graph-operations","text":"JanusGraph over HBase supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause OutOfMemoryException . Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively.","title":"Global Graph Operations"},{"location":"storage-backend/hbase/#tips-and-tricks-for-managing-an-hbase-cluster","text":"The HBase shell on the master server can be used to get an overall status check of the cluster. $HBASE_HOME /bin/hbase shell From the shell, the following commands are generally useful for understanding the status of the cluster. status 'janusgraph' status 'simple' status 'detailed' The above commands can identify if a region server has gone down. If so, it is possible to ssh into the failed region server machines and do the following: sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh stop regionserver sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh start regionserver The use of pssh can make this process easy as there is no need to log into each machine individually to run the commands. Put the IP addresses of the regionservers into a hosts.txt file and then execute the following. pssh -h host.txt sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh stop regionserver pssh -h host.txt sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh start regionserver Next, sometimes you need to restart the master server (e.g. connection refused exceptions). To do so, on the master execute the following: sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh stop master sudo -u hadoop $HBASE_HOME /bin/hbase-daemon.sh start master Finally, if an HBase cluster has already been deployed and more memory is required of the master or region servers, simply edit the $HBASE_HOME/conf/hbase-env.sh files on the respective machines with requisite -Xmx -Xms parameters. Once edited, stop/start the master and/or region servers as described previous.","title":"Tips and Tricks for Managing an HBase Cluster"},{"location":"storage-backend/inmemorybackend/","text":"InMemory Storage Backend JanusGraph ships with an in-memory storage backend which can be used through the following configuration: storage.backend = inmemory Alternatively, an in-memory JanusGraph graph can be opened directly in the Gremlin Console: graph = JanusGraphFactory . build (). set ( ' storage . backend ' , ' inmemory ' ). open () There are no additional configuration options for the in-memory storage backend. As the name suggests, this backend holds all data in memory, specifically in the heap space allocated to the java virtual machine running Janusgraph instance. Shutting down the graph or terminating the process that hosts the JanusGraph graph will irrevocably delete all data from the graph. This backend is local to a particular JanusGraph graph instance and cannot be shared across multiple JanusGraph graphs. Ideal Use Case Rapid testing The in-memory storage backend was initially developed to simplify testing (for those tests that do not require persistence) and graph exploration. Automated testing and ad-hoc prototyping remains its main purpose within Janusgraph project. Production The initial test-only implementation was further evolved to use a more compact in-memory representation, which made it suitable for production use in certain scenarios, such as Janusgraph engine being embedded into an application process, or spawned dynamically on demand, where: Ease of setup/configuration/maintenance/start up is important (just run Janusgraph from distribution jar, no management of clusters for backend DBs etc) Loss of data due to unexpected death of host process is acceptable (the backend provides a simple mechanism for making fast snapshots to handle expected restarts, and the data can always be exported on Gremlin/Tinkerpop level to say GraphSON) Size of the graph data makes it possible to host it in a single JVM process (i.e. a few tens of Gigabytes max, unless you use a specialized JVM and hardware) Higher performance is required, but no expertise/resources available to tune more complex backends. Due to its memory-only nature, in-memory backend typically performs faster than disk-based ones, in queries using simple indices and in graph modifications. However it is not specifically optimized for performance, and does not support advanced indexing functionality. Limitations Obviously the scalability is limited to the heap size of a single JVM, and no transparent resilience to failures is offered The backend offers store-level locking only, whereas a Janusgraph transaction typically changes multiple stores (e.g. vertex store and index store). This means that in scenarios where data is modified in parallel transactions, care should be taken on application level to avoid conflicting updates. At a high level, this means that you can modify unrelated parts of the graph in parallel, but you should avoid changing the same vertex or its edges in parallel transactions. The backend does not guarantee a clean rollback once commit has started. Chances of a failure in the middle of commit to an in-memory data structure are low, however this can happen - e.g. when a large heap nears saturation and the GC pause exceeds configured backend timeout. The data layout used by the backend can theoretically be susceptible to fragmentation in certain scenarios (with a lot of add/delete operations), thus reducing the amount of useful data that can be stored in a heap of specified size. The backend provides simple mechanisms to report fragmentation and defragment the storage if required. However scenarios where the level of fragmentation is high enough to be an issue are expected to be rare, and usually defragmentation is not required at all. Alternatives Generally, except for the above class of use cases, there doesn't seem to be much point in specifically 100% in-memory-only backend. Many of the other supported backends, based on mature key-value databases, can be tuned to provide high performance and resiliency, plus advanced indexing capabilities etc (provided that you have resources and expertise to host and tune them, or use them as a service). However there are a few in-memory alternatives, such as (not exhaustive list, and in no particular order): BerkeleyJE backend configured with je.log.memOnly set to true (care should be taken to avoid using it for scenarios with continuous write operations, even if the effective size of the data remains the same - continuous write operations can drive uncontrolled log growth, leading to OOM). The backend has dump/load capability, and supports compression. Aerospike-based backend (not part of Janugraph but a separate project)","title":"InMemory Storage Backend"},{"location":"storage-backend/inmemorybackend/#inmemory-storage-backend","text":"JanusGraph ships with an in-memory storage backend which can be used through the following configuration: storage.backend = inmemory Alternatively, an in-memory JanusGraph graph can be opened directly in the Gremlin Console: graph = JanusGraphFactory . build (). set ( ' storage . backend ' , ' inmemory ' ). open () There are no additional configuration options for the in-memory storage backend. As the name suggests, this backend holds all data in memory, specifically in the heap space allocated to the java virtual machine running Janusgraph instance. Shutting down the graph or terminating the process that hosts the JanusGraph graph will irrevocably delete all data from the graph. This backend is local to a particular JanusGraph graph instance and cannot be shared across multiple JanusGraph graphs.","title":"InMemory Storage Backend"},{"location":"storage-backend/inmemorybackend/#ideal-use-case","text":"","title":"Ideal Use Case"},{"location":"storage-backend/inmemorybackend/#rapid-testing","text":"The in-memory storage backend was initially developed to simplify testing (for those tests that do not require persistence) and graph exploration. Automated testing and ad-hoc prototyping remains its main purpose within Janusgraph project.","title":"Rapid testing"},{"location":"storage-backend/inmemorybackend/#production","text":"The initial test-only implementation was further evolved to use a more compact in-memory representation, which made it suitable for production use in certain scenarios, such as Janusgraph engine being embedded into an application process, or spawned dynamically on demand, where: Ease of setup/configuration/maintenance/start up is important (just run Janusgraph from distribution jar, no management of clusters for backend DBs etc) Loss of data due to unexpected death of host process is acceptable (the backend provides a simple mechanism for making fast snapshots to handle expected restarts, and the data can always be exported on Gremlin/Tinkerpop level to say GraphSON) Size of the graph data makes it possible to host it in a single JVM process (i.e. a few tens of Gigabytes max, unless you use a specialized JVM and hardware) Higher performance is required, but no expertise/resources available to tune more complex backends. Due to its memory-only nature, in-memory backend typically performs faster than disk-based ones, in queries using simple indices and in graph modifications. However it is not specifically optimized for performance, and does not support advanced indexing functionality.","title":"Production"},{"location":"storage-backend/inmemorybackend/#limitations","text":"Obviously the scalability is limited to the heap size of a single JVM, and no transparent resilience to failures is offered The backend offers store-level locking only, whereas a Janusgraph transaction typically changes multiple stores (e.g. vertex store and index store). This means that in scenarios where data is modified in parallel transactions, care should be taken on application level to avoid conflicting updates. At a high level, this means that you can modify unrelated parts of the graph in parallel, but you should avoid changing the same vertex or its edges in parallel transactions. The backend does not guarantee a clean rollback once commit has started. Chances of a failure in the middle of commit to an in-memory data structure are low, however this can happen - e.g. when a large heap nears saturation and the GC pause exceeds configured backend timeout. The data layout used by the backend can theoretically be susceptible to fragmentation in certain scenarios (with a lot of add/delete operations), thus reducing the amount of useful data that can be stored in a heap of specified size. The backend provides simple mechanisms to report fragmentation and defragment the storage if required. However scenarios where the level of fragmentation is high enough to be an issue are expected to be rare, and usually defragmentation is not required at all.","title":"Limitations"},{"location":"storage-backend/inmemorybackend/#alternatives","text":"Generally, except for the above class of use cases, there doesn't seem to be much point in specifically 100% in-memory-only backend. Many of the other supported backends, based on mature key-value databases, can be tuned to provide high performance and resiliency, plus advanced indexing capabilities etc (provided that you have resources and expertise to host and tune them, or use them as a service). However there are a few in-memory alternatives, such as (not exhaustive list, and in no particular order): BerkeleyJE backend configured with je.log.memOnly set to true (care should be taken to avoid using it for scenarios with continuous write operations, even if the effective size of the data remains the same - continuous write operations can drive uncontrolled log growth, leading to OOM). The backend has dump/load capability, and supports compression. Aerospike-based backend (not part of Janugraph but a separate project)","title":"Alternatives"},{"location":"storage-backend/scylladb/","text":"ScyllaDB ScyllaDB is a NoSQL database with a close-to-the-hardware, shared-nothing approach that optimizes raw performance, fully utilizes modern multi-core servers, and minimizes the overhead to DevOps. ScyllaDB is API-compatible with both Cassandra and DynamoDB, yet is much faster, more consistent, and with a lower TCO. \u2014 ScyllaDB Homepage ScyllaDB Setup and Connection ScyllaDB is fully compatible with Cassandra. To use it as the data storage layer: Spin up a Scylla cluster. You can do this using Scylla Cloud, using Docker, running Scylla in the cloud or on-prem. You can see a step-by-step guide on how to spin up a three node Scylla cluster using Docker in this lesson. Run JanusGraph with \u201ccql\u201d as the storage.backend. Specify the IP address of one of the Scylla nodes in your cluster as the storage.hostname. Step by Step Tutorial This Scylla University lesson provides step-by-step instructions for using JanusGraph with ScyllaDB as the data storage layer. The main steps in the lesson are: Spinning up a virtual machine Installing the prerequisites Running the JanusGraph server (using Docker) Running a Gremlin Console to connect to the new server (also in Docker) Spinning up a three-node Scylla Cluster and setting it as the data storage for the JanusGraph server Performing some basic graph operations Usage of Scylla optimized driver JanusGraph provides scylla storage.backend options in addition to generic cql option. ScyllaDB can work using any of those storage options, but the dedicated scylla backend is better optimized for ScyllaDB. Thus, it's recommended to use scylla backend option with ScyllaDB whenever possible. scylla storage option is included in janusgraph-scylla library but this library isn't shipped via janusgraph-all and this library is not part of the provided JanusGraph distributions. To make scylla option available in JanusGraph it's necessary to replace org.janusgraph:janusgraph-cql with org.janusgraph:janusgraph-scylla if JanusGraph is used in Embedded Mode. In case janusgraph-all dependency is used instead of explicit JanusGraph dependencies then the replacement to org.janusgraph:janusgraph-scylla can be done like below. Maven <dependencies> <!-- Exclude `janusgraph-cql` from `janusgraph-all` --> <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-all </artifactId> <version> 1.0.0 </version> <exclusions> <exclusion> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-cql </artifactId> </exclusion> </exclusions> </dependency> <!-- Include `janusgraph-scylla` --> <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-scylla </artifactId> <version> 1.0.0 </version> </dependency> </dependencies> Gradle /* Exclude `janusgraph-cql` from `janusgraph-all` */ compile ( \"org.janusgraph:janusgraph-all:1.0.0\" ) { exclude group: 'org.janusgraph' , module: 'janusgraph-cql' } /* Include `janusgraph-scylla` */ compile \"org.janusgraph:janusgraph-scylla:1.0.0\" Using Scylla driver in JanusGraph Server provided via the standard distribution builds Note The manual process described below is temporary and should be replaced by automated process after the issue janusgraph/janusgraph#3580 is resolved. In case scylla storage option is needed to be used in JanusGraph distribution then it's necessary to replace the next libraries in lib directory (all libraries can be downloaded via Maven Central Repository). - org.janusgraph:janusgraph-cql with org.janusgraph:janusgraph-scylla - org.janusgraph:cassandra-hadoop-util with org.janusgraph:scylla-hadoop-util - com.datastax.oss:java-driver-core with com.scylladb:java-driver-core - com.datastax.oss:java-driver-query-builder with com.scylladb:java-driver-query-builder - com.datastax.cassandra:cassandra-driver-core with com.scylladb:scylla-driver-core The versions of com.scylladb:java-driver-core and com.scylladb:java-driver-query-builder should be equal to the version found in pom.xml of org.janusgraph:janusgraph-scylla . The version of com.scylladb:scylla-driver-core should be equal to the version found in org.janusgraph:scylla-hadoop-util . After replacement is done it will be possible to use scylla as storage.backend options which will use the optimized Scylla driver.","title":"ScyllaDB"},{"location":"storage-backend/scylladb/#scylladb","text":"ScyllaDB is a NoSQL database with a close-to-the-hardware, shared-nothing approach that optimizes raw performance, fully utilizes modern multi-core servers, and minimizes the overhead to DevOps. ScyllaDB is API-compatible with both Cassandra and DynamoDB, yet is much faster, more consistent, and with a lower TCO. \u2014 ScyllaDB Homepage","title":"ScyllaDB"},{"location":"storage-backend/scylladb/#scylladb-setup-and-connection","text":"ScyllaDB is fully compatible with Cassandra. To use it as the data storage layer: Spin up a Scylla cluster. You can do this using Scylla Cloud, using Docker, running Scylla in the cloud or on-prem. You can see a step-by-step guide on how to spin up a three node Scylla cluster using Docker in this lesson. Run JanusGraph with \u201ccql\u201d as the storage.backend. Specify the IP address of one of the Scylla nodes in your cluster as the storage.hostname.","title":"ScyllaDB Setup and Connection"},{"location":"storage-backend/scylladb/#step-by-step-tutorial","text":"This Scylla University lesson provides step-by-step instructions for using JanusGraph with ScyllaDB as the data storage layer. The main steps in the lesson are: Spinning up a virtual machine Installing the prerequisites Running the JanusGraph server (using Docker) Running a Gremlin Console to connect to the new server (also in Docker) Spinning up a three-node Scylla Cluster and setting it as the data storage for the JanusGraph server Performing some basic graph operations","title":"Step by Step Tutorial"},{"location":"storage-backend/scylladb/#usage-of-scylla-optimized-driver","text":"JanusGraph provides scylla storage.backend options in addition to generic cql option. ScyllaDB can work using any of those storage options, but the dedicated scylla backend is better optimized for ScyllaDB. Thus, it's recommended to use scylla backend option with ScyllaDB whenever possible. scylla storage option is included in janusgraph-scylla library but this library isn't shipped via janusgraph-all and this library is not part of the provided JanusGraph distributions. To make scylla option available in JanusGraph it's necessary to replace org.janusgraph:janusgraph-cql with org.janusgraph:janusgraph-scylla if JanusGraph is used in Embedded Mode. In case janusgraph-all dependency is used instead of explicit JanusGraph dependencies then the replacement to org.janusgraph:janusgraph-scylla can be done like below. Maven <dependencies> <!-- Exclude `janusgraph-cql` from `janusgraph-all` --> <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-all </artifactId> <version> 1.0.0 </version> <exclusions> <exclusion> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-cql </artifactId> </exclusion> </exclusions> </dependency> <!-- Include `janusgraph-scylla` --> <dependency> <groupId> org.janusgraph </groupId> <artifactId> janusgraph-scylla </artifactId> <version> 1.0.0 </version> </dependency> </dependencies> Gradle /* Exclude `janusgraph-cql` from `janusgraph-all` */ compile ( \"org.janusgraph:janusgraph-all:1.0.0\" ) { exclude group: 'org.janusgraph' , module: 'janusgraph-cql' } /* Include `janusgraph-scylla` */ compile \"org.janusgraph:janusgraph-scylla:1.0.0\"","title":"Usage of Scylla optimized driver"},{"location":"storage-backend/scylladb/#using-scylla-driver-in-janusgraph-server-provided-via-the-standard-distribution-builds","text":"Note The manual process described below is temporary and should be replaced by automated process after the issue janusgraph/janusgraph#3580 is resolved. In case scylla storage option is needed to be used in JanusGraph distribution then it's necessary to replace the next libraries in lib directory (all libraries can be downloaded via Maven Central Repository). - org.janusgraph:janusgraph-cql with org.janusgraph:janusgraph-scylla - org.janusgraph:cassandra-hadoop-util with org.janusgraph:scylla-hadoop-util - com.datastax.oss:java-driver-core with com.scylladb:java-driver-core - com.datastax.oss:java-driver-query-builder with com.scylladb:java-driver-query-builder - com.datastax.cassandra:cassandra-driver-core with com.scylladb:scylla-driver-core The versions of com.scylladb:java-driver-core and com.scylladb:java-driver-query-builder should be equal to the version found in pom.xml of org.janusgraph:janusgraph-scylla . The version of com.scylladb:scylla-driver-core should be equal to the version found in org.janusgraph:scylla-hadoop-util . After replacement is done it will be possible to use scylla as storage.backend options which will use the optimized Scylla driver.","title":"Using Scylla driver in JanusGraph Server provided via the standard distribution builds"}]}