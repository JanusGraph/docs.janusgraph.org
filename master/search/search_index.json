{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#the-benefits-of-janusgraph","title":"The Benefits of JanusGraph","text":"<p>JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions.</p>"},{"location":"#general-janusgraph-benefits","title":"General JanusGraph Benefits","text":"<ul> <li>Support for very large graphs. JanusGraph graphs scale with the     number of machines in the cluster.</li> <li>Support for very many concurrent transactions and operational graph     processing. JanusGraph\u2019s transactional capacity scales with the     number of machines in the cluster and answers complex traversal     queries on huge graphs in milliseconds.</li> <li>Support for global graph analytics and batch graph processing     through the Hadoop framework.</li> <li>Support for geo, numeric range, and full text search for vertices     and edges on very large graphs.</li> <li>Native support for the popular property graph data model exposed by     Apache TinkerPop.</li> <li>Native support for the graph traversal language     Gremlin.</li> <li>Numerous graph-level configurations provide knobs for tuning     performance.</li> <li>Vertex-centric indices provide vertex-level querying to alleviate     issues with the infamous super node problem.</li> <li>Provides an optimized disk representation to allow for efficient use     of storage and speed of access.</li> <li>Open source under the liberal Apache 2 license.</li> </ul>"},{"location":"#benefits-of-janusgraph-with-apache-cassandra","title":"Benefits of JanusGraph with Apache Cassandra","text":"<ul> <li>Continuously available     with no single point of failure.</li> <li> <p>No read/write bottlenecks to the graph as there is no master/slave     architecture.</p> </li> <li> <p>Elastic scalability allows     for the introduction and removal of machines.</p> </li> <li>Caching layer ensures that continuously accessed data is available     in memory.</li> <li>Increase the size of the cache by adding more machines to the     cluster.</li> <li>Integration with Apache Hadoop.</li> <li>Open source under the liberal Apache 2 license.</li> </ul>"},{"location":"#benefits-of-janusgraph-with-hbase","title":"Benefits of JanusGraph with HBase","text":"<ul> <li>Tight integration with the Apache Hadoop ecosystem.</li> <li>Native support for strong consistency.</li> <li>Linear scalability with the addition of more machines.</li> <li>Strictly consistent reads and writes.</li> <li>Convenient base classes for backing Hadoop     MapReduce jobs with HBase     tables.</li> <li>Support for exporting metrics via     JMX.</li> <li>Open source under the liberal Apache 2 license.</li> </ul>"},{"location":"#janusgraph-and-the-cap-theorem","title":"JanusGraph and the CAP Theorem","text":"<p>Despite your best efforts, your system will experience enough faults that it will have to make a choice between reducing yield (i.e., stop answering requests) and reducing harvest (i.e., giving answers based on incomplete data). This decision should be based on business requirements.</p> <p>\u2014  Coda Hale</p> <p>When using a database, the CAP theorem should be thoroughly considered (C=Consistency, A=Availability, P=Partition tolerance). JanusGraph is distributed with 3 supporting backends: Apache Cassandra,  Apache HBase, and Oracle Berkeley DB Java Edition. Note that BerkeleyDB JE is a non-distributed database and is typically only used with JanusGraph for testing and exploration purposes.</p> <p>HBase gives preference to consistency at the expense of yield, i.e. the probability of completing a request. Cassandra gives preference to availability at the expense of harvest, i.e. the completeness of the answer to the query (data available/complete data).</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#version-compatibility","title":"Version Compatibility","text":"<p>The JanusGraph project is growing along with the rest of the graph and big data ecosystem and utilized storage and indexing backends. Below are version compatibilities between the various versions of components. For dependent backend systems, different minor versions are typically supported as well. It is strongly encouraged to verify version compatibility prior to deploying JanusGraph.</p> <p>Although JanusGraph may be compatible with older and no longer supported versions of its dependencies, users are warned that there are possible risks and security exposures with running software that is no longer supported or updated. Please check with the software providers to understand their supported versions. Users are strongly encouraged to use the latest versions of the software.</p>"},{"location":"changelog/#version-compatibility-matrix","title":"Version Compatibility Matrix","text":""},{"location":"changelog/#currently-supported","title":"Currently supported","text":"<p>All currently supported versions of JanusGraph are listed below. </p> <p>Info</p> <p>You are currently viewing the documentation page of JanusGraph version 1.0.0. To ensure that the information below is up to date, please double check that this is not an archived version of the documentation.</p> JanusGraph Storage Version Cassandra HBase Bigtable ScyllaDB Elasticsearch Solr TinkerPop Spark Scala 1.1.z 2 3.11.z, 4.0.z 2.6.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z 5.y 6.y, 7.y, 8.y 8.y 3.7.z 3.2.z 2.12.z 1.0.z 2 3.11.z, 4.0.z 2.5.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z 5.y 6.y, 7.y, 8.y 8.y 3.7.z 3.2.z 2.12.z <p>Info</p> <p>Even so ScyllaDB is marked as <code>N/A</code> prior version 1.0.0 it was actually supported using <code>cql</code> storage option.  The only difference is that from version 1.0.0 JanusGraph officially supports ScyllaDB using <code>scylla</code> and <code>cql</code>  storage options and have extended test coverage for ScyllaDB. </p>"},{"location":"changelog/#end-of-life","title":"End-of-Life","text":"<p>The versions of JanusGraph listed below are outdated and will no longer receive bugfixes.</p> JanusGraph Storage Version Cassandra HBase Bigtable ScyllaDB Elasticsearch Solr TinkerPop Spark Scala 0.1.z 1 1.2.z, 2.0.z, 2.1.z 0.98.z, 1.0.z, 1.1.z, 1.2.z 0.9.z, 1.0.0-preZ, 1.0.0 N/A 1.5.z 5.2.z 3.2.z 1.6.z 2.10.z 0.2.z 1 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 0.98.z, 1.0.z, 1.1.z, 1.2.z, 1.3.z 0.9.z, 1.0.0-preZ, 1.0.0 N/A 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.2.z 1.6.z 2.10.z 0.3.z 2 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.0.z, 1.1.z, 1.2.z, 1.3.z, 1.4.z 1.0.0, 1.1.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0 N/A 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.3.z 2.2.z 2.11.z 0.4.z 2 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.2.z, 1.3.z, 1.4.z, 2.1.z N/A N/A 5.y, 6.y 7.y 3.4.z 2.2.z 2.11.z 0.5.z 2 2.1.z, 2.2.z, 3.0.z, 3.11.z 1.2.z, 1.3.z, 1.4.z, 2.1.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z N/A 6.y, 7.y 7.y 3.4.z 2.2.z 2.11.z 0.6.z 2 3.0.z, 3.11.z 1.6.z, 2.2.z 1.3.0, 1.4.0, 1.5.z, 1.6.z, 1.7.z, 1.8.z, 1.9.z, 1.10.z, 1.11.z, 1.14.z N/A 6.y, 7.y 7.y, 8.y 3.5.z 3.0.z 2.12.z"},{"location":"changelog/#release-notes","title":"Release Notes","text":""},{"location":"changelog/#version-110-release-date","title":"Version 1.1.0 (Release Date: ???)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;1.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:1.1.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.11.10, 4.0.6</li> <li>Apache HBase 2.6.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>ScyllaDB 5.1.4</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.17.8, 8.10.4</li> <li>Apache Lucene 8.11.1</li> <li>Apache Solr 8.11.1</li> <li>Apache TinkerPop 3.7.3</li> <li>Java 8, 11</li> </ul> <p>Installed versions in the Pre-Packaged Distribution:</p> <ul> <li>Cassandra 4.0.6</li> <li>Elasticsearch 7.14.0</li> </ul>"},{"location":"changelog/#changes","title":"Changes","text":"<p>For more information on features and bug fixes in 1.1.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/27?closed=1</li> </ul>"},{"location":"changelog/#assets","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#upgrade-instructions","title":"Upgrade Instructions","text":""},{"location":"changelog/#inlining-vertex-properties-into-a-composite-index","title":"Inlining vertex properties into a Composite Index","text":"<p>Inlining vertex properties into a Composite Index structure can offer significant performance and efficiency benefits. See documentation on how to inline vertex properties into a composite index.</p> <p>Warning</p> <p>Important Notes on Compatibility.  1. Backward Incompatibility:     Once a JanusGraph instance adopts this new schema feature, it cannot be rolled back to a prior version of JanusGraph.    The changes in the schema structure are not compatible with earlier versions of the system. 2. Migration Considerations:     It is critical that users carefully plan their migration to this new version, as there is no automated or manual rollback process    to revert to an older version of JanusGraph once this feature is used.</p>"},{"location":"changelog/#berkeleyje-ability-to-overwrite-arbitrary-settings-applied-at-environmentconfig-creation","title":"BerkeleyJE ability to overwrite arbitrary settings applied at <code>EnvironmentConfig</code> creation","text":"<p>The new namespace <code>storage.berkeleyje.ext</code> now allows to set custom configurations which were not directly exposed by  JanusGraph. The full list of possible setting is available inside the Java class <code>com.sleepycat.je.EnvironmentConfig</code>. All configurations values should be specified as <code>String</code> and be formated the same as specified in the official sleepycat documentation. Example: <code>storage.berkeleyje.ext.je.lock.timeout=5000 ms</code></p>"},{"location":"changelog/#json-schema-initializer","title":"JSON schema initializer","text":"<p>For simplicity JSON schema initialization options has been added into JanusGraph. See documentation to learn more about JSON schema initialization process.</p>"},{"location":"changelog/#batched-queries-enhancement-introduction-of-janusgraphnoopbarriervertexonlystep","title":"Batched Queries Enhancement: Introduction of <code>JanusGraphNoOpBarrierVertexOnlyStep</code>","text":"<p>In previous versions, when a query that could benefit from batch-query optimization (multi-query) was executed without a user-defined barrier step, JanusGraph would inject a <code>NoOpBarrierStep</code> by default. This approach allowed batching for edges and properties, which do not gain advantages from multi-query optimization.</p> <p>Starting with JanusGraph 1.1.0, this behavior has been improved. The system now injects a <code>JanusGraphNoOpBarrierVertexOnlyStep</code> instead of the standard <code>NoOpBarrierStep</code> when no barrier steps are detected. This change ensures that batching is applied exclusively to vertices, which do benefit from batch queries, while excluding edges and properties from the batching process.</p> <p>If a user explicitly defines a <code>.barrier()</code> step in the query, the system will continue to use the <code>NoOpBarrierStep</code> as expected.</p>"},{"location":"changelog/#batch-query-optimizations-now-support-traversals-containing-the-drop-step","title":"Batch Query Optimizations Now Support Traversals Containing the <code>drop()</code> Step","text":"<p>Starting with JanusGraph 1.1.0, batch optimizations for vertex removal have been introduced in the <code>drop()</code> step and are enabled by default. Previously, any batch optimization would be skipped for queries containing at least one <code>drop()</code> step. However, with this update, such queries are now eligible for batch query optimization (multi-query).</p> <p>Please note that the <code>LazyBarrierStrategy</code> (a TinkerPop strategy) is disabled for any query that includes at least one <code>drop()</code> step.</p> <p>To disable the <code>drop()</code> step optimization and maintain the previous behavior, users can set the following configuration: <pre><code>query.batch.drop-step-mode=none\n</code></pre></p>"},{"location":"changelog/#version-101-release-date","title":"Version 1.0.1 (Release Date: ???)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;1.0.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:1.0.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.11.10, 4.0.6</li> <li>Apache HBase 2.5.8</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>ScyllaDB 5.1.4</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.17.8, 8.10.4</li> <li>Apache Lucene 8.11.1</li> <li>Apache Solr 8.11.1</li> <li>Apache TinkerPop 3.7.0</li> <li>Java 8, 11</li> </ul> <p>Installed versions in the Pre-Packaged Distribution:</p> <ul> <li>Cassandra 4.0.6</li> <li>Elasticsearch 7.14.0</li> </ul>"},{"location":"changelog/#changes_1","title":"Changes","text":"<p>For more information on features and bug fixes in 1.0.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/28?closed=1</li> </ul>"},{"location":"changelog/#assets_1","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#version-100-release-date-october-21-2023","title":"Version 1.0.0 (Release Date: October 21, 2023)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:1.0.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.11.10, 4.0.6</li> <li>Apache HBase 2.5.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>ScyllaDB 5.1.4</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.17.8, 8.10.4</li> <li>Apache Lucene 8.11.1</li> <li>Apache Solr 8.11.1</li> <li>Apache TinkerPop 3.7.0</li> <li>Java 8, 11</li> </ul> <p>Note</p> <p>Google Bigtable was removed from this list because there is no automatic testing in place specifically for that backend. Since the adapter for Bigtable is however just using the HBase adapter, it is also covered by the tests for HBase.</p> <p>We invite anyone who is interested in the Bigtable storage adapter to help with this by contributing so that the tests for HBase are also automatically executed for Bigtable. More information can be found in this GitHub issue: janusgraph/janusgraph#415.</p> <p>Installed versions in the Pre-Packaged Distribution:</p> <ul> <li>Cassandra 4.0.6</li> <li>Elasticsearch 7.14.0</li> </ul>"},{"location":"changelog/#changes_2","title":"Changes","text":"<p>For more information on features and bug fixes in 1.0.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/21?closed=1</li> </ul>"},{"location":"changelog/#assets_2","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#upgrade-instructions_1","title":"Upgrade Instructions","text":""},{"location":"changelog/#upgrade-tinkerpop-from-35x-to-370-breaking","title":"Upgrade TinkerPop from 3.5.x to 3.7.0 (breaking)","text":"<p>Some packages under <code>gremlin-driver</code> are moved to <code>gremlin-util</code> module. This means you need to change some classnames of serializers, e.g. from <code>org.apache.tinkerpop.gremlin.driver.ser.*</code> to <code>org.apache.tinkerpop.gremlin.util.ser.*</code>. See this for more detail.</p> <p>GraphSON serializers are renamed, e.g. from <code>org.apache.tinkerpop.gremlin.driver.ser.GraphSONMessageSerializerV3d0</code> to <code>org.apache.tinkerpop.gremlin.util.ser.GraphSONMessageSerializerV3</code>. See this for more detail.</p>"},{"location":"changelog/#string-vertex-id-support-breaking-change","title":"String vertex ID support (breaking change)","text":"<p>Users now can use custom string vertex ids. See Custom Vertex ID documentation. Prior to this change, JanusGraph automatically casts IDs of string type to long type if possible. Now this auto conversion is disabled. If you have a vertex with ID 1234, <code>g.V(\"1234\")</code> would no longer help you find the vertex - you would have to do <code>g.V(1234)</code> now.</p> <p>This feature brings about a breaking change to GraphBinary serializer. As such, users who use GraphBinary serialization format must update JanusGraph server and all clients at once, since the change is backward incompatible.</p> <p>Warning</p> <p>Even if you don't enable string vertex id feature, you are still impacted as long as you use GraphBinary serializer.</p>"},{"location":"changelog/#upgrade-of-log4j-to-version-2","title":"Upgrade of log4j to version 2","text":"<p>This change requires a new log4j configuration. You can find an example configuration in <code>conf/log4j2-server.xml</code>. As a result of the changed configuration format, we clean up all configurations. This could lead to unexpected new log lines. Please open an issue, if you see any unwanted log line.</p> <p>Note</p> <p>Log4j is only used for standalone server deployments and JanusGraph testing.</p>"},{"location":"changelog/#removal-of-cassandra-all-dependency","title":"Removal of cassandra-all dependency","text":"<p>JanusGraph had a dependency on cassandra-all only for some Hadoop-related classes. We moved these few classes into a new module cassandra-hadoop-util to reduce the dependencies of JanusGraph. If you are running embedded JanusGraph with Cassandra, you have to exclude the <code>cassandra-hadoop-util</code> from <code>janusgraph-cql</code>.</p>"},{"location":"changelog/#drop-support-for-hbase-1","title":"Drop support for HBase 1","text":"<p>We are dropping support for HBase 1.</p>"},{"location":"changelog/#drop-support-for-solr-7","title":"Drop support for Solr 7","text":"<p>We are dropping support for Solr 7.</p>"},{"location":"changelog/#drop-support-for-gryo-messageserializer","title":"Drop support for Gryo MessageSerializer","text":"<p>Support for Gryo MessageSerializer has been dropped in TinkerPop 3.6.0 and we therefore also no longer support it in JanusGraph. GraphBinary is now used as the default MessageSerializer.</p>"},{"location":"changelog/#remove-support-for-old-serialization-format-of-janusgraph-predicates","title":"Remove support for old serialization format of JanusGraph predicates","text":"<p>We are dropping support for old serialization format of JanusGraph predicates. The old predicates serialization format is only used by client older than 0.6. The change only affects GraphSON.</p>"},{"location":"changelog/#allow-removal-of-configuration-keys","title":"Allow removal of configuration keys","text":"<p>Users can now remove configuration keys in the ConfiguredGraphFactory's configuration:</p> <pre><code>ConfiguredGraphFactory.removeConfiguration(\"&lt;graph_name&gt;\", Collections.singleton(\"&lt;config_key&gt;\"))\n</code></pre> <p>Or the global configuration: <pre><code>mgmt = graph.openManagement()\nmgmt.remove(\"&lt;config_key&gt;\")\nmgmt.commit()\n</code></pre></p> <p>Note that the above commands should be used with care. They cannot be used to drop an external index backend if it has mixed indexes for instance.</p>"},{"location":"changelog/#new-index-management","title":"New index management","text":"<p>The index management has received an overhaul which enables proper index removal. The schema action <code>REMOVE_INDEX</code> is no longer available and has been replaced by <code>DISCARD_INDEX</code>. See Index Lifecycle documentation for more details.</p>"},{"location":"changelog/#totals-for-direct-index-queries-now-applies-provided-offset-and-limit","title":"<code>totals</code> for direct index queries now applies provided offset and limit","text":"<p>Direct index queries which search count for totals (<code>vertexTotals</code>, <code>edgeTotals</code>, <code>propertyTotals</code> and direct execution of  <code>IndexProvider.totals</code>) now apply provided <code>limit</code> and <code>offset</code>. Previously provided <code>limit</code> and <code>offset</code> were ignored.   For example, previously the following query would return <code>500</code> if there were <code>500</code> indexed elements: </p> <pre><code>gremlin&gt; graph.indexQuery(\"textIndex\", \"v.\\\"text\\\":fooBar\").limit(10).vertexTotals()\n</code></pre> <p>Now the above query will return <code>10</code> elements because we limited the result to <code>10</code> elements only. Same applies to <code>offset</code>. Previously the following query would return <code>500</code> if there were <code>500</code> indexed elements:</p> <pre><code>gremlin&gt; graph.indexQuery(\"textIndex\", \"v.\\\"text\\\":fooBar\").offset(10).vertexTotals()\n</code></pre> <p>Now the above query will return <code>490</code> as a result because we skip count of the first <code>10</code> elements.   <code>offset</code> provided with <code>limit</code> will apply <code>offset</code> first and <code>limit</code> last. For example, the above query will return <code>10</code> elements now if there were <code>500</code> indexed elements:</p> <pre><code>gremlin&gt; graph.indexQuery(\"textIndex\", \"v.\\\"text\\\":fooBar\").limit(10).offset(10).vertexTotals()\n</code></pre> <p>The new logic is applied similarly to Direct Index Queries <code>vertexTotals()</code>, <code>edgeTotals()</code>, <code>propertyTotals()</code> as well  as internal JanusGraph method <code>IndexProvider.totals</code>.</p>"},{"location":"changelog/#add-support-for-java-11","title":"Add support for Java 11","text":"<p>JanusGraph now officially supports Java 11 in addition to Java 8. We encourage everyone to update to Java 11.</p> <p>Note</p> <p>The pre-packaged distribution now requires Java 11.</p>"},{"location":"changelog/#batch-processing-enabled-by-default-configuration-changes","title":"Batch Processing enabled by default. Configuration changes.","text":"<p><code>query.batch</code> is now a configuration namespace. Thus, previous <code>query.batch</code> configuration is replaced by <code>query.batch.enabled</code>. <code>query.limit-batch-size</code> configuration option is changed to <code>query.batch.limited</code>.</p> <p><code>query.batch-property-prefetch</code> was replaced by a better configurable option. In case previous behaviour is desired then use <code>query.batch.has-step-mode = none</code> as replacement for <code>query.batch-property-prefetch = false</code> or use <code>query.batch.has-step-mode = all_properties</code> as replacement for <code>query.batch-property-prefetch = true</code>.</p> <p><code>query.fast-property</code> has no influence on <code>values</code>, <code>properties</code>, <code>valueMap</code>, <code>propertyMap</code>, <code>elementMap</code> anymore when  <code>query.batch.enabled</code> is <code>true</code>. By default, those steps are configured to fetch only required properties  (with separate query per property), but the behaviour can be changed with the configuration <code>query.batch.properties-mode</code>.  In case previous behavior is desired, use <code>query.batch.properties-mode = required_properties_only</code> for <code>query.fast-property = false</code>  or use <code>query.batch.properties-mode = all_properties</code> for <code>query.fast-property = true</code>. </p> <p><code>label</code> step now uses pre-fetching strategy by default. Use <code>query.batch.label-step-mode = none</code> to disable pre-fetching  optimization for <code>label</code> step.</p> <p>Batch processing allows JanusGraph to fetch a batch of vertices from the storage backend together instead of requesting each vertex individually which leads to a high number of backend queries. This was however disabled by default in JanusGraph because these batches could become much larger than what was needed for the traversal and therefore have a negative performance impact for some traversals. That is why an improved batch processing mode was added in JanusGraph 0.6.0 that limits the size of these batches retrieved from the storage backend, called Limited Batch Processing. This mode therefore solves the problem of having potentially unlimited batch sizes. That is why we now enable this mode by default as most users should benefit from this limited batch processing.</p> <p>If you want to continue using JanusGraph without batch processing, then you have to manually disable it by setting <code>query.batch.enabled</code> to <code>false</code>.</p> <p>The size of the batches can be limited by using <code>barrier()</code> steps if limited batch processing is used (<code>query.batch.limited</code> set to <code>true</code>).  A special strategy exists which already inserts <code>barrier()</code> steps by default for some steps, the <code>LazyBarrierStrategy</code>.  A new configuration option <code>query.batch.limited-size</code> exists to configure default barrier step size for batch processing  for batch cases when <code>LazyBarrierStrategy</code> not applied <code>.barrier</code> step and no user-provided barrier step exists for  batchable query part. Notice, that <code>query.batch.limited-size</code> is only used when <code>query.batch.limited</code> is <code>true</code> (default in this version).  </p>"},{"location":"changelog/#batch-registration-for-nested-batch-compatible-steps-is-changed-for-repeat-step","title":"Batch registration for nested batch compatible steps is changed for <code>repeat</code> step","text":"<p>Previously any batch compatible steps like <code>out</code>, <code>in</code>, <code>values</code>, etc. would receive vertices for batch registration  from all <code>repeat</code> parent steps, but only for their starts in case of multi-nested repeat steps  (skipping their subsequent iterations registration).  With JanusGraph 1.0.0 batches registration for the subsequent iterations of multi-nested repeat steps are used as well.  </p> <p><pre><code>g.V(startVertexId).emit().\n    repeat(__.repeat(__.in(\"connects\")).emit()).\n    until(__.loops().is(P.gt(2)))\n</code></pre> In the example above multi-nested <code>repeat</code> case would not register vertices returned from the inner <code>emit()</code> step for the next outer iteration which would result in sequential calls of <code>in(\"connects\")</code> for next outer iteration. The behaviour is now changed to register these vertices for the next child <code>repeat</code> step start.</p> <p>The behaviour can be controlled by <code>query.batch.repeat-step-mode</code> configuration option. In case the old behaviour is preferable then <code>query.batch.repeat-step-mode</code> should be set to <code>starts_only_of_all_repeat_parents</code>.</p> <p>However, in cases when transaction cache is small and repeat step traverses more than one level  deep, it could result for some vertices to be re-fetched again which would mean a waste of operation when it isn't necessary.  In such situations <code>closest_repeat_parent</code> mode might be more preferable than <code>all_repeat_parents</code>. With <code>closest_repeat_parent</code> mode vertices for batch registration will be received from the start of the closest  <code>repeat</code> step as well as the end of the closest <code>repeat</code> step (for the next iteration). Any other parent <code>repeat</code> steps  will be ignored.</p>"},{"location":"changelog/#configuredgraphfactory-now-creates-separate-indices-per-graph-in-elasticsearch","title":"ConfiguredGraphFactory now creates separate indices per graph in Elasticsearch","text":"<p>If the ConfiguredGraphFactory is used together with Elasticsearch as the index backend, then the same Elasticsearch index is used for all graphs (if the same index names were used across different graphs). Now it is possible to let JanusGraph create the index names dynamically by using the <code>graph.graphname</code> if no <code>index.[X].index-name</code> is provided in the template configuration. This is exactly like it was already the case for the CQL keyspace name for example.</p> <p>Users who don't want to use this feature can simply continue providing the index name via <code>index.[X].index-name</code> in the template configuration.</p>"},{"location":"changelog/#mixed-index-aggregation-optimization","title":"Mixed index aggregation optimization","text":"<p>A new optimization has been added to compute aggregations (min, max, sum and avg) using mixed index engine (if the aggregation function follows an indexed query). If the index backend is Elasticsearch, a <code>double</code> value is used to hold the result. As a result, aggregations on long numbers greater than 2^53 are approximate. In this case, if the accurate result is essential, the optimization can be disabled by removing the strategy <code>JanusGraphMixedIndexAggStrategy</code>: <code>g.traversal().withoutStrategies(JanusGraphMixedIndexAggStrategy.class)</code>.</p>"},{"location":"changelog/#add-support-for-elasticsearch-8","title":"Add support for ElasticSearch 8","text":"<p>JanusGraph now supports ElasticSearch 8.  Notice, <code>Mapping.PREFIX_TREE</code> mapping is no longer available for Geoshape mappings using new ElasticSearch 8 indices. <code>Mapping.PREFIX_TREE</code> is still supported in ElasticSearch 6, ElasticSearch 7, Solr, Lucene.    For ElasticSearch the new Geoshape mapping was added <code>Mapping.BKD</code>. It's recommended to use <code>Mapping.BKD</code> mapping due to better performance characteristics over <code>Mapping.PREFIX_TREE</code>.  The downside of <code>Mapping.BKD</code> is that it doesn't support Circle shapes. Thus, JanusGraph provides BKD Circle processors to convert Circle into other shapes for indexing but use Circle at the storage level. More information about Circle processors available under configuration namespace <code>index.[X].bkd-circle-processor</code>.   ElasticSearch 8 doesn't allow creating new indexes with <code>Mapping.PREFIX_TREE</code> mapping, but the existing indices using <code>Mapping.PREFIX_TREE</code> will work in ElasticSearch 8 after migration. See ElasticSearch 8 migration guide.</p>"},{"location":"changelog/#add-support-for-scylladb-driver","title":"Add support for ScyllaDB driver","text":"<p>A new module <code>janusgraph-scylla</code> provides ability to run JanusGraph with ScyllaDB Driver which is an optimized fork version of DataStax Java Driver for ScyllaDB storage backend.  For ScyllaDB storage backend you can use either <code>janusgraph-scylla</code> or <code>janusgraph-cql</code> (which is a general CQL storage driver implementation). That said, it's recommended to use <code>janusgraph-scylla</code> for ScyllaDB due to the provided internal optimizations (more about ScyllaDB driver optimizations can be found here).</p> <p>Notice that <code>janusgraph-cql</code> and <code>janusgraph-scylla</code> are mutually exclusive. Use only one module at a time and never provide both dependencies in the same classpath. See ScyllaDB Storage Backend documentation for more information about how to make <code>scylla</code> <code>storage.backend</code> options available.</p>"},{"location":"changelog/#cql-executorservice-purpose-change","title":"CQL ExecutorService purpose change","text":"<p>Previously CQL ExecutorService was used to control parallelism of both CQL IO operations and results deserialization.  Starting from JanusGraph 1.0.0 CQL ExecutorService is now used for CQL results deserialization only. All CQL IO operations  are now using internal async approach. The default pool size is now set to have a value of <code>number of cores multiplied by 2</code>. This ExecutorService is now  mandatory and cannot be disabled. The default ExecutorService core pool size is not recommended to be changed as  the default value is considered to be optimal unless users want to artificially limit parallelism of CQL results deserialization  jobs.</p>"},{"location":"changelog/#a-new-multi-query-method-added-into-keycolumnvaluestore-affects-storage-adapters-implementations-only","title":"A new multi-query method added into <code>KeyColumnValueStore</code> (affects storage adapters implementations only)","text":"<p>A new method <code>Map&lt;SliceQuery, Map&lt;StaticBuffer, EntryList&gt;&gt; getMultiSlices(MultiKeysQueryGroups&lt;StaticBuffer, SliceQuery&gt; multiKeysQueryGroups, StoreTransaction txh)</code>  is added into <code>KeyColumnValueStore</code> which is now preferred for multi-queries (batch queries) with multiple slice queries. In case a multi-query executes more than one Slice query per multi-query execution then those Slice queries will be  grouped per same key sets and the new <code>getMultiSlices</code> query will be executed with groups of different <code>SliceQuery</code> for the same sets of keys.  </p> <p>Notice, if the storage doesn't have multiQuery feature enabled - the method won't be used. Hence, it's not necessary to implement it.  </p> <p>In case storage backend has multiQuery feature enabled, then it is highly recommended to overwrite the default (non-optimized) implementation  and optimize this method execution to execute all the slice queries for all the requested keys in the shortest time possible  (for example, by using asynchronous programming, slice queries grouping, multi-threaded execution, or any other technique which  is efficient for the respective storage adapter).  </p>"},{"location":"changelog/#added-possibility-to-group-multiple-slice-queries-together-via-cql-storage-backend","title":"Added possibility to group multiple slice queries together via CQL storage backend","text":"<p>Starting from JanusGraph 1.0.0 CQL storage implementation now groups queries which fetch properties with <code>Cardinality.SINGLE</code> together into the same CQL query. The behaviour can be disabled by setting configuration <code>storage.cql.grouping.slice-allowed = false</code>. </p> <p>CQL storage implementation has also ability to group queries of different partition keys together if they belong to the same  token range or same replicas set (grouping strategy is available via <code>storage.cql.grouping.keys-class</code> configuration option).  This behaviour is disabled by default, but can be enabled via <code>storage.cql.grouping.keys-allowed = true</code>.  Please note that enabling the key grouping feature can increase the return size of related queries.  This is due to each row in a CQL query, which encompasses grouped keys, needing to return its specific key.  Moreover, it could potentially lead to less balanced load on the storage cluster. However, it reduces the amount of CQL  queries sent which may positively influence throughput in some cases as well as pricing point of some Serverless deployments.  We recommend to benchmark each use-case before enabling keys grouping (<code>storage.cql.grouping.keys-allowed</code>).</p> <p>Notice, keys grouping (<code>storage.cql.grouping.keys-allowed</code>) can be used only with storage backends which support <code>PER PARTITION LIMIT</code>.  As such, this feature can't be used with Amazon Keyspaces because it doesn't support <code>PER PARTITION LIMIT</code>.</p> <p>Different storage backends may also have restriction set on maximum keys which can be provided via <code>IN</code> operator (which is needed for grouping).  It is required to ensure that <code>storage.cql.grouping.keys-limit</code> or <code>storage.cql.grouping.slice-limit</code> is less than or equal to the  restriction provided via the storage backend. On ScyllaDB it's possible to configure this restriction using max-partition-key-restrictions-per-query  configuration option (default to <code>100</code>). On AstraDB side it is needed to be asked on AstraDB side to be changed via <code>partition_keys_in_select_failure_threshold</code> and <code>in_select_cartesian_product_failure_threshold</code> threshold  configurations (https://docs.datastax.com/en/astra-serverless/docs/plan/planning.html#_cassandra_yaml) which are set to <code>20</code> and <code>25</code> by default.</p> <p>See additional properties to control grouping configurations under the namespace <code>storage.cql.grouping</code>.</p>"},{"location":"changelog/#removal-of-deprecated-classesmethodsfunctionalities","title":"Removal of deprecated classes/methods/functionalities","text":""},{"location":"changelog/#methods","title":"Methods","text":"<ul> <li>JanusGraphIndexQuery.vertices replaced by JanusGraphIndexQuery.vertexStream</li> <li>JanusGraphIndexQuery.edges replaced by JanusGraphIndexQuery.edgeStream</li> <li>JanusGraphIndexQuery.properties replaced by JanusGraphIndexQuery.propertyStream</li> <li>IndexQueryBuilder.vertices replaced by IndexQueryBuilder.vertexStream</li> <li>IndexQueryBuilder.edges replaced by IndexQueryBuilder.edgeStream</li> <li>IndexQueryBuilder.properties replaced by IndexQueryBuilder.propertyStream</li> <li>IndexTransaction.query replaced by IndexTransaction.queryStream</li> </ul>"},{"location":"changelog/#classesinterfaces","title":"Classes/Interfaces","text":"<ul> <li>EdgeLabelDefinition class</li> <li>PropertyKeyDefinition class</li> <li>RelationTypeDefinition class</li> <li>SchemaContainer class</li> <li>SchemaElementDefinition class</li> <li>SchemaProvider interface</li> <li>VertexLabelDefinition class</li> <li>JanusGraphId class</li> <li>AllEdgesIterable class</li> <li>AllEdgesIterator class</li> <li>ConcurrentLRUCache class</li> <li>PriorityQueue class</li> <li>RemovableRelationIterable class</li> <li>RemovableRelationIterator class</li> <li>ImmutableConfiguration class</li> </ul>"},{"location":"changelog/#version-064-release-date-october-14-2023","title":"Version 0.6.4 (Release Date: October 14, 2023)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.6.4&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.6.4\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.0.14, 3.11.10</li> <li>Apache HBase 1.6.0, 2.2.7</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.14.0</li> <li>Apache Lucene 8.9.0</li> <li>Apache Solr 7.7.2, 8.11.0</li> <li>Apache TinkerPop 3.5.7</li> <li>Java 1.8</li> </ul>"},{"location":"changelog/#changes_3","title":"Changes","text":"<p>For more information on features and bug fixes in 0.6.4, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/26?closed=1</li> </ul>"},{"location":"changelog/#assets_3","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#upgrade-instructions_2","title":"Upgrade Instructions","text":""},{"location":"changelog/#default-logging-library-changed-to-reload4j","title":"Default logging library changed to Reload4j","text":"<p>The default logging library used in the pre-packaged distribution has been changed in version 0.6.3 by accident from Log4j to Logback. While this change meant that some security issues of Log4j were avoided, it was also a breaking change that was not intended. This resulted in only warnings being logged by default and also that a Log4j config file was ignored. To fix this breaking change, we change the default logging library in this release to Reload4j which is completely compatible with Log4j, but fixes the security issues of Log4j. This means that Log4j config files will continue to work with this version.</p> <p>Note that this only applies to JanusGraph 0.6. JanusGraph 1.0.0 uses Log4j2 by default.</p>"},{"location":"changelog/#version-063-release-date-february-18-2023","title":"Version 0.6.3 (Release Date: February 18, 2023)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.6.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.6.3\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.0.14, 3.11.10</li> <li>Apache HBase 1.6.0, 2.2.7</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.14.0</li> <li>Apache Lucene 8.9.0</li> <li>Apache Solr 7.7.2, 8.11.0</li> <li>Apache TinkerPop 3.5.5</li> <li>Java 1.8</li> </ul> <p>Note</p> <p>Google Bigtable was removed from this list because there is no automatic testing in place specifically for that backend. Since the adapter for Bigtable is however just using the HBase adapter, it is also covered by the tests for HBase.</p> <p>We invite anyone who is interested in the Bigtable storage adapter to help with this by contributing so that the tests for HBase are also automatically executed for Bigtable. More information can be found in this GitHub issue: janusgraph/janusgraph#415.</p>"},{"location":"changelog/#changes_4","title":"Changes","text":"<p>For more information on features and bug fixes in 0.6.3, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/24?closed=1</li> </ul>"},{"location":"changelog/#assets_4","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#version-062-release-date-may-31-2022","title":"Version 0.6.2 (Release Date: May 31, 2022)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.6.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.6.2\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.0.14, 3.11.10</li> <li>Apache HBase 1.6.0, 2.2.7</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.14.0</li> <li>Apache Lucene 8.9.0</li> <li>Apache Solr 7.7.2, 8.9.0</li> <li>Apache TinkerPop 3.5.3</li> <li>Java 1.8</li> </ul>"},{"location":"changelog/#changes_5","title":"Changes","text":"<p>For more information on features and bug fixes in 0.6.2, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/23?closed=1</li> </ul>"},{"location":"changelog/#assets_5","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#version-061-release-date-january-18-2022","title":"Version 0.6.1 (Release Date: January 18, 2022)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.6.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.6.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.0.14, 3.11.10</li> <li>Apache HBase 1.6.0, 2.2.7</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.14.0</li> <li>Apache Lucene 8.9.0</li> <li>Apache Solr 7.7.2, 8.9.0</li> <li>Apache TinkerPop 3.5.1</li> <li>Java 1.8</li> </ul>"},{"location":"changelog/#changes_6","title":"Changes","text":"<p>For more information on features and bug fixes in 0.6.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/22?closed=1</li> </ul>"},{"location":"changelog/#assets_6","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#upgrade-instructions_3","title":"Upgrade Instructions","text":""},{"location":"changelog/#graphmanager-changed-to-janusgraphmanager","title":"GraphManager changed to JanusGraphManager","text":"<p>A <code>GraphManager</code> is used to instantiate graph instances. JanusGraph Server has used the <code>DefaultGraphManager</code> from TinkerPop for this by default if no other <code>GraphManager</code> was specified in the JanusGraph Server YAML config file. The behavior of this <code>DefaultGraphManager</code> was changed in TinkerPop 3.5.0 which is included in JanusGraph 0.6.0 in how it parses config values, making it impossible to provide comma separated values, e.g., to specify multiple hostnames for the storage backend. The <code>JanusGraphManager</code> does not have this limitation which is why it is now configured as the <code>GraphManager</code> in the JanusGraph Server config files:</p> <pre><code>[...]\nchannelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer\ngraphManager: org.janusgraph.graphdb.management.JanusGraphManager\ngraphs: {\n  graph: conf/janusgraph-berkeleyje-es.properties\n}\n[...]\n</code></pre> <p>If you however want to continue using the <code>DefaultGraphManager</code>, then you can simply remove the setting again or change it to the TinkerPop <code>GraphManager</code> that has been the default before: <code>org.apache.tinkerpop.gremlin.server.util.DefaultGraphManager</code>.</p>"},{"location":"changelog/#version-060-release-date-september-3-2021","title":"Version 0.6.0 (Release Date: September 3, 2021)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.6.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.6.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 3.0.14, 3.11.10</li> <li>Apache HBase 1.6.0, 2.2.7</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.14.0</li> <li>Apache Lucene 8.9.0</li> <li>Apache Solr 7.7.2, 8.9.0</li> <li>Apache TinkerPop 3.5.3</li> <li>Java 1.8</li> </ul>"},{"location":"changelog/#changes_7","title":"Changes","text":"<p>For more information on features and bug fixes in 0.6.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/17?closed=1</li> </ul>"},{"location":"changelog/#assets_7","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#upgrade-instructions_4","title":"Upgrade Instructions","text":""},{"location":"changelog/#experimental-support-for-amazon-keyspaces","title":"Experimental support for Amazon Keyspaces","text":"<p>Amazon Keyspaces is a serverless managed Apache Cassandra-compatible database service provided by Amazon. See Deploying on Amazon Keyspaces for more details.</p>"},{"location":"changelog/#breaking-change-for-configuration-objects","title":"Breaking change for Configuration objects","text":"<p>Prior to JanusGraph 0.6.0, <code>Configuration</code> objects were from the Apache <code>commons-configuration</code> library. To comply with the TinkerPop change, JanusGraph now uses the <code>commons-configuration2</code> library. A typical usage of configuration object is to create configuration using <code>ConfigurationGraphFactory</code>. Now you would need to use the new configuration2 library. Please refer to the commons-configuration 2.0 migration guide for details. Note that this very likely does not affect gremlin console usage, since the new library is auto-imported, and the basic APIs remain the same. For java code usage, you need to import configuration2 library rather than the old configuration library.</p>"},{"location":"changelog/#breaking-change-for-gremlin-server-configs","title":"Breaking change for gremlin server configs","text":"<p><code>scriptEvaluationTimeout</code> is renamed to <code>evaluationTimeout</code>. You can refer to <code>conf/gremlin-server/gremlin-server.yaml</code> for example.</p>"},{"location":"changelog/#breaking-change-for-gremlin-eventstrategy-usage","title":"Breaking change for gremlin EventStrategy usage","text":"<p>If you are using EventStrategy, please note that now you need to register it every time you start a new transaction. An example is available at ThreadLocalTxLeakTest::eventListenersCanBeReusedAcrossTx See more background of this breaking change in this pull request.</p>"},{"location":"changelog/#disable-smart-limit-by-default-and-change-hard_max_limit","title":"Disable smart-limit by default and change HARD_MAX_LIMIT","text":"<p>Prior to 0.6.0, <code>smart-limit</code> is enabled by default. It tries to guess a small limit for each graph centric query (e.g. <code>g.V().has(\"prop\", \"value\")</code>) internally, and if more results are required by user, it queries backend again with a larger limit, and repeats until either results are exhausted or user stops the query. However, this is not the same as paging mechanism. All interim results will be fetched again in next round, making the whole query costly. Even worse, if your data backend does not return results in a consistent order, then some entries might be missing in the final results. Until JanusGraph can fully utilize the paging capacity provided by backends (e.g. Elasticsearch scroll), this option is recommended to be turned off. The exception is when you have a large number of results but you only need a few of them, then enabling <code>smart-limit</code> can reduce latency and memory usage. An example would be:</p> <pre><code>Iterator&lt;Vertex&gt; iter = graph.traversal().V().has(\"prop\", \"value\");\nwhile (iter.hasNext()) {\n    Vertex v = iter.next();\n    if (canStop()) break;\n}\n</code></pre> <p>Prior to 0.6.0, even if <code>smart-limit</code> is disabled, JanusGraph adds a <code>HARD_MAX_LIMIT</code> that is equivalent to 100,000 to avoid fetching too many results at a time. This limit is now configurable, and by default, it's Integer.MAX_VALUE which can be interpreted as no limit.</p>"},{"location":"changelog/#add-experimental-support-for-java-11","title":"Add experimental support for Java 11","text":"<p>We started to work on support for Java 11. We would like to get feedback, if everything is working as expected after upgrading to Java 11.</p>"},{"location":"changelog/#removal-of-loggingschemamaker","title":"Removal of LoggingSchemaMaker","text":"<p>The <code>schema.default=logging</code> option is not valid anymore. Use <code>schema.default=default</code> and <code>schema.logging=true</code> options together to make application behaviour unaltered, if you are using <code>LoggingSchemaMaker</code>.</p>"},{"location":"changelog/#replacing-the-server-startup-script-is-replaced","title":"Replacing the server startup script is replaced","text":"<p>The <code>gremlin-server.sh</code> is placed by <code>janusgraph-server.sh</code>. The  <code>janusgraph-server.sh</code> brings some new functionality such as easy  configuration of Java options using the <code>jvm.options</code> file.</p> <p>The <code>jvm.options</code> file contains some default configurations for JVM based  on Cassandra's JVM configurations, Elasticsearch and the old gremlin-server.sh.</p>"},{"location":"changelog/#serialization-of-janusgraph-predicates-has-changed","title":"Serialization of JanusGraph predicates has changed","text":"<p>The serialization of JanusGraph predicates has changed in this version for both  GraphSON and Gryo. The newest version of the JanusGraph Driver requires a JanusGraph  Server version of 0.6.0 and above. The server includes a fallback for clients with an  older driver to make the upgrade to version 0.6.0 easier. This means that the server  can be upgraded first without having to update all clients at the same time. The  fallback will however be removed in a future version of JanusGraph so clients should  also be upgraded.</p>"},{"location":"changelog/#graphbinary-is-now-supported","title":"GraphBinary is now supported","text":"<p>GraphBinary is a  new binary serialization format from TinkerPop that supersedes Gryo and it will  eventually also replace GraphSON. GraphBinary is language independent and has a  low serialization overhead which results in an improved performance.</p> <p>If you want to use GraphBinary, you have to add following to the <code>gremlin-server.yaml</code>  after the keyword <code>serializers</code>. This will add the support on the server site. </p> <pre><code>    - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, \n        config: { ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] }}\n    - { className: org.apache.tinkerpop.gremlin.driver.ser.GraphBinaryMessageSerializerV1, \n        config: { serializeResultToString: true }}\n</code></pre> <p>Note</p> <p>The java driver is the only driver that currently supports GraphBinary,  see Connecting to JanusGraph using Java.</p> <p>Note</p> <p>Version 1.0.0 moves everything under <code>org.apache.tinkerpop.gremlin.driver.ser</code> package to <code>org.apache.tinkerpop.gremlin.util.ser</code> package.</p> <p>Note</p> <p>Version 1.0.0 adds a breaking change to GraphBinary for Geoshape serialization, see the 1.0.0 changelog for more information.</p>"},{"location":"changelog/#new-index-selection-algorithm","title":"New index selection algorithm","text":"<p>In version 0.6.0, the index selection algorithm has changed. If the number of possible indexes for a query is small enough, the new algorithm will perform an exhaustive search to minimize the number of indexes which need to be queried. The default limit is set to 10. In order to maintain the old selection algorithm regardless of the available indexes, set the key <code>query.index-select-threshold</code> to <code>0</code>. For more information, see Configuration Reference</p>"},{"location":"changelog/#removal-of-cassandra-thrift-support","title":"Removal of Cassandra Thrift support","text":"<p>Thrift will be completely removed in Cassandra 4. All deprecated Cassandra Thrift backends were removed in JanusGraph 0.6.0. We already added support for CQL in JanusGraph 0.2.0 and we have been  encouraging users to switch from Thrift to CQL since version 0.2.1.</p> <p>This means that the following backends were removed:  <code>cassandrathrift</code>, <code>cassandra</code>, <code>astyanax</code>, and <code>embeddedcassandra</code>. Users who still use one of these Thrift backends should migrate to CQL. Our migration guide explains the  necessary steps for this. The option to run Cassandra embedded  in the same JVM as JanusGraph is however no longer supported with CQL.</p> <p>Note</p> <p>The source code for the Thrift backends will be moved into a  dedicated repository. While we do not support them any more, users can still use them  if they for some reason cannot migrate to CQL.</p>"},{"location":"changelog/#drop-support-for-cassandra-2","title":"Drop support for Cassandra 2","text":"<p>With the release of Cassandra 4, the support of Cassandra 2 will be dropped.  Therefore, you should upgrade to Cassandra 3 or higher.</p> <p>Note</p> <p>Cassandra 3 and higher doesn't support compact storage. If you have activated  or never changed the value of <code>storage.cql.storage-compact=true</code>, during the  upgrade process you have to ensure your data is correctly migrated.</p>"},{"location":"changelog/#introduction-of-a-janusgraph-server-startup-class-as-a-replacement-for-gremlin-server-startup","title":"Introduction of a JanusGraph Server startup class as a replacement for Gremlin Server startup","text":"<p>The <code>gremlin-server.sh</code> and the <code>janusgraph.sh</code> are configured to use the new JanusGraph startup class.  This new class introduces a default set of TinkerPop Serializers if no serializers are configured in  the <code>gremlin-server.yaml</code>. Furthermore, JanusGraph will log the version of JanusGraph and TinkerPop  after a shiny new JanusGraph header.</p> <p>Note</p> <p>If you have a custom script to startup JanusGraph, you propably would like to replace the Gremlin Server  class with JanusGraph Server class: </p> <p><code>org.apache.tinkerpop.gremlin.server.GremlinServer</code> =&gt; <code>org.janusgraph.graphdb.server.JanusGraphServer</code></p>"},{"location":"changelog/#drop-support-for-ganglia-metrics","title":"Drop support for Ganglia metrics","text":"<p>We are dropping Ganglia as we are using dropwizard for metrics. Dropwizard did drop Ganglia in the newest major version.</p>"},{"location":"changelog/#datastax-cassandra-driver-upgrade-from-390-to-4130","title":"DataStax cassandra driver upgrade from 3.9.0 to 4.13.0","text":"<p>All DataStax cassandra driver metrics are now disabled by default. To enable DataStax driver metrics you need to provide  a list of Session level metrics and / or Node level metrics you want to enable. To provide a list of enabled metrics,  you can use the next configuration options: <code>storage.cql.metrics.session-enabled</code> and <code>storage.cql.metrics.node-enabled</code>.  Notice, DataStax metrics are enabled only when basic metrics are enabled (i.e. <code>metrics.enabled = true</code>). See configuration references <code>storage.cql.metrics</code> for additional DataStax metrics configuration.</p> <p>An example configuration which enables some CQL Session level and Node level metrics reporting by JMX: <pre><code>metrics.enabled=true\nmetrics.jmx.enabled=true\nmetrics.jmx.domain=com.datastax.oss.driver\nmetrics.jmx.agentid=agent\nstorage.cql.metrics.session-enabled=bytes-sent,bytes-received,connected-nodes,cql-requests,throttling.delay\nstorage.cql.metrics.node-enabled=pool.open-connections,pool.available-streams,bytes-sent,cql-messages\n</code></pre></p> <p>See <code>advanced.metrics.session.enabled</code> and <code>advanced.metrics.node.enabled</code> sections in  DataStax Metrics Configuration  for a complete list of available Session level and Node level metrics.</p> <p>Due to driver upgrade the next cql configuration options have been removed:</p> <ul> <li><code>local-core-connections-per-host</code></li> <li><code>remote-core-connections-per-host</code></li> <li><code>local-max-requests-per-connection</code></li> <li><code>remote-max-requests-per-connection</code></li> <li><code>cluster-name</code></li> </ul> <p><code>storage.connection-timeout</code> is now used to control initial connection timeout to CQL storage and not request timeouts.  Please, use <code>storage.cql.request-timeout</code> to configure request timeouts instead.</p> <p>New cql configuration options should be used for upgrade:</p> <ul> <li><code>max-requests-per-connection</code></li> <li><code>session-name</code></li> </ul> <p><code>storage.cql.local-datacenter</code> is mandatory now and defaults to <code>datacenter1</code>.</p> <p>See more new cql configuration options in configuration references under <code>storage.cql</code> section.</p>"},{"location":"changelog/#automatic-configurations-of-dynamic-graph-binding","title":"Automatic configurations of dynamic graph binding","text":"<p>If the JanusGraphManager is configured, dynamic graph binding will be setup automatically,  see Dynamic Graphs.</p> <p>Note</p> <p>Breaking changes in the config of the <code>gremlin-server.yaml</code>.</p> <p>Following, classes are removed and have to be replaced by tinkerpop equivalent:</p> removed class replacement class <code>org.janusgraph.channelizers.JanusGraphWebSocketChannelizer</code> <code>org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer</code> <code>org.janusgraph.channelizers.JanusGraphHttpChannelizer</code> <code>org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer</code> <code>org.janusgraph.channelizers.JanusGraphNioChannelizer</code> <code>org.apache.tinkerpop.gremlin.server.channel.NioChannelizer</code> <code>org.janusgraph.channelizers.JanusGraphWsAndHttpChannelizer</code> <code>org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer</code>"},{"location":"changelog/#breaking-change-lucene-and-solr-fuzzy-predicates","title":"Breaking change Lucene and Solr fuzzy predicates","text":"<p>The text predicates <code>text.textFuzzy</code> and <code>text.textContainsFuzzy</code> have been updated in both the Lucene and Solr indexing backends to align with JanusGraph and Elastic. These predicates now inspect the query length to determine the Levenshtein distance, where previously they used the backend's default max distance of 2:</p> <ul> <li>0 for strings of one or two characters (exact match)</li> <li>1 for strings of three, four or five characters</li> <li>2 for strings of more than five characters</li> </ul> <p>Change Matrix:</p> text query previous result new result ah ah true true ah ai true false hop hop true true hop hap true true hop hoop true true hop hooop true false surprises surprises true true surprises surprizes true true surprises surpprises true true surprises surpprisess false false"},{"location":"changelog/#version-053-release-date-december-24-2020","title":"Version 0.5.3 (Release Date: December 24, 2020)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.5.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.5.3\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.6.2</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 7.0.0</li> <li>Apache TinkerPop 3.4.6</li> <li>Java 1.8</li> </ul>"},{"location":"changelog/#changes_8","title":"Changes","text":"<p>For more information on features and bug fixes in 0.5.3, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/20?closed=1</li> </ul>"},{"location":"changelog/#assets_8","title":"Assets","text":"<ul> <li>JavaDoc</li> <li>GitHub Release</li> <li>JanusGraph zip</li> <li>JanusGraph zip with embedded Cassandra and ElasticSearch</li> </ul>"},{"location":"changelog/#version-052-release-date-may-3-2020","title":"Version 0.5.2 (Release Date: May 3, 2020)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.5.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.5.2\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.6.2</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 7.0.0</li> <li>Apache TinkerPop 3.4.6</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.5.2, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/19?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_5","title":"Upgrade Instructions","text":""},{"location":"changelog/#elasticsearch-index-store-names-cache-now-enabled-for-any-amount-of-indexes-per-store","title":"ElasticSearch index store names cache now enabled for any amount of indexes per store","text":"<p>In JanusGraph version <code>0.5.0</code> and <code>0.5.1</code> all ElasticSearch index store names are cached for efficient index store name  retrieval and the cache is disabled if there are more than <code>50000</code> indexes available per index store.  From JanusGraph version <code>0.5.2</code> index store names cache isn't limited to <code>50000</code> but instead can be disabled by using  a new added parameter <code>enable_index_names_cache</code>. It is still recommended to disable index store names cache if more  than <code>50000</code> indexes are used per index store.</p>"},{"location":"changelog/#version-051-release-date-march-25-2020","title":"Version 0.5.1 (Release Date: March 25, 2020)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.5.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.5.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.14.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.6.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 7.0.0</li> <li>Apache TinkerPop 3.4.6</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.5.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/18?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_6","title":"Upgrade Instructions","text":""},{"location":"changelog/#two-distributed-package-is-splitted-into-two-version","title":"Two Distributed package is splitted into two version","text":"<p>The default version of the distribution package does no longer contain the <code>janusgraph.sh</code>.  This includes a packaged version of cassandra and elasticsearch. If you want to have <code>janusgraph.sh</code>,  you have to download distribution with the suffix <code>-full</code>.</p>"},{"location":"changelog/#gremlin-server-distributed-with-the-release-uses-inmemory-storage-backend-and-no-search-backend-by-default","title":"Gremlin Server distributed with the release uses inmemory storage backend and no search backend by default","text":"<p>Gremlin Server is by default configured for the inmemory storage backend and no search backend when started with  <code>bin/gremlin-server.sh</code>. You can provide configuration for another storage backend and/or search backend by providing a path to the appropriate  configuration as a second parameter (<code>./bin/gremlin-server.sh ./conf/gremlin-server/[...].yaml</code>).</p>"},{"location":"changelog/#version-050-release-date-march-10-2020","title":"Version 0.5.0 (Release Date: March 10, 2020)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.5.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.5.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 6.0.1, 6.6.0, 7.6.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 7.0.0</li> <li>Apache TinkerPop 3.4.6</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.5.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/13?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_7","title":"Upgrade Instructions","text":""},{"location":"changelog/#distributed-package-is-renamed","title":"Distributed package is renamed","text":"<p>The distribution has no longer the suffix <code>-hadoop2</code>.</p>"},{"location":"changelog/#reorder-dependency-of-hadoop","title":"Reorder dependency of Hadoop","text":"<p>Hadoop is now a dependency of supported backends. Therefore, <code>MapReduceIndexJobs</code> is now split up into different classes:</p> Old Function New Function <code>MapReduceIndexJobs.cassandraRepair</code> <code>CassandraMapReduceIndexJobsUtils.repair</code> <code>MapReduceIndexJobs.cassandraRemove</code> <code>CassandraMapReduceIndexJobsUtils.remove</code> <code>MapReduceIndexJobs.cqlRepair</code> <code>CqlMapReduceIndexJobsUtils.repair</code> <code>MapReduceIndexJobs.cqlRemove</code> <code>CqlMapReduceIndexJobsUtils.remove</code> <code>MapReduceIndexJobs.hbaseRepair</code> <code>HBaseMapReduceIndexJobsUtils.repair</code> <code>MapReduceIndexJobs.hbaseRemove</code> <code>HBaseMapReduceIndexJobsUtils.remove</code> <p>Note</p> <p>Now, you can easily support for any backend.</p> <p>Warning</p> <p><code>Cassandra3InputFormat</code> is replaced by <code>CqlInputFormat</code></p>"},{"location":"changelog/#elasticsearch-upgrade-from-660-to-761-and-drop-support-for-5x-version","title":"ElasticSearch: Upgrade from 6.6.0 to 7.6.1 and drop support for 5.x version","text":"<p>The ElasticSearch version has been changed to 7.6.1 which removes support for <code>max-retry-timeout</code> option.  That is why this option no longer available in JanusGraph. Users should be aware that by default JanusGraph setups maximum open scroll contexts to maximum value of <code>2147483647</code> with the parameter <code>setup-max-open-scroll-contexts</code> for ElasticSearch 7.y.  This option can be disabled and updated manually in ElasticSearch but you should be aware that ElasticSearch starting from version 7 has a default limit of 500 opened contexts  which most likely be reached by the normal usage of JanusGraph with ElasticSearch. By default deprecated mappings are disabled in ElasticSearch version 7.  If you are upgrading your ElasticSearch index backend to version 7 from lower versions,  it is recommended to reindex your JanusGraph indices to not use mappings.  If you are unable to reindex your indices you may setup parameter <code>use-mapping-for-es7</code> to <code>true</code>  which will tell JanusGraph to use mapping types for ElasticSearch version 7. Due to the drop of support for 5.x version, deprecated multi-type indices are no more supported.  Parameter <code>use-deprecated-multitype-index</code> is no more supported by JanusGraph.</p>"},{"location":"changelog/#berkeleydb","title":"BerkeleyDB","text":"<p>BerkeleyDB storage configured with SHARED_CACHE for better memory usage.</p>"},{"location":"changelog/#default-logging-location-has-changed","title":"Default logging location has changed","text":"<p>If you are using <code>janusgraph.sh</code> to start your instance, the default logging has been changed from <code>log</code> to <code>logs</code></p>"},{"location":"changelog/#in-memory-backend-moved-into-dedicated-module","title":"In-Memory backend moved into dedicated module","text":"<p>The built-in in-memory backend has been moved into a dedicated module. Users who use it for instance in tests, have to explicitly declare it as a dependency:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-inmemory&lt;/artifactId&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n    &lt;version&gt;0.5.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>implementation 'org.janusgraph:janusgraph-inmemory:0.5.0'\n</code></pre>"},{"location":"changelog/#version-041-release-date-january-14-2020","title":"Version 0.4.1 (Release Date: January 14, 2020)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.4.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.4.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 5.6.14, 6.0.1, 6.6.0</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 7.0.0</li> <li>Apache TinkerPop 3.4.4</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.4.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/15?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_8","title":"Upgrade Instructions","text":""},{"location":"changelog/#tinkerpop-upgrade-from-341-to-344","title":"TinkerPop: Upgrade from 3.4.1 to 3.4.4","text":"<p>Adding multiple values in the same query to a new vertex property without explicitly defined type  (i.e. using <code>Automatic Schema Maker</code> to create a property type) requires explicit usage of <code>VertexProperty.Cardinality</code>  for each call (only for the first query which defines a property) if the <code>VertexProperty.Cardinality</code> is different than  <code>VertexProperty.Cardinality.single</code>.</p>"},{"location":"changelog/#version-040-release-date-july-1-2019","title":"Version 0.4.0 (Release Date: July 1, 2019)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.4.0/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.4.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.10, 2.1.5</li> <li>Google Bigtable 1.3.0, 1.4.0, 1.5.0, 1.6.0, 1.7.0, 1.8.0, 1.9.0, 1.10.0, 1.11.0</li> <li>Oracle BerkeleyJE 7.5.11</li> <li>Elasticsearch 5.6.14, 6.0.1, 6.6.0</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 7.0.0</li> <li>Apache TinkerPop 3.4.1</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.4.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/8?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_9","title":"Upgrade Instructions","text":""},{"location":"changelog/#hbase-upgrade-from-12-to-21","title":"HBase: Upgrade from 1.2 to 2.1","text":"<p>The version of HBase that is included in the distribution of JanusGraph was upgraded from 1.2.6 to 2.1.5. HBase 2.x client is not fully backward compatible with HBase 1.x server. Users who operate their own HBase version 1.x cluster may need to upgrade their cluster to version 2.x. Optionally users may build their own distribution of JanusGraph which includes HBase 1.x from source with the maven flags -Dhbase.profile -Phbase1.</p>"},{"location":"changelog/#cassandra-upgrade-from-21-to-22","title":"Cassandra: Upgrade from 2.1 to 2.2","text":"<p>The version of Cassandra that is included in the distribution of JanusGraph was upgraded from 2.1.20 to 2.2.13. Refer to the upgrade documentation of Cassandra for detailed instructions to perform this upgrade. Users who operate their own Cassandra cluster instead of using Cassandra distributed together with JanusGraph are not affected by this upgrade. This also does not change the different versions of Cassandra that are supported by JanusGraph (see &lt;&gt; for a detailed list of the supported versions)."},{"location":"changelog/#berkeleydb-upgrade-from-74-to-75","title":"BerkeleyDB : Upgrade from 7.4 to 7.5","text":"<p>The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk  (see the BerkeleyDB changelog for reference). This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with JanusGraph can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release.</p>"},{"location":"changelog/#solr-compatible-lucene-version-changed-from-500-to-700-in-distributed-config","title":"Solr: Compatible Lucene version changed from 5.0.0 to 7.0.0 in distributed config","text":"<p>The JanusGraph distribution contains a <code>solrconfig.xml</code> file that can be used to configure Solr. The value <code>luceneMatchVersion</code> in this config that tells Solr to behave according to that Lucene version was changed from 5.0.0 to 7.0.0 as that is the default version currently used by JanusGraph. Users should generally set this value to the version of their Solr installation. If the config distributed by JanusGraph is used for an existing Solr installation that used a lower version before (like 5.0.0 from a previous versions of this file), it is highly recommended that a re-indexing is performed.</p>"},{"location":"changelog/#version-033-release-date-january-11-2020","title":"Version 0.3.3 (Release Date: January 11, 2020)","text":"MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.3.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.3.3\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.4</li> <li>Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0</li> <li>Oracle BerkeleyJE 7.4.5</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.3.3</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.3.3, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/14?closed=1</li> </ul>"},{"location":"changelog/#version-032-release-date-june-16-2019","title":"Version 0.3.2 (Release Date: June 16, 2019)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.3.2/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.3.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.3.2\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.4</li> <li>Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0</li> <li>Oracle BerkeleyJE 7.4.5</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.3.3</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.3.2, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/10?closed=1</li> </ul>"},{"location":"changelog/#version-031-release-date-october-2-2018","title":"Version 0.3.1 (Release Date: October 2, 2018)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.3.1/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.3.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.3.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.4</li> <li>Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0</li> <li>Oracle BerkeleyJE 7.4.5</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.3.3</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.3.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/7?closed=1</li> </ul>"},{"location":"changelog/#version-030-release-date-july-31-2018","title":"Version 0.3.0 (Release Date: July 31, 2018)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.3.0/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.3.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.3.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 1.2.6, 1.3.1, 1.4.4</li> <li>Google Bigtable 1.0.0, 1.1.2, 1.2.0, 1.3.0, 1.4.0</li> <li>Oracle BerkeleyJE 7.4.5</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.3.3</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.3.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/4?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_10","title":"Upgrade Instructions","text":"<p>Important</p> <p>You should back-up your data prior to attempting an upgrade! Also please note that once an upgrade has been completed you will no longer be able to connect to your graph with client versions prior to 0.3.0.</p> <p>JanusGraph 0.3.0 implements Schema Constraints which made it necessary to also introduce the concept of a schema version. There is a check to prevent client connections that either expect a different schema version or have no concept of a schema version. To perform an upgrade, the configuration option <code>graph.allow-upgrade=true</code> must be set on each graph you wish to upgrade. The graph must be opened with a 0.3.0 or greater version of JanusGraph since older versions have no concept of <code>graph.storage-version</code> and will not allow for it to be set.</p> <p>Example excerpt from <code>janusgraph.properties</code> file <pre><code># JanusGraph configuration sample: Cassandra over a socket\n#\n# This file connects to a Cassandra daemon running on localhost via\n# Thrift.  Cassandra must already be started before starting JanusGraph\n# with this file.\n\n# This option should be removed as soon as the upgrade is complete. Otherwise if this file\n# is used in the future to connect to a different graph it could cause an unintended upgrade.\ngraph.allow-upgrade=true\n\ngremlin.graph=org.janusgraph.core.JanusGraphFactory\n\n# The primary persistence provider used by JanusGraph.  This is required.\n# It should be set one of JanusGraph's built-in shorthand names for its\n# standard storage backends (shorthands: berkeleyje, cassandrathrift,\n# cassandra, astyanax, embeddedcassandra, cql, hbase, inmemory) or to the\n# full package and classname of a custom/third-party StoreManager\n# implementation.\n#\n# Default:    (no default value)\n# Data Type:  String\n# Mutability: LOCAL\nstorage.backend=cassandrathrift\n\n# The hostname or comma-separated list of hostnames of storage backend\n# servers.  This is only applicable to some storage backends, such as\n# cassandra and hbase.\n#\n# Default:    127.0.0.1\n# Data Type:  class java.lang.String[]\n# Mutability: LOCAL\nstorage.hostname=127.0.0.1\n</code></pre></p> <p>If <code>graph.allow-upgrade</code> is set to true on a graph <code>graph.storage-version</code> and <code>graph.janusgraph-version</code> will automatically be upgraded to match the version level of the server, or local client, that is opening the graph. You can verify the upgrade was successful by opening the management API and validating the values of <code>graph.storage-version</code> and <code>graph.janusgraph-version</code>.</p> <p>Once the storage version has been set you should remove <code>graph.allow-upgrade=true</code> from your properties file and reopen your graph to ensure that the upgrade was successful. </p>"},{"location":"changelog/#version-023-release-date-may-21-2019","title":"Version 0.2.3 (Release Date: May 21, 2019)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.3/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.3\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.9</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.3, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/9?closed=1</li> </ul>"},{"location":"changelog/#version-022-release-date-october-9-2018","title":"Version 0.2.2 (Release Date: October 9, 2018)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.2/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.2\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.9</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.2, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/6?closed=1</li> </ul>"},{"location":"changelog/#version-021-release-date-july-9-2018","title":"Version 0.2.1 (Release Date: July 9, 2018)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.1/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.9</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/5?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_11","title":"Upgrade Instructions","text":""},{"location":"changelog/#hbase-ttl","title":"HBase TTL","text":"<p>In JanusGraph 0.2.0, time-to-live (TTL) support was added for HBase storage backend. In order to utilize the TTL capability on HBase, the graph timestamps need to be MILLI. If the <code>graph.timestamps</code> property is not explicitly set to MILLI, the default is MICRO in JanusGraph 0.2.0, which does not work for HBase TTL. Since the <code>graph.timestamps</code> property is FIXED, a new graph needs to be created to make any change of the <code>graph.timestamps</code> property effective.</p>"},{"location":"changelog/#version-020-release-date-october-11-2017","title":"Version 0.2.0 (Release Date: October 11, 2017)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.0/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.18, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0-pre3</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.2, 6.0.0-rc1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.6</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/2?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_12","title":"Upgrade Instructions","text":""},{"location":"changelog/#elasticsearch","title":"Elasticsearch","text":"<p>JanusGraph 0.1.z is compatible with Elasticsearch 1.5.z. There were several configuration options available, including transport client, node client, and legacy configuration track. JanusGraph 0.2.0 is compatible with Elasticsearch versions from 1.y through 6.y, however it offers only a single configuration option using the REST client.</p>"},{"location":"changelog/#transport-client","title":"Transport client","text":"<p>The <code>TRANSPORT_CLIENT</code> interface has been replaced with <code>REST_CLIENT</code>. When migrating an existing graph to JanusGraph 0.2.0, the <code>interface</code> property must be set when connecting to the graph: <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.interface=REST_CLIENT\nindex.search.hostname=127.0.0.1\n</code></pre></p> <p>After connecting to the graph, the property update can be made permanent by making the change with <code>JanusGraphManagement</code>: <pre><code>mgmt = graph.openManagement()\nmgmt.set(\"index.search.elasticsearch.interface\", \"REST_CLIENT\")\nmgmt.commit()\n</code></pre></p>"},{"location":"changelog/#node-client","title":"Node client","text":"<p>A node client with JanusGraph can be configured in a few ways. If the node client was configured as a client-only or non-data node, follow the steps from the transport client section to connect to the existing cluster using the <code>REST_CLIENT</code> instead. If the node client was a data node (local-mode), then convert it into a standalone Elasticsearch node, running in a separate JVM from your application process. This can be done by using the node\u2019s configuration from the JanusGraph configuration to start a standalone Elasticsearch 1.5.z node. For example, we start with these JanusGraph 0.1.z properties:</p> <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.interface=NODE\nindex.search.conf-file=es-client.yml\nindex.search.elasticsearch.ext.node.name=alice\n</code></pre> <p>where the configuration file <code>es-client.yml</code> has properties:</p> <pre><code>node.data: true\npath.data: /var/lib/elasticsearch/data\npath.work: /var/lib/elasticsearch/work\npath.logs: /var/log/elasticsearch\n</code></pre> <p>The properties found in the configuration file <code>es-client.yml</code> and the <code>index.search.elasticsearch.ext.*</code> properties can be inserted into <code>$ES_HOME/config/elasticsearch.yml</code> so that a standalone Elasticsearch 1.5.z node can be started with the same properties. Keep in mind that if any <code>path</code> locations have relative paths, those values may need to be updated appropriately. Once the standalone Elasticsearch node is started, follow the directions in the transport client section to complete the migration to the <code>REST_CLIENT</code> interface. Note that the <code>index.search.conf-file</code> and <code>index.search.elasticsearch.ext.*</code> properties are not used by the <code>REST_CLIENT</code> interface, so they can be removed from the configuration properties.</p>"},{"location":"changelog/#legacy-configuration","title":"Legacy configuration","text":"<p>The legacy configuration track was not recommended in JanusGraph 0.1.z and is no longer supported in JanusGraph 0.2.0. Users should refer to the previous sections and migrate to the <code>REST_CLIENT</code>.</p>"},{"location":"changelog/#version-011-release-date-may-11-2017","title":"Version 0.1.1 (Release Date: May 11, 2017)","text":"<p>Documentation: https://old-docs.janusgraph.org/0.1.1/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.1.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.1.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.9</li> <li>Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4</li> <li>Google Bigtable 0.9.5.1</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.5.1</li> <li>Apache Lucene 4.10.4</li> <li>Apache Solr 5.2.1</li> <li>Apache TinkerPop 3.2.3</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.1.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/3?closed=1</li> </ul>"},{"location":"changelog/#version-010-release-date-april-11-2017","title":"Version 0.1.0 (Release Date: April 11, 2017)","text":"<p>Documentation: https://old-docs.janusgraph.org/0.1.0/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.1.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.9</li> <li>Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4</li> <li>Google Bigtable 0.9.5.1</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.5.1</li> <li>Apache Lucene 4.10.4</li> <li>Apache Solr 5.2.1</li> <li>Apache TinkerPop 3.2.3</li> <li>Java 1.8</li> </ul> <p>Features added since version Titan 1.0.0:</p> <ul> <li> <p>TinkerPop 3.2.3 compatibility</p> <ul> <li>Includes update to Spark 1.6.1</li> </ul> </li> <li> <p>Query optimizations: JanusGraphStep folds in HasId and HasContainers     can be folded in even mid-traversal</p> </li> <li> <p>Support Google Cloud Bigtable as a backend over the HBase interface</p> </li> <li> <p>Compatibility with newer versions of backend and index stores</p> <ul> <li> <p>HBase 1.2</p> </li> <li> <p>BerkeleyJE 7.3.7</p> </li> </ul> </li> <li> <p>Includes a number of bug fixes and optimizations</p> </li> </ul> <p>For more information on features and bug fixes in 0.1.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/1?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_13","title":"Upgrade Instructions","text":"<p>JanusGraph is based on the latest commit to the <code>titan11</code> branch of Titan repo.</p> <p>JanusGraph has made the following changes to Titan, so you will need to adjust your code and configuration accordingly:</p> <ol> <li> <p>module names: <code>titan-*</code> are now <code>janusgraph-*</code></p> </li> <li> <p>package names: <code>com.thinkaurelius.titan</code> are now <code>org.janusgraph</code></p> </li> <li> <p>class names: <code>Titan*</code> are now <code>JanusGraph*</code> except in cases where     this would duplicate a word, e.g., <code>TitanGraph</code> is simply     <code>JanusGraph</code> rather than <code>JanusGraphGraph</code></p> </li> </ol> <p>For more information on how to configure JanusGraph to read data which had previously been written by Titan refer to Migration from titan.</p>"},{"location":"common-questions/","title":"Common Questions","text":""},{"location":"common-questions/#accidental-type-creation","title":"Accidental type creation","text":"<p>By default, JanusGraph will automatically create property keys and edge labels when a new type is encountered. It is strongly encouraged that users explicitly schemata as documented in Schema and Data Modeling before loading any data and disable automatic type creation by setting the option <code>schema.default = none</code>.</p> <p>Automatic type creation can cause problems in multi-threaded or highly concurrent environments. Since JanusGraph needs to ensure that types are unique, multiple attempts at creating the same type will lead to locking or other exceptions. It is generally recommended to create all needed types up front or in one batch when new property keys and edge labels are needed.</p>"},{"location":"common-questions/#custom-class-datatype","title":"Custom Class Datatype","text":"<p>JanusGraph supports arbitrary objects as attribute values on properties. To use a custom class as data type in JanusGraph, either register a custom serializer or ensure that the class has a no-argument constructor and implements the <code>equals</code> method because JanusGraph will verify that it can successfully de-/serialize objects of that class. Please see Datatype and Attribute Serializer Configuration for more information.</p>"},{"location":"common-questions/#transactional-scope-for-edges","title":"Transactional Scope for Edges","text":"<p>Edges should not be accessed outside the scope in which they were originally created or retrieved.</p>"},{"location":"common-questions/#locking-exceptions","title":"Locking Exceptions","text":"<p>When defining unique types with locking enabled (i.e. requesting that JanusGraph ensures uniqueness) it is likely to encounter locking exceptions of the type <code>PermanentLockingException</code> under concurrent modifications to the graph.</p> <p>Such exceptions are to be expected, since JanusGraph cannot know how to recover from a transactional state where an earlier read value has been modified by another transaction since this may invalidate the state of the transaction. In most cases it is sufficient to simply re-run the transaction. If locking exceptions are very frequent, try to analyze and remove the source of congestion.</p>"},{"location":"common-questions/#ghost-vertices","title":"Ghost Vertices","text":"<p>When the same vertex is concurrently removed in one transaction and modified in another, both transactions will successfully commit on eventually consistent storage backends and the vertex will still exist with only the modified properties or edges. This is referred to as a ghost vertex. It is possible to guard against ghost vertices on eventually consistent backends using key uniqueness but this is prohibitively expensive in most cases. A more scalable approach is to allow ghost vertices temporarily and clearing them out in regular time intervals.</p> <p>Another option is to detect them at read-time using the option <code>checkInternalVertexExistence()</code> documented in Transaction Configuration.</p>"},{"location":"common-questions/#debug-level-logging-slows-execution","title":"Debug-level Logging Slows Execution","text":"<p>When the log level is set to <code>DEBUG</code> JanusGraph produces a lot of logging output which is useful to understand how particular queries get compiled, optimized, and executed. However, the output is so large that it will impact the query performance noticeably. Hence, use <code>INFO</code> severity or higher for production systems or benchmarking.</p>"},{"location":"common-questions/#janusgraph-outofmemoryexception-or-excessive-garbage-collection","title":"JanusGraph OutOfMemoryException or excessive Garbage Collection","text":"<p>If you experience memory issues or excessive garbage collection while running JanusGraph it is likely that the caches are configured incorrectly. If the caches are too large, the heap may fill up with cache entries. Try reducing the size of the transaction level cache before tuning the database level cache, in particular if you have many concurrent transactions. See JanusGraph Cache for more information.</p>"},{"location":"common-questions/#elasticsearch-outofmemoryexception","title":"Elasticsearch OutOfMemoryException","text":"<p>When numerous clients are connecting to Elasticsearch, it is likely that an <code>OutOfMemoryException</code> occurs. This is not due to a memory issue, but to the OS not allowing more threads to be spawned by the user (the user running Elasticsearch). To circumvent this issue, increase the number of allowed processes to the user running Elasticsearch. For example, increase the <code>ulimit -u</code> from the default 1024 to 10024.</p>"},{"location":"common-questions/#dropping-a-database","title":"Dropping a Database","text":"<p>To drop a database using the Gremlin Console you can call <code>JanusGraphFactory.drop(graph)</code>. The graph you want to drop needs to be defined prior to running the drop method.</p> <p>With ConfiguredGraphFactory <pre><code>graph = ConfiguredGraphFactory.open('example')\nConfiguredGraphFactory.drop('example');\n</code></pre></p> <p>With JanusGraphFactory <pre><code>graph = JanusGraphFactory.open('path/to/configuration.properties')\nJanusGraphFactory.drop(graph);\n</code></pre></p> <p>Note that on JanusGraph versions prior to 0.3.0 if multiple Gremlin Server instances are connecting to the graph that has been dropped it is recommended to close the graph on all active nodes by running either <code>JanusGraphFactory.close(graph)</code> or <code>ConfiguredGraphFactory.close(\"example\")</code> depending on which graph manager is in use. Closing and reopening the graph on all active nodes will prevent cached(stale) references to the graph that has been dropped. ConfiguredGraphFactory graphs that are dropped may need to have their configurations recreated using the graph configuration singleton or template configuration.</p>"},{"location":"development/","title":"Development","text":"<p>The following sections describe the JanusGraph development process.</p>"},{"location":"development/#development-decisions","title":"Development Decisions","text":"<p>Many development decisions will be made during the day-to-day work of JanusGraph contributors without any extra process overhead. However, for larger bodies of work and significant updates and additions, the following process shall be followed. Significant work may include, but is not limited to:</p> <ul> <li>A major new feature or subproject, e.g., a new storage adapter</li> <li>Incrementing the version of a core dependency such as a Apache TinkerPop</li> <li>Addition or deprecation of a public API</li> <li>Internal shared data structure or API change</li> <li>Addition of a new major dependency</li> </ul> <p>For these sorts of changes, contributors will:</p> <ul> <li>Create one or more issues in the GitHub issue tracker</li> <li>Start a DISCUSS thread on the janusgraph-dev  list where the proposed change may be discussed by committers and other community members</li> <li>When the proposer feels it appropriate, a VOTE shall be called</li> <li>Two +1 votes are required for the change to be accepted</li> </ul>"},{"location":"development/#branching","title":"Branching","text":"<p>For features that involve only one developer, developers will work in their own JanusGraph forks, managing their own branches, and submitting pull requests when ready. If multiple developers wish to collaborate on a feature, they may request that a feature branch be created in JanusGraph repository. If the developers are not committers, a JanusGraph committer will be assigned as the shepherd for that branch. The shepherd will be responsible for merging pull requests to that branch.</p>"},{"location":"development/#branch-naming-conventions","title":"Branch Naming Conventions","text":"<p>All branch names hosted in the JanusGraph repository shall be prepended with <code>Issue_#_</code>.</p>"},{"location":"development/#pull-requests","title":"Pull Requests","text":"<p>Users wishing to contribute to JanusGraph should fork the JanusGraph repository and then submit pull requests using the  GitHub pull request process. Every pull request must be associated with an existing issue. Be sure to include the relevant issue number in the title of your pull request, complete the pull request template checklist, and provide a description of what test suites were run to validate the pull request. Pull requests commit messages should clearly describe what work was accomplished. Intermediate work-in-progress commits should be squashed prior to pull request submission.</p>"},{"location":"development/#review-then-commit-rtc","title":"Review-Then-Commit (RTC)","text":"<p>Pull requests must be reviewed before being merged following a review-then-commit (RTC) process. The JanusGraph project uses the GitHub review process to review pull requests. Non-committers are welcomed and encouraged to review pull requests. Their review will be non-binding, but can be taken into consideration and are still very valuable community input. The following rules apply to the voting process.</p> <ul> <li>Approval flows<ul> <li>2 committer approvals are required to merge a pull request</li> <li>If a committer submits the pull request, it has their implicit     approval so it requires 1 additional committer approval</li> <li>1 committer approval followed a one week review period for     objections at which point a lazy consensus is assumed</li> </ul> </li> <li>One or more -1 votes within a change request will veto the pull     request until the noted issue(s) are addressed and the -1 vote is     withdrawn</li> <li>Change requests will not be considered valid without an explanation</li> </ul>"},{"location":"development/#commit-then-review-ctr","title":"Commit-Then-Review (CTR)","text":"<p>In instances where the committer deems the full RTC process unnecessary, a commit-then-review (CTR) process may be employed. The purpose of invoking CTR is to reduce the burden on the committers and minimize the turnaround time for merging trivial changes. Changes of this sort may include:</p> <ul> <li>Documentation typo or small documentation addition</li> <li>Addition of a new test case</li> <li>Backporting approved changes into other release branches</li> </ul> <p>Any commit that is made following CTR shall include the fact that it is a CTR in the commit comments. Community members who wish to review CTRs may subscribe to the JanusGraph commits list so that they will see CTRs as they come through. If another committer responds with a -1, the commit should be rolled back and the formal RTC process should be followed.</p>"},{"location":"development/#enable-automatic-backporting","title":"Enable Automatic Backporting","text":"<p>Before merging the pull request, it should be decided whether the contribution should be backported to other release branches. This should be the case for most non-breaking changes. If the change applies to something that is not present at all on other still supported release branches, like updating a dependency that was only introduced on <code>master</code> or code that was added / heavily modified on <code>master</code>, then backporting cannot work or would require too much effort. Otherwise, the appropriate labels for backporting (for example <code>backport/v0.6</code> for the branch <code>v0.6</code>) should be added before the pull requests gets merged as that allows the backporting action to automatically handle the backporting.</p>"},{"location":"development/#merging-of-pull-requests","title":"Merging of Pull Requests","text":"<p>A pull request is ready to be merged when it has been approved (see Review-Then-Commit (RTC)). It can either be merged manually with <code>git</code> commands or through the GitHub UI.</p> <p>For pull requests that should be backported (see Enable Automatic Backporting), the backporting action should create a pull request to backport the changes to other release branches after the pull request has been merged. The committer who merges the pull request should ensure that this automatic backporting succeeds. It may fail in case of merge conflicts. In that case the backporting needs to be performed manually.</p>"},{"location":"development/#manual-backporting","title":"Manual Backporting","text":"<p>The Backport CLI tool can be used to manually backport a pull request. This can be necessary if the automatic backporting of a pull request fails. After installing the CLI tool, you also need to configure it with a GitHub access token. This is explained in the README.md of the Backport CLI tool.</p> <p>You can then use the tool to backport a pull request #xyz:</p> <pre><code> backport --pr xyz\n</code></pre> <p>This will checkout the repository, apply the change from the pull request to the appropriate release branch (like <code>v0.6</code>), and then ask you to resolve any merge conflicts. After the merge conflicts are resolved, it will create a pull request that backports the changes. Note that such a backporting pull request can usually be merged via CTR after all automatic checks have been executed. If non-trivial changes were necessary to resolve merge conflicts, then waiting for a review before merging the pull request may still make sense.</p> <p>It is of course also possible to use cherry-picking to apply the commits manually to a different release branch instead of using this CLI tool.</p>"},{"location":"development/#release-policy","title":"Release Policy","text":"<p>Any JanusGraph committer may propose a release. To propose a release, simple start a new RELEASE thread on janusgraph-dev proposing the new release and requesting feedback on what should be included in the release. After consensus is reached the release manager will perform the following tasks:</p> <ul> <li>Create a release branch so that work may continue on <code>master</code></li> <li>Prepare the release artifacts</li> <li>Call a vote to approve the release on     janusgraph-dev</li> <li>Committers will be given 72 hours to review and vote on the release     artifacts</li> <li>Three +1 votes are required for a release to be approved</li> <li>One or more -1 votes with explanation will veto the release until     the noted issues are addressed and the -1 votes are withdrawn</li> </ul>"},{"location":"development/#building-janusgraph","title":"Building JanusGraph","text":"<p>To build JanusGraph you need git and Maven.</p> <ol> <li> <p>Clone the JanusGraph repository from     GitHub to a local     directory.</p> </li> <li> <p>In that directory, execute <code>mvn clean install</code>. This will build     JanusGraph and run the internal test suite. The internal test suite     has no external dependencies. Note, that running all test cases     requires a significant amount of time. To skip the tests when     building JanusGraph, execute <code>mvn clean install -DskipTests</code></p> </li> <li> <p>For comprehensive test coverage, execute     <code>mvn clean test -P comprehensive</code>. This will run additional test     covering communication to external storage backends, performance     tests and concurrency tests. The comprehensive test suite uses      HBase as external database and requires that HBase is installed.      Note, that running the comprehensive test suite requires a      significant amount of of time (&gt; 1 hour).</p> </li> </ol>"},{"location":"development/#faqs","title":"FAQs","text":"<p>Maven build causes dozens of \"[WARNING] We have a duplicate\u2026\" errors</p> <p>Make sure to use the maven-assembly-plugin when building or depending on JanusGraph.</p>"},{"location":"advanced-topics/commit-releases/","title":"Snapshot releases","text":"<p>In addition to official JanusGraph releases, JanusGraph publishes  releases for each commit. The commit releases allow users to use latest JanusGraph features without relying on official JanusGraph releases.</p> <ul> <li>Official JanusGraph releases are better tested and usually come with finalized   changes which signals that the used features are most likely to be stable for long term.</li> <li>Commit releases are not manually verified but instead verified by main CI tests as   well as scheduled full CI tests (once per week). New features in commit releases are not   guaranteed to be compatible between commits. Thus, you may expect more breaking changes between   commit releases and official releases.</li> </ul>"},{"location":"advanced-topics/commit-releases/#maven-repository-artifacts","title":"Maven repository artifacts","text":"<p>Both official and commit release are deployed to Sonatype OSS and are available in Sonatype Maven Central Repository  under the same group id <code>org.janusgraph</code>, but with different artifact id format.  </p> <p>Official JanusGraph releases have the next format for artifact id: <code>MAJOR.MINOR.PATCH</code>. Dependencies example:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:1.0.0\"\n</code></pre> <p>Artifact id for commit releases have the next format: <code>FOLLOWING_VERSION-DATE-TIME.COMMIT</code>.  </p> <ul> <li><code>FOLLOWING_VERSION</code> is the upcoming official version to be used after release is finalized (i.e. <code>0.6.3</code> if the current latest release is <code>0.6.2</code>).  It has <code>MAJOR.MINOR.PATCH</code> format.</li> <li><code>DATE</code> - date of the commit release in <code>yyyyMMdd</code> format.</li> <li><code>TIME</code> - time of the commit release in <code>HHmmss</code> format.</li> <li><code>COMMIT</code> - short commit hash of the commit used in the release.</li> </ul> <p>Dependencies example:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.6.3-20230104-164606.a49366e&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.6.3-20230104-164606.a49366e\"\n</code></pre>"},{"location":"advanced-topics/commit-releases/#janusgraph-distribution-builds","title":"JanusGraph distribution builds","text":"<p>In addition to distribution builds provided for each official JanusGraph release, snapshot distribution builds are provided for all commits.</p> <p>Info</p> <p>GitHub allows to download distribution builds only for authenticated GitHub users.</p> <p>To access the distribution build bundle for any commit, please open the commit you are interested in and select its <code>CI Release</code> details. For example: </p> <p>When <code>CI Release</code> page is opened for a specific commit, open summary and download the attached to <code>Artifacts</code> section file named <code>distribution-builds</code>. This will download <code>distribution-builds.zip</code> archive containing all distribution builds. </p> <p>Warning</p> <p>Old snapshot distribution builds are expiring in GitHub due to timing and memory limits. It's not guaranteed that the snapshot distribution build downloaded yesterday is available today. We encourage to use either official release distribution builds or newer snapshot distribution builds.</p> <p>If you see expired distribution builds or don't see any distribution builds for a specific commit, it means that it isn't available to be downloaded anymore. Thus, the distribution builds from newer commits should be used. </p>"},{"location":"advanced-topics/custom-vertex-id/","title":"Custom Vertex ID","text":"<p>JanusGraph has a built-in ID manager that allocates unique IDs for vertices, edges, and properties. By default, all these IDs are numbers of long type, but JanusGraph allows you to define custom vertex IDs.</p>"},{"location":"advanced-topics/custom-vertex-id/#configs","title":"Configs","text":"<p>There are two boolean options you can control: <code>graph.set-vertex-id</code> and <code>graph.allow-custom-vid-types</code>. Once the first option is enabled, you would be able to and must provide a vertex ID when you create a vertex. The vertex ID must be long-type, unless you turn on the second option. The second option allows to you either provide a long-type ID or a string-type ID. Note that if you turn on <code>graph.allow-custom-vid-types</code>, you must also turn on <code>graph.set-vertex-id</code>.</p> <p>Both of the two options are <code>GLOBAL_OFFLINE</code>, with default values being <code>false</code>. Being global offline means once they are set, you would have to bring down the entire JanusGraph cluster to alter the value. See the below two scenarios.</p>"},{"location":"advanced-topics/custom-vertex-id/#create-a-new-graph","title":"Create a new Graph","text":"<p>Simply set <code>graph.set-vertex-id=&lt;true/false&gt;</code> and <code>graph.allow-custom-vid-types=&lt;true/false&gt;</code> in your config file. When the graph is created, this value is loaded and persisted. After the graph is created, changing the option value in the config file would make no difference as JanusGraph simply ignores the value.</p>"},{"location":"advanced-topics/custom-vertex-id/#alter-an-existing-graph","title":"Alter an existing Graph","text":"<p>To alter a global offline config (not limited to the aforementioned two options), you need to shut down all JanusGraph instances. Then open a gremlin console, connect to JanusGraph instance and run the following:</p> <pre><code>mgmt = graph.openManagement();\nmgmt.set(\"graph.set-vertex-id\", true);\n// optional, if you want to provide string ID\nmgmt.set(\"graph.allow-custom-vid-types\", true);\nmgmt.commit();\n</code></pre> <p>It will return an error if you have open JanusGraph instances. Otherwise, it will succeed and now your global configs would have been updated.</p>"},{"location":"advanced-topics/custom-vertex-id/#custom-long-id","title":"Custom Long ID","text":"<p>To provide a long vertex ID, you must call <code>graph.getIDManager().toVertexId(long)</code> to retrieve a transformed JanusGraph ID. The input number must be positive. This is because certain range of long IDs are reserved for internal usage, so you have to always call the aforementioned utility to convert your ID. Unfortunately, this is not supported in all gremlin clients as it is a JanusGraph specific utility. You can create vertices as follows:</p> <pre><code>g = graph.traversal()\ng.addV().property(T.id, graph.getIDManager().toVertexID(123L)).next()\ng.tx().commit()\n</code></pre> <p>Note that if you want to get back the original ID you provided, you need to call <code>graph.getIDManager().fromVertexID(long)</code>.</p>"},{"location":"advanced-topics/custom-vertex-id/#custom-string-id","title":"Custom String ID","text":"<p>A string vertex ID can have arbitrary length as long as the underlying storage and index backends permit. Elasticsearch, for example, requires the length of the ID to be smaller than or equal to 512.</p> <p>The string value must consist of printable ASCII characters only, and it cannot contain JanusGraph reserved relation delimiter (a single character string). By default, this reserved delimiter is <code>-</code> (dash), which means you cannot use UUID string as a vertex ID since it contains a <code>-</code> character. You can create vertices as follows:</p> <pre><code>g = graph.traversal()\ng.addV().property(T.id, \"custom_vid_001\").next()\ng.tx().commit()\n</code></pre>"},{"location":"advanced-topics/custom-vertex-id/#override-reserved-character","title":"Override reserved Character","text":"<p>If you create a new graph (using 1.0.0 or later version), JanusGraph allows you to set <code>JANUSGRAPH_RELATION_DELIMITER</code> system property, a single character string which can be any printable ASCII character. Once it is set, JanusGraph will prohibit that specific character instead of the default <code>-</code> character. Alternatively, you can set <code>JANUSGRAPH_RELATION_DELIMITER</code> environment variable, which is evaluated if and only if the <code>JANUSGRAPH_RELATION_DELIMITER</code> system property is not present. For example, you can use the following command to set system property when opening gremlin console:</p> <pre><code>JAVA_OPTIONS=\"-DJANUSGRAPH_RELATION_DELIMITER=@\" ./bin/gremlin.sh\n</code></pre> <p>and then you should be able to use UUID as custom id since now the reserved character becomes <code>@</code> rather than <code>-</code>.</p> <pre><code>g = graph.traversal()\ng.addV().property(T.id, UUID.randomUUID().toString()).next()\ng.addV().property(T.id, \"custom-vid-001\").next()\ng.tx().commit()\n</code></pre> <p>Warning</p> <p>If you decide to replace default <code>JANUSGRAPH_RELATION_DELIMITER</code>, you MUST always set the system property or environment value consistently across all JanusGraph instances, including servers and clients, during the entire lifecycle of the graph. Otherwise, data corruption could happen. For example, if you only set it on client side, then the server will not be able to deserialize any property or edge because of the setting mismatch. Also, you must not use this setting on a legacy graph. In other words, you cannot alter this property at any time.</p>"},{"location":"advanced-topics/data-model/","title":"JanusGraph Data Model","text":"<p>JanusGraph stores graphs in adjacency list format which means that a graph is stored as a collection of vertices with their adjacency list. The adjacency list of a vertex contains all of the vertex\u2019s incident edges (and properties).</p> <p>By storing a graph in adjacency list format JanusGraph ensures that all of a vertex\u2019s incident edges and properties are stored compactly in the storage backend which speeds up traversals. The downside is that each edge has to be stored twice - once for each end vertex of the edge.</p> <p>In addition, JanusGraph maintains the adjacency list of each vertex in sort order with the order being defined by the sort key and sort order the edge labels. The sort order enables efficient retrievals of subsets of the adjacency list using vertex centric indices.</p> <p>JanusGraph stores the adjacency list representation of a graph in any storage backend that supports the Bigtable data model.</p>"},{"location":"advanced-topics/data-model/#bigtable-data-model","title":"Bigtable Data Model","text":"<p>Under the Bigtable data model each table is a collection of rows. Each row is uniquely identified by a key. Each row is comprised of an arbitrary (large, but limited) number of cells. A cell is composed of a column and value. A cell is uniquely identified by a column within a given row. Rows in the Bigtable model are called \"wide rows\" because they support a large number of cells and the columns of those cells don\u2019t have to be defined up front as is required in relational databases.</p> <p>JanusGraph has an additional requirement for the Bigtable data model: The cells must be sorted by their columns and a subset of the cells specified by a column range must be efficiently retrievable (e.g. by using index structures, skip lists, or binary search).</p> <p>In addition, a particular Bigtable implementation may keep the rows sorted in the order of their key. JanusGraph can exploit such key-order to effectively partition the graph which provides better loading and traversal performance for very large graphs. However, this is not a requirement.</p>"},{"location":"advanced-topics/data-model/#janusgraph-data-layout","title":"JanusGraph Data Layout","text":"<p>JanusGraph stores each adjacency list as a row in the underlying storage backend. The (64 bit) vertex id (which JanusGraph uniquely assigns to every vertex) is the key which points to the row containing the vertex\u2019s adjacency list. Each edge and property is stored as an individual cell in the row which allows for efficient insertions and deletions. The maximum number of cells allowed per row in a particular storage backend is therefore also the maximum degree of a vertex that JanusGraph can support against this backend.</p> <p>If the storage backend supports key-order, the adjacency lists will be ordered by vertex id, and JanusGraph can assign vertex ids such that the graph is effectively partitioned. Ids are assigned such that vertices which are frequently co-accessed have ids with small absolute difference.</p>"},{"location":"advanced-topics/data-model/#individual-edge-layout","title":"Individual Edge Layout","text":"<p>Each edge and property is stored as one cell in the rows of its adjacent vertices. They are serialized such that the byte order of the column respects the sort key of the edge label. Variable id encoding schemes and compressed object serialization are used to keep the storage footprint of each edge/cell as small as possible.</p> <p>Consider the storage layout of an individual edge as visualized in the top row of the graphic above. The dark blue boxes represent numbers that are encoded with a variable length encoding scheme to reduce the number of bytes they consume. Red boxes represent one or multiple property values (i.e. objects) that are serialized with compressed meta data referenced in the associated property key. Grey boxes represent uncompressed property values (i.e. serialized objects).</p> <p>The serialized representation of an edge starts with the edge label\u2019s unique id (as assigned by JanusGraph). This is typically a small number and compressed well with variable id encoding. The last bit of this id is offset to store whether this is an incoming or outgoing edge. Next, the property value comprising the sort key are stored. The sort key is defined with the edge label and hence the sort key objects meta data can be referenced to the edge label. After that, the id of the adjacent vertex is stored. JanusGraph does not store the actual vertex id but the difference to the id of the vertex that owns this adjacency list. It is likely that the difference is a smaller number than the absolute id and hence compresses better. The vertex id is followed by the id of this edge. Each edge is assigned a unique id by JanusGraph. This concludes the column value of the edge\u2019s cell. The value of the edge\u2019s cell contains the compressed serialization of the signature properties of the edge (as defined by the label\u2019s signature key) and any other properties that have been added to the edge in uncompressed serialization.</p> <p>The serialized representation of a property is simpler and only contains the property\u2019s key id in the column. The property id and the property value are stored in the value. If the property key is defined as <code>list()</code>, however, the property id is stored in the column as well.</p>"},{"location":"advanced-topics/eventual-consistency/","title":"Eventually-Consistent Storage Backends","text":"<p>When running JanusGraph against an eventually consistent storage backend special JanusGraph features must be used to ensure data consistency and special considerations must be made regarding data degradation.</p> <p>This page summarizes some of the aspects to consider when running JanusGraph on top of an eventually consistent storage backend like Apache Cassandra or Apache HBase.</p>"},{"location":"advanced-topics/eventual-consistency/#data-consistency","title":"Data Consistency","text":"<p>On eventually consistent storage backends, JanusGraph must obtain locks in order to ensure consistency because the underlying storage backend does not provide transactional isolation. In the interest of efficiency, JanusGraph does not use locking by default. Hence, the user has to decide for each schema element that defines a consistency constraint whether or not to use locking. Use <code>JanusGraphManagement.setConsistency(element, ConsistencyModifier.LOCK)</code> to explicitly enable locking on a schema element as shown in the following examples. <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('consistentName').dataType(String.class).make()\nindex = mgmt.buildIndex('byConsistentName', Vertex.class).addKey(name).unique().buildCompositeIndex()\nmgmt.setConsistency(name, ConsistencyModifier.LOCK) // Ensures only one name per vertex\nmgmt.setConsistency(index, ConsistencyModifier.LOCK) // Ensures name uniqueness in the graph\nmgmt.commit()\n</code></pre></p> <p>When updating an element that is guarded by a uniqueness constraint, JanusGraph uses the following protocol at the end of a transaction when calling <code>tx.commit()</code>:</p> <ol> <li>Acquire a lock on all elements that have a consistency constraint</li> <li>Re-read those elements from the storage backend and verify that they     match the state of the element in the current transaction prior to     modification. If not, the element was concurrently modified and a     PermanentLocking exception is thrown.</li> <li>Persist the state of the transaction against the storage backend.</li> <li>Release all locks.</li> </ol> <p>This is a brief description of the locking protocol which leaves out optimizations (e.g. local conflict detection) and detection of failure scenarios (e.g. expired locks).</p> <p>The actual lock application mechanism is abstracted such that JanusGraph can use multiple implementations of a locking provider. Currently, only one locking provider is included in the JanusGraph distribution:</p> <p>A locking implementation based on key-consistent read and write operations that is agnostic to the underlying storage backend as long as it supports key-consistent operations (which includes Cassandra and HBase). This is the default implementation and uses timestamp based lock applications to determine which transaction holds the lock. It requires that clocks are synchronized across all machines in the cluster.</p> <p>Warning</p> <p>The locking implementation is not robust against all failure scenarios. For instance, when a Cassandra cluster drops below quorum, consistency is no longer ensured. Hence, it is suggested to use locking-based consistency constraints sparingly with eventually consistent storage backends. For use cases that require strict and or frequent consistency constraint enforcement, it is suggested to use a storage backend that provides transactional isolation.</p>"},{"location":"advanced-topics/eventual-consistency/#data-consistency-without-locks","title":"Data Consistency without Locks","text":"<p>Because of the additional steps required to acquire a lock when committing a modifying transaction, locking is a fairly expensive way to ensure consistency and can lead to deadlock when very many concurrent transactions try to modify the same elements in the graph. Hence, locking should be used in situations where consistency is more important than write latency and the number of conflicting transactions is small.</p> <p>In other situations, it may be better to allow conflicting transactions to proceed and to resolve inconsistencies at read time. This is a design pattern commonly employed in large scale data systems and most effective when the actual likelihood of conflict is small. Hence, write transactions don\u2019t incur additional overhead and any (unlikely) conflict that does occur is detected and resolved at read time and later cleaned up. JanusGraph makes it easy to use this strategy through the following features.</p>"},{"location":"advanced-topics/eventual-consistency/#forking-edges","title":"Forking Edges","text":"<p>Because edge are stored as single records in the underlying storage backend, concurrently modifying a single edge would lead to conflict. Instead of locking, an edge label can be configured to use <code>ConsistencyModifier.FORK</code>. The following example creates a new edge label <code>related</code> and defines its consistency to FORK.</p> <pre><code>mgmt = graph.openManagement()\nrelated = mgmt.makeEdgeLabel('related').make()\nmgmt.setConsistency(related, ConsistencyModifier.FORK)\nmgmt.commit()\n</code></pre> <p>When modifying an edge whose label is configured to FORK the edge is deleted and the modified edge is added as a new one. Hence, if two concurrent transactions modify the same edge, two modified copies of the edge will exist upon commit which can be resolved during querying traversals if needed.</p> <p>Note</p> <p>Edge forking only applies to MULTI edges. Edge labels with a multiplicity constraint cannot use this strategy since a constraint is built into the edge label definition that requires an explicit lock or use the conflict resolution mechanism of the underlying storage backend.</p>"},{"location":"advanced-topics/eventual-consistency/#multi-properties","title":"Multi-Properties","text":"<p>Modifying single valued properties on vertices concurrently can result in a conflict. Similarly to edges, one can allow an arbitrary number of properties on a vertex for a particular property key defined with cardinality LIST and FORK on modification. Hence, instead of conflict one reads multiple properties. Since JanusGraph allows properties on properties, provenance information like <code>author</code> can be added to the properties to facilitate resolution at read time.</p> <p>See multi-properties to learn how to define those.</p>"},{"location":"advanced-topics/eventual-consistency/#data-inconsistency","title":"Data Inconsistency","text":""},{"location":"advanced-topics/eventual-consistency/#temporary-inconsistency","title":"Temporary Inconsistency","text":"<p>On eventually consistent storage backends, writes may not be immediately visible to the entire cluster causing temporary inconsistencies in the graph. This is an inherent property of eventual consistency, in the sense, that accepted updates must be propagated to other instances in the cluster and no guarantees are made with respect to read atomicity in the interest of performance.</p> <p>From JanusGraph\u2019s perspective, eventual consistency might cause the following temporary graph inconsistencies in addition the general inconsistency that some parts of a transaction are visible while others aren\u2019t yet.</p> <p>Stale Index entries Index entries might point to nonexistent vertices or edges. Similarly, a vertex or edge appears in the graph but is not yet indexed and hence ignored by global graph queries.</p> <p>In some situations due to server failures permanent index inconsistency can  happen. See how to deal with permanent stale index entries here.</p> <p>Half-Edges Only one direction of an edge gets persisted or deleted which might lead to the edge not being or incorrectly being retrieved.</p> <p>Note</p> <p>In order to avoid that write failures result in permanent inconsistencies in the graph it is recommended to use storage backends that support batch write atomicity and to ensure that write atomicity is enabled. To get the benefit of write atomicity, the number modifications made in a single transaction must be smaller than the configured <code>buffer-size</code> option documented in Configuration Reference. The buffer size defines the maximum number of modifications that JanusGraph will persist in a single batch. If a transaction has more modifications, the persistence will be split into multiple batches which are persisted individually which is useful for batch loading but invalidates write atomicity.</p>"},{"location":"advanced-topics/eventual-consistency/#ghost-vertices","title":"Ghost Vertices","text":"<p>A permanent inconsistency that can arise when operating JanusGraph on eventually consistent storage backend is the phenomena of ghost vertices. If a vertex gets deleted while it is concurrently being modified, the vertex might re-appear as a ghost.</p> <p>The following strategies can be used to mitigate this issue:</p> <p>Existence checks Configure transactions to (double) check for the existence of vertices prior to returning them. Please see Transaction Configuration for more information and note that this can significantly decrease performance. Note, that this does not fix the inconsistencies but hides some of them from the user.</p> <p>Regular Clean-ups Run regular batch-jobs to repair inconsistencies in the graph using JanusGraph with TinkerPop\u2019s Hadoop-Gremlin.  This is the only strategy that can address all inconsistencies and effectively repair them.</p> <p>Soft Deletes Instead of deleting vertices, they are marked as deleted which keeps them in the graph for future analysis but hides them from user-facing transactions.</p>"},{"location":"advanced-topics/executor-service/","title":"ExecutorService","text":""},{"location":"advanced-topics/executor-service/#backend-executorservice","title":"Backend ExecutorService","text":"<p>By default, JanusGraph uses <code>ExecutorService</code> to process some queries in parallel for storage backend implementations  which don't support multi-key queries (see <code>storage.parallel-backend-ops</code> configuration option). </p> <p>Info</p> <p>Officially supported backends: <code>Cassandra</code>, <code>HBase</code>, <code>ScyllaDB</code>, <code>Bigtable</code> have multi-key queries support. Thus,  these backend implementations ignore <code>storage.parallel-backend-ops</code> configuration and instead use multi-key queries. <code>BerkeleyDB</code> and <code>in-memory</code> storage backends don't have multi-key queries support. Thus, backend-ops executor  service can be used for such storage backends.</p> <p>JanusGraph allows to configure the executor service which is used via configuration options  provided in <code>storage.parallel-backend-executor-service</code> configuration section.</p> <p>Currently, JanusGraph has the following ExecutorService implementations which can be used (controlled via  <code>storage.parallel-backend-executor-service.class</code>):</p> <ul> <li><code>fixed</code> - fixed thread pool size;</li> <li><code>cached</code> - cached thread pool size;</li> <li>Custom ExecutorService;</li> </ul>"},{"location":"advanced-topics/executor-service/#custom-executorservice","title":"Custom ExecutorService","text":"<p>To use custom <code>ExecutorService</code> the configuration option <code>storage.parallel-backend-executor-service.class</code> must be  provided which must be the full class name of the <code>ExecutorService</code> implementation class.</p> <p>The provided class which implements <code>ExecutorService</code> must have either a public constructor with ExecutorServiceConfiguration argument (preferred constructor) or a public parameterless constructor.  When both constructors are available the constructor with <code>ExecutorServiceConfiguration</code> argument will be used.</p> <p>When custom <code>ExecutorService</code> is provided with <code>ExecutorServiceConfiguration</code> it isn't required to  use any of the configuration options provided in <code>ExecutorServiceConfiguration</code> but sometimes it is convenient to use the provided configurations. The configuration options provided in <code>ExecutorServiceConfiguration</code>  are typically those configuration options which are provided via configurations from <code>storage.parallel-backend-executor-service</code>.</p>"},{"location":"advanced-topics/executor-service/#cassandra-backend-executorservice","title":"Cassandra backend ExecutorService","text":"<p>Although Cassandra backend doesn't use <code>storage.parallel-backend-executor-service</code> due to having multi-key queries support, it  has its own internal ExecutorService for queries deserialization processing. Usually it's not recommended to configure this executor service because it's considered to be optimal by default. In case when the default executor service doesn't fit user's use-case for any reason, the configuration options under  <code>storage.cql.executor-service</code> can be used to modify it. The rules by which the Cassandra backend <code>ExecutorService</code> is built are the  same as the rules which are used to build parallel backend queries <code>ExecutorService</code> (described above).  The only difference is that the configuration for Cassandra backend <code>ExecutorService</code> are provided via configuration  options under <code>storage.cql.executor-service</code>.</p> <p>Warning</p> <p>By default, <code>storage.cql.executor-service</code> is configured to have a core pool size of number of processors multiplied  by 2. It's recommended to always use the default value unless there is a reason to artificially limit parallelism for  CQL slice query deserialization.</p>"},{"location":"advanced-topics/hadoop/","title":"JanusGraph with TinkerPop\u2019s Hadoop-Gremlin","text":"<p>This chapter describes how to leverage Apache Hadoop  and Apache Spark to configure JanusGraph for distributed graph processing. These steps will provide an overview on how to get started with those projects, but please refer to those project communities to become more deeply familiar with them.</p> <p>JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP.</p> <p>For the scope of the example below, Apache Spark is the computing framework and Apache Cassandra is the storage backend. The directions can be followed with other packages with minor changes to the configuration properties.</p> <p>Note</p> <p>The examples in this chapter are based on running Spark in local mode or standalone cluster mode. Additional configuration is required when using Spark on YARN or Mesos.</p>"},{"location":"advanced-topics/hadoop/#configuring-hadoop-for-running-olap","title":"Configuring Hadoop for Running OLAP","text":"<p>For running OLAP queries from the Gremlin Console, a few prerequisites need to be fulfilled. You will need to add the Hadoop configuration directory into the <code>CLASSPATH</code>, and the configuration directory needs to point to a live Hadoop cluster.</p> <p>Hadoop provides a distributed access-controlled file system. The Hadoop file system is used by Spark workers running on different machines to have a common source for file based operations. The intermediate computations of various OLAP queries may be persisted on the Hadoop file system.</p> <p>For configuring a single node Hadoop cluster, please refer to official Apache Hadoop Docs</p> <p>Once you have a Hadoop cluster up and running, we will need to specify the Hadoop configuration files in the <code>CLASSPATH</code>. The below document expects that you have those configuration files located under <code>/etc/hadoop/conf</code>.</p> <p>Once verified, follow the below steps to add the Hadoop configuration to the <code>CLASSPATH</code> and start the Gremlin Console, which will play the role of the Spark driver program. <pre><code>export HADOOP_CONF_DIR=/etc/hadoop/conf\nexport CLASSPATH=$HADOOP_CONF_DIR\nbin/gremlin.sh\n</code></pre></p> <p>Once the path to Hadoop configuration has been added to the <code>CLASSPATH</code>, we can verify whether the Gremlin Console can access the Hadoop cluster by following these quick steps: <pre><code>gremlin&gt; hdfs\n==&gt;storage[org.apache.hadoop.fs.LocalFileSystem@65bb9029] // BAD\n\ngremlin&gt; hdfs\n==&gt;storage[DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1229457199_1, ugi=user (auth:SIMPLE)]]] // GOOD\n</code></pre></p>"},{"location":"advanced-topics/hadoop/#olap-traversals","title":"OLAP Traversals","text":"<p>JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP to traverse over the graph, and parallelize queries by leveraging Apache Spark.</p>"},{"location":"advanced-topics/hadoop/#olap-traversals-with-spark-local","title":"OLAP Traversals with Spark Local","text":"<p>OLAP Examples below are showing configuration examples for directly supported  backends by JanusGraph. Additional configuration will be needed that  is specific to that storage backend. The configuration is specified by the <code>gremlin.hadoop.graphReader</code> property which specifies the class to read data from the storage backend.</p> <p>JanusGraph directly supports following graphReader classes:</p> <ul> <li><code>CqlInputFormat</code> for use with Cassandra</li> <li><code>HBaseInputFormat</code> and <code>HBaseSnapshotInputFormat</code> for use with HBase</li> </ul> <p>The following <code>.properties</code> files can be used to connect a JanusGraph instance such that it can be used with HadoopGraph to run OLAP queries.</p> read-cql.propertiesread-hbase.properties <pre><code># Copyright 2019 JanusGraph Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# Hadoop Graph Configuration\n#\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.cql.CqlInputFormat\ngremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat\n\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\ngremlin.spark.persistContext=true\n\n#\n# JanusGraph Cassandra InputFormat configuration\n#\n# These properties defines the connection properties which were used while write data to JanusGraph.\njanusgraphmr.ioformat.conf.storage.backend=cql\n# This specifies the hostname &amp; port for Cassandra data store.\njanusgraphmr.ioformat.conf.storage.hostname=127.0.0.1\njanusgraphmr.ioformat.conf.storage.port=9042\n# This specifies the keyspace where data is stored.\njanusgraphmr.ioformat.conf.storage.cql.keyspace=janusgraph\n# This defines the indexing backend configuration used while writing data to JanusGraph.\njanusgraphmr.ioformat.conf.index.search.backend=elasticsearch\njanusgraphmr.ioformat.conf.index.search.hostname=127.0.0.1\n# Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr).\n\n#\n# Apache Cassandra InputFormat configuration\n#\ncassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner\ncassandra.input.widerows=true\n\n#\n# SparkGraphComputer Configuration\n#\nspark.master=local[*]\nspark.executor.memory=1g\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator=org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator\n</code></pre> <pre><code># Copyright 2019 JanusGraph Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# Hadoop Graph Configuration\n#\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.hbase.HBaseInputFormat\ngremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat\n\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\ngremlin.spark.persistContext=true\n\n#\n# JanusGraph HBase InputFormat configuration\n#\njanusgraphmr.ioformat.conf.storage.backend=hbase\njanusgraphmr.ioformat.conf.storage.hostname=localhost\njanusgraphmr.ioformat.conf.storage.hbase.table=janusgraph\n\n#\n# SparkGraphComputer Configuration\n#\nspark.master=local[*]\nspark.executor.memory=1g\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator=org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator\n</code></pre> <p>First create a properties file with above configurations, and load the same on the Gremlin Console to run OLAP queries as follows:</p> read-cql.propertiesread-hbase.properties <pre><code>bin/gremlin.sh\n\n        \\,,,/\n        (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\ngremlin&gt; :plugin use tinkerpop.hadoop\n==&gt;tinkerpop.hadoop activated\ngremlin&gt; :plugin use tinkerpop.spark\n==&gt;tinkerpop.spark activated\ngremlin&gt; // 1. Open a the graph for OLAP processing reading in from Cassandra 3\ngremlin&gt; graph = GraphFactory.open('conf/hadoop-graph/read-cql.properties')\n==&gt;hadoopgraph[cqlinputformat-&gt;gryooutputformat]\ngremlin&gt; // 2. Configure the traversal to run with Spark\ngremlin&gt; g = graph.traversal().withComputer(SparkGraphComputer)\n==&gt;graphtraversalsource[hadoopgraph[cqlinputformat-&gt;gryooutputformat], sparkgraphcomputer]\ngremlin&gt; // 3. Run some OLAP traversals\ngremlin&gt; g.V().count()\n......\n==&gt;808\ngremlin&gt; g.E().count()\n......\n==&gt; 8046\n</code></pre> <pre><code>bin/gremlin.sh\n\n        \\,,,/\n        (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\ngremlin&gt; :plugin use tinkerpop.hadoop\n==&gt;tinkerpop.hadoop activated\ngremlin&gt; :plugin use tinkerpop.spark\n==&gt;tinkerpop.spark activated\ngremlin&gt; // 1. Open a the graph for OLAP processing reading in from HBase\ngremlin&gt; graph = GraphFactory.open('conf/hadoop-graph/read-hbase.properties')\n==&gt;hadoopgraph[hbaseinputformat-&gt;gryooutputformat]\ngremlin&gt; // 2. Configure the traversal to run with Spark\ngremlin&gt; g = graph.traversal().withComputer(SparkGraphComputer)\n==&gt;graphtraversalsource[hadoopgraph[hbaseinputformat-&gt;gryooutputformat], sparkgraphcomputer]\ngremlin&gt; // 3. Run some OLAP traversals\ngremlin&gt; g.V().count()\n......\n==&gt;808\ngremlin&gt; g.E().count()\n......\n==&gt; 8046\n</code></pre>"},{"location":"advanced-topics/hadoop/#olap-traversals-with-spark-standalone-cluster","title":"OLAP Traversals with Spark Standalone Cluster","text":"<p>The steps followed in the previous section can also be used with a Spark standalone cluster with only minor changes:</p> <ul> <li> <p>Update the <code>spark.master</code> property to point to the Spark master URL     instead of local</p> </li> <li> <p>Update the <code>spark.executor.extraClassPath</code> to enable the Spark     executor to find the JanusGraph dependency jars</p> </li> <li> <p>Copy the JanusGraph dependency jars into the location specified in     the previous step on each Spark executor machine</p> </li> </ul> <p>Note</p> <p>We have copied all the jars under janusgraph-distribution/lib into /opt/lib/janusgraph/ and the same directory structure is created across all workers, and jars are manually copied across all workers.</p> <p>The final properties file used for OLAP traversal is as follows:</p> read-cql-standalone-cluster.propertiesread-hbase-standalone-cluster.properties <pre><code># Copyright 2020 JanusGraph Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# Hadoop Graph Configuration\n#\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.cql.CqlInputFormat\ngremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat\n\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\ngremlin.spark.persistContext=true\n\n#\n# JanusGraph Cassandra InputFormat configuration\n#\n# These properties defines the connection properties which were used while write data to JanusGraph.\njanusgraphmr.ioformat.conf.storage.backend=cql\n# This specifies the hostname &amp; port for Cassandra data store.\njanusgraphmr.ioformat.conf.storage.hostname=127.0.0.1\njanusgraphmr.ioformat.conf.storage.port=9042\n# This specifies the keyspace where data is stored.\njanusgraphmr.ioformat.conf.storage.cql.keyspace=janusgraph\n# This defines the indexing backend configuration used while writing data to JanusGraph.\njanusgraphmr.ioformat.conf.index.search.backend=elasticsearch\njanusgraphmr.ioformat.conf.index.search.hostname=127.0.0.1\n# Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr).\n\n#\n# Apache Cassandra InputFormat configuration\n#\ncassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner\ncassandra.input.widerows=true\n\n#\n# SparkGraphComputer Configuration\n#\nspark.master=spark://127.0.0.1:7077\nspark.executor.memory=1g\nspark.executor.extraClassPath=/opt/lib/janusgraph/*\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator=org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator\n</code></pre> <pre><code># Copyright 2020 JanusGraph Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# Hadoop Graph Configuration\n#\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.hbase.HBaseInputFormat\ngremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat\n\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\ngremlin.spark.persistContext=true\n\n#\n# JanusGraph HBase InputFormat configuration\n#\njanusgraphmr.ioformat.conf.storage.backend=hbase\njanusgraphmr.ioformat.conf.storage.hostname=localhost\njanusgraphmr.ioformat.conf.storage.hbase.table=janusgraph\n\n#\n# SparkGraphComputer Configuration\n#\nspark.master=spark://127.0.0.1:7077\nspark.executor.memory=1g\nspark.executor.extraClassPath=/opt/lib/janusgraph/*\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator=org.janusgraph.hadoop.serialize.JanusGraphKryoRegistrator\n</code></pre> <p>Then use the properties file as follows from the Gremlin Console:</p> read-cql-standalone-cluster.propertiesread-hbase-standalone-cluster.properties <pre><code>bin/gremlin.sh\n\n        \\,,,/\n        (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\ngremlin&gt; :plugin use tinkerpop.hadoop\n==&gt;tinkerpop.hadoop activated\ngremlin&gt; :plugin use tinkerpop.spark\n==&gt;tinkerpop.spark activated\ngremlin&gt; // 1. Open a the graph for OLAP processing reading in from Cassandra 3\ngremlin&gt; graph = GraphFactory.open('conf/hadoop-graph/read-cql-standalone-cluster.properties')\n==&gt;hadoopgraph[cqlinputformat-&gt;gryooutputformat]\ngremlin&gt; // 2. Configure the traversal to run with Spark\ngremlin&gt; g = graph.traversal().withComputer(SparkGraphComputer)\n==&gt;graphtraversalsource[hadoopgraph[cqlinputformat-&gt;gryooutputformat], sparkgraphcomputer]\ngremlin&gt; // 3. Run some OLAP traversals\ngremlin&gt; g.V().count()\n......\n==&gt;808\ngremlin&gt; g.E().count()\n......\n==&gt; 8046\n</code></pre> <pre><code>bin/gremlin.sh\n\n        \\,,,/\n        (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\ngremlin&gt; :plugin use tinkerpop.hadoop\n==&gt;tinkerpop.hadoop activated\ngremlin&gt; :plugin use tinkerpop.spark\n==&gt;tinkerpop.spark activated\ngremlin&gt; // 1. Open a the graph for OLAP processing reading in from HBase\ngremlin&gt; graph = GraphFactory.open('conf/hadoop-graph/read-hbase-standalone-cluster.properties')\n==&gt;hadoopgraph[hbaseinputformat-&gt;gryooutputformat]\ngremlin&gt; // 2. Configure the traversal to run with Spark\ngremlin&gt; g = graph.traversal().withComputer(SparkGraphComputer)\n==&gt;graphtraversalsource[hadoopgraph[hbaseinputformat-&gt;gryooutputformat], sparkgraphcomputer]\ngremlin&gt; // 3. Run some OLAP traversals\ngremlin&gt; g.V().count()\n......\n==&gt;808\ngremlin&gt; g.E().count()\n......\n==&gt; 8046\n</code></pre>"},{"location":"advanced-topics/hadoop/#other-vertex-programs","title":"Other Vertex Programs","text":"<p>Apache TinkerPop provides various vertex programs. A vertex program runs on each vertex until either a termination criteria is attained or a fixed number of iterations has been reached. Due to the parallel nature of vertex programs, they can leverage parallel computing framework like Spark to improve their performance.</p> <p>Once you are familiar with how to configure JanusGraph to work with Spark, you can run all the other vertex programs provided by Apache TinkerPop, like Page Rank, Bulk Loading and Peer Pressure. See the TinkerPop VertexProgram docs for more details.</p>"},{"location":"advanced-topics/janusgraph-bus/","title":"JanusGraph Bus","text":"<p>The JanusGraph Bus describes a collection of configurable logs to which JanusGraph writes changes to the graph and its management. The JanusGraph Bus is used for internal (i.e. between multiple JanusGraph instances) and external (i.e. integration with other systems) communication.</p> <p>In particular, JanusGraph maintains three separate logs:</p>"},{"location":"advanced-topics/janusgraph-bus/#trigger-log","title":"Trigger Log","text":"<p>The purpose of the trigger log is to capture the mutations of a transaction so that the resulting changes to the graph can trigger events in other system. Such events may be propagating the change to other data stores, view maintenance, or aggregate computation.</p> <p>The trigger log consists of multiple sub-logs as configured by the user. When opening a transaction, the identifier for the trigger sub-log can be specified: <pre><code>tx = g.buildTransaction().logIdentifier(\"purchase\").start();\n</code></pre></p> <p>In this case, the identifier is \"purchase\" which means that the mutations of this transaction will be written to a log with the name \"trigger_purchase\". This gives the user control over where transactional mutations are logged. If no trigger log is specified, no trigger log entry will be created.</p>"},{"location":"advanced-topics/janusgraph-bus/#transaction-log","title":"Transaction Log","text":"<p>The transaction log is maintained by JanusGraph and contains two entries for each transaction if enabled: 1. Pre-Commit: Before the changes are persisted to the storage and indexing backends, the changes are compiled and written to the log. 2. Post-Commit: The success status of the transaction is written to the log.</p> <p>In this way, the transaction log functions as a Write-Ahead-Log (WAL). This log is not meant for consumption by the user or external systems - use trigger logs for that. It is used internally to store partial transaction persistence against eventually consistent backends.</p> <p>The transaction log can be enabled via the root-level configuration option \"log-tx\".</p>"},{"location":"advanced-topics/janusgraph-bus/#management-log","title":"Management Log","text":"<p>The management log is maintained by JanusGraph internally to communicate and persist all changes to global configuration options or the graph schema.</p>"},{"location":"advanced-topics/partitioning/","title":"Graph Partitioning","text":"<p>When JanusGraph is deployed on a cluster of multiple storage backend instances, the graph is partitioned across those machines. Since JanusGraph stores the graph in an adjacency list representation the assignment of vertices to machines determines the partitioning. By default, JanusGraph uses a random partitioning strategy that randomly assigns vertices to machines. Random partitioning is very efficient, requires no configuration, and results in balanced partitions. Currently explicit partitioning is not supported.</p> <pre><code>cluster.max-partitions = 32\nids.placement = simple \n</code></pre> <p>The configuration option <code>max-partitions</code> controls how many virtual partitions JanusGraph creates. This number should be roughly twice the number of storage backend instances. If the cluster of storage backend instances is expected to grow, estimate the size of the cluster in the foreseeable future and take this number as the baseline. Setting this number too large will unnecessarily fragment the cluster which can lead to poor performance. This number should be larger than the maximum expected number of nodes in the JanusGraph graph. It must be greater than 1 and a power of 2. </p> <p>There are two aspects to graph partitioning which can be individually controlled: edge cuts and vertex cuts.</p>"},{"location":"advanced-topics/partitioning/#edge-cut","title":"Edge Cut","text":"<p>In assigning vertices to partitions one strives to optimize the assignment such that frequently co-traversed vertices are hosted on the same machine. Assume vertex A is assigned to machine 1 and vertex B is assigned to machine 2. An edge between the vertices is called a cut edge because its end points are hosted on separate machines. Traversing this edge as part of a graph query requires communication between the machines which slows down query processing. Hence, it is desirable to reduce the edge cut for frequently traversed edges. That, in turn, requires placing the adjacent vertices of frequently traversed edges in the same partition.</p> <p>Vertices are placed in a partition by way of the assigned vertex id. A partition is essentially a sequential range of vertex ids. To place a vertex in a particular partition, JanusGraph chooses an id from the partition\u2019s range of vertex ids. JanusGraph controls the vertex-to-partition assignment through the configured placement strategy. By default, vertices created in the same transaction are assigned to the same partition. This strategy is easy to reason about and works well in situations where frequently co-traversed vertices are created in the same transaction - either by optimizing the loading strategy to that effect or because vertices are naturally added to the graph that way. However, the strategy is limited, leads to imbalanced partitions when data is loaded in large transactions and not the optimal strategy for many use cases. The user can provide a use case specific vertex placement strategy by implementing the <code>IDPlacementStrategy</code> interface and registering it in the configuration through the <code>ids.placement</code> option.</p> <p>When implementing <code>IDPlacementStrategy</code>, note that partitions are identified by an integer id in the range from 0 to the number of configured virtual partitions minus 1. For our example configuration, there are partitions 0, 1, 2, 3, ..31. Partition ids are not the same as vertex ids. Edge cuts are more meaningful when the JanusGraph servers are on the same hosts as the storage backend. If you have to make a network call to a different host on each hop of a traversal, the benefit of edge cuts and custom placement strategies can be largely nullified.</p>"},{"location":"advanced-topics/partitioning/#vertex-cut","title":"Vertex Cut","text":"<p>While edge cut optimization aims to reduce the cross communication and thereby improve query execution, vertex cuts address the hotspot issue caused by vertices with a large number of incident edges. While vertex-centric indexes effectively address query performance for large degree vertices, vertex cuts are needed to address the hot spot issue on very large graphs.</p> <p>Cutting a vertex means storing a subset of that vertex\u2019s adjacency list on each partition in the graph. In other words, the vertex and its adjacency list is partitioned thereby effectively distributing the load on that single vertex across all of the instances in the cluster and removing the hot spot.</p> <p>JanusGraph cuts vertices by label. A vertex label can be defined as partitioned which means that all vertices of that label will be partitioned across the cluster in the manner described above.</p> <pre><code>mgmt = graph.openManagement()\nmgmt.makeVertexLabel('user').make()\nmgmt.makeVertexLabel('product').partition().make()\nmgmt.commit()\n</code></pre> <p>In the example above, <code>product</code> is defined as a partitioned vertex label whereas <code>user</code> is a normal label. This configuration is beneficial for situations where there are thousands of products but millions of users and one records transactions between users and products. In that case, the product vertices will have a very high degree and the popular products turns into hot spots if they are not partitioned. However, products that don't have high degree are also partitioned, causing unnecessary overhead.</p> <p>Warning</p> <p>Vertex partition feature has high overhead and is not production-ready. We generally discourage usage of this feature. For more detail, see Discussion#2717.</p>"},{"location":"advanced-topics/partitioning/#graph-partitioning-faq","title":"Graph Partitioning FAQ","text":""},{"location":"advanced-topics/partitioning/#random-vs-explicit-partitioning","title":"Random vs. Explicit Partitioning","text":"<p>When the graph is small or accommodated by a few storage instances, it is best to use random partitioning for its simplicity. As a rule of thumb, one should strongly consider enabling explicit graph partitioning and configure a suitable partitioning heuristic when the graph grows into the 10s of billions of edges.</p>"},{"location":"advanced-topics/serializer/","title":"Datatype and Attribute Serializer Configuration","text":"<p>JanusGraph supports a number of classes for attribute values on properties. JanusGraph efficiently serializes primitives, primitive arrays and <code>Geoshape</code>, <code>UUID</code>, <code>Date</code>, <code>ObjectNode</code> and <code>ArrayNode</code>. JanusGraph supports serializing arbitrary objects as attribute values, but these require custom serializers to be defined.</p> <p>To configure a custom attribute class with a custom serializer, follow these steps:</p> <ol> <li> <p>Implement a custom <code>AttributeSerializer</code> for the custom attribute     class</p> </li> <li> <p>Add the following configuration options where [X] is the custom     attribute id that must be larger than all attribute ids for already     configured custom attributes:</p> <ol> <li> <p><code>attributes.custom.attribute[X].attribute-class = [Full attribute class name]</code></p> </li> <li> <p><code>attributes.custom.attribute[X].serializer-class = [Full serializer class name]</code></p> </li> </ol> </li> </ol> <p>For example, suppose we want to register a special integer attribute class called <code>SpecialInt</code> and have implemented a custom serializer <code>SpecialIntSerializer</code> that implements <code>AttributeSerializer</code>. We already have 9 custom attributes configured in the configuration file, so we would add the following lines  <pre><code>attributes.custom.attribute10.attribute-class = com.example.SpecialInt\nattributes.custom.attribute10.serializer-class = com.example.SpecialIntSerializer\n</code></pre></p>"},{"location":"advanced-topics/serializer/#custom-object-serialization","title":"Custom Object Serialization","text":"<p>JanusGraph supports arbitrary objects as property attributes and can serialize such objects to disk. For this default serializer to work for a custom class, the following conditions must be fulfilled:</p> <ul> <li> <p>The class must implement AttributeSerializer</p> </li> <li> <p>The class must have a no-argument constructor</p> </li> <li> <p>The class must implement the <code>equals(Object)</code> method</p> </li> </ul> <p>The last requirement is needed because JanusGraph will test both serialization and deserialization of a custom class before persisting data to disk.</p>"},{"location":"advanced-topics/stale-index/","title":"Permanent stale index inconsistency","text":"<p>In some situations due to crashes of storage database, index database, or JanusGraph instances  permanent stale index may appear. One of such cases could be a vertex removal from the graph but due to a crash during index persistence  there is a chance that index won't be updated ever.  In case we try to remove a vertex which is already gone from the storage database but which is still indexed then  the <code>IllegalStateException</code> will be thrown with the message <code>Vertex with id %vertexId% was removed.</code>.  For example, using <code>g.V().has(\"name\", \"HelloWorld\").drop().iterate(); g.tx().commit();</code> we may receive an exception  noting that some vertex has already been removed nevertheless it's record is still in the index.  The exception will be thrown anytime we try to remove such vertex from the graph. This problem is known and should be temporal limitation until this issue is fixed in JanusGraph. As for now there is the utility tool which may be used to fix permanent stale indices.  </p>"},{"location":"advanced-topics/stale-index/#staleindexrecordutil","title":"StaleIndexRecordUtil","text":"<p><code>StaleIndexRecordUtil.class</code> is available in <code>janusgraph-core</code> module and is meant to be used as a helper class  to fix permanent stale index entries. </p> <p><code>StaleIndexRecordUtil.forceRemoveVertexFromGraphIndex</code> can be used to force remove an index record for any vertex from  a graph index.  An example of using this method is below: <pre><code>// Let's say we want to remove non-existent vertex from a stale index. \n// We will assume the next constraints: \n// Vertex id is: `12345`\n// Index name is: `nameAgeIndex`\n// There are two indexed properties: `name` and `age`\n// Value of the name property is: `HelloWorld`\n// Value of the age property is: `123`\n\nJanusGraph graph = JanusGraphFactory.open(configuration);\n\nMap&lt;String, Object&gt; indexRecordPropertyValues = new HashMap&lt;&gt;();\nindexRecordPropertyValues.put(\"name\", \"HelloWorld\");\nindexRecordPropertyValues.put(\"age\", 123);\n\n// After the below method is executed index entry of the vertex 12345 should be removed from the index which \n// effectively fixes permanent stale index inconsistency\nStaleIndexRecordUtil.forceRemoveVertexFromGraphIndex(\n    12345L, // vertex id of the index record to be removed\n    indexRecordPropertyValues, // index record property values\n    graph,\n    \"nameAgeIndex\" // graph index name for which to remove the index record\n);\n</code></pre> For more in-depth information about usage of this tool as well as explanations of additional methods of this tool see JavaDoc for <code>StaleIndexRecordUtil</code>.</p>"},{"location":"advanced-topics/technical-limitations/","title":"Technical Limitations","text":"<p>There are various limitations and \"gotchas\" that one should be aware of when using JanusGraph. Some of these limitations are necessary design choices and others are issues that will be rectified as JanusGraph development continues. Finally, the last section provides solutions to common issues.</p>"},{"location":"advanced-topics/technical-limitations/#design-limitations","title":"Design Limitations","text":"<p>These limitations reflect long-term tradeoffs design tradeoffs which are either difficult or impractical to change. These limitations are unlikely to be removed in the near future.</p>"},{"location":"advanced-topics/technical-limitations/#size-limitation","title":"Size Limitation","text":"<p>JanusGraph can store up to a quintillion edges (2^60) and half as many vertices. That limitation is imposed by JanusGraph\u2019s id scheme.</p>"},{"location":"advanced-topics/technical-limitations/#datatype-definitions","title":"DataType Definitions","text":"<p>When declaring the data type of a property key using <code>dataType(Class)</code> JanusGraph will enforce that all properties for that key have the declared type, unless that type is <code>Object.class</code>. This is an equality type check, meaning that sub-classes will not be allowed. For instance, one cannot declare the data type to be <code>Number.class</code> and use <code>Integer</code> or <code>Long</code>. For efficiency reasons, the type needs to match exactly. Hence, use <code>Object.class</code> as the data type for type flexibility. In all other cases, declare the actual data type to benefit from increased performance and type safety.</p>"},{"location":"advanced-topics/technical-limitations/#edge-retrievals-are-ologk","title":"Edge Retrievals are O(log(k))","text":"<p>Retrieving an edge by id, e.g <code>tx.getEdge(edge.getId())</code>, is not a constant time operation because it requires an index call on one of its adjacent vertices. Hence, the cost of retrieving an individual edge by its id is <code>O(log(k))</code> where <code>k</code> is the number of incident edges on the adjacent vertex. JanusGraph will attempt to pick the adjacent vertex with the smaller degree.</p> <p>This also applies to index retrievals for edges via a standard or external index.</p>"},{"location":"advanced-topics/technical-limitations/#type-definitions-cannot-be-changed","title":"Type Definitions cannot be changed","text":"<p>The definition of an edge label, property key, or vertex label cannot be changed once it has been committed to the graph. However, a type can be renamed and new types can be created at runtime to accommodate an evolving schema.</p>"},{"location":"advanced-topics/technical-limitations/#reserved-keywords","title":"Reserved Keywords","text":"<p>There are certain keywords that JanusGraph uses internally for types that cannot be used otherwise. These types include vertex labels, edge labels, and property keys. The following are keywords that cannot be used:</p> <ul> <li> <p>vertex</p> </li> <li> <p>element</p> </li> <li> <p>edge</p> </li> <li> <p>property</p> </li> <li> <p>label</p> </li> <li> <p>key</p> </li> </ul> <p>For example, if you attempt to create a vertex with the label of <code>property</code>, you will receive an exception regarding protected system types.</p>"},{"location":"advanced-topics/technical-limitations/#temporary-limitations","title":"Temporary Limitations","text":"<p>These are limitations in JanusGraph\u2019s current implementation. These limitations could reasonably be removed in upcoming versions of JanusGraph.</p>"},{"location":"advanced-topics/technical-limitations/#limited-mixed-index-support","title":"Limited Mixed Index Support","text":"<p>Mixed indexes only support a subset of the data types that JanusGraph supports. See Mixed Index Data Types for a current listing. Also, mixed indexes do not currently support property keys with SET or LIST cardinality.</p>"},{"location":"advanced-topics/technical-limitations/#batch-loading-speed","title":"Batch Loading Speed","text":"<p>JanusGraph provides a batch loading mode that can be enabled through the graph configuration. However, this batch mode only facilitates faster loading into the storage backend, it does not use storage backend specific batch loading techniques that prepare the data in memory for disk storage. As such, batch loading in JanusGraph is currently slower than batch loading modes provided by single machine databases. Bulk Loading contains information on speeding up batch loading in JanusGraph.</p> <p>Another limitation related to batch loading is the failure to load millions of edges into a single vertex at once or in a short time of period. Such supernode loading can fail for some storage backends. This limitation also applies to dense index entries.</p>"},{"location":"advanced-topics/transaction-log/","title":"Transaction Log","text":"<p>JanusGraph can automatically log transactional changes for additional processing or as a record of change. To enable logging for a particular transaction, specify the name of the target log during the start of the transaction. <pre><code>tx = graph.buildTransaction().logIdentifier('addedPerson').start()\nu = tx.addVertex(label, 'human')\nu.property('name', 'proteros')\nu.property('age', 36)\ntx.commit()\n</code></pre></p> <p>Upon commit, any changes made during the transaction are logged to the user logging system into a log named <code>addedPerson</code>. The user logging system is a configurable logging backend with a JanusGraph compatible log interface. By default, the log is written to a separate store in the primary storage backend which can be configured as described below. The log identifier specified during the start of the transaction identifies the log in which the changes are recorded thereby allowing different types of changes to be recorded in separate logs for individual processing. <pre><code>tx = graph.buildTransaction().logIdentifier('battle').start()\nh = tx.traversal().V().has('name', 'hercules').next()\nm = tx.addVertex(label, 'monster')\nm.property('name', 'phylatax')\nh.addEdge('battled', m, 'time', 22)\ntx.commit()\n</code></pre></p> <p>JanusGraph provides a user transaction log processor framework to process the recorded transactional changes. The transaction log processor is opened via <code>JanusGraphFactory.openTransactionLog(JanusGraph)</code> against a previously opened JanusGraph graph instance. One can then add processors for a particular log which holds transactional changes. <pre><code>import java.util.concurrent.atomic.*;\nimport org.janusgraph.core.log.*;\nimport java.util.concurrent.*;\nlogProcessor = JanusGraphFactory.openTransactionLog(g);\ntotalHumansAdded = new AtomicInteger(0);\ntotalGodsAdded = new AtomicInteger(0);\nlogProcessor.addLogProcessor(\"addedPerson\").\n    setProcessorIdentifier(\"addedPersonCounter\").\n    setStartTimeNow().\n    addProcessor(new ChangeProcessor() {\n        @Override\n        public void process(JanusGraphTransaction tx, TransactionId txId, ChangeState changeState) {\n            for (v in changeState.getVertices(Change.ADDED)) {\n                if (v.label().equals(\"human\")) totalHumansAdded.incrementAndGet();\n            }\n        }\n    }).\n    addProcessor(new ChangeProcessor() {\n        @Override\n        public void process(JanusGraphTransaction tx, TransactionId txId, ChangeState changeState) {\n            for (v in changeState.getVertices(Change.ADDED)) {\n                if (v.label().equals(\"god\")) totalGodsAdded.incrementAndGet();\n            }\n        }\n    }).\n    build();\n</code></pre></p> <p>In this example, a log processor is built for the user transaction log named <code>addedPerson</code> to process the changes made in transactions which used the <code>addedPerson</code> log identifier. Two change processors are added to this log processor. The first processor counts the number of humans added and the second counts the number of gods added to the graph.</p> <p>When a log processor is built against a particular log, such as the <code>addedPerson</code> log in the example above, it will start reading transactional change records from the log immediately upon successful construction and initialization up to the head of the log. The start time specified in the builder marks the time point in the log where the log processor will start reading records. Optionally, one can specify an identifier for the log processor in the builder. The log processor will use the identifier to regularly persist its state of processing, i.e. it will maintain a marker on the last read log record. If the log processor is later restarted with the same identifier, it will continue reading from the last read record. This is particularly useful when the log processor is supposed to run for long periods of time and is therefore likely to fail. In such failure situations, the log processor can simply be restarted with the same identifier. It must be ensured that log processor identifiers are unique in a JanusGraph cluster in order to avoid conflicts on the persisted read markers.</p> <p>A change processor must implement the <code>ChangeProcessor</code> interface. It\u2019s <code>process()</code> method is invoked for each change record read from the log with a <code>JanusGraphTransaction</code> handle, the id of the transaction that caused the change, and a <code>ChangeState</code> container which holds the transactional changes. The change state container can be queried to retrieve individual elements that were part of the change state. In the example, all added vertices are retrieved. Refer to the API documentation for a description of all the query methods on <code>ChangeState</code>. The provided transaction id can be used to investigate the origin of the transaction which is uniquely identified by the combination of the id of the JanusGraph instance that executed the transaction (<code>txId.getInstanceId()</code>) and the instance specific transaction id (<code>txId.getTransactionId()</code>). In addition, the time of the transaction is available through <code>txId.getTransactionTime()</code>.</p> <p>Change processors are executed individually and in multiple threads. If a change processor accesses global state it must be ensured that such state allows concurrent access. While the log processor reads log records sequentially, the changes are processed in multiple threads so it cannot be guaranteed that the log order is preserved in the change processors.</p> <p>Note, that log processors run each registered change processor at least once for each record in the log which means that a single transactional change record may be processed multiple times under certain failure conditions. One cannot add or remove change processor from a running log processor. In other words, a log processor is immutable after it is built. To change log processing, start a new log processor and shut down an existing one. <pre><code>logProcessor.addLogProcessor(\"battle\").\n    setProcessorIdentifier(\"battleTimer\").\n    setStartTimeNow().\n    addProcessor(new ChangeProcessor() {\n        @Override\n        public void process(JanusGraphTransaction tx, TransactionId txId, ChangeState changeState) {\n            h = tx.V().has(\"name\", \"hercules\").toList().iterator().next();\n            for (edge in changeState.getEdges(h, Change.ADDED, Direction.OUT, \"battled\")) {\n                if (edge.&lt;Integer&gt;value(\"time\")&gt;1000)\n                    h.property(\"oldFighter\", true);\n            }\n        }\n    }).\n    build();\n</code></pre></p> <p>The log processor above processes transactions for the <code>battle</code> log identifier with a single change processor which evaluates <code>battled</code> edges that were added to Hercules. This example demonstrates that the transaction handle passed into the change processor is a normal <code>JanusGraphTransaction</code> which query the JanusGraph graph and make changes to it.</p>"},{"location":"advanced-topics/transaction-log/#transaction-log-use-cases","title":"Transaction Log Use Cases","text":""},{"location":"advanced-topics/transaction-log/#record-of-change","title":"Record of Change","text":"<p>The user transaction log can be used to keep a record of all changes made against the graph. By using separate log identifiers, changes can be recorded in different logs to distinguish separate transaction types.</p> <p>At any time, a log processor can be built which can processes all recorded changes starting from the desired start time. This can be used for forensic analysis, to replay changes against a different graph, or to compute an aggregate.</p>"},{"location":"advanced-topics/transaction-log/#downstream-updates","title":"Downstream Updates","text":"<p>It is often the case that a JanusGraph graph cluster is part of a larger architecture. The user transaction log and the log processor framework provide the tools needed to broadcast changes to other components of the overall system without slowing down the original transactions causing the change. This is particularly useful when transaction latencies need to be low and/or there are a number of other systems that need to be alerted to a change in the graph.</p>"},{"location":"advanced-topics/transaction-log/#triggers","title":"Triggers","text":"<p>The user transaction log provides the basic infrastructure to implement triggers that can scale to a large number of concurrent transactions and very large graphs. A trigger is registered with a particular change of data and either triggers an event in an external system or additional changes to the graph. At scale, it is not advisable to implement triggers in the original transaction but rather process triggers with a slight delay through the log processor framework. The second example shows how changes to the graph can be evaluated and trigger additional modifications.</p>"},{"location":"advanced-topics/transaction-log/#log-configuration","title":"Log Configuration","text":"<p>There are a number of configuration options to fine tune how the log processor reads from the log. Refer to the complete list of configuration options Configuration Reference for the options under the <code>log</code> namespace. To configure the user transaction log, use the <code>log.user</code> namespace. The options listed there allow the configuration of the number of threads to be used, the number of log records read in each batch, the read interval, and whether the transaction change records should automatically expire and be removed from the log after a configurable amount of time (TTL).</p> <p>An example configuration for user transaction log could look like below:</p> <pre><code>log.user.backend=default\nlog.user.fixed-partition=false\nlog.user.max-read-time=10000 ms\nlog.user.max-write-time=5000 ms\nlog.user.read-batch-size=1024\nlog.user.read-interval=3000 ms\nlog.user.read-lag-time=2000 ms\nlog.user.read-threads=1\nlog.user.send-batch-size=256\nlog.user.send-delay=0 ms\nlog.user.ttl=120000 ms\n</code></pre>"},{"location":"advanced-topics/transaction-log/#custom-storage-backend","title":"Custom storage backend","text":"<p>By default JanusGraph supports a single log backend implementation which is defined  by a reserved shortcut <code>default</code> in <code>log.user.backend</code> configuration option and has  the following class implementation <code>org.janusgraph.diskstorage.log.kcvs.KCVSLogManager</code>.  </p> <p><code>KCVSLogManager</code> reuses the graph's storage backend (defined by <code>storage.backend</code>) to store all logs. Usually for default management-only operations the default storage backend is good enough to manage logs  and custom log storage backend is not necessary. That said, in case User Transaction Log is used the underlying graph's storage backend might be far from optimal  in highly loaded applications for <code>log.user</code> log and a better storage might be preferred (i.e. Kafka or else).  Thus, JanusGraph allows providing custom log implementations via <code>log.[x].backend</code> option. The custom log implementation needs to implement <code>org.janusgraph.diskstorage.log.LogManager</code> interface and the  implementation class has to have a public constructor which accepts <code>org.janusgraph.diskstorage.configuration.Configuration</code>  as a single parameter. </p> <p>The main purpose of that <code>LogManager</code> implementation is to open new logging implementations which extend  <code>org.janusgraph.diskstorage.log.Log</code> interface.  For example, logs opened by <code>KCVSLogManager</code> have <code>org.janusgraph.diskstorage.log.kcvs.KCVSLog</code> implementations  which manage logs inside graph's storage backend but the user may implement <code>Log</code> which uses Kafka storage backend or else.  </p> <p>To specify any custom log implementation it's necessary to provide full class path of the <code>LogManager</code> implementing class  via <code>log.[x].backend</code> configuration option.</p>"},{"location":"basics/search-predicates/","title":"Search Predicates and Mixed Index Data Types","text":"<p>This page lists all of the comparison predicates that JanusGraph supports in global graph search and local traversals.</p> <p>Note</p> <p>Some parts of this section require mixed index, see mixed index backend.</p>"},{"location":"basics/search-predicates/#compare-predicate","title":"Compare Predicate","text":"<p>The <code>Compare</code> enum specifies the following comparison predicates used for index query construction and used in the examples above:</p> <ul> <li><code>eq</code> (equal)</li> <li><code>neq</code> (not equal)</li> <li><code>gt</code> (greater than)</li> <li><code>gte</code> (greater than or equal)</li> <li><code>lt</code> (less than)</li> <li><code>lte</code> (less than or equal)</li> </ul> <p>All comparison predicates are supported by String, numeric, Date and Instant data types. Boolean and UUID data types support the <code>eq</code> and <code>neq</code> comparison predicates.</p> <p><code>eq</code> and <code>neq</code> can be used on Boolean and UUID.</p>"},{"location":"basics/search-predicates/#text-predicate","title":"Text Predicate","text":"<p>The <code>Text</code> enum specifies the Text Search used to query for matching text or string values.  We differentiate between two types of predicates:</p> <ul> <li>Text search predicates which match against the individual words inside a text string after it has been tokenized. These predicates are not case sensitive.<ul> <li><code>textContains</code>: is true if (at least) one word inside the text string matches the query string</li> <li><code>textNotContains</code>: is true if no words inside the text string match the query string</li> <li><code>textContainsPrefix</code>: is true if (at least) one word inside the text string begins with the query string</li> <li><code>textNotContainsPrefix</code>: is true if no words inside the text string begin with the query string</li> <li><code>textContainsRegex</code>: is true if (at least) one word inside the text string matches the given regular expression</li> <li><code>textNotContainsRegex</code>: is true if no words inside the text string match the given regular expression</li> <li><code>textContainsFuzzy</code>: is true if (at least) one word inside the text string is similar to the query string (based on Levenshtein edit distance)</li> <li><code>textNotContainsFuzzy</code>:  is true if no words inside the text string are similar to the query string (based on Levenshtein edit distance)</li> <li><code>textContainsPhrase</code>: is true if the text string contains the exact sequence of words in the query string</li> <li><code>textNotContainsPhrase</code>:  is true if the text string does not contain the sequence of words in the query string</li> </ul> </li> <li>String search predicates which match against the entire string value<ul> <li><code>textPrefix</code>: if the string value starts with the given query string</li> <li><code>textNotPrefix</code>: if the string value does not start with the given query string</li> <li><code>textRegex</code>: if the string value matches the given regular expression in its entirety</li> <li><code>textNotRegex</code>: if the string value does not match the given regular expression in its entirety</li> <li><code>textFuzzy</code>: if the string value is similar to the given query string (based on Levenshtein edit distance)</li> <li><code>textNotFuzzy</code>: if the string value is not similar to the given query string (based on Levenshtein edit distance)</li> </ul> </li> </ul> <p>See Text Search for more information about full-text and string search.</p>"},{"location":"basics/search-predicates/#geo-predicate","title":"Geo Predicate","text":"<p>The <code>Geo</code> enum specifies geo-location predicates.</p> <ul> <li><code>geoIntersect</code> which holds true if the two geometric objects have at least one point in common (opposite of <code>geoDisjoint</code>).</li> <li><code>geoWithin</code> which holds true if one geometric object contains the other.</li> <li><code>geoDisjoint</code> which holds true if the two geometric objects have no points in common (opposite of <code>geoIntersect</code>).</li> <li><code>geoContains</code> which holds true if one geometric object is contained by the other.</li> </ul> <p>See Geo Mapping for more information about geo search.</p>"},{"location":"basics/search-predicates/#query-examples","title":"Query Examples","text":"<p>The following query examples demonstrate some of the predicates on the tutorial graph.</p> <pre><code>// 1) Find vertices with the name \"hercules\"\ng.V().has(\"name\", \"hercules\")\n// 2) Find all vertices with an age greater than 50\ng.V().has(\"age\", gt(50))\n// or find all vertices between 1000 (inclusive) and 5000 (exclusive) years of age and order by ascending age\ng.V().has(\"age\", inside(1000, 5000)).order().by(\"age\", asc)\n// which returns the same result set as the following query but in reverse order\ng.V().has(\"age\", inside(1000, 5000)).order().by(\"age\", desc)\n// 3) Find all edges where the place is at most 50 kilometers from the given latitude-longitude pair\ng.E().has(\"place\", geoWithin(Geoshape.circle(37.97, 23.72, 50)))\n// 4) Find all edges where reason contains the word \"loves\"\ng.E().has(\"reason\", textContains(\"loves\"))\n// or all edges which contain two words (need to chunk into individual words)\ng.E().has(\"reason\", textContains(\"loves\")).has(\"reason\", textContains(\"breezes\"))\n// or all edges which contain words that start with \"lov\"\ng.E().has(\"reason\", textContainsPrefix(\"lov\"))\n// or all edges which contain words that match the regular expression \"br[ez]*s\" in their entirety\ng.E().has(\"reason\", textContainsRegex(\"br[ez]*s\"))\n// or all edges which contain words similar to \"love\"\ng.E().has(\"reason\", textContainsFuzzy(\"love\"))\n// 5) Find all vertices older than a thousand years and named \"saturn\"\ng.V().has(\"age\", gt(1000)).has(\"name\", \"saturn\")\n</code></pre>"},{"location":"basics/search-predicates/#data-type-support","title":"Data Type Support","text":"<p>While JanusGraph's composite indexes support any data type that can be stored in JanusGraph, the mixed indexes are limited to the following data types.</p> <ul> <li>Byte</li> <li>Short</li> <li>Integer</li> <li>Long</li> <li>Float</li> <li>Double</li> <li>String</li> <li>Geoshape</li> <li>Date</li> <li>Instant</li> <li>UUID</li> </ul> <p>Additional data types will be supported in the future.</p>"},{"location":"basics/search-predicates/#geoshape-data-type","title":"Geoshape Data Type","text":"<p>The Geoshape data type supports representing a point, circle, box, line, polygon, multi-point, multi-line and multi-polygon. Index backends currently support indexing points, circles, boxes, lines, polygons, multi-point, multi-line, multi-polygon and geometry collection. Geospatial index lookups are only supported via mixed indexes.</p> <p>To construct a Geoshape use the following methods:</p> <pre><code> //lat, lng\nGeoshape.point(37.97, 23.72)\n//lat, lng, radius in km\nGeoshape.circle(37.97, 23.72, 50)\n//SW lat, SW lng, NE lat, NE lng\nGeoshape.box(37.97, 23.72, 38.97, 24.72)\n//WKT\nGeoshape.fromWkt(\"POLYGON ((35.4 48.9, 35.6 48.9, 35.6 49.1, 35.4 49.1, 35.4 48.9))\")\n//MultiPoint\nGeoshape.geoshape(Geoshape.getShapeFactory().multiPoint().pointXY(60.0, 60.0).pointXY(120.0, 60.0)\n  .build())\n//MultiLine\nGeoshape.geoshape(Geoshape.getShapeFactory().multiLineString()\n  .add(Geoshape.getShapeFactory().lineString().pointXY(59.0, 60.0).pointXY(61.0, 60.0))\n  .add(Geoshape.getShapeFactory().lineString().pointXY(119.0, 60.0).pointXY(121.0, 60.0)).build())\n//MultiPolygon\nGeoshape.geoshape(Geoshape.getShapeFactory().multiPolygon()\n  .add(Geoshape.getShapeFactory().polygon().pointXY(59.0, 59.0).pointXY(61.0, 59.0)\n    .pointXY(61.0, 61.0).pointXY(59.0, 61.0).pointXY(59.0, 59.0))\n  .add(Geoshape.getShapeFactory().polygon().pointXY(119.0, 59.0).pointXY(121.0, 59.0)\n    .pointXY(121.0, 61.0).pointXY(119.0, 61.0).pointXY(119.0, 59.0)).build())\n//GeometryCollection\nGeoshape.geoshape(Geoshape.getGeometryCollectionBuilder()\n  .add(Geoshape.getShapeFactory().pointXY(60.0, 60.0))\n  .add(Geoshape.getShapeFactory().lineString().pointXY(119.0, 60.0).pointXY(121.0, 60.0).build())\n  .add(Geoshape.getShapeFactory().polygon().pointXY(119.0, 59.0).pointXY(121.0, 59.0)\n    .pointXY(121.0, 61.0).pointXY(119.0, 61.0).pointXY(119.0, 59.0).build()).build())\n</code></pre> <p>In addition, when importing a graph via GraphSON the geometry may be represented by GeoJSON:</p> stringlistGeoJSON featureGeoJSON geometry <pre><code>\"37.97, 23.72\"\n</code></pre> <pre><code>[37.97, 23.72]\n</code></pre> <pre><code>{\n    \"type\": \"Feature\",\n    \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n    },\n    \"properties\": {\n    \"name\": \"Dinagat Islands\"\n    }\n}\n</code></pre> <pre><code>{\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n}\n</code></pre> <p>GeoJSON may be specified as Point, Circle, LineString or Polygon. Polygons must be closed. Note that unlike the JanusGraph API GeoJSON specifies coordinates as lng lat.</p>"},{"location":"basics/search-predicates/#collections","title":"Collections","text":"<p>If you are using Elasticsearch then you can index properties with SET and LIST cardinality. For instance:</p> <pre><code>mgmt = graph.openManagement()\nnameProperty = mgmt.makePropertyKey(\"names\").dataType(String.class).cardinality(Cardinality.SET).make()\nmgmt.buildIndex(\"search\", Vertex.class).addKey(nameProperty, Mapping.STRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n//Insert a vertex\nperson = graph.addVertex()\nperson.property(\"names\", \"Robert\")\nperson.property(\"names\", \"Bob\")\ngraph.tx().commit()\n//Now query it\ng.V().has(\"names\", \"Bob\").count().next() //1\ng.V().has(\"names\", \"Robert\").count().next() //1\n</code></pre>"},{"location":"basics/transactions/","title":"Transactions","text":"<p>Almost all interaction with JanusGraph is associated with a transaction. JanusGraph transactions are safe for concurrent use by multiple threads. Methods on a JanusGraph instance like <code>graph.V(...)</code> and <code>graph.tx().commit()</code> perform a <code>ThreadLocal</code> lookup to retrieve or create a transaction associated with the calling thread. Callers can alternatively forego <code>ThreadLocal</code> transaction management in favor of calling <code>graph.tx().createThreadedTx()</code>, which returns a reference to a transaction object with methods to read/write graph data and commit or rollback.</p> <p>JanusGraph transactions are not necessarily ACID. They can be so configured on BerkeleyDB, but they are not generally so on Cassandra or HBase, where the underlying storage system does not provide serializable isolation or multi-row atomic writes and the cost of simulating those properties would be substantial.</p> <p>This section describes JanusGraph\u2019s transactional semantics and API.</p>"},{"location":"basics/transactions/#transaction-handling","title":"Transaction Handling","text":"<p>Every graph operation in JanusGraph occurs within the context of a transaction. According to the TinkerPop\u2019s transactional specification, each thread opens its own transaction against the graph database with the first operation (i.e. retrieval or mutation) on the graph: <pre><code>graph = JanusGraphFactory.open(\"berkeleyje:/tmp/janusgraph\")\njuno = graph.addVertex() //Automatically opens a new transaction\njuno.property(\"name\", \"juno\")\ngraph.tx().commit() //Commits transaction\n</code></pre></p> <p>In this example, a local JanusGraph graph database is opened. Adding the vertex \"juno\" is the first operation (in this thread) which automatically opens a new transaction. All subsequent operations occur in the context of that same transaction until the transaction is explicitly stopped or the graph database is closed. If transactions are still open when <code>close()</code> is called, then the behavior of the outstanding transactions is technically undefined. In practice, any non-thread-bound transactions will usually be effectively rolled back, but the thread-bound transaction belonging to the thread that invoked shutdown will first be committed. Note, that both read and write operations occur within the context of a transaction.</p>"},{"location":"basics/transactions/#transactional-scope","title":"Transactional Scope","text":"<p>All graph elements (vertices, edges, and types) are associated with the transactional scope in which they were retrieved or created. Under TinkerPop\u2019s default transactional semantics, transactions are automatically created with the first operation on the graph and closed explicitly using <code>commit()</code> or <code>rollback()</code>. Once the transaction is closed, all graph elements associated with that transaction become stale and unavailable. However, JanusGraph will automatically transition vertices and types into the new transactional scope as shown in this example: <pre><code>graph = JanusGraphFactory.open(\"berkeleyje:/tmp/janusgraph\")\njuno = graph.addVertex() //Automatically opens a new transaction\ngraph.tx().commit() //Ends transaction\njuno.property(\"name\", \"juno\") //Vertex is automatically transitioned\n</code></pre></p> <p>Edges, on the other hand, are not automatically transitioned and cannot be accessed outside their original transaction. They must be explicitly transitioned: <pre><code>e = juno.addEdge(\"knows\", graph.addVertex())\ngraph.tx().commit() //Ends transaction\ne = g.E(e).next() //Need to refresh edge\ne.property(\"time\", 99)\n</code></pre></p>"},{"location":"basics/transactions/#transaction-failures","title":"Transaction Failures","text":"<p>When committing a transaction, JanusGraph will attempt to persist all changes to the storage backend. This might not always be successful due to IO exceptions, network errors, machine crashes or resource unavailability. Hence, transactions can fail. In fact, transactions will eventually fail in sufficiently large systems. Therefore, we highly recommend that your code expects and accommodates such failures: <pre><code>try {\n    if (g.V().has(\"name\", name).iterator().hasNext())\n        throw new IllegalArgumentException(\"Username already taken: \" + name)\n    user = graph.addVertex()\n    user.property(\"name\", name)\n    graph.tx().commit()\n} catch (Exception e) {\n    //Recover, retry, or return error message\n    println(e.getMessage())\n}\n</code></pre></p> <p>The example above demonstrates a simplified user signup implementation where <code>name</code> is the name of the user who wishes to register. First, it is checked whether a user with that name already exists. If not, a new user vertex is created and the name assigned. Finally, the transaction is committed.</p> <p>If the transaction fails, a <code>JanusGraphException</code> is thrown. There are a variety of reasons why a transaction may fail. JanusGraph differentiates between potentially temporary and permanent failures.</p> <p>Potentially temporary failures are those related to resource unavailability and IO hiccups (e.g. network timeouts). JanusGraph automatically tries to recover from temporary failures by retrying to persist the transactional state after some delay. The number of retry attempts and the retry delay are configurable (see Configuration Reference).</p> <p>Permanent failures can be caused by complete connection loss, hardware failure or lock contention. To understand the cause of lock contention, consider the signup example above and suppose a user tries to signup with username \"juno\". That username may still be available at the beginning of the transaction but by the time the transaction is committed, another user might have concurrently registered with \"juno\" as well and that transaction holds the lock on the username therefore causing the other transaction to fail. Depending on the transaction semantics one can recover from a lock contention failure by re-running the entire transaction.</p> <p>Permanent exceptions that can fail a transaction include:</p> <ul> <li> <p>PermanentLockingException(Local lock contention): Another local     thread has already been granted a conflicting lock.</p> </li> <li> <p>PermanentLockingException(Expected value mismatch for X:     expected=Y vs actual=Z): The verification that the value read in     this transaction is the same as the one in the datastore after     applying for the lock failed. In other words, another transaction     modified the value after it had been read and modified.</p> </li> </ul>"},{"location":"basics/transactions/#multi-threaded-transactions","title":"Multi-Threaded Transactions","text":"<p>JanusGraph supports multi-threaded transactions through TinkerPop\u2019s threaded transactions.  Hence, to speed up transaction processing and utilize multi-core architectures multiple threads can run concurrently in a single transaction.</p> <p>With TinkerPop\u2019s default transaction handling, each thread automatically opens its own transaction against the graph database. To open a thread-independent transaction, use the <code>createThreadedTx()</code> method. <pre><code>threadedGraph = graph.tx().createThreadedTx();\nthreads = new Thread[10];\nfor (int i=0; i&lt;threads.length; i++) {\n    threads[i]=new Thread({\n        println(\"Do something with 'threadedGraph'\");\n    });\n    threads[i].start();\n}\nfor (int i=0; i&lt;threads.length; i++) threads[i].join();\nthreadedGraph.tx().commit();\n</code></pre></p> <p>The <code>createThreadedTx()</code> method returns a new <code>Graph</code> object that represents this newly opened transaction. The graph object <code>tx</code> supports all of the methods that the original graph did, but does so without opening new transactions for each thread. This allows us to start multiple threads which all work concurrently in the same transaction and one of which finally commits the transaction when all threads have completed their work.</p> <p>JanusGraph relies on optimized concurrent data structures to support hundreds of concurrent threads running efficiently in a single transaction.</p>"},{"location":"basics/transactions/#concurrent-algorithms","title":"Concurrent Algorithms","text":"<p>Thread independent transactions started through <code>createThreadedTx()</code> are particularly useful when implementing concurrent graph algorithms. Most traversal or message-passing (ego-centric) like graph algorithms are embarrassingly parallel which means they can be parallelized and executed through multiple threads with little effort. Each of these threads can operate on a single <code>Graph</code> object returned by <code>createThreadedTx()</code> without blocking each other.</p>"},{"location":"basics/transactions/#nested-transactions","title":"Nested Transactions","text":"<p>Another use case for thread independent transactions is nested transactions that ought to be independent from the surrounding transaction.</p> <p>For instance, assume a long running transactional job that has to create a new vertex with a unique name. Since enforcing unique names requires the acquisition of a lock (see Eventually-Consistent Storage Backends for more detail) and since the transaction is running for a long time, lock congestion and expensive transactional failures are likely. <pre><code>v1 = graph.addVertex()\n//Do many other things\nv2 = graph.addVertex()\nv2.property(\"uniqueName\", \"foo\")\nv1.addEdge(\"related\", v2)\n//Do many other things\ngraph.tx().commit() // This long-running tx might fail due to contention on its uniqueName lock\n</code></pre></p> <p>One way around this is to create the vertex in a short, nested thread-independent transaction as demonstrated by the following pseudo code  </p> <pre><code>v1 = graph.addVertex()\n//Do many other things\ntx = graph.tx().createThreadedTx()\nv2 = tx.addVertex()\nv2.property(\"uniqueName\", \"foo\")\ntx.commit() // Any lock contention will be detected here\nv1.addEdge(\"related\", g.V(v2).next()) // Need to load v2 into outer transaction\n//Do many other things\ngraph.tx().commit() // Can't fail due to uniqueName write lock contention involving v2\n</code></pre>"},{"location":"basics/transactions/#common-transaction-handling-problems","title":"Common Transaction Handling Problems","text":"<p>Transactions are started automatically with the first operation executed against the graph. One does NOT have to start a transaction manually. The method <code>newTransaction</code> is used to start multi-threaded transactions only.</p> <p>Transactions are automatically started under the TinkerPop semantics but not automatically terminated. Transactions must be terminated manually with <code>commit()</code> or <code>rollback()</code>. If a <code>commit()</code> transactions fails, it should be terminated manually with <code>rollback()</code> after catching the failure. Manual termination of transactions is necessary because only the user knows the transactional boundary.</p> <p>A transaction will attempt to maintain its state from the beginning of the transaction. This might lead to unexpected behavior in multi-threaded applications as illustrated in the following artificial example  </p> <pre><code>v = g.V(4).next() // Retrieve vertex, first action automatically starts transaction\ng.V(v).bothE()\n&gt;&gt; returns nothing, v has no edges\n//thread is idle for a few seconds, another thread adds edges to v\ng.V(v).bothE()\n&gt;&gt; still returns nothing because the transactional state from the beginning is maintained\n</code></pre> <p>Such unexpected behavior is likely to occur in client-server applications where the server maintains multiple threads to answer client requests. It is therefore important to terminate the transaction after a unit of work (e.g. code snippet, query, etc). So, the example above should be:</p> <pre><code>v = g.V(4).next() // Retrieve vertex, first action automatically starts transaction\ng.V(v).bothE()\ngraph.tx().commit()\n//thread is idle for a few seconds, another thread adds edges to v\ng.V(v).bothE()\n&gt;&gt; returns the newly added edge\ngraph.tx().commit()\n</code></pre> <p>When using multi-threaded transactions via <code>newTransaction</code> all vertices and edges retrieved or created in the scope of that transaction are not available outside the scope of that transaction. Accessing such elements after the transaction has been closed will result in an exception. As demonstrated in the example above, such elements have to be explicitly refreshed in the new transaction using <code>g.V(existingVertex)</code> or <code>g.E(existingEdge)</code>.</p>"},{"location":"basics/transactions/#transaction-configuration","title":"Transaction Configuration","text":"<p>JanusGraph\u2019s <code>JanusGraph.buildTransaction()</code> method gives the user the ability to configure and start a new multi-threaded transaction  against a <code>JanusGraph</code>. Hence, it is identical to <code>JanusGraph.newTransaction()</code> with additional configuration options.</p> <p><code>buildTransaction()</code> returns a <code>TransactionBuilder</code> which allows the following aspects of a transaction to be configured:</p> <ul> <li> <p><code>readOnly()</code> - makes the transaction read-only and any attempt to     modify the graph will result in an exception.</p> </li> <li> <p><code>enableBatchLoading()</code> - enables batch-loading for an individual     transaction. This setting results in similar efficiencies as the     graph-wide setting <code>storage.batch-loading</code> due to the disabling of     consistency checks and other optimizations. Unlike     <code>storage.batch-loading</code> this option will not change the behavior of     the storage backend. Similarly, you could call <code>disableBatchLoading()</code>     to disable batch-loading for an individual transaction.</p> </li> <li> <p><code>propertyPrefetching(boolean)</code> - enables or disables property prefetching,     i.e. <code>query.fast-property</code>, for an individual transaction. If enabled,     all properties of a particular vertex will be pre-fetched on the first     vertex property access, which eliminates backend calls on subsequent     property access for the same vertex.</p> </li> <li> <p><code>multiQuery(boolean)</code> - enables or disables query batching, i.e.     <code>query.batch</code>, for an individual transaction. If enabled, queries     for a single traversal will be batched when executed against the     storage backend.</p> </li> <li> <p><code>setTimestamp(long)</code> - Sets the timestamp for this transaction as     communicated to the storage backend for persistence. Depending on     the storage backend, this setting may be ignored. For eventually     consistent backends, this is the timestamp used to resolve write     conflicts. If this setting is not explicitly specified, JanusGraph     uses the current time.</p> </li> <li> <p><code>setVertexCacheSize(long size)</code> - The number of vertices this     transaction caches in memory. The larger this number, the more     memory a transaction can potentially consume. If this number is too     small, a transaction might have to re-fetch data which causes delays     in particular for long running transactions.</p> </li> <li> <p><code>checkExternalVertexExistence(boolean)</code> - Whether this transaction     should verify the existence of vertices for user provided vertex     ids. Such checks requires access to the database which takes time.     The existence check should only be disabled if the user is     absolutely sure that the vertex must exist - otherwise data     corruption can ensue.</p> </li> <li> <p><code>checkInternalVertexExistence(boolean)</code> - Whether this transaction     should double-check the existence of vertices during query     execution. This can be useful to avoid phantom vertices on     eventually consistent storage backends. Disabled by default.     Enabling this setting can slow down query processing.</p> </li> <li> <p><code>consistencyChecks(boolean)</code> - Whether JanusGraph should enforce     schema level consistency constraints (e.g. multiplicity     constraints). Disabling consistency checks leads to better     performance but requires that the user ensures consistency     confirmation at the application level to avoid inconsistencies. USE     WITH GREAT CARE!</p> </li> <li> <p><code>skipDBCacheRead()</code> - Disables JanusGraph database level      cache access during read operations. Doesn't have any effect if database      level cache was disabled via config <code>cache.db-cache</code>.</p> </li> <li> <p><code>lazyLoadRelations()</code> - Set lazy-load for all properties and edges of the vertex: ids and values are deserialized upon demand.     When enabled, it can have a boost on large-scale read operations, if only certain types of relations are being read from the vertex.</p> </li> </ul> <p>Once, the desired configuration options have been specified, the new transaction is started via <code>start()</code> which returns a <code>JanusGraphTransaction</code>.</p>"},{"location":"basics/connecting/","title":"Introduction","text":"<p>JanusGraph can be queried from all languages for which a TinkerPop driver exists. Drivers allow sending of Gremlin traversals to a Gremlin Server like the JanusGraph Server. A list of TinkerPop drivers is available on TinkerPop\u2019s homepage.</p> <p>In addition to drivers, there exist  query languages for TinkerPop that make it easier to use Gremlin in different programming languages like Java, Python, or C#. Some of these languages even construct Gremlin traversals from completely different query languages like Cypher or SPARQL. Since JanusGraph implements TinkerPop, all of these languages can be used together with JanusGraph.</p>"},{"location":"basics/connecting/dotnet/","title":"Connecting from .NET","text":"<p>Gremlin traversals can be constructed with Gremlin.Net just like in Gremlin-Java or Gremlin-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. The main syntactical difference for Gremlin.Net is that it follows .NET naming conventions, e.g., method names use PascalCase instead of camelCase.</p>"},{"location":"basics/connecting/dotnet/#getting-started-with-janusgraph-and-gremlinnet","title":"Getting Started with JanusGraph and Gremlin.Net","text":"<p>To get started with Gremlin.Net:</p> <ol> <li> <p>Create a console application: <pre><code>dotnet new console -o GremlinExample\n</code></pre></p> </li> <li> <p>Add Gremlin.Net: <pre><code>dotnet add package Gremlin.Net -v 3.7.3\n</code></pre></p> </li> <li> <p>Create a <code>GraphTraversalSource</code> which is the basis for all Gremlin     traversals: <pre><code>using static Gremlin.Net.Process.Traversal.AnonymousTraversalSource;\n\nvar client = new GremlinClient(new GremlinServer(\"localhost\", 8182));\n// The client should be disposed on shut down to release resources\n// and to close open connections with client.Dispose()\nvar g = Traversal().WithRemote(new DriverRemoteConnection(client));\n        // Reuse 'g' across the application\n</code></pre></p> </li> <li> <p>Execute a simple traversal: <pre><code>var herculesAge = g.V().Has(\"name\", \"hercules\").Values&lt;int&gt;(\"age\").Next();\nConsole.WriteLine($\"Hercules is {herculesAge} years old.\");\n</code></pre>     The traversal can also be executed asynchronously by using     <code>Promise()</code> which is the recommended way as the underlying driver in     Gremlin.Net also works asynchronously: <pre><code>var herculesAge = await g.V().Has(\"name\", \"hercules\").Values&lt;int&gt;(\"age\").Promise(t =&gt; t.Next());\n</code></pre></p> </li> </ol>"},{"location":"basics/connecting/dotnet/#janusgraphnet-for-janusgraph-specific-types-and-predicates","title":"JanusGraph.Net for JanusGraph Specific Types and Predicates","text":"<p>JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin.Net. JanusGraph.Net is a .NET library that adds support for these types and predicates to Gremlin.Net.</p> <p>After installing the library, a message serializer needs to be configured for JanusGraph which can be done like this for GraphSON 3:</p> <pre><code>var client = new GremlinClient(new GremlinServer(\"localhost\", 8182),\n    new JanusGraphGraphSONMessageSerializer());\n</code></pre> <p>or like this for GraphBinary:</p> <pre><code>var client = new GremlinClient(new GremlinServer(\"localhost\", 8182),\n    new GraphBinaryMessageSerializer(JanusGraphTypeSerializerRegistry.Instance));\n</code></pre> <p>Refer to the documentation of JanusGraph.Net for more information about the library, including its compatibility with different JanusGraph versions and differences in support between the different serialization formats.</p>"},{"location":"basics/connecting/java/","title":"Connecting from Java","text":"<p>While it is possible to embed JanusGraph as a library inside a Java application and then directly connect to the backend, this section assumes that the application connects to JanusGraph Server. For information on how to embed JanusGraph, see the JanusGraph Examples projects.</p> <p>This section only covers how applications can connect to JanusGraph Server using the GraphBinary serialization. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources.</p>"},{"location":"basics/connecting/java/#getting-started-with-janusgraph-and-gremlin-java","title":"Getting Started with JanusGraph and Gremlin-Java","text":"<p>To get started with JanusGraph in Java:</p> <ol> <li> <p>Create an application with Maven:</p> <pre><code>mvn archetype:generate -DgroupId=com.mycompany.project\n    -DartifactId=gremlin-example\n    -DarchetypeArtifactId=maven-archetype-quickstart\n    -DinteractiveMode=false\n</code></pre> </li> <li> <p>Add dependencies on <code>janusgraph-driver</code> and <code>gremlin-driver</code> to the dependency manager:</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-driver&lt;/artifactId&gt;\n    &lt;version&gt;1.0.0&lt;/version&gt;\n&lt;/dependency&gt;\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.tinkerpop&lt;/groupId&gt;\n    &lt;artifactId&gt;gremlin-driver&lt;/artifactId&gt;\n    &lt;version&gt;3.7.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>implementation \"org.janusgraph:janusgraph-driver:1.0.0\"\nimplementation \"org.apache.tinkerpop:gremlin-driver:3.7.3\"\n</code></pre> </li> <li> <p>Add two configuration files, <code>conf/remote-graph.properties</code> and     <code>conf/remote-objects.yaml</code>:</p> conf/remote-graph.propertiesconf/remote-objects.yaml <pre><code>gremlin.remote.remoteConnectionClass=org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection\ngremlin.remote.driver.clusterFile=conf/remote-objects.yaml\ngremlin.remote.driver.sourceName=g\n</code></pre> <pre><code>hosts: [localhost]\nport: 8182\nserializer: { \n    className: org.apache.tinkerpop.gremlin.util.ser.GraphBinaryMessageSerializerV1,\n    config: { ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] }}\n</code></pre> </li> <li> <p>Create a <code>GraphTraversalSource</code> which is the basis for all Gremlin traversals:</p> <pre><code>import static org.apache.tinkerpop.gremlin.process.traversal.AnonymousTraversalSource.traversal;\n\nGraphTraversalSource g = traversal().withRemote(\"conf/remote-graph.properties\");\n// Reuse 'g' across the application\n// and close it on shut-down to close open connections with g.close()\n</code></pre> </li> <li> <p>Execute a simple traversal:</p> <pre><code>Object herculesAge = g.V().has(\"name\", \"hercules\").values(\"age\").next();\nSystem.out.println(\"Hercules is \" + herculesAge + \" years old.\");\n</code></pre> <p><code>next()</code> is a terminal step that submits the traversal to the Gremlin Server and returns a single result.</p> </li> </ol>"},{"location":"basics/connecting/java/#janusgraph-specific-types-and-predicates","title":"JanusGraph Specific Types and Predicates","text":"<p>JanusGraph specific types and predicates can be used directly from a Java application through the dependency <code>janusgraph-driver</code>.</p>"},{"location":"basics/connecting/java/#consideration-for-accessing-the-management-api","title":"Consideration for Accessing the Management API","text":"<p>Note</p> <p>We are working to replace the Management API with a language agnostic solution, see Management System</p> <p>The described connection uses GraphBinary and the <code>janusgraph-driver</code> which doesn't allow accessing the internal JanusGraph components such as <code>ManagementSystem</code>. To access the <code>ManagementSystem</code>, you have to submit java-based scripts, see Submitting Scripts, or directly accessing JanusGraph by local opening a JanusGraph instance.</p>"},{"location":"basics/connecting/python/","title":"Connecting from Python","text":"<p>Gremlin traversals can be constructed with Gremlin-Python just like in Gremlin-Java or Gremlin-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources.</p> <p>Important</p> <p>Some Gremlin step and predicate names are reserved words in Python. Those names are simply postfixed with <code>_</code> in Gremlin-Python, e.g., <code>in()</code> becomes <code>in_()</code>, <code>not()</code> becomes <code>not_()</code>, and so on. The other names affected by this are: <code>all</code>, <code>and</code>, <code>as</code>, <code>from</code>, <code>global</code>, <code>is</code>, <code>list</code>, <code>or</code>, and <code>set</code>.</p>"},{"location":"basics/connecting/python/#getting-started-with-janusgraph-and-gremlin-python","title":"Getting Started with JanusGraph and Gremlin-Python","text":"<p>To get started with Gremlin-Python:</p> <ol> <li>Install Gremlin-Python: <pre><code>pip install gremlinpython==3.7.3\n</code></pre></li> <li> <p>Create a text file <code>gremlinexample.py</code> and add the following imports     to it: <pre><code>from gremlin_python import statics\nfrom gremlin_python.structure.graph import Graph\nfrom gremlin_python.process.graph_traversal import __\nfrom gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n</code></pre></p> </li> <li> <p>Create a <code>GraphTraversalSource</code> which is the basis for all Gremlin     traversals: <pre><code>from gremlin_python.process.anonymous_traversal import traversal\n\nconnection = DriverRemoteConnection('ws://localhost:8182/gremlin', 'g')\n# The connection should be closed on shut down to close open connections with connection.close()\ng = traversal().withRemote(connection)\n# Reuse 'g' across the application\n</code></pre></p> </li> <li> <p>Execute a simple traversal: <pre><code>hercules_age = g.V().has('name', 'hercules').values('age').next()\nprint(f'Hercules is {hercules_age} years old.')\n</code></pre> <code>next()</code> is a terminal step that submits the traversal to the     Gremlin Server and returns a single result.</p> </li> </ol>"},{"location":"basics/connecting/python/#janusgraph-specific-types-and-predicates","title":"JanusGraph Specific Types and Predicates","text":"<p>JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin-Python.</p>"},{"location":"configs/","title":"Configuration","text":"<p>A JanusGraph graph database cluster consists of one or multiple JanusGraph instances. To open a JanusGraph instance, a configuration has to be provided which specifies how JanusGraph should be set up.</p> <p>A JanusGraph configuration specifies which components JanusGraph should use, controls all operational aspects of a JanusGraph deployment, and provides a number of tuning options to get maximum performance from a JanusGraph cluster.</p> <p>At a minimum, a JanusGraph configuration must define the persistence engine that JanusGraph should use as a storage backend. Storage Backends lists all supported persistence engines and how to configure them respectively. If advanced graph query support (e.g full-text search, geo search, or range queries) is required an additional indexing backend must be configured. See Index Backends for details. If query performance is a concern, then caching should be enabled. Cache configuration and tuning is described in JanusGraph Cache.</p>"},{"location":"configs/#example-configurations","title":"Example Configurations","text":"<p>Below are some example configuration files to demonstrate how to configure the most commonly used storage backends, indexing systems, and performance components. This covers only a tiny portion of the available configuration options. Refer to Configuration Reference for the complete list of all options.</p>"},{"location":"configs/#cassandraelasticsearch","title":"Cassandra+Elasticsearch","text":"<p>Sets up JanusGraph to use the Cassandra persistence engine running locally and a remote Elastic search indexing system:</p> <pre><code>storage.backend=cql\nstorage.hostname=localhost\n\nindex.search.backend=elasticsearch\nindex.search.hostname=100.100.101.1, 100.100.101.2\nindex.search.elasticsearch.client-only=true\n</code></pre>"},{"location":"configs/#hbasecaching","title":"HBase+Caching","text":"<p>Sets up JanusGraph to use the HBase persistence engine running remotely and uses JanusGraph\u2019s caching component for better performance. <pre><code>storage.backend=hbase\nstorage.hostname=100.100.101.1\nstorage.port=2181\n\ncache.db-cache = true\ncache.db-cache-clean-wait = 20\ncache.db-cache-time = 180000\ncache.db-cache-size = 0.5\n</code></pre></p>"},{"location":"configs/#berkeleydb","title":"BerkeleyDB","text":"<p>Sets up JanusGraph to use BerkeleyDB as an embedded persistence engine with Elasticsearch as an embedded indexing system. <pre><code>storage.backend=berkeleyje\nstorage.directory=/tmp/graph\n\nindex.search.backend=elasticsearch\nindex.search.directory=/tmp/searchindex\nindex.search.elasticsearch.client-only=false\nindex.search.elasticsearch.local-mode=true\n</code></pre></p> <p>Configuration Reference describes all of these configuration options in detail. The <code>conf</code> directory of the JanusGraph distribution contains additional configuration examples.</p>"},{"location":"configs/#further-examples","title":"Further Examples","text":"<p>There are several example configuration files in the <code>conf/</code> directory that can be used to get started with JanusGraph quickly. Paths to these files can be passed to <code>JanusGraphFactory.open(...)</code> as shown below:</p> <pre><code>// Connect to Cassandra on localhost using a default configuration\ngraph = JanusGraphFactory.open(\"conf/janusgraph-cql.properties\")\n// Connect to HBase on localhost using a default configuration\ngraph = JanusGraphFactory.open(\"conf/janusgraph-hbase.properties\")\n</code></pre>"},{"location":"configs/#using-configuration","title":"Using Configuration","text":"<p>How the configuration is provided to JanusGraph depends on the instantiation mode.</p>"},{"location":"configs/#janusgraphfactory","title":"JanusGraphFactory","text":""},{"location":"configs/#gremlin-console","title":"Gremlin Console","text":"<p>The JanusGraph distribution contains a command line Gremlin Console which makes it easy to get started and interact with JanusGraph. Invoke <code>bin/gremlin.sh</code> (Unix/Linux) or <code>bin/gremlin.bat</code> (Windows) to start the Console and then open a JanusGraph graph using the factory with the configuration stored in an accessible properties configuration file: <pre><code>graph = JanusGraphFactory.open('path/to/configuration.properties')\n</code></pre></p>"},{"location":"configs/#janusgraph-embedded","title":"JanusGraph Embedded","text":"<p>JanusGraphFactory can also be used to open an embedded JanusGraph graph instance from within a JVM-based user application. In that case, JanusGraph is part of the user application and the application can call upon JanusGraph directly through its public API.</p>"},{"location":"configs/#short-codes","title":"Short Codes","text":"<p>If the JanusGraph graph cluster has been previously configured and/or only the storage backend needs to be defined, JanusGraphFactory accepts a colon-separated string representation of the storage backend name and hostname or directory. <pre><code>graph = JanusGraphFactory.open('cql:localhost')\ngraph = JanusGraphFactory.open('berkeleyje:/tmp/graph')\n</code></pre></p>"},{"location":"configs/#janusgraph-server","title":"JanusGraph Server","text":"<p>JanusGraph, by itself, is simply a set of jar files with no thread of execution. There are two basic patterns for connecting to, and using a JanusGraph database:</p> <ol> <li> <p>JanusGraph can be used by embedding JanusGraph calls in a client     program where the program provides the thread of execution.</p> </li> <li> <p>JanusGraph packages a long running server process that, when     started, allows a remote client or logic running in a separate     program to make JanusGraph calls. This long running server process     is called JanusGraph Server.</p> </li> </ol> <p>For the JanusGraph Server, JanusGraph uses Gremlin Server of the Apache TinkerPop stack to service client requests. JanusGraph provides an out-of-the-box configuration for a quick start with JanusGraph Server, but the configuration can be changed to provide a wide range of server capabilities.</p> <p>Configuring JanusGraph Server is accomplished through a JanusGraph Server yaml configuration file located in the ./conf/gremlin-server directory in the JanusGraph distribution. To configure JanusGraph Server with a graph instance (<code>JanusGraph</code>), the JanusGraph Server configuration file requires the following settings:</p> <pre><code>...\ngraphs: {\n  graph: conf/janusgraph-berkeleyje.properties\n}\nscriptEngines: {\n  gremlin-groovy: {\n    plugins: { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: {classImports: [java.lang.Math], methodImports: [java.lang.Math#*]},\n               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: {files: [scripts/empty-sample.groovy]}}}}\n...\n</code></pre> <p>The entry for <code>graphs</code> defines the bindings to specific <code>JanusGraph</code> configurations. In the above case it binds <code>graph</code> to a JanusGraph configuration at <code>conf/janusgraph-berkeleyje.properties</code>. The <code>plugins</code> entry enables the JanusGraph Gremlin Plugin, which enables auto-imports of JanusGraph classes so that they can be referenced in remotely submitted scripts.</p> <p>Learn more about configuring and using JanusGraph Server in JanusGraph Server.</p>"},{"location":"configs/#server-distribution","title":"Server Distribution","text":"<p>The JanusGraph zip file contains a quick start server component that helps make it easier to get started with Gremlin Server and JanusGraph. Invoke <code>bin/janusgraph.sh start</code> to start Gremlin Server with Cassandra and Elasticsearch.</p> <p>Note</p> <p>For security reasons Elasticsearch and therefore <code>janusgraph.sh</code> must be run under a non-root account</p> <p>Note</p> <p>Starting with 0.5.1, this is just included in the full package version.</p>"},{"location":"configs/#global-configuration","title":"Global Configuration","text":"<p>JanusGraph distinguishes between local and global configuration options. Local configuration options apply to an individual JanusGraph instance. Global configuration options apply to all instances in a cluster. More specifically, JanusGraph distinguishes the following five scopes for configuration options:</p> <ul> <li> <p>LOCAL: These options only apply to an individual JanusGraph     instance and are specified in the configuration provided when     initializing the JanusGraph instance.</p> </li> <li> <p>MASKABLE: These configuration options can be overwritten for an     individual JanusGraph instance by the local configuration file. If     the local configuration file does not specify the option, its value     is read from the global JanusGraph cluster configuration.</p> </li> <li> <p>GLOBAL: These options are always read from the cluster     configuration and cannot be overwritten on an instance basis.</p> </li> <li> <p>GLOBAL_OFFLINE: Like GLOBAL, but changing these options     requires a cluster restart to ensure that the value is the same     across the entire cluster.</p> </li> <li> <p>FIXED: Like GLOBAL, but the value cannot be changed once the     JanusGraph cluster is initialized.</p> </li> </ul> <p>When the first JanusGraph instance in a cluster is started, the global configuration options are initialized from the provided local configuration file. Subsequently changing global configuration options is done through JanusGraph\u2019s management API. To access the management API, call <code>g.getManagementSystem()</code> on an open JanusGraph instance handle <code>g</code>. For example, to change the default caching behavior on a JanusGraph cluster: <pre><code>mgmt = graph.openManagement()\nmgmt.get('cache.db-cache')\n// Prints the current config setting\nmgmt.set('cache.db-cache', true)\n// Changes option\nmgmt.get('cache.db-cache')\n// Prints 'true'\nmgmt.commit()\n// Changes take effect\n</code></pre></p>"},{"location":"configs/#changing-offline-options","title":"Changing Offline Options","text":"<p>Changing configuration options does not affect running instances and only applies to newly started ones. Changing GLOBAL_OFFLINE configuration options requires restarting the cluster so that the changes take effect immediately for all instances. To change GLOBAL_OFFLINE options follow these steps:</p> <ul> <li>Close all but one JanusGraph instance in the cluster</li> <li>Connect to the single instance</li> <li>Ensure all running transactions are closed</li> <li>Ensure no new transactions are started (i.e. the cluster must be offline)</li> <li>Open the management API</li> <li>Change the configuration option(s)</li> <li>Call commit which will automatically shut down the graph instance</li> <li>Restart all instances</li> </ul> <p>Refer to the full list of configuration options in Configuration Reference for more information including the configuration scope of each option.</p>"},{"location":"configs/configuration-reference/","title":"Configuration Reference","text":"<p>This section is the authoritative reference for JanusGraph configuration options. It includes all options for storage and indexing backends that are part of the official JanusGraph distribution.</p> <p>The table is automatically generated by traversing the keys and namespaces in JanusGraph\u2019s internal configuration management API. Hence, the configuration options as listed on this page are synchronized with a particular JanusGraph release. If a reference to a configuration option in other parts of this documentation is in conflict with its representation on this page, assume the version listed here to be correct.</p>"},{"location":"configs/configuration-reference/#mutability-levels","title":"Mutability Levels","text":"<p>Each configuration option has a certain mutability level that governs whether and how it can be modified after the database is opened for the first time. The following listing describes the mutability levels.</p> <ul> <li> <p>FIXED</p> <p>Once the database has been opened, these configuration options cannot be changed for the entire life of the database</p> </li> <li> <p>GLOBAL_OFFLINE</p> <p>These options can only be changed for the entire database cluster at once when all instances are shut down</p> </li> <li> <p>GLOBAL</p> <p>These options can only be changed globally across the entire database cluster</p> </li> <li> <p>MASKABLE</p> <p>These options are global but can be overwritten by a local configuration file</p> </li> <li> <p>LOCAL     These options can only be provided through a local configuration file</p> </li> </ul> <p>Refer to Global Configuration for information on how to change non-local configuration options.</p>"},{"location":"configs/configuration-reference/#umbrella-namespace","title":"Umbrella Namespace","text":"<p>Namespaces marked with an asterisk are umbrella namespaces which means that they can accommodate an arbitrary number of sub-namespaces - each of which uniquely identified by its name. The configuration options listed under an umbrella namespace apply only to those sub-namespaces. Umbrella namespaces are used to configure multiple system components that are of the same type and hence have the same configuration options.</p> <p>For example, the <code>log</code> namespace is an umbrella namespace because JanusGraph can interface with multiple logging backends, such as the <code>user</code> log, each of which has the same core set of configuration options. To configure the send batch size of the <code>user</code> log to 100 transaction changes, one would have to set the following option in the configuration <pre><code>log.user.send-batch-size = 100\n</code></pre></p>"},{"location":"configs/configuration-reference/#configuration-namespaces-and-options","title":"Configuration Namespaces and Options","text":""},{"location":"configs/configuration-reference/#attributescustom","title":"attributes.custom *","text":"<p>Custom attribute serialization and handling</p> Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE"},{"location":"configs/configuration-reference/#cache","title":"cache","text":"<p>Configuration options that modify JanusGraph's caching behavior</p> Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data.  Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them.  This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 MASKABLE cache.db-cache-size Size of JanusGraph's database level cache.  Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 MASKABLE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE"},{"location":"configs/configuration-reference/#cluster","title":"cluster","text":"<p>Configuration options for multi-machine deployments</p> Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodes in the JanusGraph graph cluster. Must be greater than 1 and a power of 2. Integer 32 FIXED"},{"location":"configs/configuration-reference/#computer","title":"computer","text":"<p>GraphComputer related configuration</p> Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE"},{"location":"configs/configuration-reference/#graph","title":"graph","text":"<p>General configuration options</p> Name Description Datatype Default Value Mutability graph.allow-custom-vid-types Whether non long-type vertex ids are allowed. set-vertex-id must be enabled in order to use this functionality. Currently, only string-type is supported. This does not prevent users from using custom ids with long type. If your storage backend does not support unordered scan, then some scan operations will be disabled. You cannot use this feature with Berkeley DB. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false GLOBAL_OFFLINE graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL.  These types are managed globally through the storage backend and cannot be overridden by changing the local configuration.  This type of conflict usually indicates misconfiguration.  When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option.  When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.allow-upgrade Setting this to true will allow certain fixed values to be updated such as storage-version. This should only be used for upgrading. Boolean false MASKABLE graph.assign-timestamp Whether to use JanusGraph generated client-side timestamp in mutations if the backend supports it. When enabled, JanusGraph assigns one timestamp to all insertions and another slightly earlier timestamp to all deletions in the same batch. When this is disabled, mutation behavior depends on the backend. Some might use server-side timestamp (e.g. HBase) while others might use client-side timestamp generated by driver (CQL). Boolean true LOCAL graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagement APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anyway. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic vertex id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. For example, users must ensure the vertex ids are unique to avoid duplication. Must use <code>graph.getIDManager().toVertexId(long)</code> to convert your id first. Once this is enabled, you have to provide vertex id when creating new vertices. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false GLOBAL_OFFLINE graph.storage-version The version of JanusGraph storage schema with which this database was created. Automatically set on first start of graph. Should only ever be changed if upgrading to a new major release version of JanusGraph that contains schema changes String (no default value) FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored.  An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance.  This must be unique among all instances concurrently accessing the same stores or indexes.  It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL"},{"location":"configs/configuration-reference/#graphscript-eval","title":"graph.script-eval","text":"<p>Configuration options for gremlin script engine.</p> Name Description Datatype Default Value Mutability graph.script-eval.enabled Whether to enable Gremlin script evaluation. If it is enabled, a gremlin script engine will be instantiated together with the JanusGraph instance, with which one can use <code>eval</code> method to evaluate a gremlin script in plain string format. This is usually only useful when JanusGraph is used as an embedded Java library. Boolean false MASKABLE graph.script-eval.engine Full class name of script engine that implements <code>GremlinScriptEngine</code> interface. Following shorthands can be used:  - <code>GremlinLangScriptEngine</code> (A script engine that only accepts standard gremlin queries. Anything else including lambda function is not accepted. We recommend using this because it's generally safer, but it is not guaranteed that it has no security problem.)- <code>GremlinGroovyScriptEngine</code> (A script engine that accepts arbitrary groovy code. This can be dangerous and you should use it at your own risk. See https://tinkerpop.apache.org/docs/current/reference/#script-execution for potential security problems.) String GremlinLangScriptEngine MASKABLE"},{"location":"configs/configuration-reference/#gremlin","title":"gremlin","text":"<p>Gremlin configuration options</p> Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL"},{"location":"configs/configuration-reference/#ids","title":"ids","text":"<p>General configuration options for graph element IDs</p> Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size.  Setting this too low will make commits frequently block on slow reservation requests.  Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation.  When false, IDs are assigned only when the transaction commits. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE"},{"location":"configs/configuration-reference/#idsauthority","title":"ids.authority","text":"<p>Configuration options for graph element ID reservation/allocation</p> Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE"},{"location":"configs/configuration-reference/#index","title":"index *","text":"<p>Configuration options for the individual indexing backends</p> Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional.  JanusGraph can use multiple heterogeneous index backends.  Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers.  This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that the indexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maximum number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE"},{"location":"configs/configuration-reference/#indexxbkd-circle-processor","title":"index.[X].bkd-circle-processor","text":"<p>Configuration for BKD circle processors which is used for BKD Geoshape mapping.</p> Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.class Full class name of circle processor that implements <code>CircleProcessor</code> interface. The class is used for transformation of a Circle shape to another shape when BKD mapping is used. The provided implementation class should have either a public constructor which accepts configuration as a parameter (<code>org.janusgraph.diskstorage.configuration.Configuration</code>) or a public constructor with no parameters. Usually the transforming shape is a Polygon. Following shorthands can be used:  - <code>noTransformation</code> Circle processor which is not transforming a circle, but instead keep the circle shape unchanged. This implementation may be useful in situations when the user wants to control circle transformation logic on ElasticSearch side instead of application side. For example, using ElasticSearch Circle Processor or any custom plugin.  - <code>fixedErrorDistance</code> Circle processor which transforms the provided Circle into Polygon, Box, or Point depending on the configuration provided in <code>index.bkd-circle-processor.fixed</code>. The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point.  - <code>dynamicErrorDistance</code> Circle processor which calculates error distance dynamically depending on the circle radius and the specified multiplier value. The error distance calculation formula is <code>log(radius) * multiplier</code>. Configuration for this class can be provided via <code>index.bkd-circle-processor.dynamic</code>. The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. String dynamicErrorDistance MASKABLE"},{"location":"configs/configuration-reference/#indexxbkd-circle-processordynamic","title":"index.[X].bkd-circle-processor.dynamic","text":"<p>Configuration for Elasticsearch dynamic circle processor which is used for BKD Geoshape mapping.</p> Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.dynamic.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case <code>false</code> is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.dynamic.error-distance-multiplier Multiplier variable for dynamic error distance calculation in the formula <code>log(radius) * multiplier</code>. Radius and error distance specified in meters. Double 2.0 MASKABLE"},{"location":"configs/configuration-reference/#indexxbkd-circle-processorfixed","title":"index.[X].bkd-circle-processor.fixed","text":"<p>Configuration for Elasticsearch fixed circle processor which is used for BKD Geoshape mapping.</p> Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.fixed.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case <code>false</code> is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.fixed.error-distance The difference between the resulting inscribed distance from center to side and the circle\u2019s radius. Specified in meters. Double 10.0 MASKABLE"},{"location":"configs/configuration-reference/#indexxelasticsearch","title":"index.[X].elasticsearch","text":"<p>Elasticsearch index configuration</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-chunk-size-limit-bytes The total size limit in bytes of a bulk request. Mutation batches in excess of this limit will be chunked to this size. If a single bulk item exceeds this limit an exception will be thrown after the smaller bulk items are submitted. Ensure that this limit is always less than or equal to the configured limit of <code>http.max_content_length</code> on the Elasticsearch servers. For more information, refer to the Elasticsearch documentation. Integer 100000000 LOCAL index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.client-keep-alive Set a keep-alive timeout (in milliseconds) Long (no default value) GLOBAL_OFFLINE index.[X].elasticsearch.connect-timeout Sets the maximum connection timeout (in milliseconds). Integer 1000 MASKABLE index.[X].elasticsearch.enable_index_names_cache Enables cache for generated index store names. It is recommended to always enable index store names cache unless you have more then 50000 indexes per index store. Boolean true MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status.  This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.retry-error-codes Comma separated list of Elasticsearch REST client ResponseException error codes to retry. E.g. \"408,429\" String[] LOCAL index.[X].elasticsearch.retry-initial-wait Sets the initial retry wait time (in milliseconds) before exponential backoff. Long 1 LOCAL index.[X].elasticsearch.retry-limit Sets the number of attempts for configured retryable error codes. Integer 0 LOCAL index.[X].elasticsearch.retry-max-wait Sets the max retry wait time (in milliseconds). Long 1000 LOCAL index.[X].elasticsearch.retry_on_conflict Specify how many times should the operation be retried when a conflict occurs. Integer 0 MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in seconds) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.setup-max-open-scroll-contexts Whether JanusGraph should setup max_open_scroll_context to maximum value for the cluster or not. Boolean true MASKABLE index.[X].elasticsearch.socket-timeout Sets the maximum socket timeout (in milliseconds). Integer 30000 MASKABLE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x  and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-mapping-for-es7 Mapping types are deprecated in ElasticSearch 7 and JanusGraph will not use mapping types by default for ElasticSearch 7 but if you want to preserve mapping types, you can setup this parameter to true. If you are updating ElasticSearch from 6 to 7 and you don't want to reindex your indexes, you may setup this parameter to true but we do recommend to reindex your indexes and don't use this parameter. Boolean false MASKABLE"},{"location":"configs/configuration-reference/#indexxelasticsearchcreate","title":"index.[X].elasticsearch.create","text":"<p>Settings related to index creation</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index.  This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE"},{"location":"configs/configuration-reference/#indexxelasticsearchcreateext","title":"index.[X].elasticsearch.create.ext","text":"<p>Overrides for arbitrary settings applied at index creation. See Elasticsearch, The full list of possible setting is available at Elasticsearch index settings.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.ext.number_of_replicas The number of replicas each primary shard has Integer 1 MASKABLE index.[X].elasticsearch.create.ext.number_of_shards The number of primary shards that an index should have.Default value is 5 on ES 6 and 1 on ES 7 Integer (no default value) MASKABLE"},{"location":"configs/configuration-reference/#indexxelasticsearchhttpauth","title":"index.[X].elasticsearch.http.auth","text":"<p>Configuration options for HTTP(S) authentication.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.type Authentication type to be used for HTTP(S) access. Available options are <code>NONE</code>, <code>BASIC</code> and <code>CUSTOM</code>. String NONE LOCAL"},{"location":"configs/configuration-reference/#indexxelasticsearchhttpauthbasic","title":"index.[X].elasticsearch.http.auth.basic","text":"<p>Configuration options for HTTP(S) Basic authentication.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.basic.password Password for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.basic.realm Realm value for HTTP(S) authentication. If empty, any realm is accepted. String LOCAL index.[X].elasticsearch.http.auth.basic.username Username for HTTP(S) authentication. String LOCAL"},{"location":"configs/configuration-reference/#indexxelasticsearchhttpauthcustom","title":"index.[X].elasticsearch.http.auth.custom","text":"<p>Configuration options for custom HTTP(S) authenticator.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.custom.authenticator-args Comma-separated custom authenticator constructor arguments. String[] LOCAL index.[X].elasticsearch.http.auth.custom.authenticator-class Authenticator fully qualified class name. String LOCAL"},{"location":"configs/configuration-reference/#indexxelasticsearchssl","title":"index.[X].elasticsearch.ssl","text":"<p>Elasticsearch SSL configuration</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.allow-self-signed-certificates Controls the accepting of the self-signed SSL certificates. Boolean false LOCAL index.[X].elasticsearch.ssl.disable-hostname-verification Disables the SSL hostname verification if set to true. Hostname verification is enabled by default. Boolean false LOCAL index.[X].elasticsearch.ssl.enabled Controls use of the SSL connection to Elasticsearch. Boolean false LOCAL"},{"location":"configs/configuration-reference/#indexxelasticsearchsslkeystore","title":"index.[X].elasticsearch.ssl.keystore","text":"<p>Configuration options for SSL Keystore.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.keystore.keypassword The password to access the key in the SSL Keystore. If the option is not present, the value of \"storepassword\" is used. String LOCAL index.[X].elasticsearch.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.keystore.storepassword The password to access SSL Keystore. String LOCAL"},{"location":"configs/configuration-reference/#indexxelasticsearchssltruststore","title":"index.[X].elasticsearch.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL index.[X].elasticsearch.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"configs/configuration-reference/#indexxsolr","title":"index.[X].solr","text":"<p>Solr index configuration</p> Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be reused for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabled, the user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.kerberos-enabled Whether SOLR instance is Kerberized or not. Boolean false MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of <code>collection=field</code>. String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP (<code>http</code>) or using SolrCloud (<code>cloud</code>) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE"},{"location":"configs/configuration-reference/#log","title":"log *","text":"<p>Configuration options for JanusGraph's logging system</p> Name Description Datatype Default Value Mutability log.[X].backend Define the log backend to use. A reserved shortcut <code>default</code> can be used to use graph's storage backend to manage logs. A custom log implementation can be specified by providing full class path which implements <code>org.janusgraph.diskstorage.log.LogManager</code> and accepts a single parameter <code>org.janusgraph.diskstorage.configuration.Configuration</code> in the public constructor. String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not become visible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaning that all entries added to this log expire after the configured amount of time. Requires that the log implementation supports TTL. Duration (no default value) GLOBAL"},{"location":"configs/configuration-reference/#metrics","title":"metrics","text":"<p>Configuration options for metrics reporting</p> Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE"},{"location":"configs/configuration-reference/#metricsconsole","title":"metrics.console","text":"<p>Configuration options for metrics reporting to console</p> Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE"},{"location":"configs/configuration-reference/#metricscsv","title":"metrics.csv","text":"<p>Configuration options for metrics reporting to CSV file</p> Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE"},{"location":"configs/configuration-reference/#metricsgraphite","title":"metrics.graphite","text":"<p>Configuration options for metrics reporting through Graphite</p> Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE"},{"location":"configs/configuration-reference/#metricsjmx","title":"metrics.jmx","text":"<p>Configuration options for metrics reporting through JMX</p> Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE"},{"location":"configs/configuration-reference/#metricsslf4j","title":"metrics.slf4j","text":"<p>Configuration options for metrics reporting through slf4j</p> Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE"},{"location":"configs/configuration-reference/#query","title":"query","text":"<p>Configuration options for query processing</p> Name Description Datatype Default Value Mutability query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequent property access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties. This setting is applicable to direct vertex properties access (like <code>vertex.properties(\"foo\")</code> but not to <code>vertex.properties(\"foo\",\"bar\")</code> because the latter case is not a singular property access). This setting is not applicable to the next Gremlin steps: <code>valueMap</code>, <code>propertyMap</code>, <code>elementMap</code>, <code>properties</code>, <code>values</code> (configuration option <code>query.batch.properties-mode</code> should be used to configure their behavior).When <code>true</code> this setting overwrites <code>query.batch.has-step-mode</code> to <code>all_properties</code> unless <code>none</code> mode is used. Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing so limits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.hard-max-limit If smart-limit is disabled and no limit is given in the query, query optimizer adds a limit in light of possibly large result sets. It works in the same way as smart-limit except that hard-max-limit is usually a large number. Default value is Integer.MAX_VALUE which effectively disables this behavior. This option does not take effect when smart-limit is enabled. Integer 2147483647 MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.index-select-strategy Name of the index selection strategy or full class name. Following shorthands can be used: - <code>brute-force</code> (Try all combinations of index candidates and pick up optimal one)- <code>approximate</code> (Use greedy algorithm to pick up approximately optimal index candidate)- <code>threshold-based</code> (Use index-select-threshold to pick up either <code>approximate</code> or <code>threshold-based</code> strategy on runtime) String threshold-based MASKABLE query.index-select-threshold Threshold of deciding whether to use brute force enumeration algorithm or fast approximation algorithm for selecting suitable indexes. Selecting optimal indexes for a query is a NP-complete set cover problem. When number of suitable index candidates is no larger than threshold, JanusGraph uses brute force search with exponential time complexity to ensure the best combination of indexes is selected. Only effective when <code>threshold-based</code> index select strategy is chosen. Integer 10 MASKABLE query.optimizer-backend-access Whether the optimizer should be allowed to fire backend queries during the optimization phase. Allowing these will give the optimizer a chance to find more efficient execution plan but also increase the optimization overhead. Boolean true MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean false MASKABLE"},{"location":"configs/configuration-reference/#querybatch","title":"query.batch","text":"<p>Configuration options to configure batch queries optimization behavior</p> Name Description Datatype Default Value Mutability query.batch.drop-step-mode Batching mode for <code>drop()</code> step. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all</code> - Drops all vertices in a batch.- <code>none</code> - Skips drop batching optimization. String all MASKABLE query.batch.enabled Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. If <code>false</code> then all other configuration options under <code>query.batch</code> namespace are ignored. Boolean true MASKABLE query.batch.has-step-mode Properties pre-fetching mode for <code>has</code> step. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all_properties</code> - Pre-fetch all vertex properties on any property access (fetches all vertex properties in a single slice query)- <code>required_properties_only</code> - Pre-fetch necessary vertex properties for the whole chain of foldable <code>has</code> steps (uses a separate slice query per each required property)- <code>required_and_next_properties</code> - Prefetch the same properties as with <code>required_properties_only</code> mode, but also prefetchproperties which may be needed in the next properties access step like <code>values</code>, <code>properties,</code> <code>valueMap</code>, <code>elementMap</code>, or <code>propertyMap</code>.In case the next step is not one of those properties access steps then this mode behaves same as <code>required_properties_only</code>.In case the next step is one of the properties access steps with limited scope of properties, those properties will bepre-fetched together in the same multi-query.In case the next step is one of the properties access steps with unspecified scope of property keys then this modebehaves same as <code>all_properties</code>.- <code>required_and_next_properties_or_all</code> - Prefetch the same properties as with <code>required_and_next_properties</code>, but in case the next step is not<code>values</code>, <code>properties,</code> <code>valueMap</code>, <code>elementMap</code>, or <code>propertyMap</code> then acts like <code>all_properties</code>.- <code>none</code> - Skips <code>has</code> step batch properties pre-fetch optimization. String required_and_next_properties MASKABLE query.batch.label-step-mode Labels pre-fetching mode for <code>label()</code> step. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all</code> - Pre-fetch labels for all vertices in a batch.- <code>none</code> - Skips vertex labels pre-fetching optimization. String all MASKABLE query.batch.limited Configure a maximum batch size for queries against the storage backend. This can be used to ensure responsiveness if batches tend to grow very large. The used batch size is equivalent to the barrier size of a preceding <code>barrier()</code> step. If a step has no preceding <code>barrier()</code>, the default barrier of TinkerPop will be inserted. This option only takes effect if <code>query.batch.enabled</code> is <code>true</code>. Boolean true MASKABLE query.batch.limited-size Default batch size (barrier() step size) for queries. This size is applied only for cases where <code>LazyBarrierStrategy</code> strategy didn't apply <code>barrier</code> step and where user didn't apply barrier step either. This option is used only when <code>query.batch.limited</code> is <code>true</code>. Notice, value <code>2147483647</code> is considered to be unlimited. Integer 2500 MASKABLE query.batch.properties-mode Properties pre-fetching mode for <code>values</code>, <code>properties</code>, <code>valueMap</code>, <code>propertyMap</code>, <code>elementMap</code> steps. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all_properties</code> - Pre-fetch all vertex properties on non-singular property access (fetches all vertex properties in a single slice query). On single property access this mode behaves the same as <code>required_properties_only</code> mode.- <code>required_properties_only</code> - Pre-fetch necessary vertex properties only (uses a separate slice query per each required property)- <code>none</code> - Skips vertex properties pre-fetching optimization. String required_properties_only MASKABLE query.batch.repeat-step-mode Batch mode for <code>repeat</code> step. Used only when query.batch.enabled is <code>true</code>.These modes are controlling how the child steps with batch support are behaving if they are placed to the start of the <code>repeat</code>, <code>emit</code>, or <code>until</code> traversals.Supported modes:- <code>closest_repeat_parent</code> - Child start steps are receiving vertices for batching from the closest <code>repeat</code> step parent only.- <code>all_repeat_parents</code> - Child start steps are receiving vertices for batching from all <code>repeat</code> step parents.- <code>starts_only_of_all_repeat_parents</code> - Child start steps are receiving vertices for batching from the closest <code>repeat</code> step parent (both for the parent start and for next iterations) and also from all <code>repeat</code> step parents for the parent start. String all_repeat_parents MASKABLE"},{"location":"configs/configuration-reference/#schema","title":"schema","text":"<p>Schema related configuration options</p> Name Description Datatype Default Value Mutability schema.constraints Configures the schema constraints to be used by this graph. If config 'schema.constraints' is set to 'true' and 'schema.default' is set to 'none', then an 'IllegalArgumentException' is thrown for schema constraint violations. If 'schema.constraints' is set to 'true' and 'schema.default' is not set 'none', schema constraints are automatically created as described in the config option 'schema.default'. If 'schema.constraints' is set to 'false' which is the default, then no schema constraints are applied. Boolean false GLOBAL_OFFLINE schema.default Configures the DefaultSchemaMaker to be used by this graph. Either one of the following shorthands can be used:  - <code>default</code> (a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys), - <code>tp3</code> (same as default, but has LIST property keys), - <code>none</code> (automatic schema creation is disabled) - <code>ignore-prop</code> (same as none, but simply ignore unknown properties rather than throw exceptions) - or to the full package and classname of a custom/third-party implementing the interface <code>org.janusgraph.core.schema.DefaultSchemaMaker</code> String default MASKABLE schema.logging Controls whether logging is enabled for schema makers. This only takes effect if you set <code>schema.default</code> to <code>default</code> or <code>ignore-prop</code>. For <code>default</code> schema maker, warning messages will be logged before schema types are created automatically. For <code>ignore-prop</code> schema maker, warning messages will be logged before unknown properties are ignored. Boolean false MASKABLE"},{"location":"configs/configuration-reference/#schemainit","title":"schema.init","text":"<p>Configuration options for schema initialization on startup.</p> Name Description Datatype Default Value Mutability schema.init.drop-before-startup Drops the entire schema with graph data before JanusGraph schema initialization. Note that the schema will be dropped regardless of the selected initialization strategy, including when <code>schema.init.strategy</code> is set to <code>none</code>. Boolean false LOCAL schema.init.strategy Specifies the strategy for schema initialization before starting JanusGraph. You must provide the full class path of a class that implements the <code>SchemaInitStrategy</code> interface and has parameterless constructor.The following shortcuts are also available:- <code>none</code> - Skips schema initialization.- <code>json</code> - Schema initialization via provided JSON file or JSON string. String none LOCAL"},{"location":"configs/configuration-reference/#schemainitjson","title":"schema.init.json","text":"<p>Options for JSON schema initialization strategy.</p> Name Description Datatype Default Value Mutability schema.init.json.await-index-status-timeout Timeout for awaiting index status operation defined in milliseconds. If the status await timeouts the exception will be thrown during schema initialization process. Long 180000 LOCAL schema.init.json.file File path to JSON formated schema definition. String (no default value) LOCAL schema.init.json.force-close-other-instances Force closes other JanusGraph instances before schema initialization, regardless if they are active or not. This is a dangerous operation. This option exists to help people initialize schema who struggle with zombie JanusGraph instances. It's not recommended to be used unless you know what you are doing. Instead of this parameter, it's recommended to check <code>graph.unique-instance-id</code> and <code>graph.replace-instance-if-exists</code> options to not create zombie instances in the cluster. Boolean false LOCAL schema.init.json.indices-activation Indices activation type:- <code>reindex_and_enable_updated_only</code> - Reindex process will be triggered for any updated index. After this all updated indexes will be enabled.- <code>reindex_and_enable_non_enabled</code> - Reindex process will be triggered for any index which is not enabled (including previously created indices). After reindexing all indices will be enabled.- <code>skip_activation</code> - Skip reindex process for any updated indexes.- <code>force_enable_updated_only</code> - Force enable all updated indexes without running any reindex process (previous data may not be available for such indices).- <code>force_enable_non_enabled</code> - Force enable all indexes (including previously created indices) without running any reindex process (previous data may not be available for such indices). String reindex_and_enable_non_enabled LOCAL schema.init.json.skip-elements Skip creation of VertexLabel, EdgeLabel, and PropertyKey. Boolean false LOCAL schema.init.json.skip-indices Skip creation of indices. Boolean false LOCAL schema.init.json.string JSON formated schema definition string. This option takes precedence if both <code>file</code> and <code>string</code> are used. String (no default value) LOCAL"},{"location":"configs/configuration-reference/#storage","title":"storage","text":"<p>Configuration options for the storage backend.  Some options are applicable only for certain backends.</p> Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph.  This is required.  It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cql, hbase, inmemory, scylla) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers.  This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.num-mutations-parallel-threshold This parameter determines the minimum number of mutations a transaction must have before parallel processing is applied during aggregation. Leveraging parallel processing can enhance the commit times for transactions involving a large number of mutations. However, it is advisable not to set the threshold too low (e.g., 0 or 1) due to the overhead associated with parallelism synchronization. This overhead is more efficiently offset in the context of larger transactions. Integer 100 MASKABLE storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to /. String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE"},{"location":"configs/configuration-reference/#storageberkeleyje","title":"storage.berkeleyje","text":"<p>BerkeleyDB JE configuration options</p> Name Description Datatype Default Value Mutability storage.berkeleyje.cache-mode Modes that can be specified for control over caching of records in the JE in-memory cache String DEFAULT MASKABLE storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE storage.berkeleyje.shared-cache If true, the shared cache is used for all graph instances Boolean true MASKABLE"},{"location":"configs/configuration-reference/#storageberkeleyjeext","title":"storage.berkeleyje.ext","text":"<p>Overrides for arbitrary settings applied at <code>EnvironmentConfig</code> creation. The full list of possible setting is available inside the Java class <code>com.sleepycat.je.EnvironmentConfig</code>. All configurations values should be specified as <code>String</code> and be formated the same as specified in the following documentation. Notice, for compatibility reasons, it's allowed to use <code>-</code> character instead of <code>.</code> for config keys. All dashes will be replaced by dots when passing those keys to <code>EnvironmentConfig</code>.</p> Name Description Datatype Default Value Mutability storage.berkeleyje.ext.je-lock-timeout Lock timeout configuration. <code>0</code> disabled lock timeout completely. To set lock timeout via this configuration it's required to use String formated time representation. For example: <code>500 ms</code>, <code>5 min</code>, etc. See information about value constraints in the official sleepycat documentation. Notice, this option can be specified as <code>storage.berkeleyje.ext.je.lock.timeout</code> which will be treated the same as this configuration option. String (no default value) MASKABLE"},{"location":"configs/configuration-reference/#storagecql","title":"storage.cql","text":"<p>CQL storage backend options</p> Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.back-pressure-class The implementation of <code>QueryBackPressure</code> to use. The full name of the class which extends <code>QueryBackPressure</code> which has either a public constructor with <code>Configuration janusGraphConfiguration</code> and <code>Integer backPressureLimit</code> arguments (preferred constructor) or a public constructor with <code>Configuration janusGraphConfiguration</code> argument (second preferred constructor) or a public parameterless constructor. Other accepted options are: <code>semaphore</code> - fair semaphore based back pressure implementation of <code>back-pressure-limit</code> limit size (preferred implementation); <code>semaphoreReleaseProtected</code> - fair semaphore based back pressure implementation of <code>back-pressure-limit</code> limit size with protected releasing logic (meant to be used for testing); <code>passAll</code> - turned off back pressure (it is recommended to tune CQL driver for the ongoing workload when this implementation is used); String semaphore MASKABLE storage.cql.back-pressure-limit The maximum number of concurrent requests which are allowed to be processed by CQL driver. If no value is provided or the value is set to <code>0</code> then the value will be calculated based on CQL driver session provided parameters by using formula [advanced.connection.max-requests-per-connection * advanced.connection.pool.local.size * available_nodes_amount]. It's not recommended to use any value which is above this limit because it may result in CQL driver overload but it's suggested to have a lower value to keep the driver healthy under pressure. In situations when remote nodes connections are in use then the bigger value might be relevant as well to improve parallelism. Integer (no default value) MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.gc-grace-seconds The number of seconds before tombstones (deletion markers) are eligible for garbage-collection. Integer (no default value) FIXED storage.cql.heartbeat-interval The connection heartbeat interval in milliseconds. Long (no default value) MASKABLE storage.cql.heartbeat-timeout How long the driver waits for the response (in milliseconds) to a heartbeat. Long (no default value) MASKABLE storage.cql.init-wait-time Number of milliseconds to sleep after creating each keyspace/table, only needed if table creation is asynchronous, e.g. Amazon Keyspace Integer (no default value) LOCAL storage.cql.keyspace The name of JanusGraph's keyspace.  It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter. This value will be passed into CqlSessionBuilder.withLocalDatacenter. String datacenter1 MASKABLE storage.cql.local-max-connections-per-host The maximum number of connections that can be created per host for local datacenter Integer 1 MASKABLE storage.cql.max-requests-per-connection The maximum number of requests that can be executed concurrently on a connection. Integer 1024 MASKABLE storage.cql.metadata-schema-enabled Whether schema metadata is enabled. Boolean (no default value) MASKABLE storage.cql.metadata-token-map-enabled Whether token metadata is enabled. If disabled, partitioner-name must be provided. Boolean (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.partitioner-name The name of Cassandra cluster's partitioner. It will be retrieved by client if not provided. If provided, it must match the cluster's partitioner name. It can be the full class name such as <code>org.apache.cassandra.dht.ByteOrderedPartitioner</code> or the simple name such as <code>ByteOrderedPartitioner</code> String (no default value) MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database.  If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.remote-max-connections-per-host The maximum number of connections that can be created per host for remote datacenter Integer 1 MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept. This options is used when storage.cql.replication-strategy-class is set to SimpleStrategy Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace. Available strategies: SimpleStrategy,NetworkTopologyStrategy. String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. This options is used when storage.cql.replication-strategy-class is set to NetworkTopologyStrategy. <code>replication_factor</code> can be used to specify a replication factor. String[] (no default value) FIXED storage.cql.request-timeout Timeout for CQL requests in milliseconds. See DataStax Java Driver option <code>basic.request.timeout</code> for more information. Long 12000 MASKABLE storage.cql.session-leak-threshold The maximum number of live sessions that are allowed to coexist in a given VM until the warning starts to log for every new session. If the value is less than or equal to 0, the feature is disabled: no warning will be issued. See DataStax Java Driver option <code>advanced.session-leak.threshold</code> for more information. Integer (no default value) MASKABLE storage.cql.session-name Default name for the Cassandra session String JanusGraph Session MASKABLE storage.cql.speculative-retry The speculative retry policy. One of: NONE, ALWAYS, percentile, ms. String (no default value) FIXED storage.cql.ttl-enabled Whether TTL should be enabled or not. Must be turned off if the storage does not support TTL. Amazon Keyspace, for example, does not support TTL by default unless otherwise enabled. Boolean true LOCAL storage.cql.use-external-locking True to prevent JanusGraph from using its own locking mechanism. Setting this to true eliminates redundant checks when using an external locking mechanism outside of JanusGraph. Be aware that when use-external-locking is set to true, that failure to employ a locking algorithm which locks all columns that participate in a transaction upfront and unlocks them when the transaction ends, will result in a 'read uncommitted' transaction isolation level guarantee. If set to true without an appropriate external locking mechanism in place side effects such as dirty/non-repeatable/phantom reads should be expected. Boolean false MASKABLE storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE"},{"location":"configs/configuration-reference/#storagecqlexecutor-service","title":"storage.cql.executor-service","text":"<p>Configuration options for CQL executor service which is used to process deserialization of CQL queries.</p> Name Description Datatype Default Value Mutability storage.cql.executor-service.class The implementation of <code>ExecutorService</code> to use. The full name of the class which extends <code>ExecutorService</code> which has either a public constructor with <code>ExecutorServiceConfiguration</code> argument (preferred constructor) or a public parameterless constructor. Other accepted options are: <code>fixed</code> - fixed thread pool of size <code>core-pool-size</code>; <code>cached</code> - cached thread pool; String fixed LOCAL storage.cql.executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set or set to -1 the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.cql.executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the <code>core-pool-size</code>, this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for <code>fixed</code> executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.cql.executor-service.max-pool-size Maximum pool size for executor service. Ignored for <code>fixed</code> and <code>cached</code> executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.cql.executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL"},{"location":"configs/configuration-reference/#storagecqlgrouping","title":"storage.cql.grouping","text":"<p>Configuration options for controlling CQL queries grouping</p> Name Description Datatype Default Value Mutability storage.cql.grouping.keys-allowed If <code>true</code> this allows multiple partition keys to be grouped together into a single CQL query via <code>IN</code> operator based on the keys grouping strategy provided (usually grouping is done by same token-ranges or same replica sets, but may also involve shard ids for custom implementations).Notice, that any CQL query grouped with more than 1 key will require to return a row key for any column fetched.This option is useful when less amount of CQL queries is desired to be sent for read requests in expense of fetching more data (partition key per each fetched value).Notice, different storage backends may have different way of executing multi-partition <code>IN</code> queries (including, but not limited to how the checksum queries are sent for different consistency levels, processing node CPU usage, disk access pattern, etc.). Thus, a proper benchmarking is needed to determine if keys grouping is useful or not per case by case scenario.This option can be enabled only for storage backends which support <code>PER PARTITION LIMIT</code>. As such, this feature can't be used with Amazon Keyspaces because it doesn't support <code>PER PARTITION LIMIT</code>.If this option is <code>false</code> then each partition key will be executed in a separate asynchronous CQL query even when multiple keys from the same token range are queried.Notice, the default grouping strategy does not take shards into account. Thus, this might be inefficient with ScyllaDB storage backend. ScyllaDB specific keys grouping strategy should be implemented after the resolution of the ticket #232. Boolean false MASKABLE storage.cql.grouping.keys-class Full class path of the keys grouping execution strategy. The class should implement <code>org.janusgraph.diskstorage.cql.strategy.GroupedExecutionStrategy</code> interface and have a public constructor with two arguments <code>org.janusgraph.diskstorage.configuration.Configuration</code> and <code>org.janusgraph.diskstorage.cql.CQLStoreManager</code>.Shortcuts available:- <code>tokenRangeAware</code> - groups partition keys which belong to the same token range. Notice, this strategy does not take shards into account. Thus, this might be inefficient with ScyllaDB storage backend.- <code>replicasAware</code> - groups partition keys which belong to the same replica sets (same nodes). Notice, this strategy does not take shards into account. Thus, this might be inefficient with ScyllaDB storage backend.Usually <code>tokenRangeAware</code> grouping strategy provides more smaller groups where each group contain keys which are stored close to each other on a disk and may cause less disk seeks in some cases. However <code>replicasAware</code> grouping strategy groups keys per replica set which usually means fewer bigger groups to be used (i.e. less CQL requests).This option takes effect only when <code>storage.cql.grouping.keys-allowed</code> is <code>true</code>. String replicasAware MASKABLE storage.cql.grouping.keys-limit Maximum amount of the keys which can be grouped together into a single CQL query. If more keys are queried, they are going to be grouped into separate CQL queries.Notice, for ScyllaDB this option should not exceed the maximum number of distinct clustering key restrictions per query which can be changed by ScyllaDB configuration option <code>max-partition-key-restrictions-per-query</code> (https://enterprise.docs.scylladb.com/branch-2022.2/faq.html#how-can-i-change-the-maximum-number-of-in-restrictions). For AstraDB this limit is set to 20 and usually it's fixed. However, you can ask customer support for a possibility to change the default threshold to your desired configuration via <code>partition_keys_in_select_failure_threshold</code> and <code>in_select_cartesian_product_failure_threshold</code> threshold configurations (https://docs.datastax.com/en/astra-serverless/docs/plan/planning.html#_cassandra_yaml).Ensure that your storage backend allows more IN selectors than the one set via this configuration.This option takes effect only when <code>storage.cql.grouping.keys-allowed</code> is <code>true</code>. Integer 20 MASKABLE storage.cql.grouping.keys-min Minimum amount of keys to consider for grouping. Grouping will be skipped for any multi-key query which has less than this amount of keys (i.e. a separate CQL query will be executed for each key in such case).Usually this configuration should always be set to <code>2</code>. It is useful to increase the value only in cases when queries with more keys should not be grouped, but be performed separately to increase parallelism in expense of the network overhead.This option takes effect only when <code>storage.cql.grouping.keys-allowed</code> is <code>true</code>. Integer 2 MASKABLE storage.cql.grouping.slice-allowed If <code>true</code> this allows multiple Slice queries which are allowed to be performed as non-range queries (i.e. direct equality operation) to be grouped together into a single CQL query via <code>IN</code> operator. Notice, currently only operations to fetch properties with Cardinality.SINGLE are allowed to be performed as non-range queries (edges fetching or properties with Cardinality SET or LIST won't be grouped together).If this option is <code>false</code> then each Slice query will be executed in a separate asynchronous CQL query even when grouping is allowed. Boolean true MASKABLE storage.cql.grouping.slice-limit Maximum amount of grouped together slice queries into a single CQL query.Notice, for ScyllaDB this option should not exceed the maximum number of distinct clustering key restrictions per query which can be changed by ScyllaDB configuration option <code>max-partition-key-restrictions-per-query</code> (https://enterprise.docs.scylladb.com/branch-2022.2/faq.html#how-can-i-change-the-maximum-number-of-in-restrictions). For AstraDB this limit is set to 20 and usually it's fixed. However, you can ask customer support for a possibility to change the default threshold to your desired configuration via <code>partition_keys_in_select_failure_threshold</code> and <code>in_select_cartesian_product_failure_threshold</code> threshold configurations (https://docs.datastax.com/en/astra-serverless/docs/plan/planning.html#_cassandra_yaml).Ensure that your storage backend allows more IN selectors than the one set via this configuration.This option is used only when <code>storage.cql.grouping.slice-allowed</code> is <code>true</code>. Integer 20 MASKABLE"},{"location":"configs/configuration-reference/#storagecqlinternal","title":"storage.cql.internal","text":"<p>Advanced configuration of internal DataStax driver. Notice, all available configurations will be composed in the order. Non specified configurations will be skipped. By default only base configuration is enabled (which has the smallest priority. It means that you can overwrite any configuration used in base programmatic configuration by using any other configuration type). The configurations are composed in the next order (sorted by priority in descending order): <code>file-configuration</code>, <code>resource-configuration</code>, <code>string-configuration</code>, <code>url-configuration</code>, <code>base-programmatic-configuration</code> (which is controlled by <code>base-programmatic-configuration-enabled</code> property). Configurations with higher priority always overwrite configurations with lower priority. I.e. if the same configuration parameter is used in both <code>file-configuration</code> and <code>string-configuration</code> the configuration parameter from <code>file-configuration</code> will be used and configuration parameter from <code>string-configuration</code> will be ignored. See available configuration options and configurations structure here: https://docs.datastax.com/en/developer/java-driver/4.13/manual/core/configuration/reference/</p> Name Description Datatype Default Value Mutability storage.cql.internal.base-programmatic-configuration-enabled Whether to use main programmatic configuration provided by JanusGraph properties or not. We don't recommend to disable this property unless you want to disable usage of all storage.cql properties and use default configurations or other configurations. If programmatic configuration options miss some important configuration options you can provide those missing configurations with other configuration types which will be applied with programmatic configuration (see other configuration types in this section). For most use cases this option should always be <code>true</code>. JanusGraph behaviour might be unpredictable when using unspecified configuration options. Boolean true MASKABLE storage.cql.internal.file-configuration Path to file with DataStax configuration. String (no default value) LOCAL storage.cql.internal.resource-configuration Classpath resource with DataStax configuration. String (no default value) MASKABLE storage.cql.internal.string-configuration String representing DataStax configuration. String (no default value) MASKABLE storage.cql.internal.url-configuration Url where to get DataStax configuration. String (no default value) MASKABLE"},{"location":"configs/configuration-reference/#storagecqlmetrics","title":"storage.cql.metrics","text":"<p>Configuration options for CQL metrics</p> Name Description Datatype Default Value Mutability storage.cql.metrics.cql-messages-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option <code>advanced.metrics.node.cql-messages.refresh-interval</code> Long (no default value) LOCAL storage.cql.metrics.cql-messages-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option <code>advanced.metrics.node.cql-messages.highest-latency</code> Long (no default value) LOCAL storage.cql.metrics.cql-messages-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option <code>advanced.metrics.node.cql-messages.significant-digits</code> Integer (no default value) LOCAL storage.cql.metrics.cql-requests-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.cql-requests.highest-latency</code> Long (no default value) LOCAL storage.cql.metrics.cql-requests-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.cql-requests.refresh-interval</code> Long (no default value) LOCAL storage.cql.metrics.cql-requests-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.cql-requests.significant-digits</code> Integer (no default value) LOCAL storage.cql.metrics.node-enabled Comma separated list of enabled node metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: pool.open-connections, pool.available-streams, bytes-sent). String[] (no default value) LOCAL storage.cql.metrics.node-expire-after The time after which the node level metrics will be evicted in milliseconds. Long (no default value) LOCAL storage.cql.metrics.session-enabled Comma separated list of enabled session metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: bytes-sent, bytes-received, connected-nodes). String[] (no default value) LOCAL storage.cql.metrics.throttling-delay-highest-latency The largest latency that we expect to record for throttling in milliseconds. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.throttling.delay.highest-latency</code> Long (no default value) LOCAL storage.cql.metrics.throttling-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for throttling. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.throttling.delay.refresh-interval</code> Long (no default value) LOCAL storage.cql.metrics.throttling-delay-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for throttling. This must be between 0 and 5. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.throttling.delay.significant-digits</code> Integer (no default value) LOCAL"},{"location":"configs/configuration-reference/#storagecqlnetty","title":"storage.cql.netty","text":"<p>Configuration options related to the Netty event loop groups used internally by the CQL driver.</p> Name Description Datatype Default Value Mutability storage.cql.netty.admin-size The number of threads for the event loop group used for admin tasks not related to request I/O (handle cluster events, refresh metadata, schedule reconnections, etc.). If this is not set, the driver will use 2. Integer (no default value) LOCAL storage.cql.netty.io-size The number of threads for the event loop group used for I/O operations (reading and writing to Cassandra nodes). If this is not set, the driver will use <code>Runtime.getRuntime().availableProcessors() * 2</code>. Integer (no default value) LOCAL storage.cql.netty.timer-tick-duration The timer tick duration in milliseconds. This is how frequent the timer should wake up to check for timed-out tasks or speculative executions. See DataStax Java Driver option <code>advanced.netty.timer.tick-duration</code> for more information. Long (no default value) LOCAL storage.cql.netty.timer-ticks-per-wheel Number of ticks in a Timer wheel. See DataStax Java Driver option <code>advanced.netty.timer.ticks-per-wheel</code> for more information. Integer (no default value) LOCAL"},{"location":"configs/configuration-reference/#storagecqlrequest-tracker","title":"storage.cql.request-tracker","text":"<p>Configuration options for CQL request tracker and builtin request logger</p> Name Description Datatype Default Value Mutability storage.cql.request-tracker.class It is either a predefined DataStax driver value for a builtin request tracker or a full qualified class name which implements <code>com.datastax.oss.driver.internal.core.tracker.RequestTracker</code> interface. If no any value provided, the default DataStax request tracker is used, which is <code>NoopRequestTracker</code> which doesn't do anything. If <code>RequestLogger</code> value is provided, the DataStax RequestLogger is used. String (no default value) LOCAL storage.cql.request-tracker.logs-error-enabled Whether to log failed requests.Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-max-query-length The maximum length of the query string in the log message. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-value-length The maximum length for bound values in the log message. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-values The maximum number of bound values to log. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Integer (no default value) LOCAL storage.cql.request-tracker.logs-show-stack-traces Whether to log stack traces for failed queries. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-show-values Whether to log bound values in addition to the query string. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-enabled Whether to log <code>slow</code> requests.Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-threshold The threshold to classify a successful request as <code>slow</code>. In milliseconds. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Long (no default value) LOCAL storage.cql.request-tracker.logs-success-enabled Whether to log successful requests. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL"},{"location":"configs/configuration-reference/#storagecqlssl","title":"storage.cql.ssl","text":"<p>Configuration options for SSL</p> Name Description Datatype Default Value Mutability storage.cql.ssl.client-authentication-enabled Enables use of a client key to authenticate with Cassandra Boolean false LOCAL storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL storage.cql.ssl.hostname_validation Enable / disable SSL hostname validation. Boolean false LOCAL"},{"location":"configs/configuration-reference/#storagecqlsslkeystore","title":"storage.cql.ssl.keystore","text":"<p>Configuration options for SSL Keystore.</p> Name Description Datatype Default Value Mutability storage.cql.ssl.keystore.keypassword The password to access the key in SSL Keystore. String LOCAL storage.cql.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL storage.cql.ssl.keystore.storepassword The password to access the SSL Keystore. String LOCAL"},{"location":"configs/configuration-reference/#storagecqlssltruststore","title":"storage.cql.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"configs/configuration-reference/#storagehbase","title":"storage.hbase","text":"<p>HBase storage options</p> Name Description Datatype Default Value Mutability storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster.  JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances.  This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.snapshot-name The name of an existing HBase snapshot to be used by HBaseSnapshotInputFormat String janusgraph-snapshot LOCAL storage.hbase.snapshot-restore-dir The temporary directory to be used by HBaseSnapshotInputFormat to restore a snapshot. This directory should be on the same File System as the HBase root dir. String /tmp LOCAL storage.hbase.table The name of the table JanusGraph will use.  When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL"},{"location":"configs/configuration-reference/#storagelock","title":"storage.lock","text":"<p>Options for locking on eventually-consistent stores</p> Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend.  JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code.  JanusGraph generates an appropriate default value for this option at startup.  Overriding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Although this value is maskable, it is highly recommended to use the same value across JanusGraph instances in production environments. Duration 100 ms MASKABLE"},{"location":"configs/configuration-reference/#storagemeta","title":"storage.meta *","text":"<p>Meta data to include in storage backend retrievals</p> Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps. If enabled, timestamp can be retrieved by <code>element.value(ImplicitKey.TIMESTAMP.name())</code> or equivalently, <code>element.value(\"~timestamp\")</code>. Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL. If enabled, ttl can be retrieved by <code>element.value(ImplicitKey.TTL.name())</code> or equivalently, <code>element.value(\"~ttl\")</code>. Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility. If enabled, visibility can be retrieved by <code>element.value(ImplicitKey.VISIBILITY.name())</code> or equivalently, <code>element.value(\"~visibility\")</code>. Boolean true GLOBAL"},{"location":"configs/configuration-reference/#storageparallel-backend-executor-service","title":"storage.parallel-backend-executor-service","text":"<p>Configuration options for executor service which is used for parallel requests when <code>storage.parallel-backend-ops</code> is enabled.</p> Name Description Datatype Default Value Mutability storage.parallel-backend-executor-service.class The implementation of <code>ExecutorService</code> to use. The full name of the class which extends <code>ExecutorService</code> which has either a public constructor with <code>ExecutorServiceConfiguration</code> argument (preferred constructor) or a public parameterless constructor. Other accepted options are: <code>fixed</code> - fixed thread pool of size <code>core-pool-size</code>; <code>cached</code> - cached thread pool; String fixed LOCAL storage.parallel-backend-executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set or set to -1 the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.parallel-backend-executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the <code>core-pool-size</code>, this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for <code>fixed</code> executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.parallel-backend-executor-service.max-pool-size Maximum pool size for executor service. Ignored for <code>fixed</code> and <code>cached</code> executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.parallel-backend-executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL"},{"location":"configs/configuration-reference/#tx","title":"tx","text":"<p>Configuration options for transaction handling</p> Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL"},{"location":"configs/configuration-reference/#txrecovery","title":"tx.recovery","text":"<p>Configuration options for transaction recovery processes</p> Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE"},{"location":"configs/example-config/","title":"Example Graph Configuration","text":"<p>This page illustrates a number of common graph configurations. Please refer to Configuration Reference and the pages of the respective storage backend, index backend for more information.</p> <p>Also, note that the JanusGraph distribution includes local configuration files in the <code>conf/</code> directory.</p>"},{"location":"configs/example-config/#berkeleydb","title":"BerkeleyDB","text":"<pre><code>storage.backend=berkeleyje\nstorage.directory=/tmp/graph\n\nindex.search.backend=elasticsearch\nindex.search.directory=/tmp/searchindex\nindex.search.elasticsearch.client-only=false\nindex.search.elasticsearch.local-mode=true\n</code></pre> <p>This configuration file configures JanusGraph to use BerkeleyDB as an embedded storage backend, meaning, JanusGraph will start BerkeleyDB internally. The primary data will be stored in the directory <code>/tmp/graph</code>.</p> <p>In addition, this configures an embedded Elasticsearch index backend with the name <code>search</code>. JanusGraph will start Elasticsearch internally and it will not be externally accessible since <code>local-mode</code> is enabled. Elasticsearch stores all data for the <code>search</code> index in <code>/tmp/searchindex</code>. Configuring an index backend is optional.</p>"},{"location":"configs/example-config/#cassandra","title":"Cassandra","text":""},{"location":"configs/example-config/#cassandra-remote","title":"Cassandra Remote","text":"<pre><code>storage.backend=cql\nstorage.hostname=100.100.100.1, 100.100.100.2\n\nindex.search.backend=elasticsearch\nindex.search.hostname=100.100.101.1, 100.100.101.2\nindex.search.elasticsearch.client-only=true\n</code></pre> <p>This configuration file configures JanusGraph to use Cassandra as a remote storage backend. It assumes that a Cassandra cluster is running and accessible at the given IP addresses. If Cassandra is running locally, use the IP address <code>127.0.0.1</code>.</p> <p>In addition, this configures a remote Elasticsearch index backend with the name <code>search</code>. It assumes that an Elasticsearch cluster is running and accessible at the given IP addresses. Enabling <code>client-only</code> ensures that the local instance does not join the existing Elasticsearch cluster as another node but only connects to it. Configuring an index backend is optional.</p>"},{"location":"configs/example-config/#hbase","title":"HBase","text":"<pre><code>storage.backend=hbase\nstorage.hostname=127.0.0.1\nstorage.port=2181\n\nindex.search.backend=elasticsearch\nindex.search.hostname=127.0.0.1\nindex.search.elasticsearch.client-only=true\n</code></pre> <p>This configuration file configures JanusGraph to use HBase as a remote storage backend. It assumes that an HBase cluster is running and accessible at the given IP addresses through the configured port. If HBase is running locally, use the IP address <code>127.0.0.1</code>.</p> <p>The optional index backend configuration is identical to remote index configuration described above.</p>"},{"location":"configs/janusgraph-cfg/","title":"Janusgraph cfg","text":""},{"location":"configs/janusgraph-cfg/#attributescustom","title":"attributes.custom *","text":"<p>Custom attribute serialization and handling</p> Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE"},{"location":"configs/janusgraph-cfg/#cache","title":"cache","text":"<p>Configuration options that modify JanusGraph's caching behavior</p> Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data.  Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them.  This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 MASKABLE cache.db-cache-size Size of JanusGraph's database level cache.  Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 MASKABLE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#cluster","title":"cluster","text":"<p>Configuration options for multi-machine deployments</p> Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodes in the JanusGraph graph cluster. Must be greater than 1 and a power of 2. Integer 32 FIXED"},{"location":"configs/janusgraph-cfg/#computer","title":"computer","text":"<p>GraphComputer related configuration</p> Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE"},{"location":"configs/janusgraph-cfg/#graph","title":"graph","text":"<p>General configuration options</p> Name Description Datatype Default Value Mutability graph.allow-custom-vid-types Whether non long-type vertex ids are allowed. set-vertex-id must be enabled in order to use this functionality. Currently, only string-type is supported. This does not prevent users from using custom ids with long type. If your storage backend does not support unordered scan, then some scan operations will be disabled. You cannot use this feature with Berkeley DB. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false GLOBAL_OFFLINE graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL.  These types are managed globally through the storage backend and cannot be overridden by changing the local configuration.  This type of conflict usually indicates misconfiguration.  When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option.  When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.allow-upgrade Setting this to true will allow certain fixed values to be updated such as storage-version. This should only be used for upgrading. Boolean false MASKABLE graph.assign-timestamp Whether to use JanusGraph generated client-side timestamp in mutations if the backend supports it. When enabled, JanusGraph assigns one timestamp to all insertions and another slightly earlier timestamp to all deletions in the same batch. When this is disabled, mutation behavior depends on the backend. Some might use server-side timestamp (e.g. HBase) while others might use client-side timestamp generated by driver (CQL). Boolean true LOCAL graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagement APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anyway. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic vertex id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. For example, users must ensure the vertex ids are unique to avoid duplication. Must use <code>graph.getIDManager().toVertexId(long)</code> to convert your id first. Once this is enabled, you have to provide vertex id when creating new vertices. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false GLOBAL_OFFLINE graph.storage-version The version of JanusGraph storage schema with which this database was created. Automatically set on first start of graph. Should only ever be changed if upgrading to a new major release version of JanusGraph that contains schema changes String (no default value) FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored.  An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance.  This must be unique among all instances concurrently accessing the same stores or indexes.  It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL"},{"location":"configs/janusgraph-cfg/#graphscript-eval","title":"graph.script-eval","text":"<p>Configuration options for gremlin script engine.</p> Name Description Datatype Default Value Mutability graph.script-eval.enabled Whether to enable Gremlin script evaluation. If it is enabled, a gremlin script engine will be instantiated together with the JanusGraph instance, with which one can use <code>eval</code> method to evaluate a gremlin script in plain string format. This is usually only useful when JanusGraph is used as an embedded Java library. Boolean false MASKABLE graph.script-eval.engine Full class name of script engine that implements <code>GremlinScriptEngine</code> interface. Following shorthands can be used:  - <code>GremlinLangScriptEngine</code> (A script engine that only accepts standard gremlin queries. Anything else including lambda function is not accepted. We recommend using this because it's generally safer, but it is not guaranteed that it has no security problem.)- <code>GremlinGroovyScriptEngine</code> (A script engine that accepts arbitrary groovy code. This can be dangerous and you should use it at your own risk. See https://tinkerpop.apache.org/docs/current/reference/#script-execution for potential security problems.) String GremlinLangScriptEngine MASKABLE"},{"location":"configs/janusgraph-cfg/#gremlin","title":"gremlin","text":"<p>Gremlin configuration options</p> Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL"},{"location":"configs/janusgraph-cfg/#ids","title":"ids","text":"<p>General configuration options for graph element IDs</p> Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size.  Setting this too low will make commits frequently block on slow reservation requests.  Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation.  When false, IDs are assigned only when the transaction commits. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE"},{"location":"configs/janusgraph-cfg/#idsauthority","title":"ids.authority","text":"<p>Configuration options for graph element ID reservation/allocation</p> Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE"},{"location":"configs/janusgraph-cfg/#index","title":"index *","text":"<p>Configuration options for the individual indexing backends</p> Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional.  JanusGraph can use multiple heterogeneous index backends.  Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers.  This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that the indexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maximum number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxbkd-circle-processor","title":"index.[X].bkd-circle-processor","text":"<p>Configuration for BKD circle processors which is used for BKD Geoshape mapping.</p> Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.class Full class name of circle processor that implements <code>CircleProcessor</code> interface. The class is used for transformation of a Circle shape to another shape when BKD mapping is used. The provided implementation class should have either a public constructor which accepts configuration as a parameter (<code>org.janusgraph.diskstorage.configuration.Configuration</code>) or a public constructor with no parameters. Usually the transforming shape is a Polygon. Following shorthands can be used:  - <code>noTransformation</code> Circle processor which is not transforming a circle, but instead keep the circle shape unchanged. This implementation may be useful in situations when the user wants to control circle transformation logic on ElasticSearch side instead of application side. For example, using ElasticSearch Circle Processor or any custom plugin.  - <code>fixedErrorDistance</code> Circle processor which transforms the provided Circle into Polygon, Box, or Point depending on the configuration provided in <code>index.bkd-circle-processor.fixed</code>. The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point.  - <code>dynamicErrorDistance</code> Circle processor which calculates error distance dynamically depending on the circle radius and the specified multiplier value. The error distance calculation formula is <code>log(radius) * multiplier</code>. Configuration for this class can be provided via <code>index.bkd-circle-processor.dynamic</code>. The processing logic is similar to ElasticSearch Circle Processor except for some edge cases when the Circle is transformed into Box or Point. String dynamicErrorDistance MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxbkd-circle-processordynamic","title":"index.[X].bkd-circle-processor.dynamic","text":"<p>Configuration for Elasticsearch dynamic circle processor which is used for BKD Geoshape mapping.</p> Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.dynamic.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case <code>false</code> is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.dynamic.error-distance-multiplier Multiplier variable for dynamic error distance calculation in the formula <code>log(radius) * multiplier</code>. Radius and error distance specified in meters. Double 2.0 MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxbkd-circle-processorfixed","title":"index.[X].bkd-circle-processor.fixed","text":"<p>Configuration for Elasticsearch fixed circle processor which is used for BKD Geoshape mapping.</p> Name Description Datatype Default Value Mutability index.[X].bkd-circle-processor.fixed.bounding-box-fallback Allows to return bounding box for the circle which cannot be converted to proper shape with the specified error distance. In case <code>false</code> is set for this configuration an exception will be thrown whenever circle cannot be converted to another shape following error distance. Boolean true MASKABLE index.[X].bkd-circle-processor.fixed.error-distance The difference between the resulting inscribed distance from center to side and the circle\u2019s radius. Specified in meters. Double 10.0 MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxelasticsearch","title":"index.[X].elasticsearch","text":"<p>Elasticsearch index configuration</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-chunk-size-limit-bytes The total size limit in bytes of a bulk request. Mutation batches in excess of this limit will be chunked to this size. If a single bulk item exceeds this limit an exception will be thrown after the smaller bulk items are submitted. Ensure that this limit is always less than or equal to the configured limit of <code>http.max_content_length</code> on the Elasticsearch servers. For more information, refer to the Elasticsearch documentation. Integer 100000000 LOCAL index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.client-keep-alive Set a keep-alive timeout (in milliseconds) Long (no default value) GLOBAL_OFFLINE index.[X].elasticsearch.connect-timeout Sets the maximum connection timeout (in milliseconds). Integer 1000 MASKABLE index.[X].elasticsearch.enable_index_names_cache Enables cache for generated index store names. It is recommended to always enable index store names cache unless you have more then 50000 indexes per index store. Boolean true MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status.  This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.retry-error-codes Comma separated list of Elasticsearch REST client ResponseException error codes to retry. E.g. \"408,429\" String[] LOCAL index.[X].elasticsearch.retry-initial-wait Sets the initial retry wait time (in milliseconds) before exponential backoff. Long 1 LOCAL index.[X].elasticsearch.retry-limit Sets the number of attempts for configured retryable error codes. Integer 0 LOCAL index.[X].elasticsearch.retry-max-wait Sets the max retry wait time (in milliseconds). Long 1000 LOCAL index.[X].elasticsearch.retry_on_conflict Specify how many times should the operation be retried when a conflict occurs. Integer 0 MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in seconds) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.setup-max-open-scroll-contexts Whether JanusGraph should setup max_open_scroll_context to maximum value for the cluster or not. Boolean true MASKABLE index.[X].elasticsearch.socket-timeout Sets the maximum socket timeout (in milliseconds). Integer 30000 MASKABLE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x  and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-mapping-for-es7 Mapping types are deprecated in ElasticSearch 7 and JanusGraph will not use mapping types by default for ElasticSearch 7 but if you want to preserve mapping types, you can setup this parameter to true. If you are updating ElasticSearch from 6 to 7 and you don't want to reindex your indexes, you may setup this parameter to true but we do recommend to reindex your indexes and don't use this parameter. Boolean false MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchcreate","title":"index.[X].elasticsearch.create","text":"<p>Settings related to index creation</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index.  This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchcreateext","title":"index.[X].elasticsearch.create.ext","text":"<p>Overrides for arbitrary settings applied at index creation. See Elasticsearch, The full list of possible setting is available at Elasticsearch index settings.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.ext.number_of_replicas The number of replicas each primary shard has Integer 1 MASKABLE index.[X].elasticsearch.create.ext.number_of_shards The number of primary shards that an index should have.Default value is 5 on ES 6 and 1 on ES 7 Integer (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchhttpauth","title":"index.[X].elasticsearch.http.auth","text":"<p>Configuration options for HTTP(S) authentication.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.type Authentication type to be used for HTTP(S) access. Available options are <code>NONE</code>, <code>BASIC</code> and <code>CUSTOM</code>. String NONE LOCAL"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchhttpauthbasic","title":"index.[X].elasticsearch.http.auth.basic","text":"<p>Configuration options for HTTP(S) Basic authentication.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.basic.password Password for HTTP(S) authentication. String LOCAL index.[X].elasticsearch.http.auth.basic.realm Realm value for HTTP(S) authentication. If empty, any realm is accepted. String LOCAL index.[X].elasticsearch.http.auth.basic.username Username for HTTP(S) authentication. String LOCAL"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchhttpauthcustom","title":"index.[X].elasticsearch.http.auth.custom","text":"<p>Configuration options for custom HTTP(S) authenticator.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.http.auth.custom.authenticator-args Comma-separated custom authenticator constructor arguments. String[] LOCAL index.[X].elasticsearch.http.auth.custom.authenticator-class Authenticator fully qualified class name. String LOCAL"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchssl","title":"index.[X].elasticsearch.ssl","text":"<p>Elasticsearch SSL configuration</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.allow-self-signed-certificates Controls the accepting of the self-signed SSL certificates. Boolean false LOCAL index.[X].elasticsearch.ssl.disable-hostname-verification Disables the SSL hostname verification if set to true. Hostname verification is enabled by default. Boolean false LOCAL index.[X].elasticsearch.ssl.enabled Controls use of the SSL connection to Elasticsearch. Boolean false LOCAL"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchsslkeystore","title":"index.[X].elasticsearch.ssl.keystore","text":"<p>Configuration options for SSL Keystore.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.keystore.keypassword The password to access the key in the SSL Keystore. If the option is not present, the value of \"storepassword\" is used. String LOCAL index.[X].elasticsearch.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL index.[X].elasticsearch.ssl.keystore.storepassword The password to access SSL Keystore. String LOCAL"},{"location":"configs/janusgraph-cfg/#indexxelasticsearchssltruststore","title":"index.[X].elasticsearch.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL index.[X].elasticsearch.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"configs/janusgraph-cfg/#indexxsolr","title":"index.[X].solr","text":"<p>Solr index configuration</p> Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be reused for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabled, the user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.kerberos-enabled Whether SOLR instance is Kerberized or not. Boolean false MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of <code>collection=field</code>. String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP (<code>http</code>) or using SolrCloud (<code>cloud</code>) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE"},{"location":"configs/janusgraph-cfg/#log","title":"log *","text":"<p>Configuration options for JanusGraph's logging system</p> Name Description Datatype Default Value Mutability log.[X].backend Define the log backend to use. A reserved shortcut <code>default</code> can be used to use graph's storage backend to manage logs. A custom log implementation can be specified by providing full class path which implements <code>org.janusgraph.diskstorage.log.LogManager</code> and accepts a single parameter <code>org.janusgraph.diskstorage.configuration.Configuration</code> in the public constructor. String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not become visible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaning that all entries added to this log expire after the configured amount of time. Requires that the log implementation supports TTL. Duration (no default value) GLOBAL"},{"location":"configs/janusgraph-cfg/#metrics","title":"metrics","text":"<p>Configuration options for metrics reporting</p> Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE"},{"location":"configs/janusgraph-cfg/#metricsconsole","title":"metrics.console","text":"<p>Configuration options for metrics reporting to console</p> Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#metricscsv","title":"metrics.csv","text":"<p>Configuration options for metrics reporting to CSV file</p> Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#metricsgraphite","title":"metrics.graphite","text":"<p>Configuration options for metrics reporting through Graphite</p> Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#metricsjmx","title":"metrics.jmx","text":"<p>Configuration options for metrics reporting through JMX</p> Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE"},{"location":"configs/janusgraph-cfg/#metricsslf4j","title":"metrics.slf4j","text":"<p>Configuration options for metrics reporting through slf4j</p> Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#query","title":"query","text":"<p>Configuration options for query processing</p> Name Description Datatype Default Value Mutability query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequent property access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties. This setting is applicable to direct vertex properties access (like <code>vertex.properties(\"foo\")</code> but not to <code>vertex.properties(\"foo\",\"bar\")</code> because the latter case is not a singular property access). This setting is not applicable to the next Gremlin steps: <code>valueMap</code>, <code>propertyMap</code>, <code>elementMap</code>, <code>properties</code>, <code>values</code> (configuration option <code>query.batch.properties-mode</code> should be used to configure their behavior).When <code>true</code> this setting overwrites <code>query.batch.has-step-mode</code> to <code>all_properties</code> unless <code>none</code> mode is used. Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing so limits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.hard-max-limit If smart-limit is disabled and no limit is given in the query, query optimizer adds a limit in light of possibly large result sets. It works in the same way as smart-limit except that hard-max-limit is usually a large number. Default value is Integer.MAX_VALUE which effectively disables this behavior. This option does not take effect when smart-limit is enabled. Integer 2147483647 MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.index-select-strategy Name of the index selection strategy or full class name. Following shorthands can be used: - <code>brute-force</code> (Try all combinations of index candidates and pick up optimal one)- <code>approximate</code> (Use greedy algorithm to pick up approximately optimal index candidate)- <code>threshold-based</code> (Use index-select-threshold to pick up either <code>approximate</code> or <code>threshold-based</code> strategy on runtime) String threshold-based MASKABLE query.index-select-threshold Threshold of deciding whether to use brute force enumeration algorithm or fast approximation algorithm for selecting suitable indexes. Selecting optimal indexes for a query is a NP-complete set cover problem. When number of suitable index candidates is no larger than threshold, JanusGraph uses brute force search with exponential time complexity to ensure the best combination of indexes is selected. Only effective when <code>threshold-based</code> index select strategy is chosen. Integer 10 MASKABLE query.optimizer-backend-access Whether the optimizer should be allowed to fire backend queries during the optimization phase. Allowing these will give the optimizer a chance to find more efficient execution plan but also increase the optimization overhead. Boolean true MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean false MASKABLE"},{"location":"configs/janusgraph-cfg/#querybatch","title":"query.batch","text":"<p>Configuration options to configure batch queries optimization behavior</p> Name Description Datatype Default Value Mutability query.batch.drop-step-mode Batching mode for <code>drop()</code> step. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all</code> - Drops all vertices in a batch.- <code>none</code> - Skips drop batching optimization. String all MASKABLE query.batch.enabled Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. If <code>false</code> then all other configuration options under <code>query.batch</code> namespace are ignored. Boolean true MASKABLE query.batch.has-step-mode Properties pre-fetching mode for <code>has</code> step. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all_properties</code> - Pre-fetch all vertex properties on any property access (fetches all vertex properties in a single slice query)- <code>required_properties_only</code> - Pre-fetch necessary vertex properties for the whole chain of foldable <code>has</code> steps (uses a separate slice query per each required property)- <code>required_and_next_properties</code> - Prefetch the same properties as with <code>required_properties_only</code> mode, but also prefetchproperties which may be needed in the next properties access step like <code>values</code>, <code>properties,</code> <code>valueMap</code>, <code>elementMap</code>, or <code>propertyMap</code>.In case the next step is not one of those properties access steps then this mode behaves same as <code>required_properties_only</code>.In case the next step is one of the properties access steps with limited scope of properties, those properties will bepre-fetched together in the same multi-query.In case the next step is one of the properties access steps with unspecified scope of property keys then this modebehaves same as <code>all_properties</code>.- <code>required_and_next_properties_or_all</code> - Prefetch the same properties as with <code>required_and_next_properties</code>, but in case the next step is not<code>values</code>, <code>properties,</code> <code>valueMap</code>, <code>elementMap</code>, or <code>propertyMap</code> then acts like <code>all_properties</code>.- <code>none</code> - Skips <code>has</code> step batch properties pre-fetch optimization. String required_and_next_properties MASKABLE query.batch.label-step-mode Labels pre-fetching mode for <code>label()</code> step. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all</code> - Pre-fetch labels for all vertices in a batch.- <code>none</code> - Skips vertex labels pre-fetching optimization. String all MASKABLE query.batch.limited Configure a maximum batch size for queries against the storage backend. This can be used to ensure responsiveness if batches tend to grow very large. The used batch size is equivalent to the barrier size of a preceding <code>barrier()</code> step. If a step has no preceding <code>barrier()</code>, the default barrier of TinkerPop will be inserted. This option only takes effect if <code>query.batch.enabled</code> is <code>true</code>. Boolean true MASKABLE query.batch.limited-size Default batch size (barrier() step size) for queries. This size is applied only for cases where <code>LazyBarrierStrategy</code> strategy didn't apply <code>barrier</code> step and where user didn't apply barrier step either. This option is used only when <code>query.batch.limited</code> is <code>true</code>. Notice, value <code>2147483647</code> is considered to be unlimited. Integer 2500 MASKABLE query.batch.properties-mode Properties pre-fetching mode for <code>values</code>, <code>properties</code>, <code>valueMap</code>, <code>propertyMap</code>, <code>elementMap</code> steps. Used only when <code>query.batch.enabled</code> is <code>true</code>.Supported modes:- <code>all_properties</code> - Pre-fetch all vertex properties on non-singular property access (fetches all vertex properties in a single slice query). On single property access this mode behaves the same as <code>required_properties_only</code> mode.- <code>required_properties_only</code> - Pre-fetch necessary vertex properties only (uses a separate slice query per each required property)- <code>none</code> - Skips vertex properties pre-fetching optimization. String required_properties_only MASKABLE query.batch.repeat-step-mode Batch mode for <code>repeat</code> step. Used only when query.batch.enabled is <code>true</code>.These modes are controlling how the child steps with batch support are behaving if they are placed to the start of the <code>repeat</code>, <code>emit</code>, or <code>until</code> traversals.Supported modes:- <code>closest_repeat_parent</code> - Child start steps are receiving vertices for batching from the closest <code>repeat</code> step parent only.- <code>all_repeat_parents</code> - Child start steps are receiving vertices for batching from all <code>repeat</code> step parents.- <code>starts_only_of_all_repeat_parents</code> - Child start steps are receiving vertices for batching from the closest <code>repeat</code> step parent (both for the parent start and for next iterations) and also from all <code>repeat</code> step parents for the parent start. String all_repeat_parents MASKABLE"},{"location":"configs/janusgraph-cfg/#schema","title":"schema","text":"<p>Schema related configuration options</p> Name Description Datatype Default Value Mutability schema.constraints Configures the schema constraints to be used by this graph. If config 'schema.constraints' is set to 'true' and 'schema.default' is set to 'none', then an 'IllegalArgumentException' is thrown for schema constraint violations. If 'schema.constraints' is set to 'true' and 'schema.default' is not set 'none', schema constraints are automatically created as described in the config option 'schema.default'. If 'schema.constraints' is set to 'false' which is the default, then no schema constraints are applied. Boolean false GLOBAL_OFFLINE schema.default Configures the DefaultSchemaMaker to be used by this graph. Either one of the following shorthands can be used:  - <code>default</code> (a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys), - <code>tp3</code> (same as default, but has LIST property keys), - <code>none</code> (automatic schema creation is disabled) - <code>ignore-prop</code> (same as none, but simply ignore unknown properties rather than throw exceptions) - or to the full package and classname of a custom/third-party implementing the interface <code>org.janusgraph.core.schema.DefaultSchemaMaker</code> String default MASKABLE schema.logging Controls whether logging is enabled for schema makers. This only takes effect if you set <code>schema.default</code> to <code>default</code> or <code>ignore-prop</code>. For <code>default</code> schema maker, warning messages will be logged before schema types are created automatically. For <code>ignore-prop</code> schema maker, warning messages will be logged before unknown properties are ignored. Boolean false MASKABLE"},{"location":"configs/janusgraph-cfg/#schemainit","title":"schema.init","text":"<p>Configuration options for schema initialization on startup.</p> Name Description Datatype Default Value Mutability schema.init.drop-before-startup Drops the entire schema with graph data before JanusGraph schema initialization. Note that the schema will be dropped regardless of the selected initialization strategy, including when <code>schema.init.strategy</code> is set to <code>none</code>. Boolean false LOCAL schema.init.strategy Specifies the strategy for schema initialization before starting JanusGraph. You must provide the full class path of a class that implements the <code>SchemaInitStrategy</code> interface and has parameterless constructor.The following shortcuts are also available:- <code>none</code> - Skips schema initialization.- <code>json</code> - Schema initialization via provided JSON file or JSON string. String none LOCAL"},{"location":"configs/janusgraph-cfg/#schemainitjson","title":"schema.init.json","text":"<p>Options for JSON schema initialization strategy.</p> Name Description Datatype Default Value Mutability schema.init.json.await-index-status-timeout Timeout for awaiting index status operation defined in milliseconds. If the status await timeouts the exception will be thrown during schema initialization process. Long 180000 LOCAL schema.init.json.file File path to JSON formated schema definition. String (no default value) LOCAL schema.init.json.force-close-other-instances Force closes other JanusGraph instances before schema initialization, regardless if they are active or not. This is a dangerous operation. This option exists to help people initialize schema who struggle with zombie JanusGraph instances. It's not recommended to be used unless you know what you are doing. Instead of this parameter, it's recommended to check <code>graph.unique-instance-id</code> and <code>graph.replace-instance-if-exists</code> options to not create zombie instances in the cluster. Boolean false LOCAL schema.init.json.indices-activation Indices activation type:- <code>reindex_and_enable_updated_only</code> - Reindex process will be triggered for any updated index. After this all updated indexes will be enabled.- <code>reindex_and_enable_non_enabled</code> - Reindex process will be triggered for any index which is not enabled (including previously created indices). After reindexing all indices will be enabled.- <code>skip_activation</code> - Skip reindex process for any updated indexes.- <code>force_enable_updated_only</code> - Force enable all updated indexes without running any reindex process (previous data may not be available for such indices).- <code>force_enable_non_enabled</code> - Force enable all indexes (including previously created indices) without running any reindex process (previous data may not be available for such indices). String reindex_and_enable_non_enabled LOCAL schema.init.json.skip-elements Skip creation of VertexLabel, EdgeLabel, and PropertyKey. Boolean false LOCAL schema.init.json.skip-indices Skip creation of indices. Boolean false LOCAL schema.init.json.string JSON formated schema definition string. This option takes precedence if both <code>file</code> and <code>string</code> are used. String (no default value) LOCAL"},{"location":"configs/janusgraph-cfg/#storage","title":"storage","text":"<p>Configuration options for the storage backend.  Some options are applicable only for certain backends.</p> Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph.  This is required.  It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cql, hbase, inmemory, scylla) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers.  This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.num-mutations-parallel-threshold This parameter determines the minimum number of mutations a transaction must have before parallel processing is applied during aggregation. Leveraging parallel processing can enhance the commit times for transactions involving a large number of mutations. However, it is advisable not to set the threshold too low (e.g., 0 or 1) due to the overhead associated with parallelism synchronization. This overhead is more efficiently offset in the context of larger transactions. Integer 100 MASKABLE storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to /. String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operation fails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE"},{"location":"configs/janusgraph-cfg/#storageberkeleyje","title":"storage.berkeleyje","text":"<p>BerkeleyDB JE configuration options</p> Name Description Datatype Default Value Mutability storage.berkeleyje.cache-mode Modes that can be specified for control over caching of records in the JE in-memory cache String DEFAULT MASKABLE storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE storage.berkeleyje.shared-cache If true, the shared cache is used for all graph instances Boolean true MASKABLE"},{"location":"configs/janusgraph-cfg/#storageberkeleyjeext","title":"storage.berkeleyje.ext","text":"<p>Overrides for arbitrary settings applied at <code>EnvironmentConfig</code> creation. The full list of possible setting is available inside the Java class <code>com.sleepycat.je.EnvironmentConfig</code>. All configurations values should be specified as <code>String</code> and be formated the same as specified in the following documentation. Notice, for compatibility reasons, it's allowed to use <code>-</code> character instead of <code>.</code> for config keys. All dashes will be replaced by dots when passing those keys to <code>EnvironmentConfig</code>.</p> Name Description Datatype Default Value Mutability storage.berkeleyje.ext.je-lock-timeout Lock timeout configuration. <code>0</code> disabled lock timeout completely. To set lock timeout via this configuration it's required to use String formated time representation. For example: <code>500 ms</code>, <code>5 min</code>, etc. See information about value constraints in the official sleepycat documentation. Notice, this option can be specified as <code>storage.berkeleyje.ext.je.lock.timeout</code> which will be treated the same as this configuration option. String (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#storagecql","title":"storage.cql","text":"<p>CQL storage backend options</p> Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.back-pressure-class The implementation of <code>QueryBackPressure</code> to use. The full name of the class which extends <code>QueryBackPressure</code> which has either a public constructor with <code>Configuration janusGraphConfiguration</code> and <code>Integer backPressureLimit</code> arguments (preferred constructor) or a public constructor with <code>Configuration janusGraphConfiguration</code> argument (second preferred constructor) or a public parameterless constructor. Other accepted options are: <code>semaphore</code> - fair semaphore based back pressure implementation of <code>back-pressure-limit</code> limit size (preferred implementation); <code>semaphoreReleaseProtected</code> - fair semaphore based back pressure implementation of <code>back-pressure-limit</code> limit size with protected releasing logic (meant to be used for testing); <code>passAll</code> - turned off back pressure (it is recommended to tune CQL driver for the ongoing workload when this implementation is used); String semaphore MASKABLE storage.cql.back-pressure-limit The maximum number of concurrent requests which are allowed to be processed by CQL driver. If no value is provided or the value is set to <code>0</code> then the value will be calculated based on CQL driver session provided parameters by using formula [advanced.connection.max-requests-per-connection * advanced.connection.pool.local.size * available_nodes_amount]. It's not recommended to use any value which is above this limit because it may result in CQL driver overload but it's suggested to have a lower value to keep the driver healthy under pressure. In situations when remote nodes connections are in use then the bigger value might be relevant as well to improve parallelism. Integer (no default value) MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.gc-grace-seconds The number of seconds before tombstones (deletion markers) are eligible for garbage-collection. Integer (no default value) FIXED storage.cql.heartbeat-interval The connection heartbeat interval in milliseconds. Long (no default value) MASKABLE storage.cql.heartbeat-timeout How long the driver waits for the response (in milliseconds) to a heartbeat. Long (no default value) MASKABLE storage.cql.init-wait-time Number of milliseconds to sleep after creating each keyspace/table, only needed if table creation is asynchronous, e.g. Amazon Keyspace Integer (no default value) LOCAL storage.cql.keyspace The name of JanusGraph's keyspace.  It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter. This value will be passed into CqlSessionBuilder.withLocalDatacenter. String datacenter1 MASKABLE storage.cql.local-max-connections-per-host The maximum number of connections that can be created per host for local datacenter Integer 1 MASKABLE storage.cql.max-requests-per-connection The maximum number of requests that can be executed concurrently on a connection. Integer 1024 MASKABLE storage.cql.metadata-schema-enabled Whether schema metadata is enabled. Boolean (no default value) MASKABLE storage.cql.metadata-token-map-enabled Whether token metadata is enabled. If disabled, partitioner-name must be provided. Boolean (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.partitioner-name The name of Cassandra cluster's partitioner. It will be retrieved by client if not provided. If provided, it must match the cluster's partitioner name. It can be the full class name such as <code>org.apache.cassandra.dht.ByteOrderedPartitioner</code> or the simple name such as <code>ByteOrderedPartitioner</code> String (no default value) MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database.  If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.remote-max-connections-per-host The maximum number of connections that can be created per host for remote datacenter Integer 1 MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept. This options is used when storage.cql.replication-strategy-class is set to SimpleStrategy Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace. Available strategies: SimpleStrategy,NetworkTopologyStrategy. String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. This options is used when storage.cql.replication-strategy-class is set to NetworkTopologyStrategy. <code>replication_factor</code> can be used to specify a replication factor. String[] (no default value) FIXED storage.cql.request-timeout Timeout for CQL requests in milliseconds. See DataStax Java Driver option <code>basic.request.timeout</code> for more information. Long 12000 MASKABLE storage.cql.session-leak-threshold The maximum number of live sessions that are allowed to coexist in a given VM until the warning starts to log for every new session. If the value is less than or equal to 0, the feature is disabled: no warning will be issued. See DataStax Java Driver option <code>advanced.session-leak.threshold</code> for more information. Integer (no default value) MASKABLE storage.cql.session-name Default name for the Cassandra session String JanusGraph Session MASKABLE storage.cql.speculative-retry The speculative retry policy. One of: NONE, ALWAYS, percentile, ms. String (no default value) FIXED storage.cql.ttl-enabled Whether TTL should be enabled or not. Must be turned off if the storage does not support TTL. Amazon Keyspace, for example, does not support TTL by default unless otherwise enabled. Boolean true LOCAL storage.cql.use-external-locking True to prevent JanusGraph from using its own locking mechanism. Setting this to true eliminates redundant checks when using an external locking mechanism outside of JanusGraph. Be aware that when use-external-locking is set to true, that failure to employ a locking algorithm which locks all columns that participate in a transaction upfront and unlocks them when the transaction ends, will result in a 'read uncommitted' transaction isolation level guarantee. If set to true without an appropriate external locking mechanism in place side effects such as dirty/non-repeatable/phantom reads should be expected. Boolean false MASKABLE storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE"},{"location":"configs/janusgraph-cfg/#storagecqlexecutor-service","title":"storage.cql.executor-service","text":"<p>Configuration options for CQL executor service which is used to process deserialization of CQL queries.</p> Name Description Datatype Default Value Mutability storage.cql.executor-service.class The implementation of <code>ExecutorService</code> to use. The full name of the class which extends <code>ExecutorService</code> which has either a public constructor with <code>ExecutorServiceConfiguration</code> argument (preferred constructor) or a public parameterless constructor. Other accepted options are: <code>fixed</code> - fixed thread pool of size <code>core-pool-size</code>; <code>cached</code> - cached thread pool; String fixed LOCAL storage.cql.executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set or set to -1 the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.cql.executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the <code>core-pool-size</code>, this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for <code>fixed</code> executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.cql.executor-service.max-pool-size Maximum pool size for executor service. Ignored for <code>fixed</code> and <code>cached</code> executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.cql.executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL"},{"location":"configs/janusgraph-cfg/#storagecqlgrouping","title":"storage.cql.grouping","text":"<p>Configuration options for controlling CQL queries grouping</p> Name Description Datatype Default Value Mutability storage.cql.grouping.keys-allowed If <code>true</code> this allows multiple partition keys to be grouped together into a single CQL query via <code>IN</code> operator based on the keys grouping strategy provided (usually grouping is done by same token-ranges or same replica sets, but may also involve shard ids for custom implementations).Notice, that any CQL query grouped with more than 1 key will require to return a row key for any column fetched.This option is useful when less amount of CQL queries is desired to be sent for read requests in expense of fetching more data (partition key per each fetched value).Notice, different storage backends may have different way of executing multi-partition <code>IN</code> queries (including, but not limited to how the checksum queries are sent for different consistency levels, processing node CPU usage, disk access pattern, etc.). Thus, a proper benchmarking is needed to determine if keys grouping is useful or not per case by case scenario.This option can be enabled only for storage backends which support <code>PER PARTITION LIMIT</code>. As such, this feature can't be used with Amazon Keyspaces because it doesn't support <code>PER PARTITION LIMIT</code>.If this option is <code>false</code> then each partition key will be executed in a separate asynchronous CQL query even when multiple keys from the same token range are queried.Notice, the default grouping strategy does not take shards into account. Thus, this might be inefficient with ScyllaDB storage backend. ScyllaDB specific keys grouping strategy should be implemented after the resolution of the ticket #232. Boolean false MASKABLE storage.cql.grouping.keys-class Full class path of the keys grouping execution strategy. The class should implement <code>org.janusgraph.diskstorage.cql.strategy.GroupedExecutionStrategy</code> interface and have a public constructor with two arguments <code>org.janusgraph.diskstorage.configuration.Configuration</code> and <code>org.janusgraph.diskstorage.cql.CQLStoreManager</code>.Shortcuts available:- <code>tokenRangeAware</code> - groups partition keys which belong to the same token range. Notice, this strategy does not take shards into account. Thus, this might be inefficient with ScyllaDB storage backend.- <code>replicasAware</code> - groups partition keys which belong to the same replica sets (same nodes). Notice, this strategy does not take shards into account. Thus, this might be inefficient with ScyllaDB storage backend.Usually <code>tokenRangeAware</code> grouping strategy provides more smaller groups where each group contain keys which are stored close to each other on a disk and may cause less disk seeks in some cases. However <code>replicasAware</code> grouping strategy groups keys per replica set which usually means fewer bigger groups to be used (i.e. less CQL requests).This option takes effect only when <code>storage.cql.grouping.keys-allowed</code> is <code>true</code>. String replicasAware MASKABLE storage.cql.grouping.keys-limit Maximum amount of the keys which can be grouped together into a single CQL query. If more keys are queried, they are going to be grouped into separate CQL queries.Notice, for ScyllaDB this option should not exceed the maximum number of distinct clustering key restrictions per query which can be changed by ScyllaDB configuration option <code>max-partition-key-restrictions-per-query</code> (https://enterprise.docs.scylladb.com/branch-2022.2/faq.html#how-can-i-change-the-maximum-number-of-in-restrictions). For AstraDB this limit is set to 20 and usually it's fixed. However, you can ask customer support for a possibility to change the default threshold to your desired configuration via <code>partition_keys_in_select_failure_threshold</code> and <code>in_select_cartesian_product_failure_threshold</code> threshold configurations (https://docs.datastax.com/en/astra-serverless/docs/plan/planning.html#_cassandra_yaml).Ensure that your storage backend allows more IN selectors than the one set via this configuration.This option takes effect only when <code>storage.cql.grouping.keys-allowed</code> is <code>true</code>. Integer 20 MASKABLE storage.cql.grouping.keys-min Minimum amount of keys to consider for grouping. Grouping will be skipped for any multi-key query which has less than this amount of keys (i.e. a separate CQL query will be executed for each key in such case).Usually this configuration should always be set to <code>2</code>. It is useful to increase the value only in cases when queries with more keys should not be grouped, but be performed separately to increase parallelism in expense of the network overhead.This option takes effect only when <code>storage.cql.grouping.keys-allowed</code> is <code>true</code>. Integer 2 MASKABLE storage.cql.grouping.slice-allowed If <code>true</code> this allows multiple Slice queries which are allowed to be performed as non-range queries (i.e. direct equality operation) to be grouped together into a single CQL query via <code>IN</code> operator. Notice, currently only operations to fetch properties with Cardinality.SINGLE are allowed to be performed as non-range queries (edges fetching or properties with Cardinality SET or LIST won't be grouped together).If this option is <code>false</code> then each Slice query will be executed in a separate asynchronous CQL query even when grouping is allowed. Boolean true MASKABLE storage.cql.grouping.slice-limit Maximum amount of grouped together slice queries into a single CQL query.Notice, for ScyllaDB this option should not exceed the maximum number of distinct clustering key restrictions per query which can be changed by ScyllaDB configuration option <code>max-partition-key-restrictions-per-query</code> (https://enterprise.docs.scylladb.com/branch-2022.2/faq.html#how-can-i-change-the-maximum-number-of-in-restrictions). For AstraDB this limit is set to 20 and usually it's fixed. However, you can ask customer support for a possibility to change the default threshold to your desired configuration via <code>partition_keys_in_select_failure_threshold</code> and <code>in_select_cartesian_product_failure_threshold</code> threshold configurations (https://docs.datastax.com/en/astra-serverless/docs/plan/planning.html#_cassandra_yaml).Ensure that your storage backend allows more IN selectors than the one set via this configuration.This option is used only when <code>storage.cql.grouping.slice-allowed</code> is <code>true</code>. Integer 20 MASKABLE"},{"location":"configs/janusgraph-cfg/#storagecqlinternal","title":"storage.cql.internal","text":"<p>Advanced configuration of internal DataStax driver. Notice, all available configurations will be composed in the order. Non specified configurations will be skipped. By default only base configuration is enabled (which has the smallest priority. It means that you can overwrite any configuration used in base programmatic configuration by using any other configuration type). The configurations are composed in the next order (sorted by priority in descending order): <code>file-configuration</code>, <code>resource-configuration</code>, <code>string-configuration</code>, <code>url-configuration</code>, <code>base-programmatic-configuration</code> (which is controlled by <code>base-programmatic-configuration-enabled</code> property). Configurations with higher priority always overwrite configurations with lower priority. I.e. if the same configuration parameter is used in both <code>file-configuration</code> and <code>string-configuration</code> the configuration parameter from <code>file-configuration</code> will be used and configuration parameter from <code>string-configuration</code> will be ignored. See available configuration options and configurations structure here: https://docs.datastax.com/en/developer/java-driver/4.13/manual/core/configuration/reference/</p> Name Description Datatype Default Value Mutability storage.cql.internal.base-programmatic-configuration-enabled Whether to use main programmatic configuration provided by JanusGraph properties or not. We don't recommend to disable this property unless you want to disable usage of all storage.cql properties and use default configurations or other configurations. If programmatic configuration options miss some important configuration options you can provide those missing configurations with other configuration types which will be applied with programmatic configuration (see other configuration types in this section). For most use cases this option should always be <code>true</code>. JanusGraph behaviour might be unpredictable when using unspecified configuration options. Boolean true MASKABLE storage.cql.internal.file-configuration Path to file with DataStax configuration. String (no default value) LOCAL storage.cql.internal.resource-configuration Classpath resource with DataStax configuration. String (no default value) MASKABLE storage.cql.internal.string-configuration String representing DataStax configuration. String (no default value) MASKABLE storage.cql.internal.url-configuration Url where to get DataStax configuration. String (no default value) MASKABLE"},{"location":"configs/janusgraph-cfg/#storagecqlmetrics","title":"storage.cql.metrics","text":"<p>Configuration options for CQL metrics</p> Name Description Datatype Default Value Mutability storage.cql.metrics.cql-messages-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option <code>advanced.metrics.node.cql-messages.refresh-interval</code> Long (no default value) LOCAL storage.cql.metrics.cql-messages-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option <code>advanced.metrics.node.cql-messages.highest-latency</code> Long (no default value) LOCAL storage.cql.metrics.cql-messages-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-messages' node metric is enabled. See DataStax driver configuration option <code>advanced.metrics.node.cql-messages.significant-digits</code> Integer (no default value) LOCAL storage.cql.metrics.cql-requests-highest-latency The largest latency that we expect to record for requests in milliseconds. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.cql-requests.highest-latency</code> Long (no default value) LOCAL storage.cql.metrics.cql-requests-refresh-interval The interval at which percentile data is refreshed in milliseconds for requests. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.cql-requests.refresh-interval</code> Long (no default value) LOCAL storage.cql.metrics.cql-requests-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for requests. This must be between 0 and 5. Used if 'cql-requests' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.cql-requests.significant-digits</code> Integer (no default value) LOCAL storage.cql.metrics.node-enabled Comma separated list of enabled node metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: pool.open-connections, pool.available-streams, bytes-sent). String[] (no default value) LOCAL storage.cql.metrics.node-expire-after The time after which the node level metrics will be evicted in milliseconds. Long (no default value) LOCAL storage.cql.metrics.session-enabled Comma separated list of enabled session metrics. Used only when basic metrics are enabled. Check DataStax Cassandra Driver 4 documentation for available metrics (example: bytes-sent, bytes-received, connected-nodes). String[] (no default value) LOCAL storage.cql.metrics.throttling-delay-highest-latency The largest latency that we expect to record for throttling in milliseconds. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.throttling.delay.highest-latency</code> Long (no default value) LOCAL storage.cql.metrics.throttling-delay-refresh-interval The interval at which percentile data is refreshed in milliseconds for throttling. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.throttling.delay.refresh-interval</code> Long (no default value) LOCAL storage.cql.metrics.throttling-delay-significant-digits The number of significant decimal digits to which internal structures will maintain value resolution and separation for throttling. This must be between 0 and 5. Used if 'throttling.delay' session metric is enabled. See DataStax driver configuration option <code>advanced.metrics.session.throttling.delay.significant-digits</code> Integer (no default value) LOCAL"},{"location":"configs/janusgraph-cfg/#storagecqlnetty","title":"storage.cql.netty","text":"<p>Configuration options related to the Netty event loop groups used internally by the CQL driver.</p> Name Description Datatype Default Value Mutability storage.cql.netty.admin-size The number of threads for the event loop group used for admin tasks not related to request I/O (handle cluster events, refresh metadata, schedule reconnections, etc.). If this is not set, the driver will use 2. Integer (no default value) LOCAL storage.cql.netty.io-size The number of threads for the event loop group used for I/O operations (reading and writing to Cassandra nodes). If this is not set, the driver will use <code>Runtime.getRuntime().availableProcessors() * 2</code>. Integer (no default value) LOCAL storage.cql.netty.timer-tick-duration The timer tick duration in milliseconds. This is how frequent the timer should wake up to check for timed-out tasks or speculative executions. See DataStax Java Driver option <code>advanced.netty.timer.tick-duration</code> for more information. Long (no default value) LOCAL storage.cql.netty.timer-ticks-per-wheel Number of ticks in a Timer wheel. See DataStax Java Driver option <code>advanced.netty.timer.ticks-per-wheel</code> for more information. Integer (no default value) LOCAL"},{"location":"configs/janusgraph-cfg/#storagecqlrequest-tracker","title":"storage.cql.request-tracker","text":"<p>Configuration options for CQL request tracker and builtin request logger</p> Name Description Datatype Default Value Mutability storage.cql.request-tracker.class It is either a predefined DataStax driver value for a builtin request tracker or a full qualified class name which implements <code>com.datastax.oss.driver.internal.core.tracker.RequestTracker</code> interface. If no any value provided, the default DataStax request tracker is used, which is <code>NoopRequestTracker</code> which doesn't do anything. If <code>RequestLogger</code> value is provided, the DataStax RequestLogger is used. String (no default value) LOCAL storage.cql.request-tracker.logs-error-enabled Whether to log failed requests.Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-max-query-length The maximum length of the query string in the log message. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-value-length The maximum length for bound values in the log message. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Integer (no default value) LOCAL storage.cql.request-tracker.logs-max-values The maximum number of bound values to log. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Integer (no default value) LOCAL storage.cql.request-tracker.logs-show-stack-traces Whether to log stack traces for failed queries. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-show-values Whether to log bound values in addition to the query string. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-enabled Whether to log <code>slow</code> requests.Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL storage.cql.request-tracker.logs-slow-threshold The threshold to classify a successful request as <code>slow</code>. In milliseconds. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Long (no default value) LOCAL storage.cql.request-tracker.logs-success-enabled Whether to log successful requests. Can be used when <code>root.storage.cql.request-tracker.class</code> is set to <code>RequestLogger</code>. Boolean (no default value) LOCAL"},{"location":"configs/janusgraph-cfg/#storagecqlssl","title":"storage.cql.ssl","text":"<p>Configuration options for SSL</p> Name Description Datatype Default Value Mutability storage.cql.ssl.client-authentication-enabled Enables use of a client key to authenticate with Cassandra Boolean false LOCAL storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL storage.cql.ssl.hostname_validation Enable / disable SSL hostname validation. Boolean false LOCAL"},{"location":"configs/janusgraph-cfg/#storagecqlsslkeystore","title":"storage.cql.ssl.keystore","text":"<p>Configuration options for SSL Keystore.</p> Name Description Datatype Default Value Mutability storage.cql.ssl.keystore.keypassword The password to access the key in SSL Keystore. String LOCAL storage.cql.ssl.keystore.location Marks the location of the SSL Keystore. String LOCAL storage.cql.ssl.keystore.storepassword The password to access the SSL Keystore. String LOCAL"},{"location":"configs/janusgraph-cfg/#storagecqlssltruststore","title":"storage.cql.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"configs/janusgraph-cfg/#storagehbase","title":"storage.hbase","text":"<p>HBase storage options</p> Name Description Datatype Default Value Mutability storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster.  JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances.  This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.snapshot-name The name of an existing HBase snapshot to be used by HBaseSnapshotInputFormat String janusgraph-snapshot LOCAL storage.hbase.snapshot-restore-dir The temporary directory to be used by HBaseSnapshotInputFormat to restore a snapshot. This directory should be on the same File System as the HBase root dir. String /tmp LOCAL storage.hbase.table The name of the table JanusGraph will use.  When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL"},{"location":"configs/janusgraph-cfg/#storagelock","title":"storage.lock","text":"<p>Options for locking on eventually-consistent stores</p> Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend.  JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code.  JanusGraph generates an appropriate default value for this option at startup.  Overriding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Although this value is maskable, it is highly recommended to use the same value across JanusGraph instances in production environments. Duration 100 ms MASKABLE"},{"location":"configs/janusgraph-cfg/#storagemeta","title":"storage.meta *","text":"<p>Meta data to include in storage backend retrievals</p> Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps. If enabled, timestamp can be retrieved by <code>element.value(ImplicitKey.TIMESTAMP.name())</code> or equivalently, <code>element.value(\"~timestamp\")</code>. Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL. If enabled, ttl can be retrieved by <code>element.value(ImplicitKey.TTL.name())</code> or equivalently, <code>element.value(\"~ttl\")</code>. Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility. If enabled, visibility can be retrieved by <code>element.value(ImplicitKey.VISIBILITY.name())</code> or equivalently, <code>element.value(\"~visibility\")</code>. Boolean true GLOBAL"},{"location":"configs/janusgraph-cfg/#storageparallel-backend-executor-service","title":"storage.parallel-backend-executor-service","text":"<p>Configuration options for executor service which is used for parallel requests when <code>storage.parallel-backend-ops</code> is enabled.</p> Name Description Datatype Default Value Mutability storage.parallel-backend-executor-service.class The implementation of <code>ExecutorService</code> to use. The full name of the class which extends <code>ExecutorService</code> which has either a public constructor with <code>ExecutorServiceConfiguration</code> argument (preferred constructor) or a public parameterless constructor. Other accepted options are: <code>fixed</code> - fixed thread pool of size <code>core-pool-size</code>; <code>cached</code> - cached thread pool; String fixed LOCAL storage.parallel-backend-executor-service.core-pool-size Core pool size for executor service. May be ignored if custom executor service is used (depending on the implementation of the executor service).If not set or set to -1 the core pool size will be equal to number of processors multiplied by 2. Integer (no default value) LOCAL storage.parallel-backend-executor-service.keep-alive-time Keep alive time in milliseconds for executor service. When the number of threads is greater than the <code>core-pool-size</code>, this is the maximum time that excess idle threads will wait for new tasks before terminating. Ignored for <code>fixed</code> executor service and may be ignored if custom executor service is used (depending on the implementation of the executor service). Long 60000 LOCAL storage.parallel-backend-executor-service.max-pool-size Maximum pool size for executor service. Ignored for <code>fixed</code> and <code>cached</code> executor services. May be ignored if custom executor service is used (depending on the implementation of the executor service). Integer 2147483647 LOCAL storage.parallel-backend-executor-service.max-shutdown-wait-time Max shutdown wait time in milliseconds for executor service threads to be finished during shutdown. After this time threads will be interrupted (signalled with interrupt) without any additional wait time. Long 60000 LOCAL"},{"location":"configs/janusgraph-cfg/#tx","title":"tx","text":"<p>Configuration options for transaction handling</p> Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL"},{"location":"configs/janusgraph-cfg/#txrecovery","title":"tx.recovery","text":"<p>Configuration options for transaction recovery processes</p> Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE"},{"location":"getting-started/architecture/","title":"Architectural Overview","text":"<p>JanusGraph is a graph database engine. JanusGraph itself is focused on compact graph serialization, rich graph data modeling, and efficient query execution. In addition, JanusGraph utilizes Hadoop for graph analytics and batch graph processing. JanusGraph implements robust, modular interfaces for data persistence, data indexing, and client access. JanusGraph\u2019s modular architecture allows it to interoperate with a wide range of storage, index, and client technologies; it also eases the process of extending JanusGraph to support new ones.</p> <p>Between JanusGraph and the disks sits one or more storage and indexing adapters. JanusGraph comes standard with the following adapters, but JanusGraph\u2019s modular architecture supports third-party adapters.</p> <ul> <li>Data storage:<ul> <li>Apache Cassandra</li> <li>Apache HBase</li> <li>Oracle Berkeley DB Java Edition</li> </ul> </li> <li>Indices, which speed up and enable more complex queries:<ul> <li>Elasticsearch</li> <li>Apache Solr</li> <li>Apache Lucene</li> </ul> </li> </ul> <p>Broadly speaking, applications can interact with JanusGraph in two ways:</p> <ul> <li> <p>Embed JanusGraph inside the application executing     Gremlin     queries directly against the graph within the same JVM. Query     execution, JanusGraph\u2019s caches, and transaction handling all happen     in the same JVM as the application while data retrieval from the     storage backend may be local or remote.</p> </li> <li> <p>Interact with a local or remote JanusGraph instance by submitting     Gremlin queries to the server. JanusGraph natively supports the     Gremlin Server component of the Apache TinkerPop stack.</p> </li> </ul> <p></p>"},{"location":"getting-started/basic-usage/","title":"Basic Usage","text":"<p>This section offers a very short introduction to Gremlin's feature set. For a closer look at the topic, refer to Gremlin Query Language.</p> <p>The examples in this section make extensive use of a toy graph distributed with JanusGraph called The Graph of the Gods. This graph is diagrammed below. The abstract data model is known as a  Property Graph Model and this particular instance describes the relationships between the beings and places of the Roman pantheon. Moreover, special text and symbol modifiers in the diagram (e.g. bold, underline, etc.) denote different schematics/typings in the graph.</p> <p></p> visual symbol meaning bold key a graph indexed key bold key with star a graph indexed key that must have a unique value underlined key a vertex-centric indexed key hollow-head edge a functional/unique edge (no duplicates) tail-crossed edge a unidirectional edge (can only traverse in one direction)"},{"location":"getting-started/basic-usage/#loading-the-graph-of-the-gods-into-janusgraph","title":"Loading the Graph of the Gods Into JanusGraph","text":"<p>The example below will open a JanusGraph graph instance and load The Graph of the Gods dataset diagrammed above. <code>JanusGraphFactory</code> provides a set of static <code>open</code> methods, each of which takes a configuration as its argument and returns a graph instance. This tutorial demonstrates loading The Graph of the Gods using the helper class <code>GraphOfTheGodsFactory</code> with different configurations. This section skips over the configuration details, but additional information about storage backends, index backends, and their configuration are available in Storage Backends, Index Backends, and Configuration Reference.</p>"},{"location":"getting-started/basic-usage/#loading-with-an-index-backend","title":"Loading with an index backend","text":"<p>The below example calls one of these <code>open</code> methods on a configuration that uses the BerkeleyDB storage backend and the Elasticsearch index backend:</p> <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-berkeleyje-es.properties')\n==&gt;standardjanusgraph[berkeleyje:../db/berkeley]\ngremlin&gt; GraphOfTheGodsFactory.load(graph)\n==&gt;null\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardjanusgraph[berkeleyje:../db/berkeley], standard]\n</code></pre> <p>The <code>JanusGraphFactory.open() and GraphOfTheGodsFactory.load()</code> methods do the following to the newly constructed graph prior to returning it:</p> <ol> <li>Creates a collection of global and vertex-centric indices on the graph.</li> <li>Adds all the vertices to the graph along with their properties.</li> <li>Adds all the edges to the graph along with their properties.</li> </ol> <p>Please see the GraphOfTheGodsFactory source code for details.</p> <p>For those using JanusGraph/Cassandra (or JanusGraph/HBase), be sure to make use of <code>conf/janusgraph-cql-es.properties</code> (or <code>conf/janusgraph-hbase-es.properties</code>) and <code>GraphOfTheGodsFactory.load()</code>.</p> <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; GraphOfTheGodsFactory.load(graph)\n==&gt;null\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard]\n</code></pre>"},{"location":"getting-started/basic-usage/#loading-without-an-index-backend","title":"Loading without an index backend","text":"<p>You may also use the <code>conf/janusgraph-cql.properties</code>, <code>conf/janusgraph-berkeleyje.properties</code>, <code>conf/janusgraph-hbase.properties</code>, or <code>conf/janusgraph-inmemory.properties</code> configuration files to open a graph without an indexing backend configured. In such cases, you will need to use the <code>GraphOfTheGodsFactory.loadWithoutMixedIndex()</code> method to load the Graph of the Gods so that it doesn\u2019t attempt to make use of an indexing backend. <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql.properties')\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; GraphOfTheGodsFactory.loadWithoutMixedIndex(graph, true)\n==&gt;null\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard]\n</code></pre></p> <p>Info</p> <p>Using any configuration file other than <code>conf/janusgraph-inmemory.properties</code> requires that you have a dedicated backend configured and running. If you just want to quickly open a graph instance and explore some of the JanusGraph features, you could simply choose <code>conf/janusgraph-inmemory.properties</code> to open an in-memory backend.</p>"},{"location":"getting-started/basic-usage/#global-graph-indices","title":"Global Graph Indices","text":"<p>The typical pattern for accessing data in a graph database is to first locate the entry point into the graph using a graph index. That entry point is an element (or set of elements)\u2009\u2014\u2009i.e. a vertex or edge. From the entry elements, a Gremlin path description describes how to traverse to other elements in the graph via the explicit graph structure.</p> <p>Given that there is a unique index on <code>name</code> property, the Saturn vertex can be retrieved. The property map (i.e. the key/value pairs of Saturn) can then be examined. As demonstrated, the Saturn vertex has a <code>name</code> of \"saturn, \" an <code>age</code> of 10000, and a <code>type</code> of \"titan.\" The grandchild of Saturn can be retrieved with a traversal that expresses: \"Who is Saturn\u2019s grandchild?\" (the inverse of \"father\" is \"child\"). The result is Hercules.</p> <pre><code>gremlin&gt; saturn = g.V().has('name', 'saturn').next()\n==&gt;v[256]\ngremlin&gt; g.V(saturn).valueMap()\n==&gt;[name:[saturn], age:[10000]]\ngremlin&gt; g.V(saturn).in('father').in('father').values('name')\n==&gt;hercules\n</code></pre> <p>The property <code>place</code> is also in a graph index. The property <code>place</code> is an edge property. Therefore, JanusGraph can index edges in a graph index. It is possible to query The Graph of the Gods for all events that have happened within 50 kilometers of Athens (latitude:37.97 and long:23.72). Then, given that information, which vertices were involved in those events.</p> <p><pre><code>gremlin&gt; g.E().has('place', geoWithin(Geoshape.circle(37.97, 23.72, 50)))\n==&gt;e[a9x-co8-9hx-39s][16424-battled-&gt;4240]\n==&gt;e[9vp-co8-9hx-9ns][16424-battled-&gt;12520]\ngremlin&gt; g.E().has('place', geoWithin(Geoshape.circle(37.97, 23.72, 50))).as('source').inV().as('god2').select('source').outV().as('god1').select('god1', 'god2').by('name')\n==&gt;[god1:hercules, god2:hydra]\n==&gt;[god1:hercules, god2:nemean]\n</code></pre> Graph indices are one type of index structure in JanusGraph. Graph indices are automatically chosen by JanusGraph to answer which ask for all vertices (<code>g.V</code>) or all edges (<code>g.E</code>) that satisfy one or multiple constraints (e.g. <code>has</code> or <code>interval</code>). The second aspect of indexing in JanusGraph is known as vertex-centric indices. Vertex-centric indices are utilized to speed up traversals inside the graph. Vertex-centric indices are described later.</p>"},{"location":"getting-started/basic-usage/#graph-traversal-examples","title":"Graph Traversal Examples","text":"<p>Hercules, son of Jupiter and Alcmene, bore super human strength. Hercules was a Demigod because his father was a god and his mother was a human. Juno, wife of Jupiter, was furious with Jupiter\u2019s infidelity. In revenge, she blinded Hercules with temporary insanity and caused him to kill his wife and children. To atone for the slaying, Hercules was ordered by the Oracle of Delphi to serve Eurystheus. Eurystheus appointed Hercules to 12 labors.</p> <p>In the previous section, it was demonstrated that Saturn\u2019s grandchild was Hercules. This can be expressed using a <code>loop</code>. In essence, Hercules is the vertex that is 2-steps away from Saturn along the <code>in('father')</code> path. <pre><code>gremlin&gt; hercules = g.V(saturn).repeat(__.in('father')).times(2).next()\n==&gt;v[1536]\n</code></pre></p> <p>Hercules is a demigod. To prove that Hercules is half human and half god, his parent\u2019s origins must be examined. It is possible to traverse from the Hercules vertex to his mother and father. Finally, it is possible to determine the <code>type</code> of each of them\u2009\u2014\u2009yielding \"god\" and \"human.\" <pre><code>gremlin&gt; g.V(hercules).out('father', 'mother')\n==&gt;v[1024]\n==&gt;v[1792]\ngremlin&gt; g.V(hercules).out('father', 'mother').values('name')\n==&gt;jupiter\n==&gt;alcmene\ngremlin&gt; g.V(hercules).out('father', 'mother').label()\n==&gt;god\n==&gt;human\ngremlin&gt; hercules.label()\n==&gt;demigod\n</code></pre></p> <p>The examples thus far have been with respect to the genetic lines of the various actors in the Roman pantheon. The Property Graph Model is expressive enough to represent multiple types of things and relationships. In this way, The Graph of the Gods also identifies Hercules' various heroic exploits --- his famous 12 labors. In the previous section, it was discovered that Hercules was involved in two battles near Athens. It is possible to explore these events by traversing <code>battled</code> edges out of the Hercules vertex. <pre><code>gremlin&gt; g.V(hercules).out('battled')\n==&gt;v[2304]\n==&gt;v[2560]\n==&gt;v[2816]\ngremlin&gt; g.V(hercules).out('battled').valueMap()\n==&gt;[name:[nemean]]\n==&gt;[name:[hydra]]\n==&gt;[name:[cerberus]]\ngremlin&gt; g.V(hercules).outE('battled').has('time', gt(1)).inV().values('name')\n==&gt;cerberus\n==&gt;hydra\n</code></pre></p> <p>The edge property <code>time</code> on <code>battled</code> edges is indexed by the vertex-centric indices of a vertex. Retrieving <code>battled</code> edges incident to Hercules according to a constraint/filter on <code>time</code> is faster than doing a linear scan of all edges and filtering (typically <code>O(log n)</code>, where <code>n</code> is the number incident edges). JanusGraph is intelligent enough to use vertex-centric indices when available. A <code>toString()</code> of a Gremlin expression shows a decomposition into individual steps.</p> <pre><code>gremlin&gt; g.V(hercules).outE('battled').has('time', gt(1)).inV().values('name').toString()\n==&gt;[GraphStep([v[24744]],vertex), VertexStep(OUT,[battled],edge), HasStep([time.gt(1)]), EdgeVertexStep(IN), PropertiesStep([name],value)]\n</code></pre>"},{"location":"getting-started/basic-usage/#more-complex-graph-traversal-examples","title":"More Complex Graph Traversal Examples","text":"<p>In the depths of Tartarus lives Pluto. His relationship with Hercules was strained by the fact that Hercules battled his pet, Cerberus. However, Hercules is his nephew\u2009\u2014\u2009how should he make Hercules pay for his insolence?</p> <p>The Gremlin traversals below provide more examples over The Graph of the Gods. The explanation of each traversal is provided in the prior line as a <code>//</code> comment.</p>"},{"location":"getting-started/basic-usage/#cohabitors-of-tartarus","title":"Cohabitors of Tartarus","text":"<pre><code>gremlin&gt; pluto = g.V().has('name', 'pluto').next()\n==&gt;v[2048]\ngremlin&gt; // who are pluto's cohabitants?\ngremlin&gt; g.V(pluto).out('lives').in('lives').values('name')\n==&gt;pluto\n==&gt;cerberus\ngremlin&gt; // pluto can't be his own cohabitant\ngremlin&gt; g.V(pluto).out('lives').in('lives').where(is(neq(pluto))).values('name')\n==&gt;cerberus\ngremlin&gt; g.V(pluto).as('x').out('lives').in('lives').where(neq('x')).values('name')\n==&gt;cerberus\n</code></pre>"},{"location":"getting-started/basic-usage/#plutos-brothers","title":"Pluto\u2019s Brothers","text":"<pre><code>gremlin&gt; // where do pluto's brothers live?\ngremlin&gt; g.V(pluto).out('brother').out('lives').values('name')\n==&gt;sky\n==&gt;sea\ngremlin&gt; // which brother lives in which place?\ngremlin&gt; g.V(pluto).out('brother').as('god').out('lives').as('place').select('god', 'place')\n==&gt;[god:v[1024], place:v[512]]\n==&gt;[god:v[1280], place:v[768]]\ngremlin&gt; // what is the name of the brother and the name of the place?\ngremlin&gt; g.V(pluto).out('brother').as('god').out('lives').as('place').select('god', 'place').by('name')\n==&gt;[god:jupiter, place:sky]\n==&gt;[god:neptune, place:sea]\n</code></pre> <p>Finally, Pluto lives in Tartarus because he shows no concern for death. His brothers, on the other hand, chose their locations based upon their love for certain qualities of those locations.! <pre><code>gremlin&gt; g.V(pluto).outE('lives').values('reason')\n==&gt;no fear of death\ngremlin&gt; g.E().has('reason', textContains('loves'))\n==&gt;e[6xs-sg-m51-e8][1024-lives-&gt;512]\n==&gt;e[70g-zk-m51-lc][1280-lives-&gt;768]\ngremlin&gt; g.E().has('reason', textContains('loves')).as('source').values('reason').as('reason').select('source').outV().values('name').as('god').select('source').inV().values('name').as('thing').select('god', 'reason', 'thing')\n==&gt;[god:neptune, reason:loves waves, thing:sea]\n==&gt;[god:jupiter, reason:loves fresh breezes, thing:sky]\n</code></pre></p>"},{"location":"getting-started/gremlin/","title":"Gremlin Query Language","text":"<p>Gremlin is JanusGraph\u2019s query language used to retrieve data from and modify data in the graph. Gremlin is a path-oriented language which succinctly expresses complex graph traversals and mutation operations. Gremlin is a functional language whereby traversal operators are chained together to form path-like expressions. For example, \"from Hercules, traverse to his father and then his father\u2019s father and return the grandfather\u2019s name.\"</p> <p>Gremlin is a component of Apache TinkerPop. It is developed independently from JanusGraph and is supported by most graph databases. By building applications on top of JanusGraph through the Gremlin query language, users avoid vendor-lock in because their application can be migrated to other graph databases supporting Gremlin.</p> <p>This section is a brief overview of the Gremlin query language. For more information on Gremlin, refer to the following resources:</p> <ul> <li> <p>Practical Gremlin: An online book by Kelvin R. Lawrence providing an in-depth overview of Gremlin and it's interaction with JanusGraph.</p> </li> <li> <p>Complete Gremlin Manual: Reference manual for all of the Gremlin steps.</p> </li> <li> <p>Gremlin Console Tutorial: Learn how to use the Gremlin Console effectively to traverse and analyze a graph interactively.</p> </li> <li> <p>Gremlin Recipes: A collection of best practices and common traversal patterns for Gremlin.</p> </li> <li> <p>Gremlin Language Drivers:     Connect to a Gremlin Server with different programming languages,     including Go, JavaScript, .NET/C#, PHP, Python, Ruby, Scala, and     TypeScript.</p> </li> <li> <p>Gremlin Language Variants: Learn how to embed Gremlin in a host programming language.</p> </li> <li> <p>Gremlin for SQL developers: Learn Gremlin     using typical patterns found when querying data with SQL.</p> </li> </ul> <p>In addition to these resources, Connecting to JanusGraph explains how Gremlin can be used in different programming languages to query a JanusGraph Server.</p>"},{"location":"getting-started/gremlin/#introductory-traversals","title":"Introductory Traversals","text":"<p>A Gremlin query is a chain of operations/functions that are evaluated from left to right. A simple grandfather query is provided below over the Graph of the Gods dataset discussed in Getting Started. <pre><code>gremlin&gt; g.V().has('name', 'hercules').out('father').out('father').values('name')\n==&gt;saturn\n</code></pre></p> <p>The query above can be read:</p> <ol> <li><code>g</code>: for the current graph traversal.</li> <li><code>V</code>: for all vertices in the graph</li> <li><code>has('name', 'hercules')</code>: filters the vertices down to those with name property \"hercules\" (there is only one).</li> <li><code>out('father')</code>: traverse outgoing father edge\u2019s from Hercules.</li> <li>\u2018out('father')`: traverse outgoing father edge\u2019s from Hercules\u2019 father\u2019s vertex (i.e. Jupiter).</li> <li><code>name</code>: get the name property of the \"hercules\" vertex\u2019s grandfather.</li> </ol> <p>Taken together, these steps form a path-like traversal query. Each step can be decomposed and its results demonstrated. This style of building up a traversal/query is useful when constructing larger, complex query chains.</p> <pre><code>gremlin&gt; g\n==&gt;graphtraversalsource[janusgraph[cql:127.0.0.1], standard]\ngremlin&gt; g.V().has('name', 'hercules')\n==&gt;v[24]\ngremlin&gt; g.V().has('name', 'hercules').out('father')\n==&gt;v[16]\ngremlin&gt; g.V().has('name', 'hercules').out('father').out('father')\n==&gt;v[20]\ngremlin&gt; g.V().has('name', 'hercules').out('father').out('father').values('name')\n==&gt;saturn\n</code></pre> <p>For a sanity check, it is usually good to look at the properties of each return, not the assigned long id. <pre><code>gremlin&gt; g.V().has('name', 'hercules').values('name')\n==&gt;hercules\ngremlin&gt; g.V().has('name', 'hercules').out('father').values('name')\n==&gt;jupiter\ngremlin&gt; g.V().has('name', 'hercules').out('father').out('father').values('name')\n==&gt;saturn\n</code></pre></p> <p>Note the related traversal that shows the entire father family tree branch of Hercules. This more complicated traversal is provided in order to demonstrate the flexibility and expressivity of the language. A competent grasp of Gremlin provides the JanusGraph user the ability to fluently navigate the underlying graph structure. <pre><code>gremlin&gt; g.V().has('name', 'hercules').repeat(out('father')).emit().values('name')\n==&gt;jupiter\n==&gt;saturn\n</code></pre></p> <p>Some more traversal examples are provided below. <pre><code>gremlin&gt; hercules = g.V().has('name', 'hercules').next()\n==&gt;v[1536]\ngremlin&gt; g.V(hercules).out('father', 'mother').label()\n==&gt;god\n==&gt;human\ngremlin&gt; g.V(hercules).out('battled').label()\n==&gt;monster\n==&gt;monster\n==&gt;monster\ngremlin&gt; g.V(hercules).out('battled').valueMap()\n==&gt;{name=nemean}\n==&gt;{name=hydra}\n==&gt;{name=cerberus}\n</code></pre> Each step (denoted by a separating .) is a function that operates on the objects emitted from the previous step. There are numerous steps in the Gremlin language (see Gremlin Steps). By simply changing a step or order of the steps, different traversal semantics are enacted. The example below returns the name of all the people that have battled the same monsters as Hercules who themselves are not Hercules (i.e. \"co-battlers\" or perhaps, \"allies\").</p> <p>Given that The Graph of the Gods only has one battler (Hercules), another battler (for the sake of example) is added to the graph with Gremlin showcasing how vertices and edges are added to the graph. <pre><code>gremlin&gt; theseus = graph.addVertex('human')\n==&gt;v[3328]\ngremlin&gt; theseus.property('name', 'theseus')\n==&gt;null\ngremlin&gt; cerberus = g.V().has('name', 'cerberus').next()\n==&gt;v[2816]\ngremlin&gt; battle = theseus.addEdge('battled', cerberus, 'time', 22)\n==&gt;e[7eo-2kg-iz9-268][3328-battled-&gt;2816]\ngremlin&gt; battle.values('time')\n==&gt;22\n</code></pre></p> <p>When adding a vertex, an optional vertex label can be provided. An edge label must be specified when adding edges. Properties as key-value pairs can be set on both vertices and edges. When a property key is defined with SET or LIST cardinality, <code>addProperty</code> must be used when adding a respective property to a vertex. <pre><code>gremlin&gt; g.V(hercules).as('h').out('battled').in('battled').where(neq('h')).values('name')\n==&gt;theseus\n</code></pre></p> <p>The example above has 4 chained functions: <code>out</code>, <code>in</code>, <code>except</code>, and <code>values</code> (i.e. <code>name</code> is shorthand for <code>values('name')</code>). The function signatures of each are itemized below, where <code>V</code> is vertex and <code>U</code> is any object, where <code>V</code> is a subset of <code>U</code>.</p> <ol> <li><code>out: V -&gt; V</code></li> <li><code>in: V -&gt; V</code></li> <li><code>except: U -&gt; U</code></li> <li><code>values: V -&gt; U</code></li> </ol> <p>When chaining together functions, the incoming type must match the outgoing type, where <code>U</code> matches anything. Thus, the \"co-battled/ally\" traversal above is correct.</p> <p>Note</p> <p>The Gremlin overview presented in this section focused on the Gremlin-Groovy language implementation used in the Gremlin Console. Refer to Connecting to JanusGraph for information about connecting to JanusGraph with other languages than Groovy and independent of the Gremlin Console.</p>"},{"location":"getting-started/gremlin/#iterating-the-traversal","title":"Iterating the Traversal","text":"<p>One convenient feature of the Gremlin Console is that it automatically iterates all results from a query executed from the gremlin&gt; prompt. This works well within the REPL environment as it shows you the results as a String. As you transition towards writing a Gremlin application, it is important to understand how to iterate a traversal explicitly because your application\u2019s traversals will not iterate automatically. These are some of the common ways to iterate the Traversal:</p> <ul> <li><code>iterate()</code> - Zero results are expected or can be ignored.</li> <li><code>next()</code> - Get one result. Make sure to check <code>hasNext()</code> first.</li> <li><code>next(int n)</code> - Get the next <code>n</code> results. Make sure to check <code>hasNext()</code> first.</li> <li><code>toList()</code> - Get all results as a list. If there are no results, an empty list is returned.</li> </ul> <p>A Java code example is shown below to demonstrate these concepts: <pre><code>Traversal t = g.V().has(\"name\", \"pluto\"); // Define a traversal\n// Note the traversal is not executed/iterated yet\nVertex pluto = null;\nif (t.hasNext()) { // Check if results are available\n    pluto = g.V().has(\"name\", \"pluto\").next(); // Get one result\n    g.V(pluto).drop().iterate(); // Execute a traversal to drop pluto from graph\n}\n// Note the traversal can be cloned for reuse\nTraversal tt = t.asAdmin().clone();\nif (tt.hasNext()) {\n    System.err.println(\"pluto was not dropped!\");\n}\nList&lt;Vertex&gt; gods = g.V().hasLabel(\"god\").toList(); // Find all the gods\n</code></pre></p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#running-janusgraph-inside-a-docker-container","title":"Running JanusGraph inside a Docker container","text":"<p>For virtualization and easy access, JanusGraph provides a Docker image. Docker makes it easier to run servers and clients on a single machine without dealing with multiple installations. For instructions on installing and using Docker, please refer to the docker guide. With JanusGraph Server running in one container, we can either run the client in another container, or on our bare-metal machine. In the examples below, we use Gremlin Console as the client.</p>"},{"location":"getting-started/installation/#running-gremlin-console-inside-another-docker-container","title":"Running Gremlin Console inside another Docker container","text":"<p>Let's start by running a simple JanusGraph instance in Docker:</p> <pre><code>$ docker run --name janusgraph-default janusgraph/janusgraph:latest\n</code></pre> <p>The above command launches a JanusGraph Server instance. We name the container so that we can link a second container to it. The server may need a few seconds to start up so be patient and wait for the corresponding log messages to appear.</p> Example log <pre><code>SLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/opt/janusgraph/lib/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/opt/janusgraph/lib/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n0    [main] INFO  com.jcabi.manifests.Manifests  - 110 attributes loaded from 283 stream(s) in 130ms, 110 saved, 3770 ignored: [\"Agent-Class\", \"Ant-Version\", \"Archiver-Version\", \"Automatic-Module-Name\", \"Bnd-LastModified\", \"Boot-Class-Path\", \"Branch\", \"Build-Date\", \"Build-Host\", \"Build-Id\", \"Build-Java-Version\", \"Build-Jdk\", \"Build-Job\", \"Build-Number\", \"Build-Timestamp\", \"Build-Version\", \"Built-At\", \"Built-By\", \"Built-Date\", \"Built-OS\", \"Built-On\", \"Built-Status\", \"Bundle-ActivationPolicy\", \"Bundle-Activator\", \"Bundle-BuddyPolicy\", \"Bundle-Category\", \"Bundle-ClassPath\", \"Bundle-ContactAddress\", \"Bundle-Description\", \"Bundle-DocURL\", \"Bundle-License\", \"Bundle-ManifestVersion\", \"Bundle-Name\", \"Bundle-NativeCode\", \"Bundle-RequiredExecutionEnvironment\", \"Bundle-SymbolicName\", \"Bundle-Vendor\", \"Bundle-Version\", \"Can-Redefine-Classes\", \"Change\", \"Class-Path\", \"Created-By\", \"DSTAMP\", \"DynamicImport-Package\", \"Eclipse-BuddyPolicy\", \"Eclipse-ExtensibleAPI\", \"Embed-Dependency\", \"Embed-Transitive\", \"Export-Package\", \"Extension-Name\", \"Extension-name\", \"Fragment-Host\", \"Gradle-Version\", \"Gremlin-Lib-Paths\", \"Gremlin-Plugin-Dependencies\", \"Gremlin-Plugin-Paths\", \"Ignore-Package\", \"Implementation-Build\", \"Implementation-Build-Date\", \"Implementation-Title\", \"Implementation-URL\", \"Implementation-Vendor\", \"Implementation-Vendor-Id\", \"Implementation-Version\", \"Import-Package\", \"Include-Resource\", \"JCabi-Build\", \"JCabi-Date\", \"JCabi-Version\", \"Java-Vendor\", \"Java-Version\", \"Main-Class\", \"Manifest-Version\", \"Maven-Version\", \"Module-Email\", \"Module-Origin\", \"Module-Owner\", \"Module-Source\", \"Originally-Created-By\", \"Os-Arch\", \"Os-Name\", \"Os-Version\", \"Package\", \"Premain-Class\", \"Private-Package\", \"Provide-Capability\", \"Require-Bundle\", \"Require-Capability\", \"Scm-Connection\", \"Scm-Revision\", \"Scm-Url\", \"Specification-Title\", \"Specification-Vendor\", \"Specification-Version\", \"TODAY\", \"TSTAMP\", \"Time-Zone-Database-Version\", \"Tool\", \"X-Compile-Elasticsearch-Snapshot\", \"X-Compile-Elasticsearch-Version\", \"X-Compile-Lucene-Version\", \"X-Compile-Source-JDK\", \"X-Compile-Target-JDK\", \"hash\", \"implementation-version\", \"mode\", \"package\", \"service\", \"url\", \"version\"]\n1    [main] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - 3.4.1\n            \\,,,/\n            (o o)\n-----oOOo-(3)-oOOo-----\n\n100  [main] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Configuring Gremlin Server from /etc/opt/janusgraph/gremlin-server.yaml\n...\n...\n3965 [gremlin-server-boss-1] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Gremlin Server configured with worker thread pool of 1, gremlin pool of 8 and boss thread pool of 1.\n3965 [gremlin-server-boss-1] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Channel started at port 8182.\n</code></pre> <p>We can now instruct Docker to start a second container for the client and link it to the already running server. Here we use Gremlin Console (<code>gremlin.sh</code>) as the client.</p> <pre><code>$ docker run --rm --link janusgraph-default:janusgraph -e GREMLIN_REMOTE_HOSTS=janusgraph \\\n    -it janusgraph/janusgraph:latest ./bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\ngremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Configured janusgraph/172.17.0.2:8182\n</code></pre> <p>Warning</p> <p>The above command only establishes the connection to the server. It does not forward the following commands to the server by default! As a result, further commands will still be executed locally unless preceeded by <code>:&gt;</code>.</p> <p>To forward every command to the remote server, use the <code>:remote console</code> command. Further documentation can be found in the TinkerPop reference docs</p>"},{"location":"getting-started/installation/#running-gremlin-console-on-bare-metal","title":"Running Gremlin Console on Bare-metal","text":"<p>We can also run the Gremlin Console on our bare-metal machine. To make the server able to communicate with the gremlin console, we need to expose the 8182 port when launching JanusGraph Server:</p> <pre><code>$ docker run -it -p 8182:8182 janusgraph/janusgraph\n</code></pre> <p>We can now start a Gremlin Console and try to connect to the server. We need to download the released zip archive and launch the gremlin console (see local installation section for details):</p> <pre><code>$ bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\ngremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Configured localhost/127.0.0.1:8182\n</code></pre> <p>For further reading, see the JanusGraph Server section as well as the JanusGraph Docker documentation.</p>"},{"location":"getting-started/installation/#local-installation","title":"Local Installation","text":"<p>In order to run JanusGraph, Java 8 SE is required. Make sure the <code>$JAVA_HOME</code> environment variable points to the correct location where either JRE or JDK is installed. JanusGraph can be downloaded as a .zip archive from the Releases section of the project repository.</p> <pre><code>$ unzip janusgraph-1.0.0.zip\nArchive:  janusgraph-1.0.0.zip\n  creating: janusgraph-1.0.0/\n...\n</code></pre> <p>Once you have unzipped the downloaded archive, you are ready to go.</p>"},{"location":"getting-started/installation/#running-the-gremlin-console","title":"Running the Gremlin Console","text":"<p>The Gremlin Console is an interactive shell that gives you access to the data managed by JanusGraph. You can reach it by running the <code>gremlin.sh</code> script which is located in the project's <code>bin</code> directory.</p> <pre><code>$ cd janusgraph-1.0.0\n$ bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\n09:12:24 INFO  org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph  - HADOOP_GREMLIN_LIBS is set to: /usr/local/janusgraph/lib\nplugin activated: tinkerpop.hadoop\nplugin activated: janusgraph.imports\ngremlin&gt;\n</code></pre> <p>The Gremlin Console interprets commands using Apache Groovy, which is a superset of Java. Gremlin-Groovy extends Groovy by providing a set of methods for basic and advanced graph traversal functionality. For a deeper dive into Gremlin language's features, please refer to our introduction to Gremlin.</p>"},{"location":"getting-started/installation/#running-the-janusgraph-server","title":"Running the JanusGraph Server","text":"<p>In most real-world use cases, queries to a database will not be run from the exact same server the data is stored on. Instead, there will be some sort of client-server hierarchy in which the server runs the database and handles requests while multiple clients create these requests and thereby read and write entries within the database independently of one another. This behavior can also be achieved with JanusGraph.</p> <p>In order to start a server on your local machine, simply run the <code>janusgraph-server.sh</code> script instead of the <code>gremlin.sh</code> script. You can optionally pass a configuration file as a parameter. The default configuration is located at <code>conf/gremlin-server/gremlin-server.yaml</code>.</p> <p><pre><code>$ ./bin/janusgraph-server.sh start\n</code></pre> or <pre><code>$ ./bin/janusgraph-server.sh console ./conf/gremlin-server/gremlin-server-[...].yaml\n</code></pre></p> <p>Info</p> <p>The default configuration (<code>gremlin-server.yaml</code>) uses it's own inmemory backend instead of a dedicated database server. No search backend is used by default, so mixed indices aren't supported as search backend isn't specified  (Make sure you are using <code>GraphOfTheGodsFactory.loadWithoutMixedIndex(graph, true)</code> instead of <code>GraphOfTheGodsFactory.load(graph)</code> if you follow Basic Usage example). For further information about storage backends, visit the corresponding section of the documentation.</p> <p>A Gremlin server is now running on your local machine and waiting for clients to connect on the default port <code>8182</code>. To instantiate a client -- as done before -- run the <code>gremlin.sh</code> script. Again, a local Gremlin Console will show up. This time, instead of using it locally, we will connect the Gremlin Console to a remote server and redirect all of it's queries to this server. This is done by using the <code>:remote</code> command: <pre><code>gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Configured localhost/127.0.0.1:8182\n</code></pre> As you can probably tell from the log, the client and server are running on the same machine in this case. When using a different setup, all you have to do is modify the parameters in the <code>conf/remote.yaml</code> file.</p>"},{"location":"getting-started/installation/#using-the-pre-packaged-distribution","title":"Using the Pre-Packaged Distribution","text":"<p>Note</p> <p>Starting with 0.5.1, this requires to download <code>janusgraph-full-1.0.0.zip</code> instead of the default <code>janusgraph-1.0.0.zip</code>.</p> <p>The JanusGraph release comes pre-configured to run JanusGraph Server out of the box leveraging a sample Cassandra and Elasticsearch configuration to allow users to get started quickly with JanusGraph Server. This configuration defaults to client applications that can connect to JanusGraph Server via WebSocket with a custom subprotocol. There are a number of clients developed in different languages to help support the subprotocol. The most familiar client to use the WebSocket interface is the Gremlin Console. The quick-start bundle is not intended to be representative of a production installation, but does provide a way to perform development with JanusGraph Server, run tests and see how the components are wired together. To use this default configuration:</p> <ul> <li> <p>Download a copy of the current <code>janusgraph-full-$VERSION.zip</code> file from     the Releases     page</p> </li> <li> <p>Unzip it and enter the <code>janusgraph--full-$VERSION</code> directory</p> </li> <li> <p>Run <code>bin/janusgraph.sh start</code>. This step will start Gremlin Server     with Cassandra/ES forked into a separate process. Note for security     reasons Elasticsearch and therefore <code>janusgraph.sh</code> must be run     under a non-root account.</p> </li> </ul> <pre><code>$ bin/janusgraph.sh start\nForking Cassandra...\nRunning `nodetool statusthrift`.. OK (returned exit status 0 and printed string \"running\").\nForking Elasticsearch...\nConnecting to Elasticsearch (127.0.0.1:9300)... OK (connected to 127.0.0.1:9300).\nForking Gremlin-Server...\nConnecting to Gremlin-Server (127.0.0.1:8182)... OK (connected to 127.0.0.1:8182).\nRun gremlin.sh to connect.\n</code></pre>"},{"location":"getting-started/installation/#cleaning-up-after-the-pre-packaged-distribution","title":"Cleaning up after the Pre-Packaged Distribution","text":"<p>If you want to start fresh and remove the database and logs you can use the clean command with <code>janusgraph.sh</code>. The server should be stopped before running the clean operation. <pre><code>$ cd /Path/to/janusgraph/janusgraph-{project.version}/\n$ ./bin/janusgraph.sh stop\nKilling Gremlin-Server (pid 91505)...\nKilling Elasticsearch (pid 91402)...\nKilling Cassandra (pid 91219)...\n$ ./bin/janusgraph.sh clean\nAre you sure you want to delete all stored data and logs? [y/N] y\nDeleted data in /Path/to/janusgraph/janusgraph-{project.version}/db\nDeleted logs in /Path/to/janusgraph/janusgraph-{project.version}/log\n</code></pre></p>"},{"location":"index-backend/","title":"Introduction","text":"<p>While JanusGraph's composite graph indexes are natively supported through the primary storage backend, mixed graph indexes require that an indexing backend is configured. Mixed indexes provide support for geo, numeric range, and full-text search.</p> <p>The choice of index backend determines which search features are supported, as well as the performance and scalability of the index. JanusGraph currently supports three index backends: Elasticsearch, Apache Solr and Apache Lucene.</p> <p>Use Elasticsearch or Apache Solr when there is an expectation that JanusGraph will be distributed across multiple machines. Apache Lucene performs better in small scale, single machine applications. It performs better in unit tests, for instance.</p>"},{"location":"index-backend/direct-index-query/","title":"Direct Index Query","text":"<p>JanusGraph\u2019s standard global graph querying mechanism supports boolean queries for vertices or edges. In other words, an element either matches the query or it does not. There are no partial matches or result scoring.</p> <p>Some indexing backends additionally support fuzzy search queries. For those queries, a score is computed for each match to indicate the \"goodness\" of the match and results are returned in the order of their score. Fuzzy search is particularly useful when dealing with full-text search queries where matching more words is considered to be better.</p> <p>Since fuzzy search implementations and scoring algorithms differ significantly between indexing backends, JanusGraph does not support fuzzy search natively. However, JanusGraph provides a direct index query mechanism that allows search queries to be directly send to the indexing backend for evaluation (for those backends that support it).</p> <p>Use <code>Graph.indexQuery()</code> to compose a query that is executed directly against an indexing backend. This query builder expects two parameters:</p> <ol> <li> <p>The name of the indexing backend to query. This must be the name     configured in JanusGraph\u2019s configuration and used in the property     key indexing definitions</p> </li> <li> <p>The query string</p> </li> </ol> <p>The builder allows configuration of the maximum number of elements to be returned via its <code>limit(int)</code> method. The builder\u2019s <code>offset(int)</code> controls number of initial matches in the result set to skip. To retrieve all vertex or edges matching the given query in the specified indexing backend, invoke <code>vertexStream()</code> or <code>edgeStream()</code>, respectively. It is not possible to query for both vertices and edges at the same time. These methods return a <code>Stream</code> over <code>Result</code> objects. A result object contains the matched handle, retrievable via <code>getElement()</code>, and the associated score - <code>getScore()</code>.</p> <p>Consider the following example: <pre><code>ManagementSystem mgmt = graph.openManagement();\nPropertyKey text = mgmt.makePropertyKey(\"text\").dataType(String.class).make();\nmgmt.buildIndex(\"vertexByText\", Vertex.class).addKey(text).buildMixedIndex(\"search\");\nmgmt.commit();\n// ... Load vertices ...\nfor (Result&lt;Vertex&gt; result : graph.indexQuery(\"vertexByText\", \"v.text:(farm uncle berry)\").vertexStream()) {\n   System.out.println(result.getElement() + \": \" + result.getScore());\n}\n</code></pre></p>"},{"location":"index-backend/direct-index-query/#query-string","title":"Query String","text":"<p>The query string is handed directly to the indexing backend for processing and hence the query string syntax depends on what is supported by the indexing backend. For vertex queries, JanusGraph will analyze the query string for property key references starting with \"v.\" and replace those by a handle to the indexing field that corresponds to the property key. Likewise, for edge queries, JanusGraph will replace property key references starting with \"e.\". Hence, to refer to a property of a vertex, use \"v.[KEY_NAME]\" in the query string. Likewise, for edges write \"e.[KEY_NAME]\".</p> <p>Elasticsearch and Lucene support the Lucene query syntax. Refer to the Lucene documentation or the Elasticsearch documentation for more information. The query used in the example above follows the Lucene query syntax. <pre><code>graph.indexQuery(\"vertexByText\", \"v.text:(farm uncle berry)\").vertexStream()\n</code></pre></p> <p>This query matches all vertices where the text contains any of the three words (grouped by parentheses) and score matches higher the more words are matched in the text.</p> <p>In addition Elasticsearch supports wildcard queries, use \"v.*\" or \"e.*\" in the query string to query if any of the properties on the element match.</p>"},{"location":"index-backend/direct-index-query/#query-totals","title":"Query Totals","text":"<p>It is sometimes useful to know how many total results were returned from a query without having to retrieve all results. Fortunately, Elasticsearch and Solr provide a shortcut that does not involve retrieving and ranking all documents. This shortcut is exposed through the \".vertexTotals()\", \".edgeTotals()\", and \".propertyTotals()\" methods.</p> <p>The totals can be retrieved using the same query syntax as the indexQuery builder. <pre><code>graph.indexQuery(\"vertexByText\", \"v.text:(farm uncle berry)\").vertexTotals()\n</code></pre></p> <p>Note</p> <p>In JanusGraph versions earlier than 1.0.0 provided <code>limit</code> and <code>offset</code> was ignored for any <code>totals</code> queries. Starting from JanusGraph 1.0.0 provided <code>limit</code> and <code>offset</code> are applied to the final <code>totals</code> result (<code>vertexTotals()</code>, <code>edgeTotals()</code>, <code>propertyTotals()</code> as well as internal JanusGraph method <code>IndexProvider.totals</code>). This was made to have consistent behaviour between <code>totals</code> and <code>stream</code> direct index queries.</p>"},{"location":"index-backend/direct-index-query/#gotchas","title":"Gotchas","text":""},{"location":"index-backend/direct-index-query/#property-key-names","title":"Property Key Names","text":"<p>Names of property keys that contain non-alphanumeric characters must be placed in quotation marks to ensure that the query is parsed correctly. <pre><code>graph.indexQuery(\"vertexByText\", \"v.\\\"first_name\\\":john\").vertexStream()\n</code></pre></p> <p>Some property key names may be transformed by the JanusGraph indexing backend implementation. For instance, an indexing backend that does not permit spaces in field names may transform \"My Field Name\" to \"My\u2022Field\u2022Name\", or an indexing backend like Solr may append type information to the name, transforming \"myBooleanField\" to \"myBooleanField_b\". These transformations happen in the index backend\u2019s implementation of IndexProvider, in the \"mapKey2Field\" method. Indexing backends may reserve special characters (such as \u2022) and prohibit indexing of fields that contain them. For this reason it is recommended to avoid spaces and special characters in property names.</p> <p>In general, making direct index queries depends on implementation details of JanusGraph indexing backends that are normally hidden from users, so it\u2019s best to verify a query empirically against the indexing backend in use.</p>"},{"location":"index-backend/direct-index-query/#element-identifier-collision","title":"Element Identifier Collision","text":"<p>The strings \"v.\", \"e.\", and \"p.\" are used to identify a vertex, edge or property element respectively in a query. If the field name or the query value contains the same sequence of characters, this can cause a collision in the query string and parsing errors as in the following example: <pre><code>graph.indexQuery(\"vertexByText\", \"v.name:v.john\").vertexStream() //DOES NOT WORK!\n</code></pre></p> <p>To avoid such identifier collisions, use the <code>setElementIdentifier</code> method to define a unique element identifier string that does not occur in any other parts of the query: <pre><code>graph.indexQuery(\"vertexByText\", \"$v$name:v.john\").setElementIdentifier(\"$v$\").vertexStream()\n</code></pre></p>"},{"location":"index-backend/direct-index-query/#mixed-index-availability-delay","title":"Mixed Index Availability Delay","text":"<p>When a query traverses a mixed index immediately after data is inserted the changes may not be visible. In Elasticsearch the configuration option that determines this delay is index refresh interval. In Solr the primary configuration option is max time.</p>"},{"location":"index-backend/elasticsearch/","title":"Elasticsearch","text":"<p>Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected.</p> <p>\u2014  Elasticsearch Overview</p> <p>JanusGraph supports Elasticsearch as an index backend. Here are some of the Elasticsearch features supported by JanusGraph:</p> <ul> <li>Full-Text: Supports all <code>Text</code> predicates to search for text     properties that matches a given word, prefix or regular expression.</li> <li>Geo: Supports all <code>Geo</code> predicates to search for geo properties     that are intersecting, within, disjoint to or contained in a given     query geometry. Supports points, circles, boxes, lines and polygons     for indexing. Supports circles, boxes and polygons for querying     point properties and all shapes for querying non-point properties.</li> <li>Numeric Range: Supports all numeric comparisons in <code>Compare</code>.</li> <li>Flexible Configuration: Supports remote operation and open-ended     settings customization.</li> <li>Collections: Supports indexing SET and LIST cardinality     properties.</li> <li>Temporal: Nanosecond granularity temporal indexing.</li> <li>Custom Analyzer: Choose to use a custom analyzer</li> </ul> <p>Please see Version Compatibility for details on what versions of Elasticsearch will work with JanusGraph.</p> <p>Important</p> <p>JanusGraph uses sandboxed https://www.elastic.co/guide/en/elasticsearch/reference/master/modules-scripting-painless.html[Painless scripts] for inline updates, which are enabled by default in Elasticsearch.</p>"},{"location":"index-backend/elasticsearch/#running-elasticsearch","title":"Running Elasticsearch","text":"<p>JanusGraph supports connections to a running Elasticsearch cluster. JanusGraph provides two options for running local Elasticsearch instances for getting started quickly. JanusGraph server (see Getting started) automatically starts a local Elasticsearch instance. Alternatively JanusGraph releases include a full Elasticsearch distribution to allow users to manually start a local Elasticsearch instance (see this page for more information).</p> <pre><code>$ elasticsearch/bin/elasticsearch\n</code></pre> <p>Note</p> <p>For security reasons Elasticsearch must be run under a non-root account</p>"},{"location":"index-backend/elasticsearch/#elasticsearch-configuration-overview","title":"Elasticsearch Configuration Overview","text":"<p>JanusGraph supports HTTP(S) client connections to a running Elasticsearch cluster. Please see Version Compatibility for details on what versions of Elasticsearch will work with the different client types in JanusGraph.</p> <p>Note</p> <p>JanusGraph\u2019s index options start with the string \"<code>index.[X].</code>\" where \"<code>[X]</code>\" is a user-defined name for the backend. This user-defined name must be passed to JanusGraph\u2019s ManagementSystem interface when building a mixed index, as described in Mixed Index, so that JanusGraph knows which of potentially multiple configured index backends to use. Configuration snippets in this chapter use the name <code>search</code>, whereas prose discussion of options typically write <code>[X]</code> in the same position. The exact index name is not significant as long as it is used consistently in JanusGraph\u2019s configuration and when administering indices.</p> <p>Tip</p> <p>It\u2019s recommended that index names contain only alphanumeric lowercase characters and hyphens, and that they start with a lowercase letter.</p>"},{"location":"index-backend/elasticsearch/#connecting-to-elasticsearch","title":"Connecting to Elasticsearch","text":"<p>The Elasticsearch client is specified as follows: <pre><code>index.search.backend=elasticsearch\n</code></pre></p> <p>When connecting to Elasticsearch a single or list of hostnames for the Elasticsearch instances must be provided. These are supplied via JanusGraph\u2019s <code>index.[X].hostname</code> key. <pre><code>index.search.backend=elasticsearch\nindex.search.hostname=10.0.0.10:9200\n</code></pre></p> <p>Each host or host:port pair specified here will be added to the HTTP client\u2019s round-robin list of request targets. Here\u2019s a minimal configuration that will round-robin over 10.0.0.10 on the default Elasticsearch HTTP port (9200) and 10.0.0.20 on port 7777: <pre><code>index.search.backend=elasticsearch\nindex.search.hostname=10.0.0.10, 10.0.0.20:7777\n</code></pre></p>"},{"location":"index-backend/elasticsearch/#janusgraph-indexx-and-indexxelasticsearch-options","title":"JanusGraph <code>index.[X]</code> and <code>index.[X].elasticsearch</code> options","text":"<p>JanusGraph only uses default values for <code>index-name</code> and <code>health-request-timeout</code>. See Configuration Reference for descriptions of these options and their accepted values.</p> <ul> <li><code>index.[X].index-name</code></li> <li><code>index.[X].elasticsearch.health-request-timeout</code></li> </ul>"},{"location":"index-backend/elasticsearch/#rest-client-options","title":"REST Client Options","text":"<p>The REST client accepts the <code>index.[X].bulk-refresh</code> option. This option controls when changes are made visible to search. See ?refresh documentation for more information.</p>"},{"location":"index-backend/elasticsearch/#rest-client-https-configuration","title":"REST Client HTTPS Configuration","text":"<p>SSL support for HTTP can be enabled by setting the <code>index.[X].elasticsearch.ssl.enabled</code> configuration option to <code>true</code>. Note that depending on your configuration you may need to change the value of <code>index.[X].port</code> if your HTTPS port number is different from the default one for the REST API (9200).</p> <p>When SSL is enabled you may also configure the location and password of the truststore. This can be done as follows:</p> <pre><code>index.search.elasticsearch.ssl.truststore.location=/path/to/your/truststore.jks\nindex.search.elasticsearch.ssl.truststore.password=truststorepwd\n</code></pre> <p>Note that these settings apply only to Elasticsearch REST client and do not affect any other SSL connections in JanusGraph.</p> <p>Configuration of the client keystore is also supported:</p> <pre><code>index.search.elasticsearch.ssl.keystore.location=/path/to/your/keystore.jks\nindex.search.elasticsearch.ssl.keystore.storepassword=keystorepwd\nindex.search.elasticsearch.ssl.keystore.keypassword=keypwd\n</code></pre> <p>Any of the passwords can be empty.</p> <p>If needed, the SSL hostname verification can be disabled by setting the <code>index.[X].elasticsearch.ssl.disable-hostname-verification</code> property value to <code>true</code> and the support for self-signed SSL certificates can be enabled by setting <code>index.[X].elasticsearch.ssl.allow-self-signed-certificates</code> property value to <code>true</code>.</p> <p>Tip</p> <p>It is not recommended to rely on the self-signed SSL certificates or to disable the hostname verification for a production system as it significantly limits the client's ability to provide the secure communication channel with the Elasticsearch server(s). This may result in leaking the confidential data which may be a part of your JanusGraph index.</p>"},{"location":"index-backend/elasticsearch/#rest-client-http-authentication","title":"REST Client HTTP Authentication","text":"<p>REST client supports the following authentication options: Basic HTTP Authentication (username/password) and custom authentication based on the user-provided implementation.</p> <p>These authentication methods are independent from SSL client authentication described above.</p>"},{"location":"index-backend/elasticsearch/#rest-client-basic-http-authentication","title":"REST Client Basic HTTP Authentication","text":"<p>Basic HTTP Authentication is available regardless of the state of SSL support.  Optionally, an authentication realm can be specified via <code>index.[X].elasticsearch.http.auth.basic.realm</code> property.</p> <pre><code>index.search.elasticsearch.http.auth.type=basic\nindex.search.elasticsearch.http.auth.basic.username=httpuser\nindex.search.elasticsearch.http.auth.basic.password=httppassword\n</code></pre> <p>Tip</p> <p>It is highly recommended to use SSL (e.g. setting <code>index.[X].elasticsearch.ssl.enabled</code> to <code>true</code>) when using this option as the credentials can be intercepted when sent over an unencrypted connection!</p>"},{"location":"index-backend/elasticsearch/#rest-client-custom-http-authentication","title":"REST Client Custom HTTP Authentication","text":"<p>Additional authentication methods can be implemented by providing your own implementation. The custom authenticator is configured as follows:</p> <pre><code>index.search.elasticsearch.http.auth.type=custom\nindex.search.elasticsearch.http.auth.custom.authenticator-class=fully.qualified.class.Name\nindex.search.elasticsearch.http.auth.custom.authenticator-args=arg1,arg2,...\n</code></pre> <p>Argument list is optional and can be empty.</p> <p>The class specified there has to implement the <code>org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticator</code> interface or extend <code>org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticatorBase</code> convenience class. The implementation gets access to HTTP client configuration and can customize the client as needed. Refer to &lt;&gt; for more information. <p>For example, the following code snippet implements an authenticator allowing the Elasticsearch REST client to authenticate and get authorized against AWS IAM:</p> <pre><code>import java.io.IOException;\nimport java.time.LocalDateTime;\nimport java.time.ZoneOffset;\nimport org.apache.http.HttpRequestInterceptor;\nimport org.apache.http.impl.nio.client.HttpAsyncClientBuilder;\nimport org.janusgraph.diskstorage.es.rest.util.RestClientAuthenticatorBase;\nimport com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\nimport com.amazonaws.regions.DefaultAwsRegionProviderChain;\nimport com.google.common.base.Supplier;\nimport vc.inreach.aws.request.AWSSigner;\nimport vc.inreach.aws.request.AWSSigningRequestInterceptor;\n/**\n * &lt;p&gt;\n * Elasticsearch REST HTTP(S) client callback implementing AWS request signing.\n * &lt;/p&gt;\n * &lt;p&gt;\n * The signer is based on AWS SDK default provider chain, allowing multiple options for providing\n * the caller credentials. See {@link DefaultAWSCredentialsProviderChain} documentation for the details.\n * &lt;/p&gt;\n */\npublic class AWSV4AuthHttpClientConfigCallback extends RestClientAuthenticatorBase {\n    private static final String AWS_SERVICE_NAME = \"es\";\n    private HttpRequestInterceptor awsSigningInterceptor;\n    public AWSV4AuthHttpClientConfigCallback(final String[] args) {\n        // does not require any configuration\n    }\n    @Override\n    public void init() throws IOException {\n        DefaultAWSCredentialsProviderChain awsCredentialsProvider = new DefaultAWSCredentialsProviderChain();\n        final Supplier&lt;LocalDateTime&gt; clock = () -&gt; LocalDateTime.now(ZoneOffset.UTC);\n        // using default region provider chain\n        // (https://docs.aws.amazon.com/sdk-for-java/v2/developer-guide/java-dg-region-selection.html)\n        DefaultAwsRegionProviderChain regionProviderChain = new DefaultAwsRegionProviderChain();\n        final String awsRegion = regionProviderChain.getRegion();\n        final AWSSigner awsSigner = new AWSSigner(awsCredentialsProvider, awsRegion, AWS_SERVICE_NAME, clock);\n        this.awsSigningInterceptor = new AWSSigningRequestInterceptor(awsSigner);\n    }\n    @Override\n    public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpClientBuilder) {\n        return httpClientBuilder.addInterceptorLast(awsSigningInterceptor);/\n    }\n}\n</code></pre> <p>This custom authenticator does not use any constructor arguments.</p>"},{"location":"index-backend/elasticsearch/#ingest-pipelines","title":"Ingest Pipelines","text":"<p>Different ingest pipelines can be set for each mixed index. Ingest pipeline can be use to pre-process documents before indexing.  A pipeline is composed by a series of processors. Each processor transforms the document in some way. For example date processor can extract a date from a text to a date field. So you can query this date with JanusGraph without it being physically in the primary storage.</p> <ul> <li><code>index.[X].elasticsearch.ingest-pipeline.[mixedIndexName] = pipeline_id</code></li> </ul> <p>See ingest documentation for more information about ingest pipelines and processors documentation for more information about ingest processors.</p>"},{"location":"index-backend/elasticsearch/#secure-elasticsearch","title":"Secure Elasticsearch","text":"<p>Elasticsearch does not perform authentication or authorization. A client that can connect to Elasticsearch is trusted by Elasticsearch. When Elasticsearch runs on an unsecured or public network, particularly the Internet, it should be deployed with some type of external security. This is generally done with a combination of firewalling, tunneling of Elasticsearch\u2019s ports or by using Elasticsearch extensions such as X-Pack. Elasticsearch has two client-facing ports to consider:</p> <ul> <li>The HTTP REST API, usually on port 9200</li> <li>The native \"transport\" protocol, usually on port 9300</li> </ul> <p>A client uses either one protocol/port or the other, but not both simultaneously. Securing the HTTP protocol port is generally done with a combination of firewalling and a reverse proxy with SSL encryption and HTTP authentication. There are a couple of ways to approach security on the native \"transport\" protocol port:</p> <p>In addition to that, some hosted Elasticsearch services offer other methods of authentication and authorization. For example, AWS Elasticsearch Service requires the use of HTTPS and offers an option for using IAM-based access control. For that the requests sent to this service must be signed. This can be achieved by using a custom authenticator (see above).</p> <p>Tunnel Elasticsearch's native \"transport\" protocol:: This approach can be implemented with SSL/TLS tunneling (for instance via stunnel), a VPN, or SSH port forwarding. SSL/TLS tunnels require non-trivial setup and monitoring: one or both ends of the tunnel need a certificate, and the stunnel processes need to be configured and running continuously. The setup for most secure VPNs is likewise non-trivial. Some Elasticsearch service providers handle server-side tunnel management and provide a custom Elasticsearch <code>transport.type</code> to simplify the client setup.</p> <p>Add a firewall rule that allows only trusted clients to connect on Elasticsearch\u2019s native protocol port This is typically done at the host firewall level. Easy to configure, but very weak security by itself.</p>"},{"location":"index-backend/elasticsearch/#index-creation-options","title":"Index Creation Options","text":"<p>JanusGraph supports customization of the index settings it uses when creating its Elasticsearch index. It allows setting arbitrary key-value pairs on the <code>settings</code> object in the Elasticsearch <code>create index</code> request issued by JanusGraph. Here is a non-exhaustive sample of Elasticsearch index settings that can be customized using this mechanism:</p> <ul> <li><code>index.number_of_replicas</code></li> <li><code>index.number_of_shards</code></li> <li><code>index.refresh_interval</code></li> </ul> <p>Settings customized through this mechanism are only applied when JanusGraph attempts to create its index in Elasticsearch. If JanusGraph finds that its index already exists, then it does not attempt to recreate it, and these settings have no effect.</p>"},{"location":"index-backend/elasticsearch/#embedding-elasticsearch-index-creation-settings-with-createext","title":"Embedding Elasticsearch index creation settings with <code>create.ext</code>","text":"<p>JanusGraph iterates over all properties prefixed with <code>index.[X].elasticsearch.create.ext.</code>, where <code>[X]</code> is an index name such as <code>search</code>. It strips the prefix from each property key. The remainder of the stripped key will be interpreted as an Elasticsearch index creation setting. The value associated with the key is not modified. The stripped key and unmodified value are passed as part of the <code>settings</code> object in the Elasticsearch create index request that JanusGraph issues when bootstrapping on Elasticsearch. This allows embedding arbitrary index creation settings settings in JanusGraph\u2019s properties. Here\u2019s an example configuration fragment that customizes three Elasticsearch index settings using the <code>create.ext</code> config mechanism:</p> <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.create.ext.number_of_shards=15\nindex.search.elasticsearch.create.ext.number_of_replicas=3\nindex.search.elasticsearch.create.ext.shard.check_on_startup=true\n</code></pre> <p>The configuration fragment listed above takes advantage of Elasticsearch\u2019s assumption, implemented server-side, that unqualified <code>create index</code> setting keys have an <code>index.</code> prefix. It\u2019s also possible to spell out the index prefix explicitly. Here\u2019s a JanusGraph config file functionally equivalent to the one listed above, except that the <code>index.</code> prefix before the index creation settings is explicit: <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.create.ext.index.number_of_shards=15\nindex.search.elasticsearch.create.ext.index.number_of_replicas=3\nindex.search.elasticsearch.create.ext.index.shard.check_on_startup=false\n</code></pre></p> <p>Tip</p> <p>The <code>create.ext</code> mechanism for specifying index creation settings is compatible with JanusGraph\u2019s Elasticsearch configuration.</p>"},{"location":"index-backend/elasticsearch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"index-backend/elasticsearch/#connection-issues-to-remote-elasticsearch-cluster","title":"Connection Issues to remote Elasticsearch cluster","text":"<p>Check that the Elasticsearch cluster nodes are reachable on the HTTP protocol port from the JanusGraph nodes. Check the node listen port by examining the Elasticsearch node configuration logs or using a general diagnostic utility like <code>netstat</code>. Check the JanusGraph configuration.</p>"},{"location":"index-backend/elasticsearch/#optimizing-elasticsearch","title":"Optimizing Elasticsearch","text":""},{"location":"index-backend/elasticsearch/#write-optimization","title":"Write Optimization","text":"<p>For bulk loading or other write-intense applications, consider increasing Elasticsearch\u2019s refresh interval. Refer to this discussion on how to increase the refresh interval and its impact on write performance. Note, that a higher refresh interval means that it takes a longer time for graph mutations to be available in the index.</p>"},{"location":"index-backend/elasticsearch/#further-reading","title":"Further Reading","text":"<ul> <li>Please refer to the Elasticsearch homepage     and available documentation for more information on Elasticsearch     and how to setup an Elasticsearch cluster.</li> </ul>"},{"location":"index-backend/field-mapping/","title":"Field Mapping","text":""},{"location":"index-backend/field-mapping/#individual-field-mapping","title":"Individual Field Mapping","text":"<p>By default, JanusGraph will encode property keys to generate a unique field name for the property key in the mixed index. If one wants to query the mixed index directly in the external index backend can be difficult to deal with and are illegible. For this use case, the field name can be explicitly specified through a parameter. <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('bookname').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(name, Parameter.of('mapped-name', 'bookname')).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p> <p>With this field mapping defined as a parameter, JanusGraph will use the same name for the field in the <code>booksBySummary</code> index created in the external index system as for the property key. Note, that it must be ensured that the given field name is unique in the index.</p>"},{"location":"index-backend/field-mapping/#global-field-mapping","title":"Global Field Mapping","text":"<p>Instead of individually adjusting the field mapping for every key added to a mixed index, one can instruct JanusGraph to always set the field name in the external index to be identical to the property key name. This is accomplished by enabling the configuration option <code>map-name</code> which is configured per indexing backend. If this option is enabled for a particular indexing backend, then all mixed indexes defined against said backend will use field names identical to the property key names.</p> <p>However, this approach has two limitations: 1) The user has to ensure that the property key names are valid field names for the indexing backend and 2) renaming the property key will NOT rename the field name in the index which can lead to naming collisions that the user has to be aware of and avoid.</p> <p>Note, that individual field mappings as described above can be used to overwrite the default name for a particular key.</p>"},{"location":"index-backend/field-mapping/#custom-analyzer","title":"Custom Analyzer","text":"<p>By default, JanusGraph will use the default analyzer from the indexing backend for properties with Mapping.TEXT, and no analyzer for properties with Mapping.STRING. If one wants to use another analyzer, it can be explicitly specified through a parameter : ParameterType.TEXT_ANALYZER for Mapping.TEXT and ParameterType.STRING_ANALYZER for Mapping.STRING.</p>"},{"location":"index-backend/field-mapping/#for-elasticsearch","title":"For Elasticsearch","text":"<p>The name of the analyzer must be set as parameter value. <pre><code>mgmt = graph.openManagement()\nstring = mgmt.makePropertyKey('string').dataType(String.class).make()\ntext = mgmt.makePropertyKey('text').dataType(String.class).make()\ntextString = mgmt.makePropertyKey('textString').dataType(String.class).make()\nmgmt.buildIndex('string', Vertex.class).addKey(string, Mapping.STRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), 'standard')).buildMixedIndex(\"search\")\nmgmt.buildIndex('text', Vertex.class).addKey(text, Mapping.TEXT.asParameter(), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), 'english')).buildMixedIndex(\"search\")\nmgmt.buildIndex('textString', Vertex.class).addKey(text, Mapping.TEXTSTRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), 'standard'), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), 'english')).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p> <p>With these settings, JanusGraph will use the standard analyzer for property key string and the english analyzer for property key text.</p>"},{"location":"index-backend/field-mapping/#for-solr","title":"For Solr","text":"<p>The class of the tokenizer must be set as parameter value. <pre><code>mgmt = graph.openManagement()\nstring = mgmt.makePropertyKey('string').dataType(String.class).make()\ntext = mgmt.makePropertyKey('text').dataType(String.class).make()\nmgmt.buildIndex('string', Vertex.class).addKey(string, Mapping.STRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), 'org.apache.lucene.analysis.standard.StandardTokenizer')).buildMixedIndex(\"search\")\nmgmt.buildIndex('text', Vertex.class).addKey(text, Mapping.TEXT.asParameter(), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), 'org.apache.lucene.analysis.core.WhitespaceTokenizer')).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p> <p>With these settings, JanusGraph will use the standard tokenizer for property key string and the whitespace tokenizer for property key text.</p>"},{"location":"index-backend/field-mapping/#for-lucene","title":"For Lucene","text":"<p>The name of the analyzer must be set as parameter value or it defaults to KeywordAnalyzer for <code>Mapping.STRING</code> and to StandardAnalyzer for <code>Mapping.TEXT</code>.</p> <pre><code>mgmt = graph.openManagement()\nstring = mgmt.makePropertyKey('string').dataType(String.class).make()\ntext = mgmt.makePropertyKey('text').dataType(String.class).make()\nname = mgmt.makePropertyKey('name').dataType(String.class).make()\ndocument = mgmt.makePropertyKey('document').dataType(String.class).make()\nmgmt.buildIndex('string', Vertex.class).addKey(string, Mapping.STRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), org.apache.lucene.analysis.core.SimpleAnalyzer.class.getName())).buildMixedIndex(\"search\")\nmgmt.buildIndex('text', Vertex.class).addKey(text, Mapping.TEXT.asParameter(), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), org.apache.lucene.analysis.en.EnglishAnalyzer.class.getName())).buildMixedIndex(\"search\")\nmgmt.buildIndex('name', Vertex.class).addKey(string, Mapping.STRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.buildIndex('document', Vertex.class).addKey(text, Mapping.TEXT.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>With these settings, JanusGraph will use a SimpleAnalyzer analyzer for property key <code>string</code>, an EnglishAnalyzer analyzer for property key <code>text</code>,  a KeywordAnalyzer analyzer for property <code>name</code> and a StandardAnalyzer analyzer for property 'document'.</p>"},{"location":"index-backend/field-mapping/#custom-parameters","title":"Custom parameters","text":"<p>Sometimes it is required to set additional parameters on mappings (other than mapping type, mapping name and analyzer). For example, when we would like to use a different similarity algorithm (to modify the scoring algorithm of full text search) or if we want to use a custom boosting on some fields in Elasticsearch we can set custom parameters (right now only Elasticsearch supports custom parameters). The name of the custom parameter must be set through <code>ParameterType.customParameterName(\"yourProperty\")</code>.</p>"},{"location":"index-backend/field-mapping/#for-elasticsearch_1","title":"For Elasticsearch","text":"<pre><code>mgmt = graph.openManagement()\nmyProperty = mgmt.makePropertyKey('my_property').dataType(String.class).make()\nmgmt.buildIndex('custom_property_test', Vertex.class).addKey(myProperty, Mapping.TEXT.asParameter(), Parameter.of(ParameterType.customParameterName(\"boost\"), 5), Parameter.of(ParameterType.customParameterName(\"similarity\"), \"boolean\")).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>With these settings, JanusGraph will use the boost 5 and boolean similarity algorithm for property key <code>my_property</code>. Possible mapping parameters depend on Elasticsearch version. See mapping parameters for current Elasticsearch version.</p>"},{"location":"index-backend/lucene/","title":"Apache Lucene","text":"<p>Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform. Apache Lucene is an open source project available for free download.</p> <p>\u2014  Apache Lucene Homepage</p> <p>JanusGraph supports Apache Lucene as a single-machine, embedded index backend. Lucene has a slightly extended feature set and performs better in small-scale applications compared to Elasticsearch, but is limited to single-machine deployments.</p>"},{"location":"index-backend/lucene/#lucene-embedded-configuration","title":"Lucene Embedded Configuration","text":"<p>For single machine deployments, Lucene runs embedded with JanusGraph. JanusGraph starts and interfaces with Lucene internally.</p> <p>To run Lucene embedded, add the following configuration options to the graph configuration file where <code>/data/searchindex</code> specifies the directory where Lucene should store the index data:</p> <pre><code>index.search.backend=lucene\nindex.search.directory=/data/searchindex\n</code></pre> <p>In the above configuration, the index backend is named <code>search</code>. Replace <code>search</code> by a different name to change the name of the index.</p>"},{"location":"index-backend/lucene/#feature-support","title":"Feature Support","text":"<ul> <li>Full-Text: Supports all <code>Text</code> predicates to search for text     properties that matches a given word, prefix or regular expression.</li> <li>Geo: Supports <code>Geo</code> predicates to search for geo properties that     are intersecting, within, or contained in a given query geometry.     Supports points, lines and polygons for indexing. Supports circles     and boxes for querying point properties and all shapes for querying     non-point properties.</li> <li>Numeric Range: Supports all numeric comparisons in <code>Compare</code>.</li> <li>Collections: Supports indexing SET and LIST cardinality properties, except <code>Geo</code>.</li> <li>Temporal: Nanosecond granularity temporal indexing.</li> <li>Custom Analyzer: Choose to use a custom analyzer</li> <li>Not Query-normal-form: Supports queries other than Query-normal-form (QNF).      QNF for JanusGraph is a variant of CNF (conjunctive normal form) with negation inlined where possible.</li> </ul>"},{"location":"index-backend/lucene/#configuration-options","title":"Configuration Options","text":"<p>Refer to Configuration Reference for a complete listing of all Lucene specific configuration options in addition to the general JanusGraph configuration options.</p> <p>Note, that each of the index backend options needs to be prefixed with <code>index.[INDEX-NAME].</code> where <code>[INDEX-NAME]</code> stands for the name of the index backend. For instance, if the index backend is named search then these configuration options need to be prefixed with <code>index.search.</code>. To configure an index backend named search to use Lucene as the index system, set the following configuration option:</p> <pre><code>index.search.backend=lucene\n</code></pre>"},{"location":"index-backend/lucene/#further-reading","title":"Further Reading","text":"<ul> <li>Please refer to the Apache Lucene     homepage and available documentation for     more information on Lucene.</li> </ul>"},{"location":"index-backend/solr/","title":"Apache Solr","text":"<p>Solr is highly reliable, scalable and fault tolerant,  providing distributed indexing, replication and load-balanced  querying, automated failover and recovery, centralized configuration  and more. Solr powers the search and navigation features of  many of the world's largest internet sites. </p> <p>\u2014  Apache Solr Homepage</p> <p>JanusGraph supports Apache Solr as an index backend. Here are some of the Solr features supported by JanusGraph:</p> <ul> <li>Full-Text: Supports all <code>Text</code> predicates to search for text     properties that matches a given word, prefix or regular expression.</li> <li>Geo: Supports all <code>Geo</code> predicates to search for geo properties     that are intersecting, within, disjoint to or contained in a given     query geometry. Supports points, lines and polygons for indexing.     Supports circles, boxes and polygons for querying point properties     and all shapes for querying non-point properties.</li> <li>Numeric Range: Supports all numeric comparisons in <code>Compare</code>.</li> <li>TTL: Supports automatically expiring indexed elements.</li> <li>Temporal: Millisecond granularity temporal indexing.</li> <li>Custom Analyzer: Choose to use a custom analyzer</li> </ul> <p>Please see Version Compatibility for details on what versions of Solr will work with JanusGraph.</p>"},{"location":"index-backend/solr/#solr-configuration-overview","title":"Solr Configuration Overview","text":"<p>JanusGraph supports Solr running in either a SolrCloud or Solr Standalone (HTTP) configuration for use with a mixed index  (see Mixed Index).  The desired connection mode is configured via the parameter <code>mode</code> which must be set to either <code>cloud</code> or <code>http</code>, the former being the default value. For example, to explicitly specify that Solr is running in a SolrCloud configuration the following property is specified as a JanusGraph configuration property:</p> <pre><code>index.search.solr.mode=cloud\n</code></pre> <p>These are some key Solr terms:</p> <ul> <li>Core: A single index on a single machine</li> <li>Configuration: solrconfig.xml, schema.xml, and other files     required to define a core.</li> <li>Collection: A single logical index that can span multiple     cores on different machines.</li> <li>Configset: A shared configuration that can be reused by     multiple cores.</li> </ul>"},{"location":"index-backend/solr/#connecting-to-solrcloud","title":"Connecting to SolrCloud","text":"<p>When connecting to a SolrCloud cluster by setting the <code>mode</code> equal to <code>cloud</code>, the Zookeeper URL (and optionally port) must be specified so that JanusGraph can discover and interact with the Solr cluster.</p> <pre><code>index.search.backend=solr\nindex.search.solr.mode=cloud\nindex.search.solr.zookeeper-url=localhost:2181\n</code></pre> <p>A number of additional configuration options pertaining to the creation of new collections (which is only supported in SolrCloud operation mode) can be configured to control sharding behavior among other things. Refer to the Configuration Reference for a complete listing of those options.</p> <p>SolrCloud leverages Zookeeper to coordinate collection and configset information between the Solr servers. The use of Zookeeper with SolrCloud provides the opportunity to significantly reduce the amount of manual configuration required to use Solr as a back end index for JanusGraph.</p>"},{"location":"index-backend/solr/#configset-configuration","title":"Configset Configuration","text":"<p>A configset is required to create a collection. The configset is stored in Zookeeper to enable access to it across the Solr servers.</p> <ul> <li> <p>Each collection can provide its own configset when it is created, so     that each collection may have a different configuration. With this     approach, each collection must be created manually.</p> </li> <li> <p>A shared configset can be uploaded separately to Zookeeper if it     will be reused by multiple collections. With this approach,     JanusGraph can create collections automatically by using the shared     configset. Another benefit is that reusing a configset significantly     reduces the amount of data stored in Zookeeper.</p> </li> </ul>"},{"location":"index-backend/solr/#using-an-individual-configset","title":"Using an Individual Configset","text":"<p>In this example, a collection named <code>verticesByAge</code> is created manually using the default JanusGraph configuration for Solr that is found in the distribution. When the collection is created, the configuration is uploaded into Zookeeper, using the same collection name <code>verticesByAge</code> for the configset name. Refer to the Solr Reference Guide for available parameters.</p> <pre><code># create the collection\n$SOLR_HOME/bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME/conf/solr\n</code></pre> <p>Define a mixed index using <code>JanusGraphManagement</code> and the same collection name.</p> <pre><code>mgmt = graph.openManagement()\nage = mgmt.makePropertyKey(\"age\").dataType(Integer.class).make()\nmgmt.buildIndex(\"verticesByAge\", Vertex.class).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre>"},{"location":"index-backend/solr/#using-a-shared-configset","title":"Using a Shared Configset","text":"<p>When using a shared configset, it is most convenient to upload the configuration first as a one time operation. In this example, a configset named <code>janusgraph-configset</code> is uploaded in to Zookeeper using the default JanusGraph configuration for Solr that is found in the distribution. Refer to the Solr Reference Guide for available parameters.</p> <pre><code># upload the shared configset into Zookeeper\n# Solr 5\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -z localhost:2181 \\\n    -d $JANUSGRAPH_HOME/conf/solr -n janusgraph-configset\n# Solr 6 and higher\n$SOLR_HOME/bin/solr zk upconfig -d $JANUSGRAPH_HOME/conf/solr -n janusgraph-configset \\\n    -z localhost:2181\n</code></pre> <p>When configuring the SolrCloud indexing backend for JanusGraph, make sure to provide the name of the shared configset using the <code>index.search.solr.configset</code> property.</p> <pre><code>index.search.backend=solr\nindex.search.solr.mode=cloud\nindex.search.solr.zookeeper-url=localhost:2181\nindex.search.solr.configset=janusgraph-configset\n</code></pre> <p>Define a mixed index using <code>JanusGraphManagement</code> and the collection name.</p> <pre><code>mgmt = graph.openManagement()\nage = mgmt.makePropertyKey(\"age\").dataType(Integer.class).make()\nmgmt.buildIndex(\"verticesByAge\", Vertex.class).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre>"},{"location":"index-backend/solr/#connecting-to-solr-standalone-http","title":"Connecting to Solr Standalone (HTTP)","text":"<p>When connecting to Solr Standalone via HTTP by setting the <code>mode</code> equal to <code>http</code>, a single or list of URLs for the Solr instances must be provided.</p> <pre><code>index.search.backend=solr\nindex.search.solr.mode=http\nindex.search.solr.http-urls=http://localhost:8983/solr\n</code></pre> <p>Additional configuration options for controlling the maximum number of connections, connection timeout and transmission compression are available for the HTTP mode. Refer to the Configuration Reference for a complete listing of those options.</p>"},{"location":"index-backend/solr/#core-configuration","title":"Core Configuration","text":"<p>Solr Standalone is used for a single instance, and it keeps configuration information on the file system. A core must be created manually for each mixed index.</p> <p>To create a core, a <code>core_name</code> and a <code>configuration</code> directory is required. Refer to the Solr Reference Guide for available parameters. In this example, a core named <code>verticesByAge</code> is created using the default JanusGraph configuration for Solr that is found in the distribution.</p> <pre><code>$SOLR_HOME/bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME/conf/solr\n</code></pre> <p>Define a mixed index using <code>JanusGraphManagement</code> and the same core name. <pre><code>mgmt = graph.openManagement()\nage = mgmt.makePropertyKey(\"age\").dataType(Integer.class).make()\nmgmt.buildIndex(\"verticesByAge\", Vertex.class).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p>"},{"location":"index-backend/solr/#kerberos-configuration","title":"Kerberos Configuration","text":"<p>When connecting to a Solr environment that is protected by Kerberos we must specify that Kerberos is being used and reference a JAAS configuration file to properly configure the Solr Clients. This configuration is required when Kerberos is in use regardless of the mode in which Solr is operating (SolrCloud or Solr Standalone).</p> <pre><code>index.search.solr.kerberos-enabled=true\n</code></pre> <p>The JAAS configuration file is supplied by ensuring that you set the java system property <code>java.security.auth.login.config</code> with the absolute path to the file. This property should be set using JVM options. For example to run <code>gremlin.sh</code> you would need to set the <code>JAVA_OPTIONS</code> environment variable prior to running the script:</p> <pre><code>export JAVA_OPTIONS=\"-Djava.security.auth.login.config=/absolute/path/jaas.conf\"\n$JANUSGRAPH_HOME/bin/gremlin.sh\n</code></pre> <p>For details on the content required in the JAAS configuration file refer to the https://lucene.apache.org/solr/guide/7_0/kerberos-authentication-plugin.html#define-a-jaas-configuration-file[Solr Reference Guide].</p>"},{"location":"index-backend/solr/#solr-schema-design","title":"Solr Schema Design","text":""},{"location":"index-backend/solr/#dynamic-field-definition","title":"Dynamic Field Definition","text":"<p>By default, JanusGraph uses Solr\u2019s Dynamic Fields feature to define the field types for all indexed keys. This requires no extra configuration when adding property keys to a mixed index backed by Solr and provides better performance than schemaless mode.</p> <p>JanusGraph assumes the following dynamic field tags are defined in the backing Solr collection\u2019s schema.xml file. Please note that there is additional xml definition of the following fields required in a solr schema.xml file in order to use them. Reference the example schema.xml file provided in the <code>./conf/solr/schema.xml</code> directory in a JanusGraph installation for more information.</p> <pre><code>&lt;dynamicField name=\"*_i\"    type=\"int\"          indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_s\"    type=\"string\"       indexed=\"true\"  stored=\"true\" /&gt;\n&lt;dynamicField name=\"*_l\"    type=\"long\"         indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_t\"    type=\"text_general\" indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_b\"    type=\"boolean\"      indexed=\"true\" stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_f\"    type=\"float\"        indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_d\"    type=\"double\"       indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_g\"    type=\"geo\"          indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_dt\"   type=\"date\"         indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_uuid\" type=\"uuid\"         indexed=\"true\"  stored=\"true\"/&gt;\n</code></pre> <p>In JanusGraph\u2019s default configuration, property key names do not have to end with the type-appropriate suffix to take advantage of Solr\u2019s dynamic field feature. JanusGraph generates the Solr field name from the property key name by encoding the property key definition\u2019s numeric identifier and the type-appropriate suffix. This means that JanusGraph uses synthetic field names with type-appropriate suffixes behind the scenes, regardless of the property key names defined and used by application code using JanusGraph. This field name mapping can be overridden through non-default configuration. That\u2019s described in the next section.</p>"},{"location":"index-backend/solr/#manual-field-definition","title":"Manual Field Definition","text":"<p>If the user would rather manually define the field types for each of the indexed fields in a collection, the configuration option <code>dyn-fields</code> needs to be disabled. It is important that the field for each indexed property key is defined in the backing Solr schema before the property key is added to the index.</p> <p>In this scenario, it is advisable to enable explicit property key name to field mapping in order to fix the field names for their explicit definition. This can be achieved in one of two ways:</p> <ol> <li>Configuring the name of the field by providing a <code>mapped-name</code>     parameter when adding the property key to the index. See     Individual Field Mapping for more information.</li> <li>By enabling the <code>map-name</code> configuration option for the Solr index     which will use the property key name as the field name in Solr. See     Global Field Mapping for more information.</li> </ol>"},{"location":"index-backend/solr/#schemaless-mode","title":"Schemaless Mode","text":"<p>JanusGraph can also interact with a SolrCloud cluster that is configured for schemaless mode. In this scenario, the configuration option <code>dyn-fields</code> should be disabled since Solr will infer the field type from the values and not the field name.</p> <p>Note, however, that schemaless mode is recommended only for prototyping and initial application development and NOT recommended for production use.</p>"},{"location":"index-backend/solr/#troubleshooting","title":"Troubleshooting","text":""},{"location":"index-backend/solr/#collection-does-not-exist","title":"Collection Does Not Exist","text":"<p>The collection (and all of the required configuration files) must be initialized before a defined index can use the collection. See Connecting to SolrCloud for more information.</p> <p>When using SolrCloud, the Zookeeper zkCli.sh command line tool can be used to inspect the configurations loaded into Zookeeper. Also verify that the default JanusGraph configuration files are copied to the correct location under solr and that the directory where the files are copied is correct.</p>"},{"location":"index-backend/solr/#cannot-find-the-specified-configset","title":"Cannot Find the Specified Configset","text":"<p>When using SolrCloud, a configset is required to create a mixed index for JanusGraph. See Configset Configuration for more information.</p> <ul> <li>If using an individual configset, the collection must be created     manually first.</li> <li>If using a shared configset, the configset must be uploaded into     Zookeeper first.</li> </ul> <p>You can verify that the configset and its configuration files are in Zookeeper under <code>/configs</code>. Refer to the Solr Reference Guide for other Zookeeper operations.</p> <pre><code># verify the configset in Zookeeper\n# Solr 5\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd list -z localhost:2181\n# Solr 6 and higher\n$SOLR_HOME/bin/solr zk ls -r /configs/configset-name -z localhost:2181\n</code></pre>"},{"location":"index-backend/solr/#http-error-404","title":"HTTP Error 404","text":"<p>This error may be encountered when using Solr Standalone (HTTP) mode. An example of the error:</p> <pre><code>20:01:22 ERROR org.janusgraph.diskstorage.solr.SolrIndex  - Unable to save documents\nto Solr as one of the shape objects stored were not compatible with Solr.\norg.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server\nat http://localhost:8983/solr: Expected mime type application/octet-stream but got text/html.\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/&gt;\n&lt;title&gt;Error 404 Not Found&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 404&lt;/h2&gt;\n&lt;p&gt;Problem accessing /solr/verticesByAge/update. Reason:\n&lt;pre&gt;    Not Found&lt;/pre&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Make sure to create the core manually before attempting to store data into the index. See Core Configuration for more information.</p>"},{"location":"index-backend/solr/#invalid-core-or-collection-name","title":"Invalid core or collection name","text":"<p>The core or collection name is an identifier. It must consist entirely of periods, underscores, hyphens, and/or alphanumerics, and also it may not start with a hyphen.</p>"},{"location":"index-backend/solr/#connection-problems","title":"Connection Problems","text":"<p>Irrespective of the operation mode, a Solr instance or a cluster of Solr instances must be running and accessible from the JanusGraph instance(s) in order for JanusGraph to use Solr as an indexing backend. Check that the Solr cluster is running correctly and that it is visible and accessible over the network (or locally) from the JanusGraph instances.</p>"},{"location":"index-backend/solr/#jts-classnotfoundexception-with-geo-data","title":"JTS ClassNotFoundException with Geo Data","text":"<p>Solr relies on Spatial4j for geo processing. Spatial4j declares an optional dependency on JTS (\"JTS Topology Suite\"). JTS is required for some geo field definition and query functionality. If the JTS jar is not on the Solr daemon\u2019s classpath and a field in schema.xml uses a geo type, then Solr may throw a ClassNotFoundException on one of the missing JTS classes. The exception can appear when starting Solr using a schema.xml file designed to work with JanusGraph, but can also appear when invoking <code>CREATE</code> in the Solr CoreAdmin API. The exception appears in slightly different formats on the client and server sides, although the root cause is identical.</p> <p>Here\u2019s a representative example from a Solr server log:</p> <pre><code>ERROR [http-8983-exec-5] 2014-10-07 02:54:06, 665 SolrCoreResourceManager.java (line 344) com/vividsolutions/jts/geom/Geometry\njava.lang.NoClassDefFoundError: com/vividsolutions/jts/geom/Geometry\n    at com.spatial4j.core.context.jts.JtsSpatialContextFactory.newSpatialContext(JtsSpatialContextFactory.java:30)\n    at com.spatial4j.core.context.SpatialContextFactory.makeSpatialContext(SpatialContextFactory.java:83)\n    at org.apache.solr.schema.AbstractSpatialFieldType.init(AbstractSpatialFieldType.java:95)\n    at org.apache.solr.schema.AbstractSpatialPrefixTreeFieldType.init(AbstractSpatialPrefixTreeFieldType.java:43)\n    at org.apache.solr.schema.SpatialRecursivePrefixTreeFieldType.init(SpatialRecursivePrefixTreeFieldType.java:37)\n    at org.apache.solr.schema.FieldType.setArgs(FieldType.java:164)\n    at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:141)\n    at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:43)\n    at org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:190)\n    at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:470)\n    at com.datastax.bdp.search.solr.CassandraIndexSchema.readSchema(CassandraIndexSchema.java:72)\n    at org.apache.solr.schema.IndexSchema.&lt;init&gt;(IndexSchema.java:168)\n    at com.datastax.bdp.search.solr.CassandraIndexSchema.&lt;init&gt;(CassandraIndexSchema.java:54)\n    at com.datastax.bdp.search.solr.core.CassandraCoreContainer.create(CassandraCoreContainer.java:210)\n    at com.datastax.bdp.search.solr.core.SolrCoreResourceManager.createCore(SolrCoreResourceManager.java:256)\n    at com.datastax.bdp.search.solr.handler.admin.CassandraCoreAdminHandler.handleCreateAction(CassandraCoreAdminHandler.java:117)\n    ...\n</code></pre> <p>Here\u2019s what normally appears in the output of the client that issued the associated <code>CREATE</code> command to the CoreAdmin API:</p> <pre><code>org.apache.solr.common.SolrException: com/vividsolutions/jts/geom/Geometry\n    at com.datastax.bdp.search.solr.core.SolrCoreResourceManager.createCore(SolrCoreResourceManager.java:345)\n    at com.datastax.bdp.search.solr.handler.admin.CassandraCoreAdminHandler.handleCreateAction(CassandraCoreAdminHandler.java:117)\n    at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody(CoreAdminHandler.java:152)\n    ...\n</code></pre> <p>This is resolved by adding the JTS jar to the classpath of JanusGraph and/or the Solr server. JTS is not included in JanusGraph distributions by default due to its LGPL license. Users must download the JTS jar file separately and copy it into the JanusGraph and/or Solr server lib directory. If using Solr\u2019s built in web server, the JTS jar may be copied to the example/solr-webapp/webapp/WEB-INF/lib directory to include it in the classpath. Solr can be restarted, and the exception should be gone. Solr must be started once with the correct schema.xml file in place first, for the example/solr-webapp/webapp/WEB-INF/lib directory to exist.</p> <p>To determine the ideal JTS version for Solr server, first check the version of Spatial4j in use by the Solr cluster, then determine the version of JTS against which that Spatial4j version was compiled. Spatial4j declares its target JTS version in the pom for the <code>com.spatial4j:spatial4j</code> artifact. Copy the JTS jar to the server/solr-webapp/webapp/WEB-INF/lib directory in your solr installation.</p>"},{"location":"index-backend/solr/#advanced-solr-configuration","title":"Advanced Solr Configuration","text":""},{"location":"index-backend/solr/#dse-search","title":"DSE Search","text":"<p>This section covers installation and configuration of JanusGraph with DataStax Enterprise (DSE) Search. There are multiple ways to install DSE, but this section focuses on DSE\u2019s binary tarball install option on Linux. Most of the steps in this section can be generalized to the other install options for DSE.</p> <p>Install DataStax Enterprise as directed by the page Installing DataStax Enterprise using the binary tarball.</p> <p>Export <code>DSE_HOME</code> and append to <code>PATH</code> in your shell environment. Here\u2019s an example using Bash syntax: <pre><code>export DSE_HOME=/path/to/dse-version.number\nexport PATH=\"$DSE_HOME\"/bin:\"$PATH\"\n</code></pre></p> <p>Install JTS for Solr. The appropriate version varies with the Spatial4j version. As of DSE 4.5.2, the appropriate version is 1.13. <pre><code>cd $DSE_HOME/resources/solr/lib\ncurl -O 'http://central.maven.org/maven2/com/vividsolutions/jts/1.13/jts-1.13.jar'\n</code></pre></p> <p>Start DSE Cassandra and Solr in a single background daemon:</p> <pre><code># The \"dse-data\" path below was chosen to match the\n# \"Installing DataStax Enterprise using the binary tarball\"\n# documentation page from DataStax.  The exact path is not\n# significant.\ndse cassandra -s -Ddse.solr.data.dir=\"$DSE_HOME\"/dse-data/solr\n</code></pre> <p>The previous command will write some startup information to the console and to the logfile path <code>log4j.appender.R.File</code> configured in <code>$DSE_HOME/resources/cassandra/conf/log4j2-server.xml</code>.</p> <p>Once DSE with Cassandra and Solr has started normally, check the cluster health with <code>nodetool status</code>. A single-instance ring should show one node with flags *U*p and *N*ormal:</p> <pre><code>nodetool status\nNote: Ownership information does not include topology; for complete information, specify a keyspace\n= Datacenter: Solr\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Owns   Host ID                               Token                                    Rack\nUN  127.0.0.1  99.89 KB   100.0%  5484ef7b-ebce-4560-80f0-cbdcd9e9f496  -7317038863489909889                     rack1\n</code></pre> <p>Next, switch to Gremlin Console and open a JanusGraph database against the DSE instance. This will create JanusGraph\u2019s keyspace and column families.</p> <pre><code>cd $JANUSGRAPH_HOME\nbin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\ngremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql-solr.properties')\n==&gt;janusgraph[cql:[127.0.0.1]]\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[janusgraph[cql:[127.0.0.1]], standard]\ngremlin&gt;\n</code></pre> <p>Keep this Gremlin Console open. We\u2019ll take a break now to install a Solr core. Then we\u2019ll come back to this console to load some sample data.</p> <p>Next, upload configuration files for JanusGraph\u2019s Solr collection, then create the core in DSE:</p> <pre><code># Change to the directory where JanusGraph was extracted.  Later commands\n# use relative paths to the Solr config files shipped with the JanusGraph\n# distribution.\ncd $JANUSGRAPH_HOME\n\n# The name must be URL safe and should contain one dot/full-stop\n# character. The part of the name after the dot must not conflict with\n# any of JanusGraph's internal CF names.  Starting the part after the dot\n# \"solr\" will avoid a conflict with JanusGraph's internal CF names.\nCORE_NAME=janusgraph.solr1\n# Where to upload collection configuration and send CoreAdmin requests.\nSOLR_HOST=localhost:8983\n\n# The value of index.[X].solr.http-urls in JanusGraph's config file\n# should match $SOLR_HOST and $CORE_NAME.  For example, given the\n# $CORE_NAME and $SOLR_HOST values above, JanusGraph's config file would\n# contain (assuming \"search\" is the desired index alias):\n#\n# index.search.solr.http-urls=http://localhost:8983/solr/janusgraph.solr1\n#\n# The stock JanusGraph config file conf/janusgraph-cql-solr.properties\n# ships with this http-urls value.\n\n# Upload Solr config files to DSE Search daemon\nfor xml in conf/solr/{solrconfig, schema, elevate}.xml ; do\n    curl -v http://\"$SOLR_HOST\"/solr/resource/\"$CORE_NAME/$xml\" \\\n      --data-binary @\"$xml\" -H 'Content-type:text/xml; charset=utf-8'\ndone\nfor txt in conf/solr/{protwords, stopwords, synonyms}.txt ; do\n    curl -v http://\"$SOLR_HOST\"/solr/resource/\"$CORE_NAME/$txt\" \\\n      --data-binary @\"$txt\" -H 'Content-type:text/plain; charset=utf-8'\ndone\nsleep 5\n\n# Create core using the Solr config files just uploaded above\ncurl \"http://\"$SOLR_HOST\"/solr/admin/cores?action=CREATE&amp;name=$CORE_NAME\"\nsleep 5\n\n# Retrieve and print the status of the core we just created\ncurl \"http://localhost:8983/solr/admin/cores?action=STATUS&amp;core=$CORE_NAME\"\n</code></pre> <p>Now the JanusGraph database and backing Solr core are ready for use. We can test it out with the Graph of the Gods dataset. Picking up the Gremlin Console session started above: <pre><code>// Assuming graph = JanusGraphFactory.open('conf/janusgraph-cql-solr.properties')...\ngremlin&gt; GraphOfTheGodsFactory.load(graph)\n==&gt;null\n</code></pre></p> <p>Now we can run any of the queries described in Getting started. Queries involving text and geo predicates will be served by Solr. For more verbose reporting from JanusGraph and the Solr client, run <code>gremlin.sh -l DEBUG</code> and issue some index-backed queries.</p>"},{"location":"index-backend/text-search/","title":"Index Parameters and Full-Text Search","text":"<p>When defining a mixed index, a list of parameters can be optionally specified for each property key added to the index. These parameters control how the particular key is to be indexed. JanusGraph recognizes the following index parameters. Whether these are supported depends on the configured index backend. A particular index backend might also support custom parameters in addition to the ones listed here.</p>"},{"location":"index-backend/text-search/#full-text-search","title":"Full-Text Search","text":"<p>When indexing string values, that is property keys with <code>String.class</code> data type, one has the choice to either index those as text or character strings which is controlled by the <code>mapping</code> parameter type.</p> <p>When the value is indexed as text, the string is tokenized into a bag of words which allows the user to efficiently query for all matches that contain one or multiple words. This is commonly referred to as full-text search. When the value is indexed as a character string, the string is index \"as-is\" without any further analysis or tokenization. This facilitates queries looking for an exact character sequence match. This is commonly referred to as string search.</p>"},{"location":"index-backend/text-search/#full-text-search_1","title":"Full-Text Search","text":"<p>By default, strings are indexed as text. To make this indexing option explicit, one can define a mapping when indexing a property key as text.</p> <pre><code>mgmt = graph.openManagement()\nsummary = mgmt.makePropertyKey('booksummary').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(summary, Mapping.TEXT.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>This is identical to a standard mixed index definition with the only addition of an extra parameter that specifies the mapping in the index - in this case <code>Mapping.TEXT</code>.</p> <p>When a string property is indexed as text, the string value is tokenized into a bag of tokens. The exact tokenization depends on the indexing backend and its configuration. JanusGraph\u2019s default tokenization splits the string on non-alphanumeric characters and removes any tokens with less than 2 characters. The tokenization used by an indexing backend may differ (e.g. stop words are removed) which can lead to minor differences in how full-text search queries are handled for modifications inside a transaction and committed data in the indexing backend.</p> <p>When a string property is indexed as text, only full-text search predicates are supported in graph queries by the indexing backend. Full-text search is case-insensitive.</p> <ul> <li><code>textContains</code>: is true if (at least) one word inside the text     string matches the query string</li> <li><code>textContainsPrefix</code>: is true if (at least) one word inside the text     string begins with the query string</li> <li><code>textContainsRegex</code>: is true if (at least) one word inside the text     string matches the given regular expression</li> <li><code>textContainsFuzzy</code>: is true if (at least) one word inside the text     string is similar to the query String (based on Levenshtein edit     distance)</li> <li><code>textContainsPhrase</code>: is true if the text string does contain the sequence of words in the query string</li> </ul> <pre><code>import static org.janusgraph.core.attribute.Text.*\ng.V().has('booksummary', textContains('unicorns'))\ng.V().has('booksummary', textContainsPrefix('uni'))\ng.V().has('booksummary', textContainsRegex('.*corn.*'))\ng.V().has('booksummary', textContainsFuzzy('unicorn'))\ng.V().has('booksummary', textContainsPhrase('unicorn horn'))\n</code></pre> <p>The Elasticsearch backend extends this functionality and includes support for negations of the above predicates, as well as phrase matching:</p> <ul> <li><code>textNotContains</code>: is true if no words inside the text string match the query string</li> <li><code>textNotContainsPrefix</code>: is true if no words inside the text string begin with the query string</li> <li><code>textNotContainsRegex</code>: is true if no words inside the text string match the given regular expression</li> <li><code>textNotContainsFuzzy</code>:  is true if no words inside the text string are similar to the query string (based on Levenshtein edit distance)</li> <li><code>textNotContainsPhrase</code>:  is true if the text string does not contain the sequence of words in the query string</li> </ul> <p>String search predicates (see below) may be used in queries, but those require filtering in memory which can be very costly.</p>"},{"location":"index-backend/text-search/#string-search","title":"String Search","text":"<p>To index string properties as character sequences without any analysis or tokenization, specify the mapping as <code>Mapping.STRING</code>:</p> <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('bookname').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(name, Mapping.STRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>When a string mapping is configured, the string value is indexed and can be queried \"as-is\" - including stop words and non-letter characters. However, in this case the query must match the entire string value. Hence, the string mapping is useful when indexing short character sequences that are considered to be one token.</p> <p>When a string property is indexed as string, only the following predicates are supported in graph queries by the indexing backend. String search is case-sensitive.</p> <ul> <li><code>eq</code>: if the string is identical to the query string</li> <li><code>neq</code>: if the string is different than the query string</li> <li><code>textPrefix</code>: if the string value starts with the given query string</li> <li><code>textRegex</code>: if the string value matches the given regular     expression in its entirety</li> <li><code>textFuzzy</code>: if the string value is similar to the given query     string (based on Levenshtein edit distance)</li> </ul> <pre><code>import static org.apache.tinkerpop.gremlin.process.traversal.P.*\nimport static org.janusgraph.core.attribute.Text.*\ng.V().has('bookname', eq('unicorns'))\ng.V().has('bookname', neq('unicorns'))\ng.V().has('bookname', textPrefix('uni'))\ng.V().has('bookname', textRegex('.*corn.*'))\ng.V().has('bookname', textFuzzy('unicorn'))\n</code></pre> <p>The Elasticsearch backend extends this functionality and includes support for negations of the above text predicates:</p> <ul> <li><code>textNotPrefix</code>: if the string value does not start with the given query string</li> <li><code>textNotRegex</code>: if the string value does not match the given regular expression in its entirety</li> <li><code>textNotFuzzy</code>: if the string value is not similar to the given query string (based on Levenshtein edit distance)</li> </ul> <p>Full-text search predicates may be used in queries, but those require filtering in memory which can be very costly.</p>"},{"location":"index-backend/text-search/#full-text-and-string-search","title":"Full text and string search","text":"<p>If you are using Elasticsearch it is possible to index properties as both text and string allowing you to use all of the predicates for exact and fuzzy matching.</p> <pre><code>mgmt = graph.openManagement()\nsummary = mgmt.makePropertyKey('booksummary').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(summary, Mapping.TEXTSTRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>Note that the data will be stored in the index twice, once for exact matching and once for fuzzy matching.</p>"},{"location":"index-backend/text-search/#tinkerpop-text-predicates","title":"TinkerPop Text Predicates","text":"<p>It is also possible to use the TinkerPop text predicates with JanusGraph, but these predicates do not make use of indices which means that they require filtering in memory which can be very costly.</p> <pre><code>import static org.apache.tinkerpop.gremlin.process.traversal.TextP.*;\ng.V().has('bookname', startingWith('uni'))\ng.V().has('bookname', endingWith('corn'))\ng.V().has('bookname', containing('nico'))\n</code></pre>"},{"location":"index-backend/text-search/#geo-mapping","title":"Geo Mapping","text":"<p>By default, JanusGraph supports indexing geo properties with point type and querying geo properties by circle or box. To index a non-point geo property with support for querying by any geoshape type, specify the mapping as <code>Mapping.BKD</code> (preferred for ElasticSearch) or <code>Mapping.PREFIX_TREE</code> (Supported in Solr, Lucene, and ElasticSearch 6.  Deprecated in ElasticSearch 7. Support was removed in ElasticSearch 8):</p> <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('border').dataType(Geoshape.class).make()\nmgmt.buildIndex('borderIndex', Vertex.class).addKey(name, Mapping.BKD.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>When <code>Mapping.PREFIX_TREE</code> is used it is possible to specify additional  parameters to tune the configuration of the underlying prefix tree mapping.  These optional parameters include the number of levels used in the prefix  tree as well as the associated precision.</p> <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('border').dataType(Geoshape.class).make()\nmgmt.buildIndex('borderIndex', Vertex.class).addKey(name, Mapping.PREFIX_TREE.asParameter(), Parameter.of(\"index-geo-max-levels\", 18), Parameter.of(\"index-geo-dist-error-pct\", 0.0125)).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>Note that some indexing backends (e.g. Solr) may require additional external schema configuration to support and tune indexing non-point properties.</p> <p>Info</p> <p><code>Mapping.BKD</code> mapping which is preferred for ElasticSearch doesn't support Circle shape.  Instead of indexing a Circle as it was with <code>Mapping.PREFIX_TREE</code> there are Circle  processors available which transform Circle into other shapes and index those shapes instead.  Depending on the precision configuration used, search for such circles may behave differently.  The higher the precision the more points is used during indexing which may affect storage size and index time. See <code>index.[X].bkd-circle-processor</code> configuration properties for more information about BKD circle processors.</p>"},{"location":"operations/batch-processing/","title":"Batch Processing","text":"<p>In order to answer queries, JanusGraph has to perform queries against the storage backend. In general, there are two ways of doing this:</p> <ul> <li>Once data from the backend is needed, execute a backend   query and continue with the result.</li> <li>Maintain a list of what data is needed.   Once the list reaches a certain size, execute a batched   backend query to fetch all of it at once.</li> </ul> <p>The first option tends to be more responsive and consume less memory because the query can emit the first results very early without waiting for larger batches of queries to complete. It however sends many small queries to the storage backend for traversals that traverse a high number of vertices which leads to poor performance. That is why JanusGraph uses batch processing by default. Both of these options are described in greater detail below, including information about configuring batch processing.</p> <p>Note</p> <p>The default setting was changed in version 1.0.0. Older versions of JanusGraph used no batch processing (first option) by default.</p>"},{"location":"operations/batch-processing/#no-batch-processing","title":"No Batch Processing","text":"<p>In terms of graph traversals, the execution of queries is loosely coupled to the principle of Depth-First-Search.</p>"},{"location":"operations/batch-processing/#use-this-configuration-in-use-cases-where-for-example","title":"Use this configuration in use cases where for example ...","text":"<ul> <li>... each query only accesses few vertices of the graph.</li> <li>... your application does not need the full result set   immediately but rather requires a low latency for the first   results to arrive.</li> </ul>"},{"location":"operations/batch-processing/#possible-limitations","title":"Possible limitations","text":"<ul> <li>Traversing large neighborhoods can make the query slow.</li> </ul>"},{"location":"operations/batch-processing/#steps-to-explicitly-configure-this-option","title":"Steps to explicitly configure this option:","text":"<ul> <li>Ensure <code>query.batch.enabled</code> is set to <code>false</code></li> </ul>"},{"location":"operations/batch-processing/#unrestricted-batch-processing","title":"Unrestricted Batch Processing","text":"<p>Using this configuration, each step which traverses the Graph starting from a vertex (so e.g. <code>in()</code>, <code>outE()</code> and <code>values()</code> but not <code>inV()</code> or <code>otherV()</code> and also not <code>valueMap()</code>, see #2444) becomes a blocking operator which means that it produces no results until all the results of the previous step are known. Only then, a single backend query is executed and the results are passed to the next step. Manual <code>barrier()</code> steps do not affect this in any meaningful way. This way of execution can be thought of as a Breadth-First-Search.</p>"},{"location":"operations/batch-processing/#use-this-configuration-in-use-cases-where-for-example_1","title":"Use this configuration in use cases where for example ...","text":"<ul> <li>... your queries are likely to access multiple vertices in   each step.</li> <li>... there is a significant network latency between JanusGraph   and the storage backend.</li> </ul>"},{"location":"operations/batch-processing/#possible-limitations_1","title":"Possible limitations","text":"<ul> <li>Increased memory consumption</li> <li>If limit steps occur late in the query, there might be an   unnecessary overhead produced by the steps before the limit   step.</li> <li>Performing very large backend queries could stress the   storage backend.</li> </ul>"},{"location":"operations/batch-processing/#steps-to-explicitly-configure-this-option_1","title":"Steps to explicitly configure this option:","text":"<ul> <li>Ensure <code>query.batch.enabled</code> is set to <code>true</code></li> <li>Ensure <code>query.batch.limited</code> is set to <code>false</code></li> </ul>"},{"location":"operations/batch-processing/#limited-batch-processing","title":"Limited Batch Processing","text":"<p>Using this configuration, each step which traverses the Graph starting from a vertex (so e.g. <code>in()</code>, <code>outE()</code> and <code>values()</code> but not <code>inV()</code> or <code>otherV()</code>) aggregates a number of vertices first, before executing a batched backend query. This aggregation phase and backend query phase will repeat until all vertices are processed. In contrast to unrestricted batch processing where one batch corresponds to one step in the query, this approach can construct multiple batches per step.</p> <p>This is the default configuration of JanusGraph since version 1.0.0.</p>"},{"location":"operations/batch-processing/#configuring-the-batch-size","title":"Configuring the batch size","text":"<p>Although batch size does not necessarily need to be configured, it can provide an additional tuning parameter to improve the performance of a query. By default, the batch size for TinkerPop's barrier step will be provided by <code>LazyBarrierStrategy</code>, which is currently at <code>2500</code>.  For batchable cases where <code>LazyBarrierStrategy</code> doesn't inject any <code>barrier</code> steps, the barrier step will be ingected with the size configured via <code>query.batch.limited-size</code>  (which defaults to <code>2500</code>, same as with <code>LazyBarrierStrategy</code>). The batch size of each vertex step can be individually configured by prepending a <code>barrier(&lt;size&gt;)</code> step. For example, in the query below, the first <code>out()</code> step would use the default batch size of 2500 and the second <code>out()</code> step would use a manually configured batch size of 1234: <pre><code>g.V(list_of_vertices).out().barrier(1234).out()\n</code></pre> Using the same mechanism, the limit can also be increased or even effectively disabled by configuring an arbitrarily high value.</p> <p>For local traversals which start with a vertex step, the limit is best configured outside the local traversal, as seen below: <pre><code>g.V(list_of_vertices).out().barrier(1234).where(__.out())\n</code></pre> The reason this is necessary is that traversers enter local traversals one by one. As part of the local traversal, the <code>barrier(1234)</code> step would not be allowed to aggregate multiple traversers.</p> <p>A special case applies to <code>repeat()</code> steps. Because the local traversal of a <code>repeat()</code> step has two inputs (first, the step before the <code>repeat()</code> step and second, the last step of the repeated traversal, which feeds the result back to the beginning), two limits can be configured here. <pre><code>g.V(list_of_vertices).barrier(1234).repeat(__.barrier(2345).out()).times(5)\n</code></pre> Because the local traversal's output is also the input for the next iteration, the <code>barrier(1234)</code> step in front of the local traversal can only aggregate traversers once they enter the <code>repeat</code> step for the first time. For each iteration, the inner <code>barrier(2345)</code> is used to aggregate traversers from the previous iteration.</p>"},{"location":"operations/batch-processing/#use-this-configuration-in-use-cases-where-for-example_2","title":"Use this configuration in use cases where for example ...","text":"<ul> <li>... you have a mixture of traversals that traverse a high number of vertices   and traversals that only access few vertices of the graph.</li> </ul>"},{"location":"operations/batch-processing/#possible-limitations_2","title":"Possible limitations","text":"<ul> <li>Increased memory consumption (compared to no batch processing)</li> <li>The performance of queries depends on the configured batch   size.   If you use this configuration, make sure that the   latency and throughput of your queries meet your   requirements and if not, tweak the batch size accordingly.</li> </ul>"},{"location":"operations/batch-processing/#steps-to-explicitly-configure-this-option_2","title":"Steps to explicitly configure this option:","text":"<ul> <li>Ensure <code>query.batch.enabled</code> is set to <code>true</code></li> <li>Ensure <code>query.batch.limited</code> is set to <code>true</code></li> </ul>"},{"location":"operations/batch-processing/#batched-query-processing-flow","title":"Batched Query Processing Flow","text":"<p>Whenever <code>query.batch.enabled</code> is set to <code>true</code> steps compatible with batch processing are going to be  executed in batched fashion. Each storage backend may differently execute such batches, but usually  it means requesting data in parallel for multiple vertices which usually improves query performance  when the query is accessing many vertices.  </p> <p>Batched query processing takes into account two types of steps:</p> <ol> <li>Batch compatible step. This is the step which will execute batch requests. Currently, the list of such steps     is the next: <code>out()</code>, <code>in()</code>, <code>both()</code>, <code>inE()</code>, <code>outE()</code>, <code>bothE()</code>, <code>has()</code>, <code>values()</code>, <code>properties()</code>, <code>valueMap()</code>,     <code>propertyMap()</code>, <code>elementMap()</code>, <code>label()</code>, <code>drop()</code>.</li> <li>Parent step. This is a parent step which has local traversals with the same start. Such parent steps also implement the      interface <code>TraversalParent</code>. There are many such steps, but as for an example those could be: <code>and(...)</code>, <code>or(...)</code>,      <code>not(...)</code>, <code>order().by(...)</code>, <code>project(\"valueA\", \"valueB\", \"valueC\").by(...).by(...).by(...)</code>, <code>union(..., ..., ...)</code>,     <code>choose(..., ..., ...)</code>, <code>coalesce(..., ...)</code>, <code>where(...)</code>, etc. Start of such local steps should be the same, thus,     the only exception currently are steps <code>repeat()</code> and <code>match()</code> (see below on how they are processed).  </li> </ol> <p>Parent steps register their vertices for later processing with the batch compatible start step. For example, <pre><code>g.V(v1, v2, v3).union(out(\"knows\"), in(\"follows\"))\n</code></pre> In the example above vertices <code>v1</code>, <code>v2</code>, and <code>v3</code> will be registered with <code>out(\"knows\")</code> and <code>in(\"follows\")</code>  steps for batch processing because their parent step (<code>union</code>) registers any input with the batch compatible child start  steps.   Moreover, parent steps can register vertices for batch processing even with deep nested batch compatible start steps.  For example, <pre><code>g.V(v1, v2, v3).\n    and(\n        union(out(\"edge1\"), in(\"edge2\")),\n        or(\n            union(out(\"edge3\"), in(\"edge4\").optional(out(\"edge5\"))),\n            optional(out(\"edge6\")).in(\"edge7\")))\n</code></pre> In the example above vertices <code>v1</code>, <code>v2</code>, and <code>v3</code> will be registered with <code>out(\"edge1\")</code>, <code>in(\"edge2\")</code>, <code>out(\"edge3\")</code>,  <code>in(\"edge4\")</code>, and <code>out(\"edge6\")</code> steps for batch processing because they all can be considered as starts of the most root parent step (<code>and</code> step). That said, those vertices won't be registered for batch processing with steps <code>out(\"edge5\")</code> or <code>in(\"edge7\")</code> because those steps  are either not starting steps or starting steps of other parent steps. As such, <code>out(\"edge5\")</code> will be registered with  any vertex returned from <code>in(\"edge4\")</code> step, and <code>in(\"edge7\")</code> will be registered with any vertex returned from <code>optional(out(\"edge6\"))</code> step.  </p>"},{"location":"operations/batch-processing/#batch-processing-for-repeat-step","title":"Batch processing for <code>repeat</code> step","text":"<p>Repeat step doesn't follow the rules of other parent steps and registers vertices to child steps differently.  Currently, TinkerPop's default implementation is using Breadth-First Search instead of Depth-Fist Search (as used for other steps). </p> <p>JanusGraph applies repeat step vertices to the start of local <code>repeat</code> step, start of local <code>emit</code> step in case it is  placed before <code>repeat</code> step, and start of local <code>until</code> step in case it is placed before <code>repeat</code> step.  Moreover, for any next iteration JanusGraph applies result of the local <code>repeat</code> step (end step) to the beginning of the  local <code>repeat</code> step (start step) as well as start steps of <code>emit</code> and <code>until</code> traversals. </p>"},{"location":"operations/batch-processing/#use-cases-for-batch-requests-per-level-loop","title":"Use-cases for batch requests per level (<code>loop</code>):","text":"<ol> <li> <p>Simple example.     <pre><code>g.V(v1, v2, v3).repeat(out(\"knows\")).emit()\n</code></pre>     In the above example vertices <code>v1</code>, <code>v2</code>, and <code>v3</code> will be registered with <code>out(\"knows\")</code> step because it's the start      step with batch support. Moreover, the result of all iterations on the same level (<code>loop</code>) of <code>out(\"knows\")</code> will be registered      back to <code>out(\"knows\")</code> for the next level (<code>loop</code>) iterations and so on until <code>out(\"knows\")</code> stops emitting any results. </p> </li> <li> <p>Example with custom <code>emit</code> traversal after <code>repeat</code>.     <pre><code>g.V(v1, v2, v3).repeat(out(\"knows\")).emit(out(\"follows\"))\n</code></pre>     The above example's vertices registration flow is the same as in example <code>1</code>, but the difference is that <code>out(\"follows\")</code>      will receive vertices for registration from <code>out(\"knows\")</code> the same way as <code>out(\"knows\")</code> step itself receives vertices      for registration from itself. Notice, the same logic would apply for <code>until</code> step if it were <code>until</code> instead of <code>emit</code> here.</p> </li> <li> <p>Example with custom <code>emit</code> traversal before <code>repeat</code>.     <pre><code>g.V(v1, v2, v3).emit(out(\"follows\")).repeat(out(\"knows\"))\n</code></pre>     The above example's vertices registration flow is the same as in example <code>2</code>, but the difference is that <code>out(\"follows\")</code>     will receive vertices for registration both from <code>out(\"knows\")</code> and the start vertices <code>v1</code>, <code>v2</code>, <code>v3</code>. In other words,     vertices registration sources for <code>out(\"knows\")</code> and <code>out(\"follows\")</code> are the same in this case. Notice, the same logic      would apply for <code>until</code> step if it were <code>until</code> instead of <code>emit</code> here.</p> </li> <li> <p>Example with custom <code>emit</code> and <code>until</code> traversals before <code>repeat</code>.     <pre><code>g.V(v1, v2, v3).emit(out(\"follows\")).until(out(\"feeds\")).repeat(out(\"knows\"))\n</code></pre>     In the above example all 3 steps <code>out(\"follows\")</code>, <code>out(\"feeds\")</code>, and <code>out(\"knows\")</code> have the same vertices      registration flow where they receive vertices both from query start (<code>v1</code>, <code>v2</code>, <code>v3</code>) and from local repeat end      step (<code>out(\"knows\")</code>).</p> </li> <li> <p>Example with custom <code>emit</code> and <code>until</code> traversals after <code>repeat</code>.     <pre><code>g.V(v1, v2, v3).repeat(out(\"knows\")).emit(out(\"follows\")).until(out(\"feeds\"))\n</code></pre>     The above example's vertices registration flow is the same as in example <code>4</code>, but the difference is that      <code>out(\"follows\")</code> and <code>out(\"feeds\")</code> won't receive vertices registration from the query start (<code>v1</code>, <code>v2</code>, <code>v3</code>).</p> </li> <li> <p>Example with custom <code>until</code> traversal before <code>repeat</code> and <code>emit(true)</code> after <code>repeat</code>.     <pre><code>g.V(v1, v2, v3).until(out(\"feeds\")).repeat(out(\"knows\")).emit()\n</code></pre>     The above example's vertices registration flow is the same as in example <code>4</code>, except that the <code>emit</code> traversal      doesn't have any start step which supports batching. Thus, <code>emit</code> traversal doesn't receive batched vertices registration. </p> </li> </ol>"},{"location":"operations/batch-processing/#use-cases-for-batch-requests-per-iteration","title":"Use-cases for batch requests per iteration:","text":"<p>In most cases (like the above examples <code>1 - 6</code> and other cases) TinkerPop's default <code>repeat</code> step implementation executes  the local <code>repeat</code> traversal for the whole level (<code>loop</code>) before <code>emit</code> or <code>until</code> is executed. In other words <code>repeat</code> traversal  is executed multiple times (multiple iterations on the same <code>loop</code>) before <code>emit</code> or <code>until</code> first execution on the current <code>loop</code>.  This gives JanusGraph possibility to make larger batch requests containing vertices from multiple <code>repeat</code> traversal iterations  which efficiently executes batch requests.   That said, there are 3 use-cases when the execution flow is different and TinkerPop executes <code>until</code> or <code>emit</code> traversals after each  <code>repeat</code> traversal iteration. In such case <code>until</code> or <code>emit</code> steps will execute batch requests for vertices collected on the  current iteration only and not for vertices collected from all iterations of the same level (<code>loop</code>).    </p> <pre><code>g.V(v1, v2, v3).emit().repeat(out(\"knows\")).until(out(\"feeds\"))\ng.V(v1, v2, v3).emit(out(\"follows\")).repeat(out(\"knows\")).until(out(\"feeds\"))\ng.V(v1, v2, v3).until(out(\"feeds\")).repeat(out(\"knows\")).emit(out(\"follows\"))\n</code></pre> <p>The above 3 examples show the pattern when <code>until</code> or <code>emit</code> executes batches per iteration instead of per level.  In case any <code>emit</code> step is placed before <code>repeat</code> step while <code>until</code> step is placed after <code>repeat</code> step. In case  <code>until</code> step is placed before <code>repeat</code> step while non-true <code>emit</code> step is placed after <code>repeat</code> step. In all other cases <code>repeat</code> step will be executed for the whole <code>loop</code> and only after that <code>emit</code> or <code>until</code> will be executed.</p> <p>These limitations might be resolved after JanusGraph adds support for DFS repeat step execution  (see issue #3787).</p>"},{"location":"operations/batch-processing/#multi-nested-repeat-step-modes","title":"Multi-nested <code>repeat</code> step modes:","text":"<p>By default, in cases when batch start steps have multiple <code>repeat</code> step parents the batch registration is considering all <code>repeat</code>  parent steps. However, in cases when transaction cache is small and repeat step traverses more than one level deep, it could result for some vertices to be re-fetched again or vertices which don't need to be fetched due to early  cycle end could potentially be fetched into the transaction cache. It would mean a waste of operation when it isn't necessary.  </p> <p>Thus, JanusGraph provides a configuration option <code>query.batch.repeat-step-mode</code> to control multi-repeat step behaviour:</p> <ul> <li><code>closest_repeat_parent</code> (default option) - consider the closest <code>repeat</code> step only.    <pre><code>g.V().repeat(and(repeat(out(\"knows\")).emit())).emit()\n</code></pre>    In the example above, <code>out(\"knows\")</code> will be receiving vertices for batching from the <code>and</code> step input for the first iterations     as well as the <code>out(\"knows\")</code> step output for the next iterations. </li> <li><code>all_repeat_parents</code> - consider registering vertices from the start and end of each <code>repeat</code> step parent.    <pre><code>g.V().repeat(and(repeat(out(\"knows\")).emit())).emit()\n</code></pre>    In the example above, <code>out(\"knows\")</code> will be receiving vertices for batching from the most outer <code>repeat</code> step input     (for the first iterations), the most outer <code>repeat</code> step output (which is <code>and</code> output) (for the first iterations),    the <code>and</code> step input (for the first iterations), and from the <code>out(\"knows\")</code> output (for the next iterations).</li> <li><code>starts_only_of_all_repeat_parents</code> - consider registering vertices from the start of each <code>repeat</code> step parent.     <pre><code>g.V().repeat(and(repeat(out(\"knows\")).emit())).emit()\n</code></pre>    In the example above, <code>out(\"knows\")</code> will be receiving vertices for batching from the most outer <code>repeat</code> step input    (for the first iterations), the <code>and</code> step input (for the first iterations), and from the <code>out(\"knows\")</code> output     (for the next iterations).</li> </ul>"},{"location":"operations/batch-processing/#batch-processing-for-match-step","title":"Batch processing for <code>match</code> step","text":"<p>Currently, JanusGraph supports vertices registration for batch processing inside individual local traversals of the <code>match</code>  step, but not between those local traversals. Also, JanusGraph doesn't register start of the <code>match</code> step with any  of the local traversals of the <code>match</code> step. Thus, performance for <code>match</code> step might be limited. This is a temporary  limitation until this feature is implemented (see issue #3788).</p>"},{"location":"operations/batch-processing/#batch-processing-for-properties","title":"Batch processing for properties","text":"<p>Some of the Gremlin steps with enabled optimization may prefetch vertex properties in batches.  As for now, JanusGraph uses slice queries to query part of the row data. A single-slice query contains  the start key and the end key to define a slice of data JanusGraph is interested in. As JanusGraph doesn't support multi-range slice queries right now it can either fetch a single property  in a single Slice query or all properties in a single slice query. Thus, users have to decide the tradeoff between  different properties fetching approaches and decide when they want to fetch all properties in a single slice query (which is usually faster but unnecessary properties might be fetched) or to fetch only requested properties in  separate slice query per each property (might be slightly slower but will fetch only the requested properties). </p> <p>See issue #3816 which will allow fetching only requested  properties via a single slice query.  </p> <p>See configuration option <code>query.fast-property</code> which may be used to pre-fetch all properties on a first singular property  access when direct vertex properties are requested (for example <code>vertex.properties(\"foo\")</code>). See configuration option <code>query.batch.has-step-mode</code> to control properties pre-fetching behaviour for <code>has</code> step. See configuration option <code>query.batch.properties-mode</code> to control properties pre-fetching behaviour for <code>values</code>,  <code>properties</code>, <code>valueMap</code>, <code>propertyMap</code>, and <code>elementMap</code> steps. See configuration option <code>query.batch.label-step-mode</code> to control labels pre-fetching behaviour for <code>label</code> step. See configuration option <code>query.batch.drop-step-mode</code> to control drop batching behaviour for <code>drop</code> step.  </p>"},{"location":"operations/bulk-loading/","title":"Bulk Loading","text":"<p>There are a number of configuration options and tools that make ingesting large amounts of graph data into JanusGraph more efficient. Such ingestion is referred to as bulk loading in contrast to the default transactional loading where small amounts of data are added through individual transactions.</p> <p>There are a number of use cases for bulk loading data into JanusGraph, including:</p> <ul> <li> <p>Introducing JanusGraph into an existing environment with existing     data and migrating or duplicating this data into a new JanusGraph     cluster.</p> </li> <li> <p>Using JanusGraph as an end point of an     ETL     process.</p> </li> <li> <p>Adding an existing or external graph datasets (e.g. publicly     available RDF datasets) to a running     JanusGraph cluster.</p> </li> <li> <p>Updating a JanusGraph graph with results from a graph analytics job.</p> </li> </ul> <p>This page describes configuration options and tools that make bulk loading more efficient in JanusGraph. Please observe the limitations and assumptions for each option carefully before proceeding to avoid data loss or data corruption.</p> <p>This documentation focuses on JanusGraph specific optimization. In addition, consider improving the chosen storage backend and (optional) index backend for high write performance. Please refer to the documentation of the respective backend for more information.</p>"},{"location":"operations/bulk-loading/#configuration-options","title":"Configuration Options","text":""},{"location":"operations/bulk-loading/#batch-loading","title":"Batch Loading","text":"<p>Enabling the <code>storage.batch-loading</code> configuration option will have the biggest positive impact on bulk loading times for most applications. Enabling batch loading disables JanusGraph internal consistency checks in a number of places. Most importantly, it disables locking. In other words, JanusGraph assumes that the data to be loaded into JanusGraph is consistent with the graph and hence disables its own checks in the interest of performance.</p> <p>In many bulk loading scenarios it is significantly cheaper to ensure data consistency prior to loading the data than ensuring data consistency while loading it into the database. The <code>storage.batch-loading</code> configuration option exists because of this observation.</p> <p>For example, consider the use case of bulk loading existing user profiles into JanusGraph. Furthermore, assume that the username property key has a unique composite index defined on it, i.e. usernames must be unique across the entire graph. If the user profiles are imported from another database, username uniqueness might already guaranteed. If not, it is simple to sort the profiles by name and filter out duplicates or writing a Hadoop job that does such filtering. Now, we can enable <code>storage.batch-loading</code> which significantly reduces the bulk loading time because JanusGraph does not have to check for every added user whether the name already exists in the database.</p> <p>Important: Enabling <code>storage.batch-loading</code> requires the user to ensure that the loaded data is internally consistent and consistent with any data already in the graph. In particular, concurrent type creation can lead to severe data integrity issues when batch loading is enabled. Hence, we strongly encourage disabling automatic type creation by setting <code>schema.default = none</code> in the graph configuration.</p>"},{"location":"operations/bulk-loading/#optimizing-id-allocation","title":"Optimizing ID Allocation","text":""},{"location":"operations/bulk-loading/#id-block-size","title":"ID Block Size","text":"<p>Each newly added vertex or edge is assigned a unique id. JanusGraph\u2019s id pool manager acquires ids in blocks for a particular JanusGraph instance. The id block acquisition process is expensive because it needs to guarantee globally unique assignment of blocks. Increasing <code>ids.block-size</code> reduces the number of acquisitions but potentially leaves many ids unassigned and hence wasted. For transactional workloads the default block size is reasonable, but during bulk loading vertices and edges are added much more frequently and in rapid succession. Hence, it is generally advisable to increase the block size by a factor of 10 or more depending on the number of vertices to be added per machine.</p> <p>Rule of thumb: Set <code>ids.block-size</code> to the number of vertices you expect to add per JanusGraph instance per hour.</p> <p>Important: All JanusGraph instances MUST be configured with the same value for <code>ids.block-size</code> to ensure proper id allocation. Hence, be careful to shut down all JanusGraph instances prior to changing this value.</p>"},{"location":"operations/bulk-loading/#id-acquisition-process","title":"ID Acquisition Process","text":"<p>When id blocks are frequently allocated by many JanusGraph instances in parallel, allocation conflicts between instances will inevitably arise and slow down the allocation process. In addition, the increased write load due to bulk loading may further slow down the process to the point where JanusGraph considers it failed and throws an exception. There are three configuration options that can be tuned to avoid this.</p> <p>1) <code>ids.authority.wait-time</code> configures the time in milliseconds the id pool manager waits for an id block application to be acknowledged by the storage backend. The shorter this time, the more likely it is that an application will fail on a congested storage cluster.</p> <p>Rule of thumb: Set this to the sum of the 95th percentile read and write times measured on the storage backend cluster under load. Important: This value should be the same across all JanusGraph instances.</p> <p>2) <code>ids.renew-timeout</code> configures the number of milliseconds JanusGraph\u2019s id pool manager will wait in total while attempting to acquire a new id block before failing.</p> <p>Rule of thumb: Set this value to be as large feasible to not have to wait too long for unrecoverable failures. The only downside of increasing it is that JanusGraph will try for a long time on an unavailable storage backend cluster.</p>"},{"location":"operations/bulk-loading/#optimizing-writes-and-reads","title":"Optimizing Writes and Reads","text":""},{"location":"operations/bulk-loading/#buffer-size","title":"Buffer Size","text":"<p>JanusGraph buffers writes and executes them in small batches to reduce the number of requests against the storage backend. The size of these batches is controlled by <code>storage.buffer-size</code>. When executing a lot of writes in a short period of time, it is possible that the storage backend can become overloaded with write requests. In that case, increasing <code>storage.buffer-size</code> can avoid failure by increasing the number of writes per request and thereby lowering the number of requests.</p> <p>However, increasing the buffer size increases the latency of the write request and its likelihood of failure. Hence, it is not advisable to increase this setting for transactional loads and one should carefully experiment with this setting during bulk loading.</p>"},{"location":"operations/bulk-loading/#read-and-write-robustness","title":"Read and Write Robustness","text":"<p>During bulk loading, the load on the cluster typically increases making it more likely for read and write operations to fail (in particular if the buffer size is increased as described above). <code>storage.read-attempts</code> and <code>storage.write-attempts</code> configure how many times JanusGraph will attempt to execute a read or write operation against the storage backend before giving up. If it is expected that there is a high load on the backend during bulk loading, it is generally advisable to increase these configuration options.</p> <p><code>storage.attempt-wait</code> specifies the number of milliseconds that JanusGraph will wait before re-attempting a failed backend operation. A higher value can ensure that operation re-tries do not further increase the load on the backend.</p>"},{"location":"operations/bulk-loading/#strategies","title":"Strategies","text":""},{"location":"operations/bulk-loading/#parallelizing-the-load","title":"Parallelizing the Load","text":"<p>By parallelizing the bulk loading across multiple machines, the load time can be greatly reduced if JanusGraph\u2019s storage backend cluster is large enough to serve the additional requests. This is essentially the approach JanusGraph with TinkerPop\u2019s Hadoop-Gremlin takes to bulk loading data into JanusGraph using MapReduce.</p> <p>If Hadoop cannot be used for parallelizing the bulk loading process, here are some high level guidelines for effectively parallelizing the loading process:</p> <ul> <li> <p>In some cases, the graph data can be decomposed into multiple     disconnected subgraphs. Those subgraphs can be loaded independently     in parallel across multiple machines (for instance, using BatchGraph     as described above).</p> </li> <li> <p>If the graph cannot be decomposed, it is often beneficial to load in     multiple steps where the last two steps can be parallelized across     multiple machines:</p> <ol> <li> <p>Make sure the vertex and edge data sets are de-duplicated and     consistent.</p> </li> <li> <p>Set <code>batch-loading=true</code>. Possibly optimize additional     configuration settings described above.</p> </li> <li> <p>Add all the vertices with their properties to the graph (but no     edges). Maintain a (distributed) map from vertex id (as defined     by the loaded data) to JanusGraph\u2019s internal vertex id (i.e.     <code>vertex.getId()</code>) which is a 64 bit long id.</p> </li> <li> <p>Add all the edges using the map to look-up JanusGraph\u2019s vertex     id and retrieving the vertices using that id.</p> </li> </ol> </li> </ul>"},{"location":"operations/bulk-loading/#qa","title":"Q&amp;A","text":"<ul> <li>What should I do to avoid the following exception during     batch-loading: <code>java.io.IOException: ID renewal thread on partition [X] did not complete in time.</code>?     This exception is mostly likely caused by repeated time-outs during     the id allocation phase due to highly stressed storage backend.     Refer to the section on ID Allocation Optimization above.</li> </ul>"},{"location":"operations/cache/","title":"JanusGraph Cache","text":""},{"location":"operations/cache/#caching","title":"Caching","text":"<p>JanusGraph employs multiple layers of data caching to facilitate fast graph traversals. The caching layers are listed here in the order they are accessed from within a JanusGraph transaction. The closer the cache is to the transaction, the faster the cache access and the higher the memory footprint and maintenance overhead.</p>"},{"location":"operations/cache/#transaction-level-caching","title":"Transaction-Level Caching","text":"<p>Within an open transaction, JanusGraph maintains two caches:</p> <ul> <li> <p>Vertex Cache: Caches accessed vertices and their adjacency list (or     subsets thereof) so that subsequent access is significantly faster     within the same transaction. Hence, this cache speeds up iterative     traversals.</p> </li> <li> <p>Index Cache: Caches the results for index queries so that subsequent     index calls can be served from memory instead of calling the index     backend and (usually) waiting for one or more network round trips.</p> </li> </ul> <p>The size of both of those is determined by the transaction cache size. The transaction cache size can be configured via <code>cache.tx-cache-size</code> or on a per transaction basis by opening a transaction via the transaction builder <code>graph.buildTransaction()</code> and using the <code>setVertexCacheSize(int)</code> method.</p>"},{"location":"operations/cache/#vertex-cache","title":"Vertex Cache","text":"<p>The vertex cache contains vertices and the subset of their adjacency list (properties and edges) that has been retrieved in a particular transaction. The maximum number of vertices maintained in this cache is equal to the transaction cache size. If the transaction workload is an iterative traversal, the vertex cache will significantly speed it up. If the same vertex is not accessed again in the transaction, the transaction level cache will make no difference.</p> <p>Note, that the size of the vertex cache on heap is not only determined by the number of vertices it may hold but also by the size of their adjacency list. In other words, vertices with large adjacency lists (i.e. many incident edges) will consume more space in this cache than those with smaller lists.</p> <p>Furthermore note, that modified vertices are pinned in the cache, which means they cannot be evicted since that would entail loosing their changes. Therefore, transaction which contain a lot of modifications may end up with a larger than configured vertex cache.</p> <p>Assuming your vertex is not evicted from cache, or it is evicted from cache but your program context still holds the reference to the vertex, then its properties and edges are cached together with the vertex. This means once a property is queried, any subsequent reads will hit the cache. In case you want to force JanusGraph to read from the data storage again (provided you have disabled database-level cache), or you simply want to save memory, you could clear the cache of that vertex manually. Note this operation is not gremlin compliant, so you need to cast your vertex into CacheVertex type to do the refresh:</p> <pre><code>// first read automatically caches the property together with v\nv.property(\"prop\").value();\n// force refresh to clear the cache\n((CacheVertex) v).refresh();\n// now a subsequent read will look up in JanusGraph's database-level\n// cache, and then backend storage read in case of cache miss\nv.property(\"prop\").value();\n</code></pre> <p>Note that refresh operation cannot guarantee your current transaction reads the latest data from an eventual consistent backend. You should not attempt to achieve Compare-And-Set (CAS) via refresh operation, even though it might be helpful to detect conflicts among transactions in some cases.</p>"},{"location":"operations/cache/#index-cache","title":"Index Cache","text":"<p>The index cache contains the results of index queries executed in the context of this transaction. Subsequent identical index calls will be served from this cache and are therefore significantly cheaper. If the same index call never occurs twice in the same transaction, the index cache makes no difference.</p> <p>Each entry in the index cache is given a weight equal to <code>2 + result set size</code> and the total weight of the cache will not exceed half of the transaction cache size.</p>"},{"location":"operations/cache/#database-level-caching","title":"Database Level Caching","text":"<p>The database level cache contains vertices and the subset of their adjacency list (properties and edges) across multiple transactions and beyond the duration of a single transaction. The database level cache is shared by all transactions across a database. It is more space efficient than the transaction level caches but also slightly slower to access. In contrast to the transaction level caches, the database level caches do not expire immediately after closing a transaction. Hence, the database level cache significantly speeds up graph traversals for read heavy workloads across transactions. A read looks up in transaction-level cache first, and then database-level cache.</p> <p>Configuration Reference lists all of the configuration options that pertain to JanusGraph\u2019s database level cache. This page attempts to explain their usage.</p> <p>Most importantly, the database level cache is disabled by default in the current release version of JanusGraph. To enable it, set <code>cache.db-cache=true</code>.</p>"},{"location":"operations/cache/#cache-expiration-time","title":"Cache Expiration Time","text":"<p>The most important setting for performance and query behavior is the cache expiration time which is configured via <code>cache.db-cache-time</code>. The cache will hold graph elements for at most that many milliseconds. If an element expires, the data will be re-read from the storage backend on the next access.</p> <p>If there is only one JanusGraph instance accessing the storage backend or if this instance is the only one modifying the graph, the cache expiration can be set to 0 which disables cache expiration. This allows the cache to hold elements indefinitely (unless they are evicted due to space constraints or on update) which provides the best cache performance. Since no other JanusGraph instance is modifying the graph, there is no danger of holding on to stale data.</p> <p>If there are multiple JanusGraph instances accessing the storage backend, the time should be set to the maximum time that can be allowed between another JanusGraph instance modifying the graph and this JanusGraph instance seeing the data. If any change should be immediately visible to all JanusGraph instances, the database level cache should be disabled in a distributed setup. However, for most applications it is acceptable that a particular JanusGraph instance sees remote modifications with some delay. The larger the maximally allowed delay, the better the cache performance. Note, that a given JanusGraph instance will always immediately see its own modifications to the graph irrespective of the configured cache expiration time.</p>"},{"location":"operations/cache/#cache-size","title":"Cache Size","text":"<p>The configuration option <code>cache.db-cache-size</code> controls how much heap space JanusGraph\u2019s database level cache is allowed to consume. The larger the cache, the more effective it will be. However, large cache sizes can lead to excessive GC and poor performance.</p> <p>The cache size can be configured as a percentage (expressed as a decimal between 0 and 1) of the total heap space available to the JVM running JanusGraph or as an absolute number of bytes.</p> <p>Note, that the cache size refers to the amount of heap space that is exclusively occupied by the cache. JanusGraph\u2019s other data structures and each open transaction will occupy additional heap space. If additional software layers are running in the same JVM, those may occupy a significant amount of heap space as well (e.g. Gremlin Server, etc).  Be conservative in your heap memory estimation. Configuring a cache  that is too large can lead to out-of-memory exceptions and excessive GC.</p> <p>In practice, you might observe JanusGraph uses more memory than configured for database level cache. This is a known limitation due to difficulty of estimating size of deserialized objects.</p>"},{"location":"operations/cache/#clean-up-wait-time","title":"Clean Up Wait Time","text":"<p>When a vertex is locally modified (e.g. an edge is added) all of the vertex\u2019s related database level cache entries are marked as expired and eventually evicted. This will cause JanusGraph to refresh the vertex\u2019s data from the storage backend on the next access and re-populate the cache.</p> <p>However, when the storage backend is eventually consistent, the modifications that triggered the eviction may not yet be visible. By configuring <code>cache.db-cache-clean-wait</code>, the cache will wait for at least this many milliseconds before repopulating the cache with the entry retrieved from the storage backend.</p> <p>If JanusGraph runs locally or against a storage backend that guarantees immediate visibility of modifications, this value can be set to 0.</p>"},{"location":"operations/cache/#storage-backend-caching","title":"Storage Backend Caching","text":"<p>Each storage backend maintains its own data caching layer. These caches benefit from compression, data compactness, coordinated expiration and are often maintained off heap which means that large caches can be used without running into garbage collection issues. While these caches can be significantly larger than the database level cache, they are also slower to access.</p> <p>The exact type of caching and its properties depends on the particular storage backend. Please refer to the respective documentation for more information about the caching infrastructure and how to optimize it.</p>"},{"location":"operations/configured-graph-factory/","title":"ConfiguredGraphFactory","text":"<p>The JanusGraph Server can be configured to use the <code>ConfiguredGraphFactory</code>. The <code>ConfiguredGraphFactory</code> is an access point to your graphs, similar to the <code>JanusGraphFactory</code>. These graph factories provide methods for dynamically managing the graphs hosted on the server.</p>"},{"location":"operations/configured-graph-factory/#overview","title":"Overview","text":"<p><code>JanusGraphFactory</code> is a class that provides an access point to your graphs by providing a Configuration object each time you access the graph.</p> <p><code>ConfiguredGraphFactory</code> provides an access point to your graphs for which you have previously created configurations using the <code>ConfigurationManagementGraph</code>. It also offers an access point to manage graph configurations.</p> <p><code>ConfigurationManagementGraph</code> allows you to manage graph configurations.</p> <p><code>JanusGraphManager</code> is an internal server component that tracks graph references, provided your graphs are configured to use it.</p>"},{"location":"operations/configured-graph-factory/#configuredgraphfactory-versus-janusgraphfactory","title":"ConfiguredGraphFactory versus JanusGraphFactory","text":"<p>However, there is an important distinction between these two graph factories:</p> <ol> <li>The <code>ConfiguredGraphFactory</code> can only be used if you have configured     your server to use the <code>ConfigurationManagementGraph</code> APIs at server     start.</li> </ol> <p>The benefits of using the <code>ConfiguredGraphFactory</code> are that:</p> <ol> <li> <p>You only need to supply a <code>String</code> to access your graphs, as opposed     to the <code>JanusGraphFactory</code>-- which requires you to specify     information about the backend you wish to use when accessing a     graph-- every time you open a graph.</p> </li> <li> <p>If your ConfigurationManagementGraph is configured with a     distributed storage backend then your graph configurations are     available to all JanusGraph nodes in your cluster.</p> </li> </ol>"},{"location":"operations/configured-graph-factory/#how-does-the-configuredgraphfactory-work","title":"How Does the ConfiguredGraphFactory Work?","text":"<p>The <code>ConfiguredGraphFactory</code> provides an access point to graphs under two scenarios:</p> <ol> <li> <p>You have already created a configuration for your specific graph     object using the <code>ConfigurationManagementGraph#createConfiguration</code>.     In this scenario, your graph is opened using the previously created     configuration for this graph.</p> </li> <li> <p>You have already created a template configuration using the     <code>ConfigurationManagementGraph#createTemplateConfiguration</code>. In this     scenario, we create a configuration for the graph you are creating     by copying over all attributes stored in your template configuration     and appending the relevant graphName attribute, and we then open the     graph according to that specific configuration.</p> </li> </ol>"},{"location":"operations/configured-graph-factory/#accessing-the-graphs","title":"Accessing the Graphs","text":"<p>You can either use <code>ConfiguredGraphFactory.create(\"graphName\")</code> or <code>ConfiguredGraphFactory.open(\"graphName\")</code>. Learn more about the difference between these two options by reading the section below about the <code>ConfigurationManagementGraph</code>.</p> <p>You can also access your graphs by using the bindings. Read more about this in the Graph and Traversal Bindings section.</p> <p>Listing the Graphs</p> <p><code>ConfiguredGraphFactory.getGraphNames()</code> will return a set of graph names for which you have created configurations using the <code>ConfigurationManagementGraph</code> APIs.</p> <p><code>JanusGraphFactory.getGraphNames()</code> on the other hand returns a set of graph names for which you have instantiated and the references are stored inside the <code>JanusGraphManager</code>.</p>"},{"location":"operations/configured-graph-factory/#dropping-a-graph","title":"Dropping a Graph","text":"<p><code>ConfiguredGraphFactory.drop(\"graphName\")</code> will drop the graph database, deleting all data in storage and indexing backends. The graph can be open or closed (will be closed as part of the drop operation). Furthermore, this will also remove any existing graph configuration in the <code>ConfigurationManagementGraph</code>.</p> <p>Important</p> <p>This is an irreversible operation that will delete all graph and index data.</p> <p>Important</p> <p>To ensure all graph representations are consistent across all JanusGraph  nodes in your cluster, this removes the graph from the <code>JanusGraphManager</code>  graph cache on every node in the cluster, assuming each node has been  properly configured to use the <code>JanusGraphManager</code>. Learn more about this  feature and how to configure your server to use said feature here.</p>"},{"location":"operations/configured-graph-factory/#configuring-janusgraph-server-for-configuredgraphfactory","title":"Configuring JanusGraph Server for ConfiguredGraphFactory","text":"<p>Note</p> <p>Starting with JanusGraph v0.6.0, every JanusGraph Server is started with  the JanusGraphManager, if <code>ConfigurationManagementGraph</code>is configured in  the <code>graphs</code> section and the default GraphManager is not overwritten.</p> <p>To be able to use the <code>ConfiguredGraphFactory</code>, you must configure your server to use the <code>ConfigurationManagementGraph</code> APIs. To do this, you have to inject a graph variable named \"ConfigurationManagementGraph\" in your server\u2019s YAML\u2019s <code>graphs</code> map. For example:</p> <pre><code>graphManager: org.janusgraph.graphdb.management.JanusGraphManager\ngraphs: {\n  ConfigurationManagementGraph: conf/JanusGraph-configurationmanagement.properties\n}\n</code></pre> <p>In this example, our <code>ConfigurationManagementGraph</code> graph will be configured using the properties stored inside <code>conf/JanusGraph-configurationmanagement.properties</code>, which for example, look like:</p> <pre><code>gremlin.graph=org.janusgraph.core.ConfiguredGraphFactory\nstorage.backend=cql\ngraph.graphname=ConfigurationManagementGraph\nstorage.hostname=127.0.0.1\n</code></pre> <p>Assuming the GremlinServer started successfully and the <code>ConfigurationManagementGraph</code> was successfully instantiated, then all the APIs available on the <code>ConfigurationManagementGraph</code> <code>Singleton</code> will also act upon said graph. Furthermore, this is the graph that will be used to access the configurations used to create/open graphs using the <code>ConfiguredGraphFactory</code>.</p> <p>Important</p> <p>The <code>pom.xml</code> included in the JanusGraph distribution lists this dependency as optional, but the <code>ConfiguredGraphFactory</code> makes use of the <code>JanusGraphManager</code>, which requires a declared dependency on the <code>org.apache.tinkerpop:gremlin-server</code>. So if you run into <code>NoClassDefFoundError</code> errors, then be sure to update according to this message.</p>"},{"location":"operations/configured-graph-factory/#configurationmanagementgraph","title":"ConfigurationManagementGraph","text":"<p>The <code>ConfigurationManagementGraph</code> is a <code>Singleton</code> that allows you to create/update/remove configurations that you can use to access your graphs using the <code>ConfiguredGraphFactory</code>. See above on configuring your server to enable use of these APIs.</p> <p>Important</p> <p>The ConfiguredGraphFactory offers an access point to manage your graph configurations managed by the <code>ConfigurationManagementGraph</code>, so instead of acting upon the <code>Singleton</code> itself, you may act upon the corresponding <code>ConfiguredGraphFactory</code> static methods. For example, you may use <code>ConfiguredGraphFactory.removeTemplateConfiguration()</code> instead of <code>ConfiguredGraphFactory.getInstance().removeTemplateConfiguration()</code>.</p>"},{"location":"operations/configured-graph-factory/#graph-configurations","title":"Graph Configurations","text":"<p>The <code>ConfigurationManagementGraph</code> singleton allows you to create configurations used to open specific graphs, referenced by the <code>graph.graphname</code> property. For example:</p> <pre><code>map = new HashMap&lt;String, Object&gt;();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nmap.put(\"graph.graphname\", \"graph1\");\nConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));\n</code></pre> <p>Then you could access this graph on any JanusGraph node using: <pre><code>ConfiguredGraphFactory.open(\"graph1\");\n</code></pre></p>"},{"location":"operations/configured-graph-factory/#template-configuration","title":"Template Configuration","text":"<p>The <code>ConfigurationManagementGraph</code> also allows you to create one template configuration, which you can use to create many graphs using the same configuration template. For example: <pre><code>map = new HashMap&lt;String, Object&gt;();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nConfiguredGraphFactory.createTemplateConfiguration(new MapConfiguration(map));\n</code></pre></p> <p>After doing this, you can create graphs using the template configuration: <pre><code>ConfiguredGraphFactory.create(\"graph2\");\n</code></pre></p> <p>This method will first create a new configuration for \"graph2\" by copying over all the properties associated with the template configuration and storing it on a configuration for this specific graph. This means that this graph can be accessed in, on any JanusGraph node, in the future by doing: <pre><code>ConfiguredGraphFactory.open(\"graph2\");\n</code></pre></p>"},{"location":"operations/configured-graph-factory/#updating-configurations","title":"Updating Configurations","text":"<p>All interactions with both the <code>JanusGraphFactory</code> and the <code>ConfiguredGraphFactory</code> that interact with configurations that define the property <code>graph.graphname</code> go through the <code>JanusGraphManager</code> which keeps track of graph references created on the given JVM. Think of it as a graph cache. For this reason:</p> <p>Important</p> <p>Any updates to a graph configuration results in the eviction of the relevant graph from the graph cache on  every node in the JanusGraph cluster, assuming each node has been configured properly to use the <code>JanusGraphManager</code>.  Learn more about this feature and how to configure your server to use said feature here.</p> <p>Since graphs created using the template configuration first create a configuration for that graph in question using a copy and create method, this means that:</p> <p>Important</p> <p>Any updates to a specific graph created using the template configuration are not guaranteed to take effect on the specific graph until: 1. The relevant configuration is removed: <code>ConfiguredGraphFactory.removeConfiguration(\"graph2\");</code> 2. The graph is recreated using the template configuration: <code>ConfiguredGraphFactory.create(\"graph2\");</code></p>"},{"location":"operations/configured-graph-factory/#update-examples","title":"Update Examples","text":"<p>1) We migrated our Cassandra data to a new server with a new IP address:</p> <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nmap.put(\"graph.graphname\", \"graph1\");\nConfiguredGraphFactory.createConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n\n// Update configuration\nmap = new HashMap();\nmap.put(\"storage.hostname\", \"10.0.0.1\");\nConfiguredGraphFactory.updateConfiguration(\"graph1\",\nnew MapConfiguration(map));\n\n// We can verify the configuration was updated by\n// retrieving the configuration for this graph\nConfiguredGraphFactory.getConfiguration(\"graph1\");\n\n// We are now guaranteed to use the updated configuration\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n</code></pre> <p>2) We added an Elasticsearch node to our setup: <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nmap.put(\"graph.graphname\", \"graph1\");\nConfiguredGraphFactory.createConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n\n// Update configuration\nmap = new HashMap();\nmap.put(\"index.search.backend\", \"elasticsearch\");\nmap.put(\"index.search.hostname\", \"127.0.0.1\");\nmap.put(\"index.search.elasticsearch.transport-scheme\", \"http\");\nConfiguredGraphFactory.updateConfiguration(\"graph1\",\nnew MapConfiguration(map));\n\n// We can verify the configuration was updated by\n// retrieving the configuration for this graph\nConfiguredGraphFactory.getConfiguration(\"graph1\");\n\n// We are now guaranteed to use the updated configuration\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n</code></pre></p> <p>3) Update a graph configuration that was created using a template configuration that has been updated: <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nConfiguredGraphFactory.createTemplateConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.create(\"graph1\");\n\n// Update template configuration\nmap = new HashMap();\nmap.put(\"index.search.backend\", \"elasticsearch\");\nmap.put(\"index.search.hostname\", \"127.0.0.1\");\nmap.put(\"index.search.elasticsearch.transport-scheme\", \"http\");\nConfiguredGraphFactory.updateTemplateConfiguration(new\nMapConfiguration(map));\n\n// Remove Configuration\nConfiguredGraphFactory.removeConfiguration(\"graph1\");\n\n// Recreate\nConfiguredGraphFactory.create(\"graph1\");\n\n// Now this graph's configuration is guaranteed to be updated.\n// We can verify it by retrieving the configuration for this graph\nConfiguredGraphFactory.getConfiguration(\"graph1\");\n</code></pre></p>"},{"location":"operations/configured-graph-factory/#janusgraphmanager","title":"JanusGraphManager","text":"<p>The <code>JanusGraphManager</code> is a <code>Singleton</code> adhering to the TinkerPop graphManager specifications.</p> <p>In particular, the <code>JanusGraphManager</code> provides:</p> <ol> <li> <p>a coordinated mechanism by which to instantiate graph references on     a given JanusGraph node</p> </li> <li> <p>a graph reference tracker (or cache)</p> </li> </ol> <p>Any graph you create using the <code>graph.graphname</code> property will go through the <code>JanusGraphManager</code> and thus be instantiated in a coordinated fashion. The graph reference will also be placed in the graph cache on the JVM in question.</p> <p>Thus, any graph you open using the <code>graph.graphname</code> property that has already been instantiated on the JVM in question will be retrieved from the graph cache.</p> <p>This is why updates to your configurations require a few steps to guarantee correctness.</p>"},{"location":"operations/configured-graph-factory/#how-to-use-the-janusgraphmanager","title":"How To Use The JanusGraphManager","text":"<p>This is a new configuration option you can use when defining a property in your configuration that defines how to access a graph. All configurations that include this property will result in the graph instantiation happening through the <code>JanusGraphManager</code> (process explained above).</p> <p>For backwards compatibility, any graphs that do not supply this parameter but supplied at server start in your graphs object in your .yaml file, these graphs will be bound through the JanusGraphManager denoted by their <code>key</code> supplied for that graph. For example, if your .yaml graphs object looks like:</p> <pre><code>graphManager: org.janusgraph.graphdb.management.JanusGraphManager\ngraphs {\n  graph1: conf/graph1.properties,\n  graph2: conf/graph2.properties\n}\n</code></pre> <p>but <code>conf/graph1.properties</code> and <code>conf/graph2.properties</code> do not include the property <code>graph.graphname</code>, then these graphs will be stored in the JanusGraphManager and thus bound in your gremlin script executions as <code>graph1</code> and <code>graph2</code>, respectively.</p>"},{"location":"operations/configured-graph-factory/#automatic-separation-of-graphs-in-backends","title":"Automatic Separation of Graphs in Backends","text":"<p>For convenience, if your configuration used to open a graph specifies <code>graph.graphname</code>, but does not specify the backend\u2019s storage directory, table name, keyspace name, or index name, then the relevant parameter will automatically be set to the value of <code>graph.graphname</code>. However, if you supply one of those parameters, that value will always take precedence. And if you supply neither, they default to the configuration option\u2019s default value.</p> <p>One special case is <code>storage.root</code> configuration option. This is a new configuration option used to specify the base of the directory that will be used for any backend requiring local storage directory access. If you supply this parameter, you must also supply the <code>graph.graphname</code> property, and the absolute storage directory will be equal to the value of the <code>graph.graphname</code> property appended to the value of the <code>storage.root</code> property.</p> <p>Below are some example use cases:</p> <p>1) Create a template configuration for my Cassandra backend such that each graph created using this configuration gets a unique keyspace equivalent to the <code>String</code> &lt;graphName&gt; provided to the factory: <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nConfiguredGraphFactory.createTemplateConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.create(\"graph1\"); //keyspace === graph1\ng2 = ConfiguredGraphFactory.create(\"graph2\"); //keyspace === graph2\ng3 = ConfiguredGraphFactory.create(\"graph3\"); //keyspace === graph3\n</code></pre></p> <p>2) Create a template configuration for my BerkeleyJE backend such that each graph created using this configuration gets a unique storage directory equivalent to the \"&lt;storage.root&gt;/&lt;graph.graphname&gt;\": <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"berkeleyje\");\nmap.put(\"storage.root\", \"/data/graphs\");\nConfiguredGraphFactory.createTemplateConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.create(\"graph1\"); //storage directory === /data/graphs/graph1\ng2 = ConfiguredGraphFactory.create(\"graph2\"); //storage directory === /data/graphs/graph2\ng3 = ConfiguredGraphFactory.create(\"graph3\"); //storage directory === /data/graphs/graph3\n</code></pre></p>"},{"location":"operations/configured-graph-factory/#graph-and-traversal-bindings","title":"Graph and Traversal Bindings","text":"<p>Graphs created using the ConfiguredGraphFactory are bound to the executor context on the Gremlin Server by the \"graph.graphname\" property, and the graph's traversal reference is bound to the context by \"<code>&lt;graphname&gt;_traversal</code>\". This means, on subsequent connections to the server after the first time you create/open a graph, you can access the graph and traversal references by the \"<code>&lt;graphname&gt;</code>\" and \"<code>&lt;graphname&gt;_traversal</code>\" properties.</p> <p>Learn more about this feature and how to configure your server to use said feature here.</p> <p>Important</p> <p>If you are connected to a remote Gremlin Server using the Gremlin Console and a sessioned connection, then you will have to reconnect to the server to bind the variables. This is also true for any sessioned WebSocket connection.</p> <p>Important</p> <p>The JanusGraphManager rebinds every graph stored on the ConfigurationManagementGraph (or those for which you have created configurations) every 20 seconds. This means your graph and traversal bindings for graphs created using the ConfiguredGraphFactory will be available on all JanusGraph nodes with a maximum of a 20 second lag. It also means that a binding will still be available on a node after a server restart.</p>"},{"location":"operations/configured-graph-factory/#binding-example","title":"Binding Example","text":"<pre><code>gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Configured localhost/127.0.0.1:8182\ngremlin&gt; :remote console\n==&gt;All scripts will now be sent to Gremlin Server - [localhost/127.0.0.1:8182] - type ':remote console' to return to local mode\ngremlin&gt; ConfiguredGraphFactory.open(\"graph1\")\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; graph1\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; graph1_traversal\n==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard]\n</code></pre>"},{"location":"operations/configured-graph-factory/#examples","title":"Examples","text":"<p>It is recommended to use a sessioned connection when creating a Configured Graph Factory template. If a sessioned connection is not used the Configured Graph Factory Template creation must be sent to the server as a single line using semi-colons. See details on sessions can be found in Connecting to Gremlin Server.</p> <pre><code>gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml session\n==&gt;Configured localhost/127.0.0.1:8182\n\ngremlin&gt; :remote console\n==&gt;All scripts will now be sent to Gremlin Server - [localhost:8182]-[5206cdde-b231-41fa-9e6c-69feac0fe2b2] - type ':remote console' to return to local mode\n\ngremlin&gt; ConfiguredGraphFactory.open(\"graph\");\nPlease create configuration for this graph using the\nConfigurationManagementGraph API.\n\ngremlin&gt; ConfiguredGraphFactory.create(\"graph\");\nPlease create a template Configuration using the\nConfigurationManagementGraph API.\n\ngremlin&gt; map = new HashMap();\ngremlin&gt; map.put(\"storage.backend\", \"cql\");\ngremlin&gt; map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; map.put(\"GraphName\", \"graph1\");\ngremlin&gt; ConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));\nPlease include in your configuration the property \"graph.graphname\".\n\ngremlin&gt; map = new HashMap();\ngremlin&gt; map.put(\"storage.backend\", \"cql\");\ngremlin&gt; map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; map.put(\"graph.graphname\", \"graph1\");\ngremlin&gt; ConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));\n==&gt;null\n\ngremlin&gt; ConfiguredGraphFactory.open(\"graph1\").vertices();\n\ngremlin&gt; map = new HashMap(); map.put(\"storage.backend\",\n\"cql\"); map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; map.put(\"graph.graphname\", \"graph1\");\ngremlin&gt; ConfiguredGraphFactory.createTemplateConfiguration(new MapConfiguration(map));\nYour template configuration may not contain the property\n\"graph.graphname\".\n\ngremlin&gt; map = new HashMap();\ngremlin&gt; map.put(\"storage.backend\",\n\"cql\"); map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; ConfiguredGraphFactory.createTemplateConfiguration(new MapConfiguration(map));\n==&gt;null\n\n// Each graph is now acting in unique keyspaces equivalent to the\ngraphnames.\ngremlin&gt; g1 = ConfiguredGraphFactory.open(\"graph1\");\ngremlin&gt; g2 = ConfiguredGraphFactory.create(\"graph2\");\ngremlin&gt; g3 = ConfiguredGraphFactory.create(\"graph3\");\ngremlin&gt; g2.addVertex();\ngremlin&gt; l = [];\ngremlin&gt; l &lt;&lt; g1.vertices().size();\n==&gt;0\ngremlin&gt; l &lt;&lt; g2.vertices().size();\n==&gt;1\ngremlin&gt; l &lt;&lt; g3.vertices().size();\n==&gt;0\n\n// After a graph is created, you must access it using .open()\ngremlin&gt; g2 = ConfiguredGraphFactory.create(\"graph2\"); g2.vertices().size();\nConfiguration for graph \"graph2\" already exists.\n\ngremlin&gt; g2 = ConfiguredGraphFactory.open(\"graph2\"); g2.vertices().size();\n==&gt;1\n</code></pre>"},{"location":"operations/container/","title":"JanusGraph Container","text":"<p>Note</p> <p>even though the examples below and in the Docker Compose config files (<code>*.yml</code>) use the <code>latest</code> image, when running a service in production, be sure to specify a specific numeric version to avoid unexpected behavior changes due to <code>latest</code> pointing to a new release version, see our Docker tagging Policy.</p>"},{"location":"operations/container/#usage","title":"Usage","text":""},{"location":"operations/container/#start-a-janusgraph-server-instance","title":"Start a JanusGraph Server instance","text":"<p>The default configuration uses the Oracle Berkeley DB Java Edition storage backend and the Apache Lucene indexing backend:</p> <pre><code>docker run --rm --name janusgraph-default docker.io/janusgraph/janusgraph:latest\n</code></pre>"},{"location":"operations/container/#connecting-with-gremlin-console","title":"Connecting with Gremlin Console","text":"<p>Start a JanusGraph container and connect to the <code>janusgraph</code> server remotely using Gremlin Console:</p> <pre><code>$ docker run --rm --link janusgraph-default:janusgraph -e GREMLIN_REMOTE_HOSTS=janusgraph \\\n    -it docker.io/janusgraph/janusgraph:latest ./bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\nplugin activated: tinkerpop.server\nplugin activated: tinkerpop.utilities\nplugin activated: tinkerpop.hadoop\nplugin activated: tinkerpop.spark\nplugin activated: tinkerpop.tinkergraph\ngremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Configured janusgraph/172.17.0.2:8182\ngremlin&gt; :&gt; g.addV('person').property('name', 'chris')\n==&gt;v[4160]\ngremlin&gt; :&gt; g.V().values('name')\n==&gt;chris\n</code></pre>"},{"location":"operations/container/#using-docker-compose","title":"Using Docker Compose","text":"<p>Start a JanusGraph Server instance using <code>docker-compose.yml</code>:</p> <pre><code>docker-compose -f docker-compose.yml up\n</code></pre> <p>Start a JanusGraph container running Gremlin Console in the same network using <code>docker-compose.yml</code>:</p> <pre><code>docker-compose -f docker-compose.yml run --rm \\\n    -e GREMLIN_REMOTE_HOSTS=janusgraph janusgraph ./bin/gremlin.sh\n</code></pre>"},{"location":"operations/container/#initialization","title":"Initialization","text":"<p>When the container is started it will execute files with the extension <code>.groovy</code> that are found in <code>/docker-entrypoint-initdb.d</code> with the Gremlin Console. These scripts are only executed after the JanusGraph Server instance was started. So, they can connect to it and execute Gremlin traversals.</p> <p>For example, to add a vertex to the graph, create a file <code>/docker-entrypoint-initdb.d/add-vertex.groovy</code> with the following content:</p> <pre><code>g = traversal().withRemote('conf/remote-graph.properties')\ng.addV('demigod').property('name', 'hercules').iterate()\n</code></pre>"},{"location":"operations/container/#generate-config","title":"Generate Config","text":"<p>JanusGraph-Docker has a single utility method. This method writes the JanusGraph Configuration and show the config afterward.</p> <pre><code>docker run --rm -it docker.io/janusgraph/janusgraph:latest janusgraph show-config\n</code></pre> <p>Default config locations are <code>/etc/opt/janusgraph/janusgraph.properties</code> and <code>/etc/opt/janusgraph/janusgraph-server.yaml</code>.</p>"},{"location":"operations/container/#configuration","title":"Configuration","text":"<p>The JanusGraph image provides multiple methods for configuration, including using environment variables to set options and using bind-mounted configuration.</p>"},{"location":"operations/container/#docker-environment-variables","title":"Docker environment variables","text":"<p>The environment variables supported by the JanusGraph image are summarized below.</p> Variable Description Default <code>JANUS_PROPS_TEMPLATE</code> JanusGraph properties file template (see below). <code>berkeleyje-lucene</code> <code>janusgraph.*</code> Any JanusGraph configuration option to override in the template properties file, specified with an outer <code>janusgraph</code> namespace (e.g., <code>janusgraph.storage.hostname</code>). See JanusGraph Configuration for available options. no default value <code>gremlinserver.*</code> Any Gremlin Server configuration option to override in the default configuration (YAML) file, specified with an outer <code>gremlinserver</code> namespace (e.g., <code>gremlinserver.threadPoolWorker</code>). You can set or update nested options using additional dots (e.g., <code>gremlinserver.graphs.graph</code>). See Gremlin Server Configuration for available options. See Gremlin Server Environment Variable Syntax section below for help editing gremlin server configuration using environment variables. no default value` <code>JANUS_SERVER_TIMEOUT</code> Timeout (seconds) used when waiting for Gremlin Server before executing initialization scripts. <code>30</code> <code>JANUS_STORAGE_TIMEOUT</code> Timeout (seconds) used when waiting for the storage backend before starting Gremlin Server. <code>60</code> <code>GREMLIN_REMOTE_HOSTS</code> Optional hostname for external Gremlin Server instance. Enables a container running Gremlin Console to connect to a remote server using <code>conf/remote.yaml</code> (or <code>remote-objects.yaml</code>). no default value <code>JANUS_INITDB_DIR</code> Defines the location of the initialization scripts. <code>/docker-entrypoint-initdb.d</code>"},{"location":"operations/container/#properties-template","title":"Properties template","text":"<p>The <code>JANUS_PROPS_TEMPLATE</code> environment variable is used to define the base JanusGraph properties file. Values in the template properties file are used unless an alternate value for a given property is provided in the environment. The common usage will be to specify a template for the general environment (e.g., <code>cassandra-es</code>) and then provide additional individual configuration to override/extend the template. The available templates depend on the JanusGraph version (see <code>conf/janusgraph*.properties</code>).</p> <code>JANUS_PROPS_TEMPLATE</code> Supported Versions <code>berkeleyje</code> all <code>berkeleyje-es</code> all <code>berkeleyje-lucene</code> (default) all <code>cassandra-es</code> &lt;=0.5.3 <code>cql-es</code> &gt;=0.2.1 <code>cql</code> &gt;=0.5.3 <code>inmemory</code> &gt;=0.5.3"},{"location":"operations/container/#example-berkeleyje-lucene","title":"Example: Berkeleyje-Lucene","text":"<p>Start a JanusGraph instance using the default <code>berkeleyje-lucene</code> template with custom storage and server settings:</p> <pre><code>docker run --name janusgraph-default \\\n    -e janusgraph.storage.berkeleyje.cache-percentage=80 \\\n    -e gremlinserver.threadPoolWorker=2 \\\n    docker.io/janusgraph/janusgraph:latest\n</code></pre> <p>Inspect the configuration:</p> <pre><code>$ docker exec janusgraph-default sh -c 'cat /etc/opt/janusgraph/janusgraph.properties | grep ^[a-z]'\ngremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=berkeleyje\nstorage.directory=/var/lib/janusgraph/data\nindex.search.backend=lucene\nstorage.berkeleyje.cache-percentage=80\nindex.search.directory=/var/lib/janusgraph/index\n\n$ docker exec janusgraph-default grep threadPoolWorker /etc/opt/janusgraph/janusgraph-server.yaml\nthreadPoolWorker: 2\n</code></pre>"},{"location":"operations/container/#example-cassandra-es-with-docker-compose","title":"Example: Cassandra-ES with Docker Compose","text":"<p>Start a JanusGraph instance with Cassandra and Elasticsearch using the <code>cql-es</code> template through <code>docker-compose-cql-es.yml</code>:</p> <pre><code>docker-compose -f docker-compose-cql-es.yml up\n</code></pre> <p>Inspect the configuration using <code>docker-compose-cql-es.yml</code>:</p> <pre><code>$ docker-compose -f docker-compose-cql-es.yml exec \\\n      janusgraph sh -c 'cat /etc/opt/janusgraph/janusgraph.properties | grep ^[a-z]'\ngremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=cql\nstorage.hostname=jce-cassandra\ncache.db-cache = true\ncache.db-cache-clean-wait = 20\ncache.db-cache-time = 180000\ncache.db-cache-size = 0.25\nindex.search.backend=elasticsearch\nindex.search.hostname=jce-elastic\nindex.search.elasticsearch.client-only=true\nstorage.directory=/var/lib/janusgraph/data\nindex.search.directory=/var/lib/janusgraph/index\n</code></pre>"},{"location":"operations/container/#gremlin-server-environment-variable-syntax","title":"Gremlin Server Environment Variable Syntax","text":"<p>Environment Variables that start with the prefix <code>gremlinserver.</code> or <code>gremlinserver%d.</code> are used to edit the base janusgraph-server.yaml file. The text after the prefix in the environment variable name should follow a specific syntax. This syntax is implemented using the yq write and delete commands and the yq documentation can be used as a reference for this syntax. Secondly, the value of the environment variable will be used to set the value of the key specified in the environment variable name.</p> <p>Let's take a look at a few examples:</p>"},{"location":"operations/container/#nested-properties","title":"Nested Properties","text":"<p>For example, say we want to add a configuration property <code>graphs.ConfigurationMangementGraph</code> with the value <code>conf/JanusGraph-configurationmanagement.properties</code>:</p> <pre><code>$ docker run --rm -it -e gremlinserver.graphs.ConfigurationManagementGraph=\\\nconf/JanusGraph-configurationmanagement.properties docker.io/janusgraph/janusgraph:latest janusgraph show-config\n...\ngraphs:\n  graph: conf/janusgraph-cql-es-server.properties\n  ConfigurationManagementGraph: conf/JanusGraph-configurationmanagement.properties\nscriptEngines:\n...\n</code></pre>"},{"location":"operations/container/#delete-a-component","title":"Delete a component","text":"<p>To delete a component append %d to the 'gremlinserver.' prefix before the closing dot and then select the component following the prefix. Don't forget the trailing '='. For example to delete the graphs.graph configuration property we can do the following:</p> <pre><code>$ docker run --rm -it -e gremlinserver%d.graphs.graph= docker.io/janusgraph/janusgraph:latest janusgraph show-config\n...\nchannelizer: org.apache.tinkerpop.gremlin.server.channel.WebSocketChannelizer\ngraphs: {}\nscriptEngines:\n...\n</code></pre>"},{"location":"operations/container/#append-item-and-alternate-indexing-syntax","title":"Append item and alternate indexing syntax","text":"<p>This example shows how to append an item to a list. This can be done by adding \"[+]\" at the end of the environment variable name. This example also shows how to use square bracket syntax as an alternative to the dot syntax. This alternate syntax is useful if one of the keys in the property path contains special characters as we see in the example below.</p> <pre><code>$ docker run --rm -it -e gremlinserver.scriptEngines.gremlin-groovy\\\n.plugins[\"org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin\"]\\\n.files[+]=/scripts/another-script.groovy docker.io/janusgraph/janusgraph:latest janusgraph show-config\n...\nscriptEngines:\n  gremlin-groovy:\n    plugins:\n      org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin: {}\n      org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: {}\n      org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: {}\n      org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin:\n        classImports:\n        - java.lang.Math\n        methodImports:\n        - java.lang.Math#*\n      org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin:\n        files:\n        - scripts/empty-sample.groovy\n        - /scripts/another-script.groovy\n...\n</code></pre>"},{"location":"operations/container/#mounted-configuration","title":"Mounted Configuration","text":"<p>By default, the container stores both the <code>janusgraph.properties</code> and <code>janusgraph-server.yaml</code> files in the <code>JANUS_CONFIG_DIR</code> directory which maps to <code>/etc/opt/janusgraph</code>. When the container starts, it updates those files using the environment variable values. If you have a specific configuration and do not wish to use environment variables to configure JanusGraph, you can mount a directory containing your own version of those configuration files into the container through a bind mount, e.g., <code>-v /local/path/on/host:/etc/opt/janusgraph:ro</code>. You'll need to bind the files as read-only, however, if you do not wish to have the environment variables override the values in that file.</p>"},{"location":"operations/container/#example-with-mounted-configuration","title":"Example with mounted configuration","text":"<p>Start a JanusGraph instance with mounted configuration using <code>docker-compose-mount.yml</code>:</p> <pre><code>$ docker-compose -f docker-compose-mount.yml up\njanusgraph-mount | chown: changing ownership of '/etc/opt/janusgraph/janusgraph.properties': Read-only file system\n...\n</code></pre>"},{"location":"operations/container/#default-user-janusgraph","title":"Default user JanusGraph","text":"<p>Note: The default user of the image changed for all version beginning with the newest image version of 0.5.3.</p> <p>The user is created with uid 999 and gid 999 and user's a home dir is <code>/var/lib/janusgraph</code>. </p> <p>Following folder are created with these user rights: * <code>/var/lib/janusgraph</code> * <code>/etc/opt/janusgraph</code> * <code>/opt/janusgraph</code> * <code>/docker-entrypoint-initdb.d</code></p>"},{"location":"operations/container/#image-tagging-policy","title":"Image Tagging Policy","text":"<p>Here's the policy we follow for tagging our container images:</p> Tag Config Support level Docker base image latest latest JanusGraph release no breaking changes guarantees eclipse-temurin:11-jre x.x newest patch-level version of JanusGraph expect breaking changes eclipse-temurin:11-jre x.x.x, x.x.x-revision defined JanusGraph version static tag eclipse-temurin:11-jre x.x.x-SNAPSHOT latest snapshot build expect breaking changes eclipse-temurin:11-jre x.x.x-SNAPSHOT-revision latest snapshot build static tag eclipse-temurin:11-jre"},{"location":"operations/deployment/","title":"Deployment Scenarios","text":"<p>JanusGraph offers a wide choice of storage and index backends which results in great flexibility of how it can be deployed. This chapter presents a few possible deployment scenarios to help with the complexity that comes with this flexibility.</p> <p>Before discussing the different deployment scenarios, it is important to understand the roles of JanusGraph itself and that of the backends. First of all, applications only communicate directly with JanusGraph, mostly by sending Gremlin traversals for execution. JanusGraph then communicates with the configured backends to execute the received traversal. When JanusGraph is used in the form of JanusGraph Server, then there is nothing like a master JanusGraph Server. Applications can therefore connect to any JanusGraph Server instance. They can also use a load-balancer to schedule requests to the different instances. The JanusGraph Server instances themselves don\u2019t communicate to each other directly which makes it easy to scale them when the need arises to process more traversals.</p> <p>Note</p> <p>The scenarios presented in this chapter are only examples of how JanusGraph can be deployed. Each deployment needs to take into account the concrete use cases and production needs.</p>"},{"location":"operations/deployment/#getting-started-scenario","title":"Getting Started Scenario","text":"<p>This scenario is the scenario most users probably want to choose when they are just getting started with JanusGraph. It offers scalability and fault tolerance with a minimum number of servers required. JanusGraph Server runs together with an instance of the storage backend and optionally also an instance of the index backend on every server.</p> <p></p> <p>A setup like this can be extended by simply adding more servers of the same kind or by moving one of the components onto dedicated servers. The latter describes a growth path to transform the deployment into the Advanced Scenario.</p> <p>Any of the scalable storage backends can be used with this scenario. Note however that for Scylla some configuration is required when it is hosted co-located with other services like in this scenario. When an index backend should be used in this scenario then it also needs to be one that is scalable.</p>"},{"location":"operations/deployment/#advanced-scenario","title":"Advanced Scenario","text":"<p>The advanced scenario is an evolution of the Getting Started Scenario.  Instead of hosting the JanusGraph Server instances together with the storage backend and optionally also the index backend, they are now separated on different servers. The advantage of hosting the different components (JanusGraph Server, storage/index backend) on different servers is that they can be scaled and managed independently of each other. This offers a higher flexibility at the cost of having to maintain more servers.</p> <p></p> <p>Since this scenario offers independent scalability of the different components, it of course makes most sense to also use scalable backends.</p>"},{"location":"operations/deployment/#minimalist-scenario","title":"Minimalist Scenario","text":"<p>It is also possible to host JanusGraph Server together with the backend(s) on just one server. This is especially attractive for testing purposes or for example when JanusGraph just supports a single application which can then also run on the same server.</p> <p></p> <p>Opposed to the previous scenarios, it makes most sense to use backends for this scenario that are not scalable. The in-memory backend can be used for testing purposes or Berkeley DB for production and Lucene as the optional index backend.</p>"},{"location":"operations/deployment/#embedded-janusgraph","title":"Embedded JanusGraph","text":"<p>Instead of connecting to the JanusGraph Server from an application it is also possible to embed JanusGraph as a library inside a JVM based application. While this reduces the administrative and network overhead, it makes it impossible to scale JanusGraph independently of the application. Embedded JanusGraph can be deployed as a variation of any of the other scenarios. JanusGraph just moves from the server(s) directly into the application as its now just used as a library instead of an independent service. You would need to introduce <code>janusgraph-core</code> dependency to your project, as well as the modules needed for your selected backends. For example, if you have a Maven project and you use Cassandra and Lucene as your backends, You should add the following dependencies into your <code>pom.xml</code>:</p> <pre><code>&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n        &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n        &lt;version&gt;${janusgraph.version}&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n        &lt;artifactId&gt;janusgraph-cql&lt;/artifactId&gt;\n        &lt;version&gt;${janusgraph.version}&lt;/version&gt;\n    &lt;/dependency&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n        &lt;artifactId&gt;janusgraph-lucene&lt;/artifactId&gt;\n        &lt;version&gt;${janusgraph.version}&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <p>Then you could start a JanusGraph instance in your application code, similar to  what you do in gremlin console:</p> <pre><code>import org.janusgraph.core.JanusGraph;\nimport org.janusgraph.core.JanusGraphFactory;\nimport org.janusgraph.core.schema.JanusGraphManagement;\n\npublic class MyGraphApp {\n    public static void main(String[] args) {\n        JanusGraph graph = JanusGraphFactory.open(\"/path/to/your/config/file\");\n        JanusGraphManagement mgmt = graph.openManagement();\n        mgmt.printSchema();\n        mgmt.commit();\n        graph.close();\n    }\n}\n</code></pre>"},{"location":"operations/deployment/#gremlin-parser","title":"Gremlin Parser","text":"<p>JanusGraph server could accept adhoc gremlin queries in plain string format, while embedded JanusGraph requires you to write Java code for any query. You could develop your own DSL (domain specific language) on top of JanusGraph to expose the graph query capabilities to end users. Alternatively, you could leverage the built-in Gremlin parser to take any gremlin query in plain string format, parse them and execute them in your application with JanusGraph embedded. To leverage gremlin parser, check out graph.script-eval. Once you turn on the script evaluation option, you could then evaluate gremlin queries using <code>JanusGraph::eval</code> method:</p> <pre><code>graph.eval(/* gremlin script */ \"g.V().count().next()\",                 /* commit */ false);\ngraph.eval(/* gremlin script */ \"g.addV().next();g.V().count().next()\", /* commit */ true)\n</code></pre> <p>As shown above, you could pass in a single gremlin query or a gremlin script in plain string format. Optionally, you could commit or rollback the script after the execution. This is useful when you want to prevent end users from updating the graph.</p>"},{"location":"operations/dynamic-graphs/","title":"Dynamic Graphs","text":"<p>JanusGraph supports dynamically creating graphs. This is deviation from the way in which standard Gremlin Server implementations allow one to access a graph. Traditionally, users create bindings to graphs at server-start, by configuring the gremlin-server.yaml file accordingly. For example, if the <code>graphs</code> section of your yaml file looks like this: <pre><code>graphs {\n  graph1: conf/graph1.properties,\n  graph2: conf/graph2.properties\n}\n</code></pre></p> <p>then you will access your graphs on the Gremlin Server using the fact that the String <code>graph1</code> will be bound to the graph opened on the server as per its supplied properties file, and the same holds true for <code>graph2</code>.</p> <p>However, if we use the <code>ConfiguredGraphFactory</code> to dynamically create graphs, then those graphs are managed by the JanusGraphManager and the graph configurations are managed by the ConfigurationManagementGraph. This is especially useful because it 1. allows you to define graph configurations post-server-start and 2. allows the graph configurations to be managed in a persisted and distributed nature across your JanusGraph cluster.</p> <p>To properly use the <code>ConfiguredGraphFactory</code>, you must configure every Gremlin Server in your cluster to use the <code>JanusGraphManager</code> and the <code>ConfigurationManagementGraph</code>. This procedure is explained in detail here.</p>"},{"location":"operations/dynamic-graphs/#graph-reference-consistency","title":"Graph Reference Consistency","text":"<p>If you configure all your JanusGraph servers to use the ConfiguredGraphFactory, JanusGraph will ensure all graph representations are-up-to-date across all JanusGraph nodes in your cluster.</p> <p>For example, if you update or delete the configuration to a graph on one JanusGraph node, then we must evict that graph from the cache on every JanusGraph node in the cluster. Otherwise, we may have inconsistent graph representations across your cluster. JanusGraph automatically handles this eviction using a messaging log queue through the backend system that the graph in question is configured to use.</p> <p>If one of your servers is configured incorrectly, then it may not be able to successfully remove the graph from the cache.</p> <p>Important</p> <p>Any updates to your TemplateConfiguration will not result in the updating of graphs/graph configurations previously created using said template configuration. If you want to update the individual graph configurations, you must do so using the available update APIs. These update APIs will then result in the graph cache eviction across all JanusGraph nodes in your cluster.</p>"},{"location":"operations/dynamic-graphs/#dynamic-graph-and-traversal-bindings","title":"Dynamic Graph and Traversal Bindings","text":"<p>JanusGraph has the ability to bind dynamically created graphs and their traversal references to <code>&lt;graph.graphname&gt;</code> and <code>&lt;graph.graphname&gt;_traversal</code>, respectively, across all JanusGraph nodes in your cluster, with a maximum of a 20s lag for the binding to take effect on any node in the cluster. Read more about this here.</p> <p>JanusGraph accomplishes this by having each node in your cluster poll the <code>ConfigurationManagementGraph</code> for all graphs for which you have created configurations. The <code>JanusGraphManager</code> will then open said graph with its persisted configuration, store it in its graph cache, and bind the <code>&lt;graph.graphname&gt;</code> to the graph reference on the <code>GremlinExecutor</code> as well as bind <code>&lt;graph.graphname&gt;_traversal</code> to the graph\u2019s traversal reference on the <code>GremlinExecutor</code>.</p> <p>This allows you to access a dynamically created graph and its traversal reference by their string bindings, on every node in your JanusGraph cluster. This is particularly important to be able to work with Gremlin Server clients and use TinkerPops\u2019s withRemote functionality.</p> <p>Note</p> <p>To set up your cluster to bind dynamically created graphs and their traversal references, you must configure each node to use  the ConfiguredGraphFactory.</p>"},{"location":"operations/dynamic-graphs/#using-tinkerpops-withremote-functionality","title":"Using TinkerPop\u2019s withRemote Functionality","text":"<p>Since traversal references are bound on the JanusGraph servers, we can make use of TinkerPop\u2019s withRemote functionality. This will allow one to run gremlin queries locally, against a remote graph reference. Traditionally, one runs queries against remote Gremlin Servers by sending String script representations, which are processed on the remote server and the response serialized and sent back. However, TinkerPop also allows for the use of <code>remoteGraph</code>, which could be useful if you are building a TinkerPop compliant graph infrastructure that is easily transferable to multiple implementations.</p> <p>To use this functionality in JanusGraph, we must first ensure we have created a graph on the remote JanusGraph cluster:</p> <p><code>ConfiguredGraphFactory.create(\"graph1\");</code></p> <p>Next, we must wait 20 seconds to ensure the traversal reference is bound on every JanusGraph node in the remote cluster.</p> <p>Finally, we can locally make use of the <code>withRemote</code> method to access a local reference to a remote graph: <pre><code>gremlin&gt; cluster = Cluster.open('conf/remote-objects.yaml')\n==&gt;localhost/127.0.0.1:8182\ngremlin&gt; g = traversal().withRemote(DriverRemoteConnection.using(cluster, \"graph1_traversal\"))\n==&gt;graphtraversalsource[emptygraph[empty], standard]\n</code></pre></p> <p>For completion, the above <code>conf/remote-objects.yaml</code> should tell the <code>Cluster</code> API how to access the remote JanusGraph servers; for example, it may look like: <pre><code>hosts: [remoteaddress1.com, remoteaddress2.com]\nport: 8182\nusername: admin\npassword: password\nconnectionPool: { enableSsl: true }\nserializer: \n    className: org.apache.tinkerpop.gremlin.util.ser.GraphBinaryMessageSerializerV1\n    config: \n        ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry]\n</code></pre></p>"},{"location":"operations/management/","title":"Management System","text":"<p>JanusGraph Management System provides methods to define, update, and inspect the schema of a JanusGraph graph, and more. Checkout the JanusGraph Management API documentation for all core APIs available.</p> <p>JanusGraph Management System behaves like a transaction in that it opens a transactional scope. As such, it needs to be closed via its commit or rollback methods, unless otherwise specified. <pre><code>mgmt = graph.openManagement()\n// do something\nmgmt.commit()\n</code></pre></p> <p>Note</p> <p>We strongly encourage all users of JanusGraph not to use JanusGraph\u2019s APIs outside of the management system, and always use standard Gremlin query language for any queries.</p>"},{"location":"operations/management/#schema-management","title":"Schema Management","text":"<p>Management System allows you to view, update, and create vertex labels, edge labels, and property keys. See schema management for details.</p>"},{"location":"operations/management/#index-management","title":"Index Management","text":"<p>Management System allows you to manage both vertex-centric indexes and graph indexes. See index management for details.</p>"},{"location":"operations/management/#consistency-management","title":"Consistency Management","text":"<p>Management System allows you to set the consistency level of individual schema elements. See Eventual Consistency for details.</p>"},{"location":"operations/management/#ghost-vertex-removal","title":"Ghost Vertex Removal","text":"<p>Management System allows you to purge ghost vertices in the graph. It uses a local thread pool to initiate multiple threads to scan your entire graph, detecting and purging ghost vertices as well as their incident edges, leveraging GhostVertexRemover By default, the concurrency level is the number of available processors on your machine. You can also configure the number of threads as shown in the example below. If your graph is huge, you could consider running GhostVertexRemover on a MapReduce cluster. <pre><code>mgmt = graph.openManagement()\n// by default, concurrency level = the number of available processors\nmgmt.removeGhostVertices().get()\n// alternatively, you could also configure the concurrency\nmgmt.removeGhostVertices(4).get()\n// it is not necessary to commit here, since GhostVertexRemover commits\n// periodically and automatically, but it is a good habit to do so\n// calling rollback() won't really rollback the ghost vertex removal process\nmgmt.commit()\n</code></pre></p>"},{"location":"operations/migrating-thrift/","title":"Migrating from Thirft","text":"<p>This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by a JanusGraph Thrift backend to a JanusGraph CQL Backend.</p> <p>Note</p> <p>The following backends are no longer supported: <code>cassandrathrift</code>, <code>cassandra</code>, <code>astyanax</code>, and <code>embeddedcassandra</code></p>"},{"location":"operations/migrating-thrift/#configuration-replacements","title":"Configuration Replacements","text":"Old configuration New CQL configuration .backend=cassandrathrift .backend=cql .backend=cassandra .backend=cql .backend=astyanax .backend=cql .backend=embeddedcassandra .backend=cql .port=9160 .port=9042 .cassandra.atomic-batch-mutate .cql.atomic-batch-mutate .cassandra.compaction-strategy-class .cql.compaction-strategy-class .cassandra.compaction-strategy-options .cql.compaction-strategy-options .cassandra.compression .cql.compression"},{"location":"operations/migrating-thrift/#embedded-mode","title":"Embedded Mode","text":"<p>JanusGraph does no longer support the embedded mode of Cassandra.  Therefore, users have to deploy Cassandra as a external instance,  see Cassandra.</p> <p>The JanusGraph distribution already comes with a version of Cassandra  included in the archive <code>janusgraph-full-1.0.0.zip</code> that is used by the <code>janusgraph.sh</code> script.</p>"},{"location":"operations/migrating-titan/","title":"Migrating from Titan","text":"<p>This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by Titan. Please note after migrating to version 0.3.0,  or later, of JanusGraph you will not be able to connect to a graph using a Titan client. </p>"},{"location":"operations/migrating-titan/#configuration","title":"Configuration","text":"<p>When connecting to an existing Titan data store the <code>graph.titan-version</code> property should already be set in the global configuration to Titan version <code>1.0.0</code>. The ID store name in JanusGraph is configurable via the <code>ids.store-name</code> property whereas in Titan it was a constant. If the <code>graph.titan-version</code> has been set in the existing global configuration, then you do not need to explicitly set the ID store as it will default to <code>titan_ids</code>.</p>"},{"location":"operations/migrating-titan/#cassandra","title":"Cassandra","text":"<p>The default keyspace used by Titan was <code>titan</code> and in order to reuse that existing keyspace the <code>storage.cql.keyspace</code> property needs to be set accordingly. <pre><code>storage.cql.keyspace=titan\n</code></pre></p> <p>These configuration options allow JanusGraph to read data from a Cassandra database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in Cassandra before attempting to use it with the JanusGraph release.</p>"},{"location":"operations/migrating-titan/#hbase","title":"HBase","text":"<p>The name of the table used by Titan was <code>titan</code> and in order to reuse that existing table the <code>storage.hbase.table</code> property needs to be set accordingly. <pre><code>storage.hbase.table=titan\n</code></pre></p> <p>These configuration options allow JanusGraph to read data from an HBase database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in HBase before attempting to use it with the JanusGraph release.</p>"},{"location":"operations/migrating-titan/#berkeleydb","title":"BerkeleyDB","text":"<p>The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk. This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with Titan can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release.</p>"},{"location":"operations/monitoring/","title":"Monitoring JanusGraph","text":""},{"location":"operations/monitoring/#metrics-in-janusgraph","title":"Metrics in JanusGraph","text":"<p>JanusGraph supports Metrics. JanusGraph can measure the following:</p> <ul> <li> <p>The number of transactions begun, committed, and rolled back</p> </li> <li> <p>The number of attempts and failures of each storage backend     operation type</p> </li> <li> <p>The response time distribution of each storage backend operation     type</p> </li> </ul>"},{"location":"operations/monitoring/#configuring-metrics-collection","title":"Configuring Metrics Collection","text":"<p>To enable Metrics collection, set the following in JanusGraph\u2019s properties file: <pre><code># Required to enable Metrics in JanusGraph\nmetrics.enabled = true\n</code></pre></p> <p>This setting makes JanusGraph record measurements at runtime using Metrics classes like Timer, Counter, Histogram, etc. To access these measurements, one or more Metrics reporters must be configured as described in the section Configuring Metrics Reporting.</p>"},{"location":"operations/monitoring/#customizing-the-default-metric-names","title":"Customizing the Default Metric Names","text":"<p>JanusGraph prefixes all metric names with \"org.janusgraph\" by default. This prefix can be set through the <code>metrics.prefix</code> configuration property. For example, to shorten the default \"org.janusgraph\" prefix to just \"janusgraph\": <pre><code># Optional\nmetrics.prefix = janusgraph\n</code></pre></p>"},{"location":"operations/monitoring/#transaction-specific-metrics-names","title":"Transaction-Specific Metrics Names","text":"<p>Each JanusGraph transaction may optionally specify its own Metrics name prefix, overriding both the default Metrics name prefix and the <code>metrics.prefix</code> configuration property. For example, the prefix could be changed to the name of the frontend application that opened the JanusGraph transaction. Note that Metrics maintains a ConcurrentHashMap of metric names and their associated objects in memory, so it\u2019s probably a good idea to keep the number of distinct metric prefixes small.</p> <p>To do this, call <code>TransactionBuilder.setMetricsPrefix(String)</code>: <pre><code>JanusGraph graph = ...;\nTransactionBuilder tbuilder = graph.buildTransaction();\nJanusGraphTransaction tx = tbuilder.groupName(\"foobar\").start();\n</code></pre></p>"},{"location":"operations/monitoring/#separating-metrics-by-backend-store","title":"Separating Metrics by Backend Store","text":"<p>JanusGraph combines the Metrics for its various internal storage backend handles by default. All Metrics for storage backend interactions follow the pattern \"&lt;prefix&gt;.stores.&lt;opname&gt;\", regardless of whether they come from the ID store, edge store, etc. When <code>metrics.merge-basic-metrics = false</code> is set in JanusGraph\u2019s properties file, the \"stores\" string in metric names is replaced by \"idStore\", \"edgeStore\", \"vertexIndexStore\", or \"edgeIndexStore\".</p>"},{"location":"operations/monitoring/#index-provider-metrics","title":"Index Provider Metrics","text":"<p>JanusGraph collects basic metrics for mixed index operations. These metrics  can be found here <code>&lt;prefix&gt;.indexProvider.&lt;INDEX-NAME&gt;.&lt;opname&gt;</code>.</p>"},{"location":"operations/monitoring/#configuring-metrics-reporting","title":"Configuring Metrics Reporting","text":"<p>JanusGraph supports the following Metrics reporters:</p> <ul> <li>Console</li> <li>CSV</li> <li>Ganglia</li> <li>Graphite</li> <li>JMX</li> <li>Slf4j</li> <li>User-provided/Custom</li> </ul> <p>Each reporter type is independent and can coexist with other reporter types. For example, it\u2019s possible to configure JMX and Slf4j Metrics reporters to operate simultaneously. Just set all their respective configuration keys in janusgraph.properties (and enable metrics as directed above).</p>"},{"location":"operations/monitoring/#console-reporter","title":"Console Reporter","text":"Metrics Console Reporter Configuration Options Config Key Required? Value Default metrics.console.interval yes Milliseconds to wait between dumping metrics to the console null <p>Example janusgraph.properties snippet that prints metrics to the console once a minute: <pre><code>metrics.enabled = true\n# Required; specify logging interval in milliseconds\nmetrics.console.interval = 60000\n</code></pre></p>"},{"location":"operations/monitoring/#csv-file-reporter","title":"CSV File Reporter","text":"Metrics CSV Reporter Configuration Options Config Key Required? Value Default metrics.csv.interval yes Milliseconds to wait between writing CSV lines null metrics.csv.directory yes Directory in which CSV files are written (will be created if it does not exist) null <p>Example janusgraph.properties snippet that writes CSV files once a minute to the directory <code>./foo/bar/</code> (relative to the process\u2019s working directory): <pre><code>metrics.enabled = true\n# Required; specify logging interval in milliseconds\nmetrics.csv.interval = 60000\nmetrics.csv.directory = foo/bar\n</code></pre></p>"},{"location":"operations/monitoring/#graphite-reporter","title":"Graphite Reporter","text":"Metrics Graphite Reporter Configuration Options Config Key Required? Value Default metrics.graphite.hostname yes IP address or hostname to which Graphite plaintext protocol data are sent null metrics.graphite.interval yes Milliseconds to wait between pushing data to Graphite null metrics.graphite.port no Port to which Graphite plaintext protocol reports are sent 2003 metrics.graphite.prefix no Arbitrary string prepended to all metric names sent to Graphite null <p>Example janusgraph.properties snippet that sends metrics to a Graphite server on 192.168.0.1 every minute: <pre><code>metrics.enabled = true\n# Required; IP or hostname string\nmetrics.graphite.hostname = 192.168.0.1\n# Required; specify logging interval in milliseconds\nmetrics.graphite.interval = 60000\n</code></pre></p>"},{"location":"operations/monitoring/#jmx-reporter","title":"JMX Reporter","text":"Metrics JMX Reporter Configuration Options Config Key Required? Value Default metrics.jmx.enabled yes Boolean false metrics.jmx.domain no Metrics will appear in this JMX domain Metrics\u2019s own default metrics.jmx.agentid no Metrics will be reported with this JMX agent ID Metrics\u2019s own default <p>Example janusgraph.properties snippet: <pre><code>metrics.enabled = true\n# Required\nmetrics.jmx.enabled = true\n# Optional; if omitted, then Metrics uses its default values\nmetrics.jmx.domain = foo\nmetrics.jmx.agentid = baz\n</code></pre></p>"},{"location":"operations/monitoring/#slf4j-reporter","title":"Slf4j Reporter","text":"Metrics Slf4j Reporter Configuration Options Config Key Required? Value Default metrics.slf4j.interval yes Milliseconds to wait between dumping metrics to the logger null metrics.slf4j.logger no Slf4j logger name to use \"metrics\" <p>Example janusgraph.properties snippet that logs metrics once a minute to the logger named <code>foo</code>: <pre><code>metrics.enabled = true\n# Required; specify logging interval in milliseconds\nmetrics.slf4j.interval = 60000\n# Optional; uses Metrics default when unset\nmetrics.slf4j.logger = foo\n</code></pre></p>"},{"location":"operations/monitoring/#user-providedcustom-reporter","title":"User-Provided/Custom Reporter","text":"<p>In case the Metrics reporter configuration options listed above are insufficient, JanusGraph provides a utility method to access the single <code>MetricRegistry</code> instance which holds all of its measurements. <pre><code>com.codahale.metrics.MetricRegistry janusgraphRegistry = \n    org.janusgraph.util.stats.MetricManager.INSTANCE.getRegistry();\n</code></pre></p> <p>Code that accesses <code>janusgraphRegistry</code> this way can then attach non-standard reporter types or standard reporter types with exotic configurations to <code>janusgraphRegistry</code>. This approach is also useful if the surrounding application already has a framework for Metrics reporter configuration, or if the application needs multiple differently-configured instances of one of JanusGraph\u2019s supported reporter types. For instance, one could use this approach to setup multiple unicast Graphite reporters whereas JanusGraph\u2019s properties configuration is limited to just one Graphite reporter.</p>"},{"location":"operations/recovery/","title":"Failure &amp; Recovery","text":"<p>JanusGraph is a highly available and robust graph database. In large scale JanusGraph deployments failure is inevitable. This page describes some failure situations and how JanusGraph can handle them.</p>"},{"location":"operations/recovery/#transaction-failure","title":"Transaction Failure","text":"<p>Transactions can fail for a number of reasons. If the transaction fails before the commit the changes will be discarded and the application can retry the transaction in coherence with the business logic. Likewise, locking or other consistency failures will cause an exception prior to persistence and hence can be retried. The persistence stage of a transaction is when JanusGraph starts persisting data to the various backend systems.</p> <p>JanusGraph first persists all graph mutations to the storage backend. This persistence is executed as one batch mutation to ensure that the mutation is committed atomically for those backends supporting atomicity. If the batch mutation fails due to an exception in the storage backend, the entire transaction is failed.</p> <p>If the primary persistence into the storage backend succeeds but secondary persistence into the indexing backends or the logging system fail, the transaction is still considered to be successful because the storage backend is the authoritative source of the graph.</p> <p>However, this can create inconsistencies with the indexes and logs. To automatically repair such inconsistencies, JanusGraph can maintain a transaction write-ahead log which is enabled through the configuration. <pre><code>tx.log-tx = true\ntx.max-commit-time = 10000\n</code></pre></p> <p>The max-commit-time property is used to determine when a transaction has failed. If the persistence stage of the transaction takes longer than this time, JanusGraph will attempt to recover it if necessary. Hence, this time out should be configured as a generous upper bound on the maximum duration of persistence. Note, that this does not include the time spent before commit.</p> <p>In addition, a separate process must be setup that reads the log to identify partially failed transaction and repair any inconsistencies caused. It is suggested to run the transaction repair process on a separate machine connected to the cluster to isolate failures. Configure a separately controlled process to run the following where the start time specifies the time since epoch where the recovery process should start reading from the write-ahead log. <pre><code>recovery = JanusGraphFactory.startTransactionRecovery(graph, startTime, TimeUnit.MILLISECONDS);\n</code></pre></p> <p>Enabling the transaction write-ahead log causes an additional write operation for mutating transactions which increases the latency. Also note, that additional space is required to store the log. The transaction write-ahead log has a configurable time-to-live of 2 days which means that log entries expire after that time to keep the storage overhead small. Refer to Configuration Reference for a complete list of all log related configuration options to fine tune logging behavior.</p>"},{"location":"operations/recovery/#janusgraph-instance-failure","title":"JanusGraph Instance Failure","text":"<p>JanusGraph is robust against individual instance failure in that other instances of the JanusGraph cluster are not impacted by such failure and can continue processing transactions without loss of performance while the failed instance is restarted.</p> <p>However, some schema related operations - such as installing indexes - require the coordination of all JanusGraph instances. For this reason, JanusGraph maintains a record of all running instances. If an instance fails, i.e. is not properly shut down, JanusGraph considers it to be active and expects its participation in cluster-wide operations which subsequently fail because this instances did not participate in or did not acknowledge the operation.</p> <p>In this case, the user must manually remove the failed instance record from the cluster and then retry the operation. To remove the failed instance, open a management transaction against any of the running JanusGraph instances, inspect the list of running instances to identify the failed one, and finally remove it. <pre><code>mgmt = graph.openManagement()\nmgmt.getOpenInstances() //all open instances\n==&gt;7f0001016161-dunwich1(current)\n==&gt;7f0001016161-atlantis1\nmgmt.forceCloseInstance('7f0001016161-atlantis1') //remove an instance\nmgmt.commit()\n</code></pre></p> <p>The unique identifier of the current JanusGraph instance is marked with the suffix <code>(current)</code> so that it can be easily identified. This instance cannot be closed via the <code>forceCloseInstance</code> method and instead should be closed via <code>g.close()</code></p> <p>It must be ensured that the manually removed instance is indeed no longer active. Removing an active JanusGraph instance from a cluster can cause data inconsistencies. Hence, use this method with great care in particular when JanusGraph is operated in an environment where instances are automatically restarted.</p>"},{"location":"operations/server/","title":"JanusGraph Server","text":"<p>JanusGraph uses the Gremlin Server  engine as the server component to process and answer client queries and extends it with convenience features for JanusGraph.  From now on, we will call this JanusGraph Server.</p> <p>JanusGraph Server must be started manually in order to use it. JanusGraph Server provides a way to remotely execute Gremlin traversals against one or more JanusGraph instances hosted within it. This section will describe how to use the WebSocket configuration, as well as describe how to configure JanusGraph Server to handle HTTP endpoint interactions. For information about how to connect to a JanusGraph Server from different languages refer to Connecting to JanusGraph.</p>"},{"location":"operations/server/#starting-a-janusgraph-server","title":"Starting a JanusGraph Server","text":"<p>JanusGraph Server comes packaged with a script called <code>bin/janusgraph-server.sh</code> to get it started: <pre><code>$ ./bin/janusgraph-server.sh console\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/var/lib/janusgraph/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/var/lib/janusgraph/lib/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n0    [main] INFO  org.janusgraph.graphdb.server.JanusGraphServer  -                                                                       \n   mmm                                mmm                       #     \n     #   mmm   m mm   m   m   mmm   m\"   \"  m mm   mmm   mmmm   # mm  \n     #  \"   #  #\"  #  #   #  #   \"  #   mm  #\"  \" \"   #  #\" \"#  #\"  # \n     #  m\"\"\"#  #   #  #   #   \"\"\"m  #    #  #     m\"\"\"#  #   #  #   # \n \"mmm\"  \"mm\"#  #   #  \"mm\"#  \"mmm\"   \"mmm\"  #     \"mm\"#  ##m#\"  #   # \n                                                         #            \n                                                         \"            \n[...]\n2240 [gremlin-server-boss-1] INFO  org.apache.tinkerpop.gremlin.server.GremlinServer  - Channel started at port 8182.\n</code></pre></p> <p>JanusGraph Server is configured by the provided YAML file <code>conf/gremlin-server/gremlin-server.yaml</code>.  That file tells JanusGraph Server many things and is based on the Gremlin Server config, see Gremlin Server.</p>"},{"location":"operations/server/#usage-of-janusgraph-serversh","title":"Usage of janusgraph-server.sh","text":"<p>The JanusGraph Server can be started in the foreground with stdout logging or detached. </p> <pre><code>$ ./bin/janusgraph-server.sh\nUsage: ./bin/janusgraph-server.sh {start [conf file]|stop|restart [conf file]|status|console|usage &lt;group&gt; &lt;artifact&gt; &lt;version&gt;|&lt;conf file&gt;}\n\n    start           Start the server in the background. Configuration file can be specified as a second argument\n                    or as JANUSGRAPH_YAML environment variable. If configuration file is not specified\n                    or has invalid path than JanusGraph server will try to use the default configuration file\n                    at relative location conf/gremlin-server/gremlin-server.yaml\n    stop            Stop the server\n    restart         Stop and start the server. To use previously used configuration it should be specified again\n                    as described in \"start\" command\n    status          Check if the server is running\n    console         Start the server in the foreground. Same rules are applied for configurations as described\n                    in \"start\" command\n    usage           Print out this help message\n\nIn case command is not specified and the configuration is specified as the first argument, JanusGraph Server will\n be started in the foreground using the specified configuration (same as with \"console\" command).\n</code></pre>"},{"location":"operations/server/#env-variables","title":"Env variables","text":"Variable Description Default Value <code>$JANUSGRAPH_HOME</code> Root directory of a default janusgraph installation. (default directory below janusgraph-server.sh) <code>$JANUSGRAPH_CONF</code> Config directory containing all kinds of server and graph configs. <code>$JANUSGRAPH_HOME/conf</code> <code>$LOG_DIR</code> Log directory <code>\"$JANUSGRAPH_HOME/logs\"</code> <code>$LOG_FILE</code> Default log file <code>\"$LOG_DIR/janusgraph.log\"</code> <code>$PID_DIR</code> <code>\"$JANUSGRAPH_HOME/run\"</code> <code>$PID_FILE</code> <code>\"$PID_DIR/janusgraph.pid\"</code> <code>$JANUSGRAPH_YAML</code> JanusGraph Server config path <code>\"$JANUSGRAPH_CONF/gremlin-server/gremlin-server.yaml\"</code> <code>$JANUSGRAPH_LIB</code> JanusGraph library directory <code>\"$JANUSGRAPH_HOME/lib\"</code> <code>$JAVA_HOME</code> If not set java home fallback to <code>java</code>. NOT_SET <code>$JAVA_OPTIONS_FILE</code> <code>\"$JANUSGRAPH_CONF/jvm.options\"</code> <code>$JAVA_OPTIONS</code> NOT_SET <code>$CP</code> Can be used to override the classpath's. (expert mode) NOT_SET <code>$DEBUG</code> If you enable debug by creating this env, bash debug will be enabled. NOT_SET"},{"location":"operations/server/#configure-jvmoptions","title":"Configure jvm.options","text":"<p>JanusGraph runs on the JVM which is configurable for special use cases. Therefore, JanusGraph provides a <code>jvm.options</code> file with some default options.</p> <pre><code># Copyright 2020 JanusGraph Authors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#################\n# HEAP SETTINGS #\n#################\n\n-Xms4096m\n-Xmx4096m\n\n\n########################\n# GENERAL JVM SETTINGS #\n########################\n\n\n# enable thread priorities, primarily so we can give periodic tasks\n# a lower priority to avoid interfering with client workload\n-XX:+UseThreadPriorities\n\n# allows lowering thread priority without being root on linux - probably\n# not necessary on Windows but doesn't harm anything.\n# see http://tech.stolsvik.com/2010/01/linux-java-thread-priorities-workar\n-XX:ThreadPriorityPolicy=42\n\n# Enable heap-dump if there's an OOM\n-XX:+HeapDumpOnOutOfMemoryError\n\n# Per-thread stack size.\n-Xss256k\n\n# Make sure all memory is faulted and zeroed on startup.\n# This helps prevent soft faults in containers and makes\n# transparent hugepage allocation more effective.\n-XX:+AlwaysPreTouch\n\n# Enable thread-local allocation blocks and allow the JVM to automatically\n# resize them at runtime.\n-XX:+UseTLAB\n-XX:+ResizeTLAB\n-XX:+UseNUMA\n\n\n####################\n# GREMLIN SETTINGS #\n####################\n\n-Dgremlin.io.kryoShimService=org.janusgraph.hadoop.serialize.JanusGraphKryoShimService\n\n\n#################\n#  GC SETTINGS  #\n#################\n\n### CMS Settings\n\n-XX:+UseParNewGC\n-XX:+UseConcMarkSweepGC\n-XX:+CMSParallelRemarkEnabled\n-XX:SurvivorRatio=8\n-XX:MaxTenuringThreshold=1\n-XX:CMSInitiatingOccupancyFraction=75\n-XX:+UseCMSInitiatingOccupancyOnly\n-XX:CMSWaitDuration=10000\n-XX:+CMSParallelInitialMarkEnabled\n-XX:+CMSEdenChunksRecordAlways\n-XX:+CMSClassUnloadingEnabled\n</code></pre>"},{"location":"operations/server/#janusgraph-server-as-a-websocket-endpoint","title":"JanusGraph Server as a WebSocket Endpoint","text":"<p>The default configuration described in Getting Started  is already a WebSocket configuration. If you want to alter the default configuration to work with your own Cassandra or HBase environment rather than use the quick start environment, follow these steps:</p> <p>To Configure JanusGraph Server For WebSocket</p> <ol> <li> <p>Test a local connection to a JanusGraph database first. This step     applies whether using the Gremlin Console to test the connection, or     whether connecting from a program. Make appropriate changes in a     properties file in the <code>./conf</code> directory for your environment. For     example, edit <code>./conf/janusgraph-hbase.properties</code> and make sure the     storage.backend, storage.hostname and storage.hbase.table parameters     are specified correctly. For more information on configuring     JanusGraph for various storage backends, see     Storage Backends. Make sure the properties file contains the     following line: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\n</code></pre></p> </li> <li> <p>Once a local configuration is tested and you have a working     properties file, copy the properties file from the <code>./conf</code>     directory to the <code>./conf/gremlin-server</code> directory. <pre><code>cp conf/janusgraph-hbase.properties\nconf/gremlin-server/socket-janusgraph-hbase-server.properties\n</code></pre></p> </li> <li> <p>Copy <code>./conf/gremlin-server/gremlin-server.yaml</code> to a new file     called <code>socket-gremlin-server.yaml</code>. Do this in case you need to     refer to the original version of the file <pre><code>cp conf/gremlin-server/gremlin-server.yaml\nconf/gremlin-server/socket-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>Edit the <code>socket-gremlin-server.yaml</code> file and make the following     updates:</p> <ol> <li> <p>If you are planning to connect to JanusGraph Server from     something other than localhost, update the IP address for host: <pre><code>host: 10.10.10.100\n</code></pre></p> </li> <li> <p>Update the graphs section to point to your new properties file     so the JanusGraph Server can find and connect to your JanusGraph     instance: <pre><code>graphs: { graph:\n    conf/gremlin-server/socket-janusgraph-hbase-server.properties}\n</code></pre></p> </li> </ol> </li> <li> <p>Start the JanusGraph Server, specifying the yaml file you just     configured: <pre><code>bin/janusgraph-server.sh console ./conf/gremlin-server/socket-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>The JanusGraph Server should now be running in WebSocket mode and     can be tested by following the instructions in Connecting to Gremlin Server</p> </li> </ol> <p>Important</p> <p>Do not use <code>bin/janusgraph.sh</code>. That starts the default configuration, which starts a separate Cassandra/Elasticsearch environment.</p>"},{"location":"operations/server/#janusgraph-server-as-a-http-endpoint","title":"JanusGraph Server as a HTTP Endpoint","text":"<p>The default configuration described in Getting Started is a WebSocket configuration. If you want to alter the default configuration in order to use JanusGraph Server as an HTTP endpoint for your JanusGraph database, follow these steps:</p> <ol> <li> <p>Test a local connection to a JanusGraph database first. This step     applies whether using the Gremlin Console to test the connection, or     whether connecting from a program. Make appropriate changes in a     properties file in the <code>./conf</code> directory for your environment. For     example, edit <code>./conf/janusgraph-hbase.properties</code> and make sure the     storage.backend, storage.hostname and storage.hbase.table parameters     are specified correctly. For more information on configuring     JanusGraph for various storage backends, see     Storage Backends. Make sure the properties file contains the     following line: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\n</code></pre></p> </li> <li> <p>Once a local configuration is tested and you have a working     properties file, copy the properties file from the <code>./conf</code>     directory to the <code>./conf/gremlin-server</code> directory. <pre><code>cp conf/janusgraph-hbase.properties conf/gremlin-server/http-janusgraph-hbase-server.properties\n</code></pre></p> </li> <li> <p>Copy <code>./conf/gremlin-server/gremlin-server.yaml</code> to a new file     called <code>http-gremlin-server.yaml</code>. Do this in case you need to refer     to the original version of the file <pre><code>cp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/http-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>Edit the <code>http-gremlin-server.yaml</code> file and make the following     updates:</p> <ol> <li> <p>If you are planning to connect to JanusGraph Server from     something other than localhost, update the IP address for host: <pre><code>host: 10.10.10.100\n</code></pre></p> </li> <li> <p>Update the channelizer setting to specify the HttpChannelizer: <pre><code>channelizer: org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer\n</code></pre></p> </li> <li> <p>Update the graphs section to point to your new properties file     so the JanusGraph Server can find and connect to your JanusGraph     instance: <pre><code>graphs: { graph:\n    conf/gremlin-server/http-janusgraph-hbase-server.properties}\n</code></pre></p> </li> </ol> </li> <li> <p>Start the JanusGraph Server, specifying the yaml file you just     configured: <pre><code>bin/janusgraph-server.sh console ./conf/gremlin-server/http-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>The JanusGraph Server should now be running in HTTP mode and     available for testing. curl can be used to verify the server is     working: <pre><code>curl -XPOST -Hcontent-type:application/json -d *{\"gremlin\":\"g.V().count()\"}* [IP for JanusGraph server host](http://):8182 \n</code></pre></p> </li> </ol>"},{"location":"operations/server/#janusgraph-server-as-both-a-websocket-and-http-endpoint","title":"JanusGraph Server as Both a WebSocket and HTTP Endpoint","text":"<p>As of JanusGraph 0.2.0, you can configure your <code>gremlin-server.yaml</code> to accept both WebSocket and HTTP connections over the same port. This can be achieved by changing the channelizer in any of the previous examples as follows. <pre><code>channelizer: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer\n</code></pre></p>"},{"location":"operations/server/#advanced-janusgraph-server-configurations","title":"Advanced JanusGraph Server Configurations","text":""},{"location":"operations/server/#authentication-over-http","title":"Authentication over HTTP","text":"<p>Important</p> <p>In the following example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords.</p>"},{"location":"operations/server/#http-basic-authentication","title":"HTTP Basic authentication","text":"<p>To enable Basic authentication in JanusGraph Server include the following configuration in your <code>gremlin-server.yaml</code>.</p> <pre><code> authentication: {\n   authenticator: org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.JanusGraphSimpleAuthenticator,\n   authenticationHandler: org.apache.tinkerpop.gremlin.server.handler.HttpBasicAuthenticationHandler,\n   config: {\n     defaultUsername: user,\n     defaultPassword: password,\n     credentialsDb: conf/janusgraph-credentials-server.properties\n    }\n }\n</code></pre> <p>Verify that basic authentication is configured correctly. For example</p> <pre><code>curl -v -XPOST http://localhost:8182 -d '{\"gremlin\": \"g.V().count()\"}'\n</code></pre> <p>should return a 401 if the authentication is configured correctly and</p> <p><pre><code>curl -v -XPOST http://localhost:8182 -d '{\"gremlin\": \"g.V().count()\"}' -u user:password\n</code></pre> should return a 200 and the result of 4 if authentication is configured correctly.</p>"},{"location":"operations/server/#authentication-over-websocket","title":"Authentication over WebSocket","text":"<p>Authentication over WebSocket occurs through a Simple Authentication and Security Layer (https://en.wikipedia.org/wiki/Simple_Authentication_and_Security_Layer[SASL]) mechanism.</p> <p>To enable SASL authentication include the following configuration in the <code>gremlin-server.yaml</code></p> <pre><code>authentication: {\n  authenticator: org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.JanusGraphSimpleAuthenticator,\n  authenticationHandler: org.apache.tinkerpop.gremlin.server.handler.SaslAuthenticationHandler,\n  config: {\n    defaultUsername: user,\n    defaultPassword: password,\n    credentialsDb: conf/janusgraph-credentials-server.properties\n  }\n}\n</code></pre> <p>Important</p> <p>In the preceding example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords.</p> <p>If you are connecting through the gremlin console, your remote yaml file should amend the <code>username</code> and <code>password</code> properties with the appropriate values.</p> <pre><code>username: user\npassword: password\n</code></pre>"},{"location":"operations/server/#authentication-over-http-and-websocket","title":"Authentication over HTTP and WebSocket","text":"<p>If you are using the combined channelizer for both HTTP and WebSocket you can use the SaslAndHMACAuthenticator to authorize through either WebSocket through SASL, HTTP through basic auth, and HTTP through hash-based message authentication code (https://en.wikipedia.org/wiki/Hash-based_message_authentication_code[HMAC]) Auth. HMAC is a token based authentication designed to be used over HTTP. You first acquire a token via the <code>/session</code> endpoint and then use that to authenticate. It is used to amortize the time spent encrypting the password using basic auth.</p> <p>The <code>gremlin-server.yaml</code> should include the following configurations</p> <pre><code>authentication: {\n  authenticator: org.janusgraph.graphdb.tinkerpop.gremlin.server.auth.SaslAndHMACAuthenticator,\n  authenticationHandler: org.janusgraph.graphdb.tinkerpop.gremlin.server.handler.SaslAndHMACAuthenticationHandler,\n  config: {\n    defaultUsername: user,\n    defaultPassword: password,\n    hmacSecret: secret,\n    credentialsDb: conf/janusgraph-credentials-server.properties\n  }\n}\n</code></pre> <p>Important</p> <p>In the preceding example, credentialsDb should be different from the graph(s) you are using. It should be configured with the correct backend and a different keyspace, table, or storage directory as appropriate for the configured backend. This graph will be used for storing usernames and passwords.</p> <p>Important</p> <p>Note the hmacSecret here. This should be the same across all running JanusGraph servers if you want to be able to use the same HMAC token on each server.</p> <p>For HMAC authentication over HTTP, this creates a <code>/session</code> endpoint that provides a token that expires after an hour by default. This timeout for the token can be configured through the <code>tokenTimeout</code> configuration option in the <code>authentication.config</code> map. This value is a Long value and in milliseconds.</p> <p>You can obtain the token using curl by issuing a get request to the <code>/session</code> endpoint. For example</p> <pre><code>curl http://localhost:8182/session -XGET -u user:password\n\n{\"token\": \"dXNlcjoxNTA5NTQ2NjI0NDUzOkhrclhYaGhRVG9KTnVSRXJ5U2VpdndhalJRcVBtWEpSMzh5WldqRTM4MW89\"}\n</code></pre> <p>You can then use that token for authentication by using the \"Authorization: Token\" header. For example</p> <pre><code>curl -v http://localhost:8182/session -XPOST -d '{\"gremlin\": \"g.V().count()\"}' -H \"Authorization: Token dXNlcjoxNTA5NTQ2NjI0NDUzOkhrclhYaGhRVG9KTnVSRXJ5U2VpdndhalJRcVBtWEpSMzh5WldqRTM4MW89\"\n</code></pre>"},{"location":"operations/server/#extending-janusgraph-server","title":"Extending JanusGraph Server","text":"<p>Note</p> <p>We currently are refactoring JanusGraph Server. If you like to get information or want to give input, see issue #2119.</p> <p>It is possible to extend Gremlin Server with other means of communication by implementing the interfaces that it provides and leverage this with JanusGraph. See more details in the appropriate TinkerPop documentation.</p>"},{"location":"schema/","title":"Schema and Data Modeling","text":"<p>Each JanusGraph graph has a schema comprised of the edge labels, property keys, and vertex labels used therein. A JanusGraph schema can either be explicitly or implicitly defined. Users are encouraged to explicitly define the graph schema during application development. An explicitly defined schema is an important component of a robust graph application and greatly improves collaborative software development. Note, that a JanusGraph schema can be evolved over time without any interruption of normal database operations. Extending the schema does not slow down query answering and does not require database downtime.</p> <p>The schema type - i.e. edge label, property key, or vertex label - is assigned to elements in the graph - i.e. edge, properties or vertices respectively - when they are first created. The assigned schema type cannot be changed for a particular element. This ensures a stable type system that is easy to reason about.</p> <p>Beyond the schema definition options explained in this section, schema types provide performance tuning options that are discussed in Advanced Schema.</p>"},{"location":"schema/#displaying-schema-information","title":"Displaying Schema Information","text":"<p>There are methods to view specific elements of the graph schema within the management API. These methods are <code>mgmt.printIndexes()</code>, <code>mgmt.printPropertyKeys()</code>, <code>mgmt.printVertexLabels()</code>, and <code>mgmt.printEdgeLabels()</code>. There is also a method that displays all the combined output named <code>printSchema()</code>.</p> <pre><code>mgmt = graph.openManagement()\nmgmt.printSchema()\n</code></pre>"},{"location":"schema/#defining-edge-labels","title":"Defining Edge Labels","text":"<p>Each edge connecting two vertices has a label which defines the semantics of the relationship. For instance, an edge labeled <code>friend</code> between vertices A and B encodes a friendship between the two individuals.</p> <p>To define an edge label, call <code>makeEdgeLabel(String)</code> on an open graph or management transaction and provide the name of the edge label as the argument. Edge label names must be unique in the graph. This method returns a builder for edge labels that allows to define its multiplicity. The multiplicity of an edge label defines a multiplicity constraint on all edges of this label, that is, a maximum number of edges between pairs of vertices. JanusGraph recognizes the following multiplicity settings.</p>"},{"location":"schema/#edge-label-multiplicity","title":"Edge Label Multiplicity","text":"<ul> <li>MULTI: Allows multiple edges of the same label between any pair     of vertices. In other words, the graph is a multi graph with     respect to such edge label. There is no constraint on edge     multiplicity.</li> <li>SIMPLE: Allows at most one edge of such label between any pair     of vertices. In other words, the graph is a simple graph with     respect to the label. Ensures that edges are unique for a given     label and pairs of vertices.</li> <li>MANY2ONE: Allows at most one outgoing edge of such label on any     vertex in the graph but places no constraint on incoming edges. The     edge label <code>mother</code> is an example with MANY2ONE multiplicity since     each person has at most one mother but mothers can have multiple     children.</li> <li>ONE2MANY: Allows at most one incoming edge of such label on any     vertex in the graph but places no constraint on outgoing edges. The     edge label <code>winnerOf</code> is an example with ONE2MANY multiplicity since     each contest is won by at most one person but a person can win     multiple contests.</li> <li>ONE2ONE: Allows at most one incoming and one outgoing edge of     such label on any vertex in the graph. The edge label marriedTo is     an example with ONE2ONE multiplicity since a person is married to     exactly one other person.</li> </ul> <p>The default multiplicity is MULTI. The definition of an edge label is completed by calling the <code>make()</code> method on the builder which returns the defined edge label as shown in the following example. <pre><code>mgmt = graph.openManagement()\nfollow = mgmt.makeEdgeLabel('follow').multiplicity(MULTI).make()\nmother = mgmt.makeEdgeLabel('mother').multiplicity(MANY2ONE).make()\nmgmt.commit()\n</code></pre></p>"},{"location":"schema/#defining-property-keys","title":"Defining Property Keys","text":"<p>Properties on vertices and edges are key-value pairs. For instance, the property <code>name='Daniel'</code> has the key <code>name</code> and the value <code>'Daniel'</code>. Property keys are part of the JanusGraph schema and can constrain the allowed data types and cardinality of values.</p> <p>To define a property key, call <code>makePropertyKey(String)</code> on an open graph or management transaction and provide the name of the property key as the argument. Property key names must be unique in the graph, and it is recommended to avoid spaces or special characters in property names. This method returns a builder for the property keys.</p> <p>Note</p> <p>During property key creation, consider creating also graph indices for better  performance, see Index Performance.</p>"},{"location":"schema/#property-key-data-type","title":"Property Key Data Type","text":"<p>Use <code>dataType(Class)</code> to define the data type of a property key. JanusGraph will enforce that all values associated with the key have the configured data type and thereby ensures that data added to the graph is valid. For instance, one can define that the <code>name</code> key has a String data type. Note that primitive types are not supported. Use the corresponding wrapper class, e.g. <code>Integer</code> instead of <code>int</code>.</p> <p>Define the data type as <code>Object.class</code> in order to allow any (serializable) value to be associated with a key. However, it is encouraged to use concrete data types whenever possible. Configured data types must be concrete classes and not interfaces or abstract classes. JanusGraph enforces class equality, so adding a sub-class of a configured data type is not allowed.</p> <p>JanusGraph natively supports the following data types.</p> <p>Native JanusGraph Data Types</p> Name Description String Character sequence Character Individual character Boolean true or false Byte byte value Short short value Integer integer value Long long value Float 4 byte floating point number Double 8 byte floating point number Date Specific instant in time (<code>java.util.Date</code>) Geoshape Geographic shape like point, circle or box UUID Universally unique identifier (<code>java.util.UUID</code>)"},{"location":"schema/#property-key-cardinality","title":"Property Key Cardinality","text":"<p>Use <code>cardinality(Cardinality)</code> to define the allowed cardinality of the values associated with the key on any given vertex.</p> <ul> <li>SINGLE: Allows at most one value per element for such key. In     other words, the key\u2192value mapping is unique for all elements in the     graph. The property key <code>birthDate</code> is an example with SINGLE     cardinality since each person has exactly one birth date.</li> <li>LIST: Allows an arbitrary number of values per element for such     key. In other words, the key is associated with a list of values     allowing duplicate values. Assuming we model sensors as vertices in     a graph, the property key <code>sensorReading</code> is an example with LIST     cardinality to allow lots of (potentially duplicate) sensor readings     to be recorded.</li> <li>SET: Allows multiple values but no duplicate values per element     for such key. In other words, the key is associated with a set of     values. The property key <code>name</code> has SET cardinality if we want to     capture all names of an individual (including nick name, maiden     name, etc).</li> </ul> <p>The default cardinality setting is SINGLE. Note, that property keys used on edges and properties have cardinality SINGLE. Attaching multiple values for a single key on an edge or property is not supported.</p> <pre><code>mgmt = graph.openManagement()\nbirthDate = mgmt.makePropertyKey('birthDate').dataType(Long.class).cardinality(Cardinality.SINGLE).make()\nname = mgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.SET).make()\nsensorReading = mgmt.makePropertyKey('sensorReading').dataType(Double.class).cardinality(Cardinality.LIST).make()\nmgmt.commit()\n</code></pre>"},{"location":"schema/#relation-types","title":"Relation Types","text":"<p>Edge labels and property keys are jointly referred to as relation types. Names of relation types must be unique in the graph which means that property keys and edge labels cannot have the same name. There are methods in the JanusGraph API to query for the existence or retrieve relation types which encompasses both property keys and edge labels.</p> <pre><code>mgmt = graph.openManagement()\nif (mgmt.containsRelationType('name'))\n    name = mgmt.getPropertyKey('name')\nmgmt.getRelationTypes(EdgeLabel.class)\nmgmt.commit()\n</code></pre>"},{"location":"schema/#defining-vertex-labels","title":"Defining Vertex Labels","text":"<p>Like edges, vertices have labels. Unlike edge labels, vertex labels are optional. Vertex labels are useful to distinguish different types of vertices, e.g. user vertices and product vertices.</p> <p>Although labels are optional at the conceptual and data model level, JanusGraph assigns all vertices a label as an internal implementation detail. Vertices created by the <code>addVertex</code> methods use JanusGraph\u2019s default label.</p> <p>To create a label, call <code>makeVertexLabel(String).make()</code> on an open graph or management transaction and provide the name of the vertex label as the argument. Vertex label names must be unique in the graph.</p> <pre><code>mgmt = graph.openManagement()\nperson = mgmt.makeVertexLabel('person').make()\nmgmt.commit()\n// Create a labeled vertex\nperson = graph.addVertex(label, 'person')\n// Create an unlabeled vertex\nv = graph.addVertex()\ngraph.tx().commit()\n</code></pre>"},{"location":"schema/#automatic-schema-maker","title":"Automatic Schema Maker","text":"<p>If an edge label, property key, or vertex label has not been defined explicitly, it will be defined implicitly when it is first used during the addition of an edge, vertex or the setting of a property. The <code>DefaultSchemaMaker</code> configured for the JanusGraph graph defines such types.</p> <p>By default, implicitly created edge labels have multiplicity MULTI and implicitly created property keys have cardinality SINGLE. Data types of implicitly created property keys are inferred as long as they are natively supported by JanusGraph. <code>Object.class</code> is used if and only if the given value is not any of the <code>Native JanusGraph Data Types</code>. Users can control automatic schema element creation by implementing and registering their own <code>DefaultSchemaMaker</code>.</p> <p>When defining a cardinality for a vertex property which differs from SINGLE,  the cardinality should be used for all values of the vertex property in the  first query (i.e. the query which defines a new vertex property key).</p> <p>It is strongly encouraged to explicitly define all schema elements and to disable automatic schema creation by setting <code>schema.default=none</code> in the JanusGraph graph configuration.</p>"},{"location":"schema/#changing-schema-elements","title":"Changing Schema Elements","text":"<p>The definition of an edge label, property key, or vertex label cannot be changed once its committed into the graph. However, the names of schema elements can be changed via <code>JanusGraphManagement.changeName(JanusGraphSchemaElement, String)</code> as shown in the following example where the property key <code>place</code> is renamed to <code>location</code>.</p> <pre><code>mgmt = graph.openManagement()\nplace = mgmt.getPropertyKey('place')\nmgmt.changeName(place, 'location')\nmgmt.commit()\n</code></pre> <p>Note, that schema name changes may not be immediately visible in currently running transactions and other JanusGraph graph instances in the cluster. While schema name changes are announced to all JanusGraph instances through the storage backend, it may take a while for the schema changes to take effect and it may require a instance restart in the event of certain failure conditions - like network partitions - if they coincide with the rename. Hence, the user must ensure that either of the following holds:</p> <ul> <li>The renamed label or key is not currently in active use (i.e.     written or read) and will not be in use until all JanusGraph     instances are aware of the name change.</li> <li>Running transactions actively accommodate the brief intermediate     period where either the old or new name is valid based on the     specific JanusGraph instance and status of the name-change     announcement. For instance, that could mean transactions query for     both names simultaneously.</li> </ul> <p>Should the need arise to re-define an existing schema type, it is recommended to change the name of this type to a name that is not currently (and will never be) in use. After that, a new label or key can be defined with the original name, thereby effectively replacing the old one. However, note that this would not affect vertices, edges, or properties previously written with the existing type. Redefining existing graph elements is not supported online and must be accomplished through a batch graph transformation.</p> <p>Be careful that if you change a property name that is part of some mixed index, you shall not reuse that property name in the same index. For example, the following code will fail.</p> <pre><code>name = mgmt.makePropertyKey(\"name\").dataType(String.class).make()\nmgmt.buildIndex(\"nameIndex\", Vertex.class).addKey(name).buildMixedIndex(\"search\")\nmgmt.commit()\n\nmgmt = graph.openManagement()\nmgmt.changeName(mgmt.getPropertyKey(\"name\"), \"oldName\");\nname = mgmt.makePropertyKey(\"name\").dataType(String.class).make()\nmgmt.addIndexKey(mgmt.getGraphIndex(\"nameIndex\"), name)\nmgmt.commit()\n\n// the following query will throw an exception\ngraph.traversal().V().has(\"name\", textContains(\"value\")).hasNext()\n</code></pre> <p>The reason is, JanusGraph does not attempt to alter the field name stored in the mixed index backend when you call <code>mgmt.changeName</code>. Instead, JanusGraph maintains a dual mapping between property name and index field name. If you create a new index using the old name and add it to the same index, conflicts will occur because JanusGraph cannot figure out which property should the index field map to.</p>"},{"location":"schema/#schema-constraints","title":"Schema Constraints","text":"<p>The definition of the schema allows users to configure explicit property and connection constraints. Properties can be bound to specific vertex label and/or edge labels. Moreover, connection constraints allow users to explicitly define which two vertex labels can be connected by an edge label. These constraints can be used to ensure that a graph matches a given domain model. For example for the graph of the gods, a <code>god</code> can be a brother of another <code>god</code>, but not of a <code>monster</code> and a <code>god</code> can have a property <code>age</code>, but <code>location</code> can not have a property <code>age</code>. These constraints are disabled by default.</p> <p>Enable these schema constraints by setting <code>schema.constraints=true</code>. This setting depends on the setting <code>schema.default</code>. If config <code>schema.default</code> is set to <code>none</code>, then an <code>IllegalArgumentException</code> is thrown for schema constraint violations. If <code>schema.default</code> is not set <code>none</code>, schema constraints are automatically created, but no exception is thrown. Activating schema constraints has no impact on the existing data, because these schema constraints are only applied during the insertion process. So reading of data is not affected at all by those constraints.</p> <p>Multiple properties can be bound to a vertex using <code>JanusGraphManagement.addProperties(VertexLabel, PropertyKey...)</code>, for example:</p> <pre><code>mgmt = graph.openManagement()\nperson = mgmt.makeVertexLabel('person').make()\nname = mgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.SET).make()\nbirthDate = mgmt.makePropertyKey('birthDate').dataType(Long.class).cardinality(Cardinality.SINGLE).make()\nmgmt.addProperties(person, name, birthDate)\nmgmt.commit()\n</code></pre> <p>Multiple properties can be bound to an edge using <code>JanusGraphManagement.addProperties(EdgeLabel, PropertyKey...)</code>, for example:</p> <pre><code>mgmt = graph.openManagement()\nfollow = mgmt.makeEdgeLabel('follow').multiplicity(MULTI).make()\nname = mgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.SET).make()\nmgmt.addProperties(follow, name)\nmgmt.commit()\n</code></pre> <p>Connections can be defined using <code>JanusGraphManagement.addConnection(EdgeLabel, VertexLabel out, VertexLabel in)</code> between an outgoing, an incoming and an edge, for example:</p> <pre><code>mgmt = graph.openManagement()\nperson = mgmt.makeVertexLabel('person').make()\ncompany = mgmt.makeVertexLabel('company').make()\nworks = mgmt.makeEdgeLabel('works').multiplicity(MULTI).make()\nmgmt.addConnection(works, person, company)\nmgmt.commit()\n</code></pre>"},{"location":"schema/advschema/","title":"Advanced Schema","text":"<p>This page describes some of the advanced schema definition options that JanusGraph provides. For general information on JanusGraph\u2019s schema and how to define it, refer to Schema and Data Modeling.</p>"},{"location":"schema/advschema/#static-vertices","title":"Static Vertices","text":"<p>Vertex labels can be defined as static which means that vertices with that label cannot be modified outside the transaction in which they were created. <pre><code>mgmt = graph.openManagement()\ntweet = mgmt.makeVertexLabel('tweet').setStatic().make()\nmgmt.commit()\n</code></pre></p> <p>Static vertex labels are a method of controlling the data lifecycle and useful when loading data into the graph that should not be modified after its creation.</p>"},{"location":"schema/advschema/#edge-and-vertex-ttl","title":"Edge and Vertex TTL","text":"<p>Edge and vertex labels can be configured with a time-to-live (TTL). Edges and vertices with such labels will automatically be removed from the graph when the configured TTL has passed after their initial creation. TTL configuration is useful when loading a large amount of data into the graph that is only of temporary use. Defining a TTL removes the need for manual clean up and handles the removal very efficiently. For example, it would make sense to TTL event edges such as user-page visits when those are summarized after a certain period of time or simply no longer needed for analytics or operational query processing.</p> <p>The following storage backends support edge and vertex TTL.</p> <ul> <li>CQL compatible storage backends</li> <li>HBase</li> <li>BerkeleyDB - supports only hour-discrete TTL, thus the minimal TTL is one hour.</li> </ul>"},{"location":"schema/advschema/#edge-ttl","title":"Edge TTL","text":"<p>Edge TTL is defined on a per-edge label basis, meaning that all edges of that label have the same time-to-live. Note that the backend must support cell level TTL. Currently only CQL, HBase and BerkeleyDB support this. <pre><code>mgmt = graph.openManagement()\nvisits = mgmt.makeEdgeLabel('visits').make()\nmgmt.setTTL(visits, Duration.ofDays(7))\nmgmt.commit()\n</code></pre></p> <p>Note, that modifying an edge resets the TTL for that edge. Also note, that the TTL of an edge label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label.</p>"},{"location":"schema/advschema/#property-ttl","title":"Property TTL","text":"<p>Property TTL is very similar to edge TTL and defined on a per-property key basis, meaning that all properties of that key have the same time-to-live. Note that the backend must support cell level TTL. Currently only CQL, HBase and BerkeleyDB support this. <pre><code>mgmt = graph.openManagement()\nsensor = mgmt.makePropertyKey('sensor').cardinality(Cardinality.LIST).dataType(Double.class).make()\nmgmt.setTTL(sensor, Duration.ofDays(21))\nmgmt.commit()\n</code></pre></p> <p>As with edge TTL, modifying an existing property resets the TTL for that property and modifying the TTL for a property key might not immediately take effect.</p>"},{"location":"schema/advschema/#vertex-ttl","title":"Vertex TTL","text":"<p>Vertex TTL is defined on a per-vertex label basis, meaning that all vertices of that label have the same time-to-live. The configured TTL applies to the vertex, its properties, and all incident edges to ensure that the entire vertex is removed from the graph. For this reason, a vertex label must be defined as static before a TTL can be set to rule out any modifications that would invalidate the vertex TTL. Vertex TTL only applies to static vertex labels. Note that the backend must support store level TTL. Currently only CQL, HBase and BerkeleyDB support this. <pre><code>mgmt = graph.openManagement()\ntweet = mgmt.makeVertexLabel('tweet').setStatic().make()\nmgmt.setTTL(tweet, Duration.ofHours(36))\nmgmt.commit()\n</code></pre></p> <p>Note, that the TTL of a vertex label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label.</p>"},{"location":"schema/advschema/#multi-properties","title":"Multi-Properties","text":"<p>As discussed in Schema and Data Modeling, JanusGraph supports property keys with SET and LIST cardinality. Hence, JanusGraph supports multiple properties with the same key on a single vertex. Furthermore, JanusGraph treats properties similarly to edges in that single-valued property annotations are allowed on properties as shown in the following example. <pre><code>mgmt = graph.openManagement()\nmgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.LIST).make()\nmgmt.commit()\nv = graph.addVertex()\np1 = v.property('name', 'Dan LaRocque')\np1.property('source', 'web')\np2 = v.property('name', 'dalaro')\np2.property('source', 'github')\ngraph.tx().commit()\nv.properties('name')\n==&gt; Iterable over all name properties\n</code></pre> These features are useful in a number of applications such as those where attaching provenance information (e.g. who added a property, when and from where?) to properties is necessary. Support for higher cardinality properties and property annotations on properties is also useful in high-concurrency, scale-out design patterns as described in Eventually-Consistent Storage Backends.</p> <p>Vertex-centric indexes and global graph indexes are supported for properties in the same manner as they are supported for edges. Refer to Indexing for Better Performance for information on defining these indexes for edges and use the corresponding API methods to define the same indexes for properties.</p>"},{"location":"schema/advschema/#unidirected-edges","title":"Unidirected Edges","text":"<p>Unidirected edges are edges that can only be traversed in the out-going direction. Unidirected edges have a lower storage footprint but are limited in the types of traversals they support. Unidirected edges are conceptually similar to hyperlinks in the world-wide-web in the sense that the out-vertex can traverse through the edge, but the in-vertex is unaware of its existence. <pre><code>mgmt = graph.openManagement()\nmgmt.makeEdgeLabel('author').unidirected().make()\nmgmt.commit()\n</code></pre></p> <p>Note, that unidirected edges do not get automatically deleted when their in-vertices are deleted. The user must ensure that such inconsistencies do not arise or resolve them at query time by explicitly checking vertex existence in a transaction. See the discussion in Ghost Vertices for more information.</p>"},{"location":"schema/schema-init-strategies/","title":"Schema Initialization Strategies","text":"<p>Besides the ability to provide groovy code to JanusGraph server to initialize schema or execute any custom logic, JanusGraph provides ability to execute  custom schema initialization strategies on startup.  These strategies are made to help users define and maintain their schema easier,  but they are not replacement to custom schema management processes which can  be developed by using <code>JanusGraphManagement</code> directly. </p> <p>Each time JanusGraph instance is started via <code>JanusGraphFactory</code> or <code>ConfiguredGraphFactory</code>,  a schema initialization strategy gets executed before startup.  Schema initialization strategy can be chosen via <code>schema.init.strategy</code> configuration parameter.  By default <code>none</code> is selected, meaning schema initialization process is skipped.</p> <p>Parameter <code>schema.init.schema-drop-before-startup</code> can be used to configure  schema and data removal on startup (can be convenient in testing environments).</p>"},{"location":"schema/schema-init-strategies/#json-schema-initialization","title":"JSON Schema Initialization","text":"<p>When <code>json</code> configuration option is selected in <code>schema.init.strategy</code> it  triggers schema initialization via JSON formated schema definition which  can be provided via file or string.</p> <p>Configurations for this strategy are provided under <code>schema.init.json</code> namespace.  The easiest way to set up JSON schema initialization it by providing JSON schema  file path to <code>schema.init.json.file</code> or directly insert JSON schema into  <code>schema.init.json.string</code> configuration option.</p>"},{"location":"schema/schema-init-strategies/#json-schema-format","title":"JSON Schema Format","text":"<p>Schema defined via JSON must be a deserialized JSON version of  <code>org.janusgraph.core.schema.json.definition.JsonSchemaDefinition</code> class where top level object elements will be the following: - <code>vertexLabels</code> - <code>edgeLabels</code> - <code>propertyKeys</code> - <code>compositeIndexes</code> - <code>vertexCentricEdgeIndexes</code> - <code>vertexCentricPropertyIndexes</code> - <code>mixedIndexes</code></p> <p>Each value is an array representing a list of vertex labels, edges labels, property  keys, composite indexes, vertex-centric edge indexes, vertex-centric property indexes,  or mixed indexes.</p>"},{"location":"schema/schema-init-strategies/#json-vertex-label-definition","title":"JSON Vertex Label Definition","text":"<p>Each vertex label object consists of the following keys: - <code>label</code> - <code>string</code> datatype. (Required) - <code>staticVertex</code> - <code>boolean</code> datatype. - <code>partition</code> - <code>boolean</code> datatype. - <code>ttl</code> - <code>number</code> datatype.</p>"},{"location":"schema/schema-init-strategies/#json-edge-label-definition","title":"JSON Edge Label Definition","text":"<p>Each edge label object consists of the following keys: - <code>label</code> - <code>string</code> datatype. (Required) - <code>multiplicity</code> - <code>string</code> datatype. Allowed values: <code>MULTI</code>, <code>SIMPLE</code>, <code>ONE2MANY</code>, <code>MANY2ONE</code>, <code>ONE2ONE</code>. - <code>unidirected</code> - <code>boolean</code> datatype. - <code>ttl</code> - <code>number</code> datatype.</p>"},{"location":"schema/schema-init-strategies/#json-property-key-definition","title":"JSON Property Key Definition","text":"<p>Each property key object consists of the following keys: - <code>key</code> - <code>string</code> datatype. (Required) - <code>className</code> - <code>string</code> datatype. This must be a full class path of the selected property datatype. For example, <code>java.lang.String</code>, <code>java.lang.Long</code>,<code>org.janusgraph.core.attribute.Geoshape</code>. (Required) - <code>cardinality</code> - <code>string</code> datatype. Allowed values: <code>SINGLE</code>, <code>LIST</code>, <code>SET</code>. - <code>ttl</code> - <code>number</code> datatype.</p>"},{"location":"schema/schema-init-strategies/#json-composite-index-definition","title":"JSON Composite Index Definition","text":"<p>Each composite index object consists of the following keys: - <code>name</code> - <code>string</code> datatype. (Required) - <code>typeClass</code> - <code>string</code> datatype. Full class path of the datatype of an indexed element. Allowed values: <code>org.apache.tinkerpop.gremlin.structure.Vertex</code>, <code>org.apache.tinkerpop.gremlin.structure.Edge</code>. (Required) - <code>indexOnly</code> - <code>string</code> datatype. Vertex or Edge label. - <code>unique</code> - <code>boolean</code> datatype. - <code>consistency</code> - <code>string</code> datatype. Allowed values: <code>DEFAULT</code>, <code>LOCK</code>, <code>FORK</code>. - <code>keys</code> - an array of objects representing <code>org.janusgraph.core.schema.json.definition.index.JsonIndexedPropertyKeyDefinition</code> (see definition below after indexes definition). These are used index keys. (Required) - <code>inlinePropertyKeys</code> - an array of property keys to be inlined into the composite index. Currently supported for vertex composite indexes only. See documentation for more information about this feature.</p>"},{"location":"schema/schema-init-strategies/#json-mixed-index-definition","title":"JSON Mixed Index Definition","text":"<p>Each mixed index object consists of the following keys: - <code>name</code> - <code>string</code> datatype. (Required) - <code>typeClass</code> - <code>string</code> datatype. Full class path of the datatype of an indexed element. Allowed values: <code>org.apache.tinkerpop.gremlin.structure.Vertex</code>, <code>org.apache.tinkerpop.gremlin.structure.Edge</code>. (Required) - <code>indexOnly</code> - <code>string</code> datatype. Vertex or Edge label. - <code>indexBackend</code> - <code>string</code> datatype. Name for index backend configuration. (Required) - <code>keys</code> - an array of objects representing <code>org.janusgraph.core.schema.json.definition.index.JsonIndexedPropertyKeyDefinition</code> (see definition below after indexes definition). These are used index keys. (Required)</p>"},{"location":"schema/schema-init-strategies/#json-vertex-centric-edge-index-definition","title":"JSON Vertex-Centric Edge Index Definition","text":"<p>Each vertex-centric edge index object consists of the following keys: - <code>name</code> - <code>string</code> datatype. (Required) - <code>propertyKeys</code> - an array of strings (each value has <code>string</code> datatype). These are the Edge properties which will be used for the index. (Required) - <code>order</code> - <code>string</code> datatype. Allowed values: <code>asc</code>, <code>desc</code>. - <code>indexedEdgeLabel</code> - <code>string</code> datatype. Edge label to be indexed. (Required) - <code>direction</code> - <code>string</code> datatype. Allowed values: <code>OUT</code>, <code>IN</code>, <code>BOTH</code>.</p>"},{"location":"schema/schema-init-strategies/#json-vertex-centric-property-index-definition","title":"JSON Vertex-Centric Property Index Definition","text":"<p>Each vertex-centric property index object consists of the following keys: - <code>name</code> - <code>string</code> datatype. (Required) - <code>propertyKeys</code> - an array of strings (each value has <code>string</code> datatype). These are the meta-properties which will be used for the index. (Required) - <code>order</code> - <code>string</code> datatype. Allowed values: <code>asc</code>, <code>desc</code>. - <code>indexedPropertyKey</code> - <code>string</code> datatype. Property key to be indexed. (Required)</p>"},{"location":"schema/schema-init-strategies/#json-definition-of-property-keys-defined-in-composite-and-mixed-indexes-keys","title":"JSON Definition of Property Keys defined in Composite and Mixed indexes (<code>keys</code>)","text":"<p>Each property key object defined as <code>keys</code> representation of composite or mixed index consists of the following keys: - <code>propertyKey</code> - <code>string</code> datatype. (Required) - <code>parameters</code> - an array of objects representing <code>org.janusgraph.core.schema.json.definition.JsonParameterDefinition</code>. These are optional parameters to let index know of additional configurations for the property key. (See description below)</p>"},{"location":"schema/schema-init-strategies/#json-definition-of-parameters-defined-in-property-keys-for-composite-and-mixed-indexes-parameters","title":"JSON Definition of Parameters defined in Property Keys for Composite and Mixed indexes (<code>parameters</code>)","text":"<p>Each parameter is a configuration for to let underlying index backend configure relative properties better. Each such parameter consists of the following keys: - <code>key</code> - <code>string</code> datatype. (Required) - <code>value</code> - <code>string</code> datatype. (Required) - <code>parser</code> - <code>string</code> datatype. This must be a full class pass of the parser which will be used to parse <code>value</code> of the parameter or a pre-defined shortcut. This parser must implement <code>org.janusgraph.core.schema.json.parser.JsonParameterParser</code> interface and have a parameterless constructor. If none is provided then <code>string</code> parser is used by default.</p> <p>Pre-defined <code>parser</code> shortcuts: - <code>string</code> - doesn't change <code>value</code> and uses it as is (<code>String</code> datatype). - <code>enum</code> - replaces the provided string (defined as <code>&lt;full class path&gt;.&lt;enum option&gt;</code>) to actual enum value.  For example, if <code>value</code> has a string <code>org.janusgraph.core.schema.Mapping.STRING</code> it will be replaced to actual <code>STRING</code> enum,  and it won't be treated as a string. - <code>boolean</code> - parses value to <code>Boolean</code>. - <code>byte</code> - parses value to <code>Byte</code>. - <code>short</code> - parses value to <code>Short</code>. - <code>integer</code> - parses value to <code>Integer</code>. - <code>long</code> - parses value to <code>Long</code>. - <code>float</code> - parses value to <code>Float</code>. - <code>double</code> - parses value to <code>Double</code>.</p> <p>Parameters may include custom keys defined as <code>ParameterType.customParameterName(\"&lt;your custom key&gt;\")</code>.  To define such keys in JSON it's necessary to use specific prefix for <code>key</code> - %`custom%` (in total the prefix consists of 10 characters.  In case some characters are not rendered correctly here, you can always reference to <code>org.janusgraph.graphdb.types.ParameterType.CUSTOM_PARAMETER_PREFIX</code>  to find out those 10 prefix characters). As such, to define custom keys like <code>similarity</code> you should write your key as: %`custom%`similarity</p>"},{"location":"schema/schema-init-strategies/#json-schema-definition-example","title":"JSON Schema definition example","text":"<p>Following the rules above (defined in <code>JSON Schema Format</code>) an example of JSON schema definition could look like the one below.</p> <p>Note</p> <p>Below schema is valid and tested using Cassandra and ElasticSearch. It uses some of the parameters related to  either eventual consistent databases and ElasticSearch in particular. Thus, some of the database specific schema definitions  may not work properly with other databases (like <code>similarity</code> or <code>string-analyzer</code>). The schema below is used for demonstration purposes only.</p> <pre><code>{\n    \"vertexLabels\": [\n        {\n            \"label\": \"normalVertex\"\n        },\n        {\n            \"label\": \"partitionedVertex\",\n            \"partition\": true\n        },\n        {\n            \"label\": \"unmodifiableVertex\",\n            \"staticVertex\": true\n        },\n        {\n            \"label\": \"temporaryVertexForTwoHours\",\n            \"staticVertex\": true,\n            \"ttl\": 7200000\n        }\n    ],\n    \"edgeLabels\": [\n        {\n            \"label\": \"normalSimpleEdge\",\n            \"multiplicity\": \"SIMPLE\",\n            \"unidirected\": false\n        },\n        {\n            \"label\": \"unidirectedMultiEdge\",\n            \"multiplicity\": \"MULTI\",\n            \"unidirected\": true\n        },\n        {\n            \"label\": \"temporaryEdgeForOneHour\",\n            \"ttl\": 3600000\n        },\n        {\n            \"label\": \"edgeWhichUsesLocksInEventualConsistentDBs\",\n            \"consistency\": \"LOCK\"\n        },\n        {\n            \"label\": \"edgeWhichUsesForkingInEventualConsistentDBs\",\n            \"consistency\": \"FORK\"\n        }\n    ],\n    \"propertyKeys\": [\n        {\n            \"key\": \"normalProperty\",\n            \"className\": \"java.lang.Long\"\n        },\n        {\n            \"key\": \"stringProperty\",\n            \"className\": \"java.lang.String\",\n            \"cardinality\": \"SINGLE\"\n        },\n        {\n            \"key\": \"anotherStringProperty\",\n            \"className\": \"java.lang.String\",\n            \"cardinality\": \"SINGLE\"\n        },\n        {\n            \"key\": \"listProperty\",\n            \"className\": \"java.lang.Long\",\n            \"cardinality\": \"LIST\"\n        },\n        {\n            \"key\": \"geoshapeProperty\",\n            \"className\": \"org.janusgraph.core.attribute.Geoshape\"\n        },\n        {\n            \"key\": \"setProperty\",\n            \"className\": \"java.lang.String\",\n            \"cardinality\": \"SET\"\n        },\n        {\n            \"key\": \"propertyWhichUsesLocksInEventualConsistentDBs\",\n            \"className\": \"java.lang.Long\",\n            \"cardinality\": \"SET\",\n            \"consistency\": \"LOCK\"\n        },\n        {\n            \"key\": \"temporaryPropertyForOneHour\",\n            \"className\": \"java.lang.Long\",\n            \"ttl\": 3600000\n        }\n    ],\n    \"compositeIndexes\": [\n        {\n            \"name\": \"simpleCompositeIndex\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"normalProperty\"\n                }\n            ]\n        },\n        {\n            \"name\": \"indexOnlyForNormalVerticesOnListProperty\",\n            \"indexOnly\": \"normalVertex\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"listProperty\"\n                }\n            ]\n        },\n        {\n            \"name\": \"compositeIndexOnEdge\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Edge\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"normalProperty\"\n                }\n            ]\n        },\n        {\n            \"name\": \"uniqueCompositeIndexWithLocking\",\n            \"indexOnly\": \"unmodifiableVertex\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"unique\": true,\n            \"consistency\": \"LOCK\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"stringProperty\"\n                }\n            ]\n        },\n        {\n            \"name\": \"multiKeysCompositeIndex\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"normalProperty\"\n                },\n                {\n                    \"propertyKey\": \"anotherStringProperty\"\n                }\n            ]\n        },\n        {\n            \"name\": \"compositeIndexWithInlinedProperties\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"setProperty\"\n                }\n            ],\n            \"inlinePropertyKeys\": [\"normalProperty\", \"stringProperty\", \"anotherStringProperty\"]\n        }\n    ],\n    \"vertexCentricEdgeIndexes\": [\n        {\n            \"name\": \"vertexCentricBothDirectionsEdgeIndex\",\n            \"indexedEdgeLabel\": \"normalSimpleEdge\",\n            \"direction\": \"BOTH\",\n            \"propertyKeys\": [\n                \"normalProperty\"\n            ],\n            \"order\": \"asc\"\n        },\n        {\n            \"name\": \"vertexCentricUnidirectedEdgeIndexOnMultipleProperties\",\n            \"indexedEdgeLabel\": \"unidirectedMultiEdge\",\n            \"direction\": \"OUT\",\n            \"propertyKeys\": [\n                \"stringProperty\",\n                \"anotherStringProperty\"\n            ],\n            \"order\": \"desc\"\n        }\n    ],\n    \"vertexCentricPropertyIndexes\": [\n        {\n            \"name\": \"normalVertexCentricPropertyKey\",\n            \"indexedPropertyKey\": \"listProperty\",\n            \"propertyKeys\": [\n                \"normalProperty\"\n            ],\n            \"order\": \"asc\"\n        }\n    ],\n    \"mixedIndexes\": [\n        {\n            \"name\": \"simpleMixedIndexOnMultipleProperties\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"indexBackend\": \"search\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"normalProperty\"\n                },\n                {\n                    \"propertyKey\": \"geoshapeProperty\"\n                }\n            ]\n        },\n        {\n            \"name\": \"mixedIndexWithParametersOnProperties\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Vertex\",\n            \"indexBackend\": \"search\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"stringProperty\",\n                    \"parameters\": [\n                        {\n                            \"key\": \"string-analyzer\",\n                            \"value\": \"standard\",\n                            \"parser\": \"string\"\n                        },\n                        {\n                            \"key\": \"mapping\",\n                            \"value\": \"org.janusgraph.core.schema.Mapping.STRING\",\n                            \"parser\": \"enum\"\n                        }\n                    ]\n                }\n            ]\n        },\n        {\n            \"name\": \"mixedIndexWithCustomParameterKeyAndParserFullClassPath\",\n            \"indexOnly\": \"unidirectedMultiEdge\",\n            \"typeClass\": \"org.apache.tinkerpop.gremlin.structure.Edge\",\n            \"indexBackend\": \"search\",\n            \"keys\": [\n                {\n                    \"propertyKey\": \"anotherStringProperty\",\n                    \"parameters\": [\n                        {\n                            \"key\": \"%`custom%`similarity\",\n                            \"value\": \"boolean\",\n                            \"parser\": \"string\"\n                        },\n                        {\n                            \"key\": \"mapping\",\n                            \"value\": \"org.janusgraph.core.schema.Mapping.TEXTSTRING\",\n                            \"parser\": \"org.janusgraph.core.schema.json.parser.EnumJsonParameterParser\"\n                        }\n                    ]\n                }\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"schema/schema-init-strategies/#json-schema-initialization-flow","title":"JSON schema initialization flow","text":"<p>Currently, JSON schema initialization flow is simple and doesn't include any schema update or schema migration features.  The flow is split on multiple phases: 1) Creation of simple elements: <code>PropertyKey</code>, <code>VertexLabel</code>, <code>EdgeLabel</code>. 2) Creation of indices: Composite indexes, Vertex-centric Edge indexes, Vertex-centric Property indexes, Mixed indexes. 3) Indexes activation phase (configured via <code>schema.init.json.indices-activation</code>).</p> <p>Note</p> <p>Current implementation of JSON schema importer only creates elements, but it never updates them even if you change  definition of your property, edge, vertex, or index. If an element with such name exists - schema importer will skip it  without doing any updates on the element. Also, it never deletes any existing schema elements from the graph. For schema migration and removal processes use <code>graph.openManagement()</code> directly.</p> <p>Users who struggle to initialize schema due to created zombie JanusGraph instances in the cluster may leverage  configuration option <code>schema.init.json.force-close-other-instances</code>. This option will automatically close all  JanusGraph instance in the cluster (including activate instances). </p> <p>Warning</p> <p>When using <code>schema.init.json.force-close-other-instances</code> JanusGraph will force-close any other instances in  the cluster. However, they may not know about it and continue to work in the cluster without receiving any schema  updates. This could lead to split brain problem and corrupt current data.  It's advised to use <code>graph.unique-instance-id</code> and <code>graph.replace-instance-if-exists</code> options instead to prevent  creation of JanusGraph zombie instances.</p>"},{"location":"schema/schema-init-strategies/#json-schema-definition-api","title":"JSON Schema Definition API","text":"<p>It's also possible to manually trigger JSON schema definition API instead of relaying on startup schema  initialization process. All helper methods for JSON schema initialization are located in  <code>org.janusgraph.core.schema.JsonSchemaInitStrategy</code>.</p> <pre><code>// Schema from file\nJsonSchemaInitStrategy.initializeSchemaFromFile(graph, \"/path/to/schema.json\")\n\n// Schema from string\nJsonSchemaInitStrategy.initializeSchemaFromString(graph, \"{ \\\"vertexLabels\\\": [ { \\\"label\\\": \\\"my_vertex\\\" } ] }\")\n</code></pre> <p>Also, there are additional <code>initializeSchemaFromFile</code> and <code>initializeSchemaFromString</code> methods where it's possible to  provide all configuration options directly instead of fetching them from the configuration of the graph: - <code>initializeSchemaFromFile(JanusGraph graph, boolean createSchemaElements, boolean createSchemaIndices, IndicesActivationType indicesActivationType, boolean forceRollBackActiveTransactions, boolean forceCloseOtherInstances, long indexStatusTimeout, String jsonSchemaFilePath)</code> - <code>initializeSchemaFromString(JanusGraph graph, boolean createSchemaElements, boolean createSchemaIndices, IndicesActivationType indicesActivationType, boolean forceRollBackActiveTransactions, boolean forceCloseOtherInstances, long indexStatusTimeout, String jsonSchemaString)</code></p>"},{"location":"schema/index-management/index-lifecycle/","title":"Index Lifecycle","text":"<p>JanusGraph uses only indexes which have status <code>ENABLED</code>.  When the index is created it will not be used by JanusGraph until it is enabled.  After the index is build you should wait until it is registered (i.e. available) by JanusGraph: <pre><code>//Wait for the index to become available (i.e. wait for status REGISTERED)\nManagementSystem.awaitGraphIndexStatus(graph, \"myIndex\").call();\n</code></pre></p> <p>After the index is registered we should either enable the index (if we are sure that the current data should not be indexed by the newly created index) or we should reindex current data so that it would be available in the newly created index.</p> <p>Reindex the existing data and automatically enable the index example: <pre><code>mgmt = graph.openManagement();\nmgmt.updateIndex(mgmt.getGraphIndex(\"myIndex\"), SchemaAction.REINDEX).get();\nmgmt.commit();\n</code></pre></p> <p>Enable the index without reindexing existing data example: <pre><code>mgmt = graph.openManagement();\nmgmt.updateIndex(mgmt.getGraphIndex(\"myAnotherIndex\"), SchemaAction.ENABLE_INDEX).get();\nmgmt.commit();\n</code></pre></p>"},{"location":"schema/index-management/index-lifecycle/#index-states-and-transitions","title":"Index states and transitions","text":"<p>The diagram below shows the possible states of an index in JanusGraph. Transitions which should be used with caution are shown in grey. These are either not recommended to be used in production scenarios or have more complex implications to be aware of.</p> <p></p>"},{"location":"schema/index-management/index-lifecycle/#states-schemastatus","title":"States (SchemaStatus)","text":"<p>An index can be in one of the following states:</p> INSTALLED <p>The index has been created on the current JanusGraph instance, but it is not yet known to all instances in the cluster. This state is not shown in the diagram above because it is not stable in terms of synchronization between instances.</p> REGISTERED <p>The index is known to all instances in the cluster. However, it is not in use yet. In this state, the index contains no data and will not be used in queries.</p> ENABLED <p>The index is enabled and can be used to answer queries. Furthermore, this is the only state in which insertions and deletions in the graph are forwarded to the index.</p> DISABLED <p>The index is disabled and will not be used to answer queries. It also does no longer receive graph updates. Therefore, it is recommended to use <code>REINDEX</code> to re-enable the index.</p> DISCARDED <p>The index has been decommissioned and its contents have been deleted. The only remaining artifact is the schema vertex representing the index.</p>"},{"location":"schema/index-management/index-lifecycle/#actions-schemaaction","title":"Actions (SchemaAction)","text":"<p>The following actions can be performed on an index to change its state via <code>mgmt.updateIndex()</code>:</p> REGISTER_INDEX Registers the index with all instances in the cluster. After an index is installed, it must be registered with all JanusGraph instances. REINDEX Re-builds the index from the ground up using the data stored in the graph. Depending on the size of the graph, this action can take a long time to complete. Reindexing is possible in all stable states and automatically enables the index once finished. ENABLE_INDEX Enables the index so that it can be used by the query processing engine. An index must be registered before it can be enabled. If enabling the index manually instead of performing a reindex, be aware that past modifications of the graph are not represented in the index. Enabling a previously disabled index without <code>REINDEX</code> may cause index queries to return ghost vertices, which have been deleted from the graph while the index was disabled. DISABLE_INDEX Disables the index temporarily so that it is no longer used to answer queries. The index also stops receiving updates in the graph. DISCARD_INDEX Removes all data from the index, preparing it for removal. This operation is supported for all composite indices and for some mixed index backends. For more information on index removal see Removal. MARK_DISCARDED All indices which can not be discarded using DISCARD_INDEX have to be discarded manually in the backend. As this is not recognized by JanusGraph automatically, the state can be set to DISCARDED manually via this action. DROP_INDEX Removes the index from the schema and communicates the change to other instances in the cluster. After an index has been dropped, a new index is allowed to use the same name again."},{"location":"schema/index-management/index-performance/","title":"Indexing for Better Performance","text":"<p>JanusGraph supports two different kinds of indexing to speed up query processing: graph indexes and vertex-centric indexes (a.k.a. relation indexes). Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Graph indexes make these global retrieval operations efficient on large graphs. Vertex-centric indexes speed up the actual traversal through the graph, in particular when traversing through vertices with many incident edges.</p>"},{"location":"schema/index-management/index-performance/#graph-index","title":"Graph Index","text":"<p>Graph indexes are global index structures over the entire graph which allow efficient retrieval of vertices or edges by their properties for sufficiently selective conditions. For instance, consider the following queries <pre><code>g.V().has('name', 'hercules')\ng.E().has('reason', textContains('loves'))\n</code></pre></p> <p>The first query asks for all vertices with the name <code>hercules</code>. The second asks for all edges where the property reason contains the word <code>loves</code>. Without a graph index answering those queries would require a full scan over all vertices or edges in the graph to find those that match the given condition which is very inefficient and infeasible for huge graphs.</p> <p>JanusGraph distinguishes between two types of graph indexes: composite and mixed indexes. Composite indexes are very fast and efficient but limited to equality lookups for a particular, previously-defined combination of property keys. Mixed indexes can be used for lookups on any combination of indexed keys and support multiple condition predicates in addition to equality depending on the backing index store.</p> <p>Both types of indexes are created through the JanusGraph management system and the index builder returned by <code>JanusGraphManagement.buildIndex(String, Class)</code> where the first argument defines the name of the index and the second argument specifies the type of element to be indexed (e.g. <code>Vertex.class</code>). The name of a graph index must be unique. Graph indexes built against newly defined property keys, i.e. property keys that are defined in the same management transaction as the index, are immediately available. The same applies to graph indexes that are constrained to a label that is created in the same management transaction as the index. Graph indexes built against property keys that are already in use without being constrained to a newly created label require the execution of a reindex procedure  to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the index will not be available. It is encouraged to define graph indexes in the same transaction as the initial schema.</p> <p>Note</p> <p>In the absence of an index, JanusGraph will default to a full graph scan in order to retrieve the desired list of vertices. While this produces the correct result set, the graph scan can be very inefficient and lead to poor overall system performance in a production environment. Enable the <code>force-index</code> configuration option in production deployments of JanusGraph to prohibit graph scans.</p> <p>Info</p> <p>See index lifecycle documentation for more information about index states.</p>"},{"location":"schema/index-management/index-performance/#composite-index","title":"Composite Index","text":"<p>Composite indexes are stored in a separate store called <code>graphIndex</code>. For example, if your storage backend is Cassandra, you will see a table named <code>graphIndex</code> under your namespace. Composite indexes retrieve vertices or edges by one or a (fixed) composition of multiple keys. Consider the following composite index definitions. <pre><code>graph.tx().rollback() //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\nage = mgmt.getPropertyKey('age')\nmgmt.buildIndex('byNameComposite', Vertex.class).addKey(name).buildCompositeIndex()\nmgmt.buildIndex('byNameAndAgeComposite', Vertex.class).addKey(name).addKey(age).buildCompositeIndex()\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameComposite').call()\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameAndAgeComposite').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameComposite\"), SchemaAction.REINDEX).get()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameAndAgeComposite\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>First, two property keys <code>name</code> and <code>age</code> are already defined. Next, a simple composite index on just the name property key is built. JanusGraph will use this index to answer the following query. <pre><code>g.V().has('name', 'hercules')\n</code></pre></p> <p>The second composite graph index includes both keys. JanusGraph will use this index to answer the following query. <pre><code>g.V().has('age', 30).has('name', 'hercules')\n</code></pre></p> <p>Note, that all keys of a composite graph index must be found in the query\u2019s equality conditions for this index to be used. For example, the following query cannot be answered with either of the indexes because it only contains a constraint on <code>age</code> but not <code>name</code>. <pre><code>g.V().has('age', 30)\n</code></pre></p> <p>Also note, that composite graph indexes can only be used for equality constraints like those in the queries above. The following query would be answered with just the simple composite index defined on the <code>name</code> key because the age constraint is not an equality constraint. <pre><code>g.V().has('name', 'hercules').has('age', inside(20, 50))\n</code></pre></p> <p>Composite indexes do not require configuration of an external indexing backend and are supported through the primary storage backend. Hence, composite index modifications are persisted through the same transaction as graph modifications which means that those changes are atomic and/or consistent if the underlying storage backend supports atomicity and/or consistency.</p> <p>Note</p> <p>A composite index may comprise just one or multiple keys. A composite index with just one key is sometimes referred to as a key-index.</p>"},{"location":"schema/index-management/index-performance/#index-uniqueness","title":"Index Uniqueness","text":"<p>Composite indexes can also be used to enforce property uniqueness in the graph. If a composite graph index is defined as <code>unique()</code> there can be at most one vertex or edge for any given concatenation of property values associated with the keys of that index. For instance, to enforce that names are unique across the entire graph the following composite graph index would be defined. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\nmgmt.buildIndex('byNameUnique', Vertex.class).addKey(name).unique().buildCompositeIndex()\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameUnique').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameUnique\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>Note</p> <p>To enforce uniqueness against an eventually consistent storage backend, the consistency of the index must be explicitly set to enabling locking.</p>"},{"location":"schema/index-management/index-performance/#mixed-index","title":"Mixed Index","text":"<p>Mixed indexes retrieve vertices or edges by any combination of previously added property keys. Mixed indexes provide more flexibility than composite indexes and support additional condition predicates beyond equality. On the other hand, mixed indexes are slower for most equality queries than composite indexes.</p> <p>Unlike composite indexes, mixed indexes require the configuration of an indexing backend and use that indexing backend to execute lookup operations. JanusGraph can support multiple indexing backends in a single installation. Each indexing backend must be uniquely identified by name in the JanusGraph configuration which is called the indexing backend name. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\nage = mgmt.getPropertyKey('age')\nmgmt.buildIndex('nameAndAge', Vertex.class).addKey(name).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'nameAndAge').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"nameAndAge\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>The example above defines a mixed index containing the property keys <code>name</code> and <code>age</code>. The definition refers to the indexing backend name <code>search</code> so that JanusGraph knows which configured indexing backend it should use for this particular index. The <code>search</code> parameter specified in the buildMixedIndex call must match the second clause in the JanusGraph configuration definition like this: index.search.backend If the index was named solrsearch then the configuration definition would appear like this: index.solrsearch.backend.</p> <p>The mgmt.buildIndex example specified above uses text search as its default behavior. An index statement that explicitly defines the index as a text index can be written as follows: <pre><code>mgmt.buildIndex('nameAndAge',Vertex.class).addKey(name,Mapping.TEXT.asParameter()).addKey(age,Mapping.TEXT.asParameter()).buildMixedIndex(\"search\")\n</code></pre></p> <p>See Index Parameters and Full-Text Search for more information on text and string search options, and see the documentation section specific to the indexing backend in use for more details on how each backend handles text versus string searches.</p> <p>While the index definition example looks similar to the composite index above, it provides greater query support and can answer any of the following queries. <pre><code>g.V().has('name', textContains('hercules')).has('age', inside(20, 50))\ng.V().has('name', textContains('hercules'))\ng.V().has('age', lt(50))\ng.V().has('age', outside(20, 50))\ng.V().has('age', lt(50).or(gte(60)))\ng.V().or(__.has('name', textContains('hercules')), __.has('age', inside(20, 50)))\n</code></pre></p> <p>Mixed indexes support full-text search, range search, geo search and others. Refer to Search Predicates and Data Types for a list of predicates supported by a particular indexing backend.</p> <p>Note</p> <p>Unlike composite indexes, mixed indexes do not support uniqueness.</p>"},{"location":"schema/index-management/index-performance/#adding-property-keys","title":"Adding Property Keys","text":"<p>Property keys can be added to an existing mixed index which allows subsequent queries to include this key in the query condition. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nlocation = mgmt.makePropertyKey('location').dataType(Geoshape.class).make()\nnameAndAge = mgmt.getGraphIndex('nameAndAge')\nmgmt.addIndexKey(nameAndAge, location)\nmgmt.commit()\n//Previously created property keys already have the status ENABLED, but\n//our newly created property key \"location\" needs to REGISTER so we wait for both statuses\nManagementSystem.awaitGraphIndexStatus(graph, 'nameAndAge').status(SchemaStatus.REGISTERED, SchemaStatus.ENABLED).call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"nameAndAge\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>To add a newly defined key, we first retrieve the existing index from the management transaction by its name and then invoke the <code>addIndexKey</code> method to add the key to this index.</p> <p>If the added key is defined in the same management transaction, it will be immediately available for querying. If the property key has already been in use, adding the key requires the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the key will not be available in the mixed index.</p>"},{"location":"schema/index-management/index-performance/#mapping-parameters","title":"Mapping Parameters","text":"<p>When adding a property key to a mixed index - either through the index builder or the <code>addIndexKey</code> method - a list of parameters can be optionally specified to adjust how the property value is mapped into the indexing backend. Refer to the mapping parameters overview for a complete list of parameter types supported by each indexing backend.</p>"},{"location":"schema/index-management/index-performance/#ordering","title":"Ordering","text":"<p>The order in which the results of a graph query are returned can be defined using the <code>order().by()</code> directive. The <code>order().by()</code> method expects two parameters:</p> <ul> <li> <p>The name of the property key by which to order the results. The     results will be ordered by the value of the vertices or edges for     this property key.</p> </li> <li> <p>The sort order: either ascending <code>asc</code> or descending <code>desc</code></p> </li> </ul> <p>For example, the query <code>g.V().has('name', textContains('hercules')).order().by('age', desc).limit(10)</code> retrieves the ten oldest individuals with hercules in their name.</p> <p>When using <code>order().by()</code> it is important to note that:</p> <ul> <li> <p>Composite graph indexes do not natively support ordering search     results. All results will be retrieved and then sorted in-memory.     For large result sets, this can be very expensive.</p> </li> <li> <p>Mixed indexes support ordering natively and efficiently. However,     the property key used in the order().by() method must have been     previously added to the mixed indexed for native result ordering     support. This is important in cases where the the order().by() key     is different from the query keys. If the property key is not part of     the index, then sorting requires loading all results into memory.</p> </li> </ul>"},{"location":"schema/index-management/index-performance/#label-constraint","title":"Label Constraint","text":"<p>In many cases it is desirable to only index vertices or edges with a particular label. For instance, one may want to index only gods by their name and not every single vertex that has a name property. When defining an index it is possible to restrict the index to a particular vertex or edge label using the <code>indexOnly</code> method of the index builder. The following creates a composite index for the property key <code>name</code> that indexes only vertices labeled <code>god</code>. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\ngod = mgmt.getVertexLabel('god')\nmgmt.buildIndex('byNameAndLabel', Vertex.class).addKey(name).indexOnly(god).buildCompositeIndex()\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameAndLabel').call()\n//You can check the indexOnly constraint built just now\nmgmt = graph.openManagement()\nmgmt.getIndexOnlyConstraint(\"byNameAndLabel\")\n//Reindex the existing data\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameAndLabel\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>Label restrictions similarly apply to mixed indexes. When a composite index with label restriction is defined as unique, the uniqueness constraint only applies to properties on vertices or edges for the specified label.</p>"},{"location":"schema/index-management/index-performance/#inlining-vertex-properties-into-a-composite-index","title":"Inlining vertex properties into a Composite Index","text":"<p>Inlining vertex properties into a Composite Index structure can offer significant performance and efficiency benefits.</p> <ol> <li> <p>Performance Improvements Faster Querying: Inlining vertex properties directly within the index allows the search engine to retrieve all relevant data from the index itself.  This means, queries don\u2019t need to make additional calls to data stores to fetch full vertex information, significantly reducing lookup time.</p> </li> <li> <p>Data Locality In distributed storages, having inlined properties ensures that more complete data exists within individual partitions or shards. This reduces cross-node network calls and improves the overall query performance by ensuring data is more local to the request being processed.</p> </li> <li> <p>Cost of Indexing vs. Storage Trade-off While inlining properties increases the size of the index (potentially leading to more extensive index storage requirements),  it is often a worthwhile trade-off for performance, mainly when query speed is critical.  This is a typical pattern in systems optimized for read-heavy workloads.</p> </li> </ol>"},{"location":"schema/index-management/index-performance/#usage","title":"Usage","text":"<p>In order to take advantage of the inlined properties feature, JanusGraph Transaction should be set to use <code>.propertyPrefetching(false)</code></p> <p>Example:</p> <pre><code>//Build index\nmgmt.buildIndex(\"composite\", Vertex.class)\n    .addKey(idKey)\n    .addInlinePropertyKey(nameKey)\n    .buildCompositeIndex()\nmgmt.commit()\n\n//Query\ntx = graph.buildTransaction()\n    .propertyPrefetching(false) //this is important\n    .start()\n\ntx.traversal().V().has(\"id\", 100).next().value(\"name\")\n</code></pre>"},{"location":"schema/index-management/index-performance/#composite-versus-mixed-indexes","title":"Composite versus Mixed Indexes","text":"<ol> <li> <p>Use a composite index for exact match index retrievals. Composite     indexes do not require configuring or operating an external index     system and are often significantly faster than mixed indexes.</p> <ol> <li>As an exception, use a mixed index for exact matches when the     number of distinct values for query constraint is relatively     small or if one value is expected to be associated with many     elements in the graph (i.e. in case of low selectivity).</li> </ol> </li> <li> <p>Use a mixed indexes for numeric range, full-text or geo-spatial     indexing. Also, using a mixed index can speed up the order().by()     queries.</p> </li> <li> <p>A composite index requires all fields to be present, while a mixed     index only needs at least one field to be present. For example, say     you have a composite index with <code>key1</code> and <code>key2</code>, a mixed index with     <code>key1</code> and <code>key3</code>. If you add a vertex with only property <code>key1</code>, then     JanusGraph will create a new mixed index entry but not a composite index     entry.</p> </li> </ol>"},{"location":"schema/index-management/index-performance/#vertex-centric-indexes","title":"Vertex-centric Indexes","text":"<p>Vertex-centric indexes, also known as Relation indexes, are local index structures built individually per vertex. They are stored together with edges and properties in <code>edgeStore</code>. There are two types of vertex-centric indexes, edge indexes and property indexes.</p>"},{"location":"schema/index-management/index-performance/#edge-indexes","title":"Edge Indexes","text":"<p>In large graphs vertices can have thousands of incident edges. Traversing through those vertices can be very slow because a large subset of the incident edges has to be retrieved and then filtered in memory to match the conditions of the traversal. Vertex-centric indexes can speed up such traversals by using localized index structures to retrieve only those edges that need to be traversed.</p> <p>Suppose that Hercules battled hundreds of monsters in addition to the three captured in the introductory Graph of the Gods. Without a vertex-centric index, a query asking for those monsters battled between time point <code>10</code> and <code>20</code> would require retrieving all <code>battled</code> edges even though there are only a handful of matching edges. <pre><code>h = g.V().has('name', 'hercules').next()\ng.V(h).outE('battled').has('time', inside(10, 20)).inV()\n</code></pre></p> <p>Building a vertex-centric index by time speeds up such traversal queries. Note, this initial index example already exists in the Graph of the Gods as an index named <code>edges</code>. As a result, running the steps below will result in a uniqueness constraint error. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\ntime = mgmt.getPropertyKey('time')\nbattled = mgmt.getEdgeLabel('battled')\nmgmt.buildEdgeIndex(battled, 'battlesByTime', Direction.BOTH, Order.desc, time)\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitRelationIndexStatus(graph, 'battlesByTime', 'battled').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getRelationIndex(battled, \"battlesByTime\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>This example builds a vertex-centric index which indexes <code>battled</code> edges in both direction by time in descending order. A vertex-centric index is built against a particular edge label which is the first argument to the index construction method <code>JanusGraphManagement.buildEdgeIndex()</code>. The index only applies to edges of this label - <code>battled</code> in the example above. The second argument is a unique name for the index. The third argument is the edge direction in which the index is built. The index will only apply to traversals along edges in this direction. In this example, the vertex-centric index is built in both direction which means that time restricted traversals along <code>battled</code> edges can be served by this index in both the <code>IN</code> and <code>OUT</code> direction. JanusGraph will maintain a vertex-centric index on both the in- and out-vertex of <code>battled</code> edges. Alternatively, one could define the index to apply to the <code>OUT</code> direction only which would speed up traversals from Hercules to the monsters but not in the reverse direction. This would only require maintaining one index and hence half the index maintenance and storage cost. The last two arguments are the sort order of the index and a list of property keys to index by. The sort order is optional and defaults to ascending order (i.e. <code>Order.ASC</code>). The list of property keys must be non-empty and defines the keys by which to index the edges of the given label. A vertex-centric index can be defined with multiple keys. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\ntime = mgmt.getPropertyKey('time')\nrating = mgmt.makePropertyKey('rating').dataType(Double.class).make()\nbattled = mgmt.getEdgeLabel('battled')\nmgmt.buildEdgeIndex(battled, 'battlesByRatingAndTime', Direction.OUT, Order.desc, rating, time)\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitRelationIndexStatus(graph, 'battlesByRatingAndTime', 'battled').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getRelationIndex(battled, 'battlesByRatingAndTime'), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>This example extends the schema by a <code>rating</code> property on <code>battled</code> edges and builds a vertex-centric index which indexes <code>battled</code> edges in the out-going direction by rating and time in descending order. Note, that the order in which the property keys are specified is important because vertex-centric indexes are prefix indexes. This means, that <code>battled</code> edges are indexed by <code>rating</code> first and <code>time</code> second. <pre><code>h = g.V().has('name', 'hercules').next()\ng.V(h).outE('battled').property('rating', 5.0) //Add some rating properties\ng.V(h).outE('battled').has('rating', gt(3.0)).inV()\ng.V(h).outE('battled').has('rating', 5.0).has('time', inside(10, 50)).inV()\ng.V(h).outE('battled').has('time', inside(10, 50)).inV()\n</code></pre></p> <p>Hence, the <code>battlesByRatingAndTime</code> index can speed up the first two but not the third query.</p>"},{"location":"schema/index-management/index-performance/#property-indexes","title":"Property Indexes","text":"<p>Similar to edge indexes, property indexes can be built to traverse properties based on associated meta-properties efficiently. The following toy example illustrates the usage of property indexes.</p> <pre><code>graph = JanusGraphFactory.open(\"inmemory\")\nmgmt = graph.openManagement()\ntimestamp = mgmt.makePropertyKey(\"timestamp\").dataType(Integer.class).make()\namount = mgmt.makePropertyKey(\"amount\").dataType(Integer.class).cardinality(Cardinality.LIST).make()\nmgmt.buildPropertyIndex(amount, 'amountByTime', Order.desc, timestamp)\nmgmt.commit()\n\nbob = graph.addVertex()\nbob.property(\"amount\", 100, \"timestamp\", 1600000000)\nbob.property(\"amount\", 200, \"timestamp\", 1500000000)\nbob.property(\"amount\", -150, \"timestamp\", 1550000000)\ngraph.tx().commit()\n\ng = graph.traversal()\ng.V(bob).properties(\"amount\").has(\"timestamp\", P.gt(1500000000))\n</code></pre> <p>In the above example, we model a number of transactions Bob has made. <code>amount</code> is a vertex property that records the amount of money involved in each transaction, while <code>timestamp</code> is a meta property of <code>amount</code> which records the time that a particular record happens. By creating a property index, we can efficiently retrieve transaction amounts by timestamp. Alternatively, we can also model the transactions as edges, then both <code>amount</code> and <code>timestamp</code> will be edge properties, and we could use Edge indexes to speed up the query.</p> <p>Multiple vertex-centric indexes can be built for the same edge label in order to support different constraint traversals. JanusGraph\u2019s query optimizer attempts to pick the most efficient index for any given traversal. Vertex-centric indexes only support equality and range/interval constraints.</p> <p>Note</p> <p>The property keys used in a vertex-centric index must have an explicitly defined data type (i.e. not <code>Object.class</code>) which supports a native sort order. This means not only that they must implement <code>Comparable</code>  but that their serializer must implement <code>OrderPreservingSerializer</code>.  The types that are currently supported are <code>Boolean</code>, <code>UUID</code>, <code>Byte</code>, <code>Float</code>, <code>Long</code>, <code>String</code>,  <code>Integer</code>, <code>Date</code>, <code>Double</code>, <code>Character</code>, and <code>Short</code></p> <p>If the vertex-centric index is built against either an edge label or at least one property key that is defined in the same management transaction, the index will be immediately available for querying. If both the edge label and all of the indexed property keys have already been in use, building a vertex-centric index against it requires the execution of a reindex procedure to ensure that the index contains all previously added edges. Until the reindex procedure has completed, the index will not be available.</p> <p>Note</p> <p>JanusGraph automatically builds vertex-centric indexes per edge label and property key. That means, even with thousands of incident <code>battled</code> edges, queries like <code>g.V(h).out('mother')</code> or <code>g.V(h).values('age')</code> are efficiently answered by the local index.</p> <p>Vertex-centric indexes cannot speed up unconstrained traversals which require traversing through all incident edges of a particular label. Those traversals will become slower as the number of incident edges increases. Often, such traversals can be rewritten as constrained traversals that can utilize a vertex-centric index to ensure acceptable performance at scale.</p>"},{"location":"schema/index-management/index-performance/#using-vertex-centric-indexes-on-adjacent-vertex-ids","title":"Using vertex-centric indexes on adjacent vertex ids","text":"<p>In some cases it is relevant to find an edge based on properties of the adjacent vertex. Let's say we want to find out whether or not Hercules has battled Cerberus. <pre><code>h = g.V().has('name', 'hercules').next()\ng.V(h).out('battled').has('name', 'cerberus').hasNext()\n</code></pre></p> <p>A query like this can not use a vertex centric index because it filters on vertex properties rather than edge properties. But by restructuring the query, we can achieve exactly this. As both vertices are known, the vertex ids can be used to select the edge. <pre><code>h = g.V().has('name', 'hercules').next()\nc = g.V().has('name', 'cerberus').next()\n</code></pre></p> <p>In contrast to the name \"Cebereus\", which is a property of the adjacent vertex, the id of this vertex is already saved within the connecting edge itself. Therefore, this query runs much faster if hercules has battled many opponents: <pre><code>g.V(h).outE('battled').where(inV().is(c)).hasNext()\n</code></pre></p> <p>... or even shorter: <pre><code>g.V(h).out('battled').is(c).limit(1).hasNext()\n</code></pre></p> <p>Assuming there is a global index on the <code>name</code> property, this improves the performance a lot, because it's not necessary to fetch every adjacent vertex anymore. In addition, a vertex-centric index on adjacent vertex ids does not need to be constructed and maintained explicitly. Due to the data model, efficient access to edges by their adjacent vertex id is already provided by the storage backend.</p>"},{"location":"schema/index-management/index-performance/#ordered-traversals","title":"Ordered Traversals","text":"<p>The following queries specify an order in which the incident edges are to be traversed. Use the <code>localLimit</code> command to retrieve a subset of the edges (in a given order) for EACH vertex that is traversed. <pre><code>h = g.V().has('name', 'hercules').next()\ng.V(h).local(outE('battled').order().by('time', desc).limit(10)).inV().values('name')\ng.V(h).local(outE('battled').has('rating', 5.0).order().by('time', desc).limit(10)).values('place')\n</code></pre></p> <p>The first query asks for the names of the 10 most recently battled monsters by Hercules. The second query asks for the places of the 10 most recent battles of Hercules that are rated 5 stars. In both cases, the query is constrained by an order on a property key with a limit on the number of elements to be returned.</p> <p>Such queries can also be efficiently answered by vertex-centric indexes if the order key matches the key of the index and the requested order (i.e. ascending or descending) is the same as the one defined for the index. The <code>battlesByTime</code> index would be used to answer the first query and <code>battlesByRatingAndTime</code> applies to the second. Note, that the <code>battlesByRatingAndTime</code> index cannot be used to answer the first query because an equality constraint on <code>rating</code> must be present for the second key in the index to be effective.</p>"},{"location":"schema/index-management/index-reindexing/","title":"Reindexing","text":"<p>Graph Index and  Vertex-centric Indexes  describe how to build graph-global and vertex-centric indexes to improve query performance. These indexes are immediately available if the indexed keys or labels have been newly defined in the same management transaction. In this case, there is no need to reindex the graph and this section can be skipped. If the indexed keys and labels already existed prior to index construction it is necessary to reindex the entire graph in order to ensure that the index contains previously added elements. This section describes the reindexing process.</p> <p>Warning</p> <p>Reindexing is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.</p>"},{"location":"schema/index-management/index-reindexing/#overview","title":"Overview","text":"<p>JanusGraph can begin writing incremental index updates right after an index is defined. However, before the index is complete and usable, JanusGraph must also take a one-time read pass over all existing graph elements associated with the newly indexed schema type(s). Once this reindexing job has completed, the index is fully populated and ready to be used. The index must then be enabled to be used during query processing.</p>"},{"location":"schema/index-management/index-reindexing/#prior-to-reindex","title":"Prior to Reindex","text":"<p>The starting point of the reindexing process is the construction of an index. Refer to Indexing for Better Performance for a complete discussion of global graph and vertex-centric indexes. Note, that a global graph index is uniquely identified by its name. A vertex-centric index is uniquely identified by the combination of its name and the edge label or property key on which the index is defined - the name of the latter is referred to as the index type in this section and only applies to vertex-centric indexes.</p> <p>After building a new index against existing schema elements it is recommended to wait a few minutes for the index to be announced to the cluster. Note the index name (and the index type in case of a vertex-centric index) since this information is needed when reindexing.</p>"},{"location":"schema/index-management/index-reindexing/#preparing-to-reindex","title":"Preparing to Reindex","text":"<p>There is a choice between two execution frameworks for reindex jobs:</p> <ul> <li>MapReduce</li> <li>JanusGraphManagement</li> </ul> <p>Reindex on MapReduce supports large, horizontally-distributed databases. Reindex on JanusGraphManagement spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine.</p> <p>Reindexing requires:</p> <ul> <li>The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when     building a new index)</li> <li>The index type (a string\u2009\u2014\u2009the name of the edge label or property     key on which the vertex-centric index is built). This applies only     to vertex-centric indexes - leave blank for global graph indexes.</li> </ul>"},{"location":"schema/index-management/index-reindexing/#executing-a-reindex-job-on-mapreduce","title":"Executing a Reindex Job on MapReduce","text":"<p>The recommended way to generate and run a reindex job on MapReduce is through the <code>MapReduceIndexManagement</code> class. Here is a rough outline of the steps to run a reindex job using this class:</p> <ul> <li>Open a <code>JanusGraph</code> instance</li> <li>Pass the graph instance into <code>MapReduceIndexManagement</code>'s constructor</li> <li>Call <code>updateIndex(&lt;index&gt;, SchemaAction.REINDEX)</code> on the <code>MapReduceIndexManagement</code> instance</li> <li>If the index has not yet been enabled, enable it through <code>JanusGraphManagement</code></li> </ul> <p>This class implements an <code>updateIndex</code> method that supports only the <code>REINDEX</code> and <code>DISCARD_INDEX</code> actions for its <code>SchemaAction</code> parameter. The class starts a Hadoop MapReduce job using the Hadoop configuration and jars on the classpath. Both Hadoop 1 and 2 are supported. This class gets metadata about the index and storage backend (e.g. the Cassandra partitioner) from the <code>JanusGraph</code> instance given to its constructor. <pre><code>graph = JanusGraphFactory.open(...)\nmgmt = graph.openManagement()\nmr = new MapReduceIndexManagement(graph)\nmr.updateIndex(mgmt.getRelationIndex(mgmt.getRelationType(\"battled\"), \"battlesByTime\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p>"},{"location":"schema/index-management/index-reindexing/#reindex-example-on-mapreduce","title":"Reindex Example on MapReduce","text":"<p>The following Gremlin snippet outlines all steps of the MapReduce reindex process in one self-contained example using minimal dummy data against the Cassandra storage backend. <pre><code>// Open a graph\ngraph = JanusGraphFactory.open(\"conf/janusgraph-cql-es.properties\")\ng = graph.traversal()\n\n// Define a property\nmgmt = graph.openManagement()\ndesc = mgmt.makePropertyKey(\"desc\").dataType(String.class).make()\nmgmt.commit()\n\n// Insert some data\ngraph.addVertex(\"desc\", \"foo bar\")\ngraph.addVertex(\"desc\", \"foo baz\")\ngraph.tx().commit()\n\n// Run a query -- note the planner warning recommending the use of an index\ng.V().has(\"desc\", containsText(\"baz\"))\n\n// Create an index\nmgmt = graph.openManagement()\n\ndesc = mgmt.getPropertyKey(\"desc\")\nmixedIndex = mgmt.buildIndex(\"mixedExample\", Vertex.class).addKey(desc).buildMixedIndex(\"search\")\nmgmt.commit()\n\n// Rollback or commit transactions on the graph which predate the index definition\ngraph.tx().rollback()\n\n// Block until the SchemaStatus transitions from INSTALLED to REGISTERED\nreport = ManagementSystem.awaitGraphIndexStatus(graph, \"mixedExample\").call()\n\n// Run a JanusGraph-Hadoop job to reindex\nmgmt = graph.openManagement()\nmr = new MapReduceIndexManagement(graph)\nmr.updateIndex(mgmt.getGraphIndex(\"mixedExample\"), SchemaAction.REINDEX).get()\n\n// Enable the index\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"mixedExample\"), SchemaAction.ENABLE_INDEX).get()\nmgmt.commit()\n\n// Block until the SchemaStatus is ENABLED\nmgmt = graph.openManagement()\nreport = ManagementSystem.awaitGraphIndexStatus(graph, \"mixedExample\").status(SchemaStatus.ENABLED).call()\nmgmt.rollback()\n\n// Run a query -- JanusGraph will use the new index, no planner warning\ng.V().has(\"desc\", containsText(\"baz\"))\n\n// Concerned that JanusGraph could have read cache in that last query, instead of relying on the index?\n// Start a new instance to rule out cache hits.  Now we're definitely using the index.\ngraph.close()\ngraph = JanusGraphFactory.open(\"conf/janusgraph-cql-es.properties\")\ng.V().has(\"desc\", containsText(\"baz\"))\n</code></pre></p>"},{"location":"schema/index-management/index-reindexing/#executing-a-reindex-job-on-managementsystem","title":"Executing a Reindex job on ManagementSystem","text":"<p>To run a reindex job on ManagementSystem, invoke <code>ManagementSystem.updateIndex</code> with the <code>SchemaAction.REINDEX</code> argument. For example: <pre><code>m = graph.openManagement()\ni = m.getGraphIndex('indexName')\nm.updateIndex(i, SchemaAction.REINDEX).get()\nm.commit()\n</code></pre></p> <p>ManagementSystem uses a local thread pool to run reindexing jobs concurrently. By default, the concurrency level equals the number of available processors. If you want to change the concurrency level, you can add a parameter like this: <pre><code>// only use one thread to run reindexing\nm.updateIndex(i, SchemaAction.REINDEX, 1).get()\n</code></pre></p>"},{"location":"schema/index-management/index-reindexing/#example-for-managementsystem","title":"Example for ManagementSystem","text":"<p>The following loads some sample data into a BerkeleyDB-backed JanusGraph database, defines an index after the fact, reindexes using ManagementSystem, and finally enables and uses the index: <pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load some data from a file without any predefined schema\ngraph = JanusGraphFactory.open('conf/janusgraph-berkeleyje.properties')\ng = graph.traversal()\nm = graph.openManagement()\nm.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.LIST).make()\nm.makePropertyKey('lang').dataType(String.class).cardinality(Cardinality.LIST).make()\nm.makePropertyKey('age').dataType(Integer.class).cardinality(Cardinality.LIST).make()\nm.commit()\ngraph.io(IoCore.gryo()).readGraph('data/tinkerpop-modern.gio')\ngraph.tx().commit()\n\n// Run a query -- note the planner warning recommending the use of an index\ng.V().has('name', 'lop')\ngraph.tx().rollback()\n\n// Create an index\nm = graph.openManagement()\nm.buildIndex('names', Vertex.class).addKey(m.getPropertyKey('name')).buildCompositeIndex()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from INSTALLED to REGISTERED\nManagementSystem.awaitGraphIndexStatus(graph, 'names').status(SchemaStatus.REGISTERED).call()\n\n// Reindex using JanusGraphManagement\nm = graph.openManagement()\ni = m.getGraphIndex('names')\nm.updateIndex(i, SchemaAction.REINDEX)\nm.commit()\n\n// Wait for the index to be enabled\nManagementSystem.awaitGraphIndexStatus(graph, 'names').status(SchemaStatus.ENABLED).call()\n\n// Run a query -- JanusGraph will use the new index, no planner warning\ng.V().has('name', 'lop')\ngraph.tx().rollback()\n\n// Concerned that JanusGraph could have read cache in that last query, instead of relying on the index?\n// Start a new instance to rule out cache hits.  Now we're definitely using the index.\ngraph.close()\ngraph = JanusGraphFactory.open(\"conf/janusgraph-berkeleyje.properties\")\ng = graph.traversal()\ng.V().has('name', 'lop')\n</code></pre></p>"},{"location":"schema/index-management/index-reindexing/#common-problems","title":"Common problems","text":""},{"location":"schema/index-management/index-reindexing/#illegalargumentexception-when-starting-job","title":"IllegalArgumentException when starting job","text":"<p>When a reindexing job is started shortly after a the index has been built, the job might fail with an exception like one of the following:</p> <pre><code>The index mixedExample is in an invalid state and cannot be indexed.\nThe following index keys have invalid status: desc has status INSTALLED\n(status must be one of [REGISTERED, ENABLED, DISABLED])\n\nThe index mixedExample is in an invalid state and cannot be indexed.\nThe index has status INSTALLED, but one of [REGISTERED, ENABLED, DISABLED] is required\n</code></pre> <p>When an index is built, its existence is broadcast to all other JanusGraph instances in the cluster. Those must acknowledge the existence of the index before the reindexing process can be started. The acknowledgments can take a while to come in depending on the size of the cluster and the connection speed. Hence, one should wait a few minutes after building the index and before starting the reindex process.</p> <p>Note, that the acknowledgment might fail due to JanusGraph instance failure. In other words, the cluster might wait indefinitely on the acknowledgment of a failed instance. In this case, the user must manually remove the failed instance from the cluster registry as described in Failure &amp; Recovery. After the cluster state has been restored, the acknowledgment process must be reinitiated by manually registering the index again in the management system.</p> <pre><code>mgmt = graph.openManagement()\nrindex = mgmt.getRelationIndex(mgmt.getRelationType(\"battled\"),\"battlesByTime\")\nmgmt.updateIndex(rindex, SchemaAction.REGISTER_INDEX).get()\ngindex = mgmt.getGraphIndex(\"byName\")\nmgmt.updateIndex(gindex, SchemaAction.REGISTER_INDEX).get()\nmgmt.commit()\n</code></pre> <p>After waiting a few minutes for the acknowledgment to arrive the reindex job should start successfully.</p>"},{"location":"schema/index-management/index-reindexing/#could-not-find-index","title":"Could not find index","text":"<p>This exception in the reindexing job indicates that an index with the given name does not exist or that the name has not been specified correctly. When reindexing a global graph index, only the name of the index as defined when building the index should be specified. When reindexing a global graph index, the name of the index must be given in addition to the name of the edge label or property key on which the vertex-centric index is defined.</p>"},{"location":"schema/index-management/index-removal/","title":"Removal","text":"<p>Warning</p> <p>Index removal is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.</p>"},{"location":"schema/index-management/index-removal/#overview","title":"Overview","text":"<p>Index removal is a multi-stage process. In the first stage, one JanusGraph instance deactivates the index by setting its state to <code>DISABLED</code>. At that point, JanusGraph stops using the index to answer queries and stops incrementally updating the index. Index-related data in the storage backend remains present but ignored.</p> <p>The second stage depends on whether the index is mixed or composite. All composite indices can be deleted via JanusGraph. As with reindexing, removal can be done through either MapReduce or JanusGraphManagement. However, not all mixed index backends allow automatic removal. Index backends for which JanusGraph does not provide an automated deletion mechanism\u2009\u2014\u2009currently Lucene and Solr\u2009\u2014\u2009must be manually dropped in the index backend. For supported mixed index backends, the removal process is analogue to the composite index removal, with the exception that neither MapReduce nor multi threading in JanusGraphManagement is necessary.</p> <p>Discarding an index deletes everything associated with the index except its schema definition and its <code>DISCARDED</code> state. Removing this schema stub for the index is the third step.</p>"},{"location":"schema/index-management/index-removal/#preparing-for-index-removal","title":"Preparing for Index Removal","text":"<p>If the index is currently enabled, it should first be disabled. This is done through the <code>ManagementSystem</code>. <pre><code>mgmt = graph.openManagement()\nrindex = mgmt.getRelationIndex(mgmt.getRelationType(\"battled\"), \"battlesByTime\")\nmgmt.updateIndex(rindex, SchemaAction.DISABLE_INDEX).get()\ngindex = mgmt.getGraphIndex(\"byName\")\nmgmt.updateIndex(gindex, SchemaAction.DISABLE_INDEX).get()\nmgmt.commit()\n</code></pre></p> <p>Once the status of all keys on the index changes to <code>DISABLED</code>, the index is ready to be removed. A utility in ManagementSystem can automate the wait-for-<code>DISABLED</code> step: <pre><code>ManagementSystem.awaitGraphIndexStatus(graph, 'byName').status(SchemaStatus.DISABLED).call()\n</code></pre></p> <p>After a composite index is <code>DISABLED</code>, there is a choice between two execution frameworks for its removal:</p> <ul> <li>MapReduce</li> <li>ManagementSystem</li> </ul> <p>Index removal on MapReduce supports large, horizontally-distributed databases. Index removal on ManagementSystem spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine.</p> <p>Index removal requires:</p> <ul> <li>The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when     building a new index)</li> <li>The index type (a string\u2009\u2014\u2009the name of the edge label or property     key on which the vertex-centric index is built). This applies only     to vertex-centric indices - leave blank for global graph indices.</li> </ul>"},{"location":"schema/index-management/index-removal/#executing-an-index-removal-job-on-mapreduce","title":"Executing an index removal job on MapReduce","text":"<p>Info</p> <p>This method applies to the following types of indices: </p> <ul> <li>All composite indices</li> </ul> <p>As with reindexing, the recommended way to generate and run an index removal job on MapReduce is through the <code>MapReduceIndexManagement</code> class. Here is a rough outline of the steps to run an index removal job using this class:</p> <ul> <li>Open a <code>JanusGraph</code> instance</li> <li>If the index has not yet been disabled, disable it through <code>JanusGraphManagement</code></li> <li>Pass the graph instance into <code>MapReduceIndexManagement</code>'s constructor</li> <li>Call <code>updateIndex(&lt;index&gt;, SchemaAction.DISCARD_INDEX)</code></li> </ul> <p>A commented code example follows in the next subsection.</p>"},{"location":"schema/index-management/index-removal/#example-for-mapreduce","title":"Example for MapReduce","text":"<pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load the \"Graph of the Gods\" sample data\ngraph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\ng = graph.traversal()\nGraphOfTheGodsFactory.load(graph)\n\ng.V().has('name', 'jupiter')\n\n// Disable the \"name\" composite index\nm = graph.openManagement()\nm.updateIndex(m.getGraphIndex('name'), SchemaAction.DISABLE_INDEX).get()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from ENABLED to DISABLED\nManagementSystem.awaitGraphIndexStatus(graph, 'name').status(SchemaStatus.DISABLED).call()\n\n// Delete the indexed data using MapReduceIndexJobs\nm = graph.openManagement()\nmr = new MapReduceIndexManagement(graph)\nfuture = mr.updateIndex(m.getGraphIndex('name'), SchemaAction.DISCARD_INDEX)\nm.commit()\ngraph.tx().commit()\nfuture.get()\n\n// Block until the SchemaStatus transitions from DISABLED to DISCARDED\nManagementSystem.awaitGraphIndexStatus(graph, 'name').status(SchemaStatus.DISCARDED).call()\n\n// Index still shows up in management interface as DISCARDED -- it can now be dropped entirely\nm = graph.openManagement()\nm.updateIndex(m.getGraphIndex('name'), SchemaAction.DROP_INDEX).get()\nm.commit()\n\n// JanusGraph should issue a warning about this query requiring a full scan\ng.V().has('name', 'jupiter')\n</code></pre>"},{"location":"schema/index-management/index-removal/#executing-an-index-removal-job-on-managementsystem","title":"Executing an index removal job on ManagementSystem","text":"<p>Info</p> <p>This method applies to the following types of indices:</p> <ul> <li>All composite indices</li> <li>Mixed indices (Elasticsearch only)</li> </ul> <p>To run an index removal job on ManagementSystem, invoke <code>ManagementSystem.updateIndex</code> with the <code>SchemaAction.DISCARD_INDEX</code> argument. For example: <pre><code>m = graph.openManagement()\nm.updateIndex(m.getGraphIndex('indexName'), SchemaAction.DISCARD_INDEX).get()\nm.commit()\n</code></pre></p> <p>Similar to reindex, ManagementSystem uses a local thread pool to execute index removal job concurrently. The concurrency level is equal to the number of available processors. If you want to change the default concurrency level, you can add a parameter as follows: <pre><code>// Use only one thread to execute index removal job\nm.updateIndex(m.getGraphIndex('indexName'), SchemaAction.DISCARD_INDEX, 1).get()\n</code></pre></p>"},{"location":"schema/index-management/index-removal/#example-for-managementsystem","title":"Example for ManagementSystem","text":"<p>The following loads some indexed sample data into a BerkeleyDB-backed JanusGraph database, then disables and removes the index through ManagementSystem: <pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load the \"Graph of the Gods\" sample data\ngraph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\ng = graph.traversal()\nGraphOfTheGodsFactory.load(graph)\n\ng.V().has('name', 'jupiter')\n\n// Disable the \"name\" composite index\nm = graph.openManagement()\nm.updateIndex(m.getGraphIndex('name'), SchemaAction.DISABLE_INDEX).get()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from ENABLED to DISABLED\nManagementSystem.awaitGraphIndexStatus(graph, 'name').status(SchemaStatus.DISABLED).call()\n\n// Delete the index using JanusGraphManagement\nm = graph.openManagement()\nfuture = m.updateIndex(m.getGraphIndex('name'), SchemaAction.DISCARD_INDEX)\nm.commit()\ngraph.tx().commit()\nfuture.get()\n\n// Block until the SchemaStatus transitions from DISABLED to DISCARDED\nManagementSystem.awaitGraphIndexStatus(graph, 'name').status(SchemaStatus.DISCARDED).call()\n\n// Index still shows up in management interface as DISCARDED -- it can now be dropped entirely\nm = graph.openManagement()\nm.updateIndex(m.getGraphIndex('name'), SchemaAction.DROP_INDEX).get()\nm.commit()\n\n// JanusGraph should issue a warning about this query requiring a full scan\ng.V().has('name', 'jupiter')\n</code></pre></p>"},{"location":"schema/index-management/index-removal/#executing-a-manual-index-removal-for-mixed-indices","title":"Executing a manual index removal for mixed indices","text":"<p>Info</p> <p>This method applies to the following types of indices:</p> <ul> <li>Mixed indices which can not be removed by Janusgraph automatically</li> </ul> <p>If an index backend does not support automatic removal, you will receive an exception when trying to execute <code>DISCARD_INDEX</code>. This currently applies to all mixed indices except for Elasticsearch. For those index backends, indices have to be removed by hand in the index backend without the help of JanusGraph. In order to inform JanusGraph that an index has been removed manually, the transition to the state <code>DISCARDED</code> can be manually triggered by executing <code>MARK_DISCARDED</code>.</p> <pre><code>m = graph.openManagement()\nm.updateIndex(m.getGraphIndex('indexName'), SchemaAction.MARK_DISCARDED).get()\nm.commit()\n</code></pre> <p>Warning</p> <p>This action does not execute any operation except overwriting the state of the index. Any data stored in the index backend will still be present.</p> <p>It is recommended to execute this step before actually touching the backend and deleting the data. This way, it is guaranteed that no instance concurrently re-enables the index. Once the index has entered the state <code>DISCARDED</code>, all indexed data can be safely deleted.</p>"},{"location":"schema/index-management/index-removal/#example-for-manual-mixed-index-removal","title":"Example for manual mixed index removal","text":"<pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load the \"Graph of the Gods\" sample data\ngraph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\ng = graph.traversal()\nGraphOfTheGodsFactory.load(graph)\n\ng.V().has(\"name\", \"jupiter\")\n\n// Disable the \"name\" composite index\nm = graph.openManagement()\nm.updateIndex(m.getGraphIndex(\"name\"), SchemaAction.DISABLE_INDEX).get()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from ENABLED to DISABLED\nManagementSystem.awaitGraphIndexStatus(graph, \"name\").status(SchemaStatus.DISABLED).call()\n\n// Since JanusGraph is not capable of removing the index, we have to set the state manually.\nm = graph.openManagement()\nfuture = m.updateIndex(m.getGraphIndex(\"name\"), SchemaAction.MARK_DISCARDED)\nm.commit()\ngraph.tx().commit()\nfuture.get()\n\n// Block until the SchemaStatus transitions from DISABLED to DISCARDED\nManagementSystem.awaitGraphIndexStatus(graph, \"name\").status(SchemaStatus.DISCARDED).call()\n\n/*\n * At this point, the index is marked as DISCARDED,\n * so it is ensured that no instance will ever be\n * able to use it again. You are now safe to manually\n * perform the necessary operations in your index\n * backend to remove the indexed data by hand.\n */\n\n// Index still shows up in management interface as DISCARDED -- it can now be dropped entirely\nm = graph.openManagement()\nm.updateIndex(m.getGraphIndex(\"name\"), SchemaAction.DROP_INDEX).get()\nm.commit()\n\n// JanusGraph should issue a warning about this query requiring a full scan\ng.V().has(\"name\", \"jupiter\")\n</code></pre>"},{"location":"storage-backend/","title":"Introduction","text":"<p>JanusGraph\u2019s data storage layer is pluggable. Implementations of the pluggable storage layer\u2009\u2014\u2009that is, the software component tells JanusGraph how to talk to its data store\u2009\u2014\u2009are called storage backends.</p> <p>This section describes configuration and administration of JanusGraph\u2019s standard storage backends.</p> <p>This section only addresses configuration of the stock storage backends that come with JanusGraph.</p>"},{"location":"storage-backend/bdb/","title":"Oracle Berkeley DB Java Edition","text":"<p>Oracle Berkeley DB Java Edition is an open source, embeddable, transactional storage engine written entirely in Java. It takes full advantage of the Java environment to simplify development and deployment. The architecture of Oracle Berkeley DB Java Edition supports very high performance and concurrency for both read-intensive and write-intensive workloads.</p> <p>\u2014  Oracle Berkeley DB Java Edition Homepage</p> <p>The Oracle Berkeley DB Java Edition storage backend runs in the same JVM as JanusGraph and provides local persistence on a single machine. Hence, the BerkeleyDB storage backend requires that all of the graph data fits on the local disk and all of the frequently accessed graph elements fit into main memory. This imposes a practical limitation of graphs with 10-100s million vertices on commodity hardware. However, for graphs of that size the BerkeleyDB storage backend exhibits high performance because all data can be accessed locally within the same JVM.</p>"},{"location":"storage-backend/bdb/#berkeleydb-je-setup","title":"BerkeleyDB JE Setup","text":"<p>Since BerkeleyDB runs in the same JVM as JanusGraph, connecting the two only requires a simple configuration and no additional setup: <pre><code>JanusGraph g = JanusGraphFactory.build().\n    set(\"storage.backend\", \"berkeleyje\").\n    set(\"storage.directory\", \"/data/graph\").\nopen();\n</code></pre></p> <p>In the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/bdb/#berkeleydb-specific-configuration","title":"BerkeleyDB Specific Configuration","text":"<p>Refer to Configuration Reference for a complete listing of all BerkeleyDB specific configuration options in addition to the general JanusGraph configuration options. BerkeleyDB configured with SHARED_CACHE those multiple graphs will make better use of memory because the cache LRU algorithm is applied across all information in all graphs sharing the cache.</p> <p>When configuring BerkeleyDB it is recommended to consider the following BerkeleyDB specific configuration options:</p> <ul> <li>transactions: Enables transactions and detects conflicting     database operations. CAUTION: While disabling transactions can     lead to better performance it can cause to inconsistencies and even     corrupt the database if multiple JanusGraph instances interact with     the same instance of BerkeleyDB.</li> <li>cache-percentage: The percentage of JVM heap space (configured     via -Xmx) to be allocated to BerkeleyDB for its cache. Try to give     BerkeleyDB as much space as possible without causing memory problems     for JanusGraph. For instance, if JanusGraph only runs short     transactions, use a value of 80 or higher.</li> </ul>"},{"location":"storage-backend/bdb/#ideal-use-case","title":"Ideal Use Case","text":"<p>The BerkeleyDB storage backend is best suited for small to medium size graphs with up to 100 million vertices on commodity hardware. For graphs of that size, it will likely deliver higher performance than the distributed storage backends. Note, that BerkeleyDB is also limited in the number of concurrent requests it can handle efficiently because it runs on a single machine. Hence, it is not well suited for applications with many concurrent users mutating the graph, even if that graph is small to medium size.</p> <p>Since BerkeleyDB runs in the same JVM as JanusGraph, this storage backend is ideally suited for unit testing of application code using JanusGraph.</p>"},{"location":"storage-backend/bdb/#global-graph-operations","title":"Global Graph Operations","text":"<p>JanusGraph backed by BerkeleyDB supports global graph operations such as iterating over all vertices or edges. However, note that such operations need to scan the entire database which can require a significant amount of time for larger graphs.</p> <p>In order to not run out of memory, it is advised to disable transactions (<code>storage.transactions=false</code>) when iterating over large graphs. Having transactions enabled requires BerkeleyDB to acquire read locks on the data it is reading. When iterating over the entire graph, these read locks can easily require more memory than is available.</p>"},{"location":"storage-backend/bdb/#additional-berkeleydb-je-configuration-options","title":"Additional BerkeleyDB JE configuration options","text":"<p>It's possible to set additional BerkeleyDB JE configuration which are not  directly exposed by JanusGraph by leveraging <code>storage.berkeleyje.ext</code>  namespace. </p> <p>JanusGraph iterates over all properties prefixed with <code>storage.berkeleyje.ext.</code>. It strips the prefix from each property key.  Any dash character (<code>-</code>) wil be replaced by dot character (<code>.</code>) in the  remainder of the stripped key. The final string will be interpreted as a parameter  ke for <code>com.sleepycat.je.EnvironmentConfig</code>.  Thus, both options <code>storage.berkeleyje.ext.je.lock.timeout</code> and  <code>storage.berkeleyje.ext.je-lock-timeout</code> will be treated  the same (as <code>storage.berkeleyje.ext.je.lock.timeout</code>).  The value associated with the key is not modified.  This allows embedding arbitrary settings in JanusGraph\u2019s properties. Here\u2019s an example configuration fragment that customizes three BerkeleyDB settings  using the <code>storage.berkeleyje.ext.</code> config mechanism:</p> <pre><code>storage.backend=berkeleyje\nstorage.berkeleyje.ext.je.lock.timeout=5000 ms\nstorage.berkeleyje.ext.je.lock.deadlockDetect=false\nstorage.berkeleyje.ext.je.txn.timeout=5000 ms\nstorage.berkeleyje.ext.je.log.fileMax=100000000\n</code></pre>"},{"location":"storage-backend/bdb/#deadlock-troubleshooting","title":"Deadlock troubleshooting","text":"<p>In concurrent environment deadlocks are possible when using BerkeleyDB JE storage  backend.  It may be complicated to deal with deadlocks in use-cases when multiple threads are  modifying same vertices (including edges creation between affected vertices).  More insights on this topic can be found in the GitHub issue  #1623.</p> <p>Some users suggest the following configuration to deal with deadlocks: <pre><code>storage.berkeleyje.isolation-level=READ_UNCOMMITTED\nstorage.berkeleyje.lock-mode=LockMode.READ_UNCOMMITTED\nstorage.berkeleyje.ext.je.lock.timeout=0\nstorage.lock.wait-time=5000\nids.authority.wait-time=2000\ntx.max-commit-time=30000\n</code></pre></p>"},{"location":"storage-backend/bigtable/","title":"Google Cloud Bigtable","text":"<p>Cloud Bigtable is Google\u2019s NoSQL Big Data database service. It\u2019s the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail.</p> <p>Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it\u2019s a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis.</p> <p>\u2014  Google Cloud Bigtable Homepage</p>"},{"location":"storage-backend/bigtable/#bigtable-setup","title":"Bigtable Setup","text":"<p>Bigtable implements the HBase interface for all data access operations, and requires a few configuration options to connect.</p>"},{"location":"storage-backend/bigtable/#connecting-to-bigtable","title":"Connecting to Bigtable","text":"<p>Configuring JanusGraph to connect to Bigtable is achieved by using the <code>hbase</code> backend, along with a custom connection implementation, the project id of the Google Cloud Platform project containing the Bigtable instance, and the Cloud Bigtable instance id you are connecting to.</p> <p>Example: <pre><code>storage.backend=hbase\nstorage.hbase.ext.hbase.client.connection.impl=com.google.cloud.bigtable.hbase2_x.BigtableConnection\nstorage.hbase.ext.google.bigtable.project.id=&lt;Google Cloud Platform project id&gt;\nstorage.hbase.ext.google.bigtable.instance.id=&lt;Bigtable instance id&gt;\n</code></pre></p>"},{"location":"storage-backend/cassandra/","title":"Apache Cassandra","text":"<p>The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra\u2019s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages. The largest known Cassandra cluster has over 75,000 nodes storing over 10 PB of data.</p> <p>\u2014  Apache Cassandra Homepage</p> <p>The following sections outline the various ways in which JanusGraph can be used in concert with Apache Cassandra.</p>"},{"location":"storage-backend/cassandra/#cassandra-storage-backend","title":"Cassandra Storage Backend","text":"<p>Cassandra has two protocols for clients to use: CQL and Thrift.  With Cassandra 4.0, Thrift support will be removed in Cassandra. JanusGraph just supports the CQL storage backend.</p> <p>Note</p> <p>If security is enabled on Cassandra, the user must have <code>CREATE permission on &lt;all keyspaces&gt;</code>, otherwise the keyspace must be created ahead of time by an administrator including the required tables or the user must have <code>CREATE permission on &lt;the configured keyspace&gt;</code>. The create table file containing the required tables is located in <code>conf/cassandra/cassandraTables.cql</code>. Please define your keyspace before executing it.</p>"},{"location":"storage-backend/cassandra/#local-server-mode","title":"Local Server Mode","text":"<p>Cassandra can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and Cassandra communicate with one another via a <code>localhost</code> socket. Running JanusGraph over Cassandra requires the following setup steps:</p> <ol> <li>Download Cassandra, unpack     it, and set filesystem paths in <code>conf/cassandra.yaml</code>.</li> <li>Connecting Gremlin Server to Cassandra using the default     configuration files provided in the pre-packaged distribution.</li> <li> <p>Start Cassandra by invoking <code>bin/cassandra -f</code> on the command line     in the directory where Cassandra was unpacked. Read output to check     that Cassandra started successfully.</p> <p>Now, you can create a Cassandra JanusGraph as follows <pre><code>JanusGraph g = JanusGraphFactory.build().\nset(\"storage.backend\", \"cql\").\nset(\"storage.hostname\", \"127.0.0.1\").\nopen();\n</code></pre></p> </li> </ol> <p>In the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/cassandra/#local-container-mode","title":"Local Container Mode","text":"<p>Cassandra does not have a native install for Windows or OSX. One of the easiest ways to run Cassandra on OSX, Windows, or Linux is to use a Docker Container. You can download and run Cassandra with a single Docker command. It is important to install a version that is supported by the version of JanusGraph you intend to use. The compatible versions can be found under the Tested Compatibility section of the specific release on the Releases page. The Cassandra Docker Hub page can be referenced for the available versions and useful commands. A description of the ports can be found here. Port 9160 is used for the Thrift client API. Port 9042 is for CQL native clients. Ports 7000, 7001 and 7099 are for inter-node communication. Version 3.11 of Cassandra was the latest compatible version for JanusGraph 0.2.0 and is specified in the reference command below.</p> <pre><code>docker run --name jg-cassandra -d -e CASSANDRA_START_RPC=true -p 9160:9160 \\\n  -p 9042:9042 -p 7199:7199 -p 7001:7001 -p 7000:7000 cassandra:3.11\n</code></pre>"},{"location":"storage-backend/cassandra/#remote-server-mode","title":"Remote Server Mode","text":"<p>When the graph needs to scale beyond the confines of a single machine, then Cassandra and JanusGraph are logically separated into different machines. In this model, the Cassandra cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the Cassandra cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph.</p> <p>For example, suppose we have a running Cassandra cluster where one of the machines has the IP address 77.77.77.77, then connecting JanusGraph with the cluster is accomplished as follows (comma separate IP addresses to reference more than one machine): <pre><code>JanusGraph graph = JanusGraphFactory.build().\n  set(\"storage.backend\", \"cql\").\n  set(\"storage.hostname\", \"77.77.77.77\").\n  open();\n</code></pre></p> <p>In the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/cassandra/#remote-server-mode-with-gremlin-server","title":"Remote Server Mode with Gremlin Server","text":"<p>Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph.</p> <p>Start Gremlin Server using <code>bin/janusgraph-server.sh</code> and then in an external Gremlin Console session using <code>bin/gremlin.sh</code> you can send Gremlin commands over the wire: <pre><code>:plugin use tinkerpop.server\n:remote connect tinkerpop.server conf/remote.yaml\n:&gt; g.addV()\n</code></pre></p> <p>In this case, each Gremlin Server would be configured to connect to the Cassandra cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server.</p> <pre><code>...\ngraphs: {\n  g: conf/janusgraph-cql.properties\n}\nscriptEngines: {\n  gremlin-groovy: {\n    plugins: { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: {classImports: [java.lang.Math], methodImports: [java.lang.Math#*]},\n               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: {files: [scripts/empty-sample.groovy]}}}}\n...\n</code></pre> <p>For more information about Gremlin Server see the Apache TinkerPop documentation</p>"},{"location":"storage-backend/cassandra/#cql-specific-configuration","title":"CQL Specific Configuration","text":"<p>Refer to Configuration Reference for a complete listing of all Cassandra specific configuration options in addition to the general JanusGraph configuration options.</p> <p>When configuring CQL it is recommended to consider the following CQL specific configuration options:</p> <ul> <li>read-consistency-level: Cassandra consistency level for read     operations</li> <li>write-consistency-level: Cassandra consistency level for write     operations</li> <li>replication-factor: The replication factor to use. The higher     the replication factor, the more robust the graph database is to     machine failure at the expense of data duplication. The default     value should be overwritten for production system to ensure     robustness. A value of 3 is recommended. This replication factor     can only be set when the keyspace is initially created. On an     existing keyspace, this value is ignored.</li> <li>keyspace: The name of the keyspace to store the JanusGraph graph     in. Allows multiple JanusGraph graphs to co-exist in the same     Cassandra cluster.</li> </ul> <p>More information on Cassandra consistency levels and acceptable values can be found here. In general, higher levels are more consistent and robust but have higher latency.</p>"},{"location":"storage-backend/cassandra/#global-graph-operations","title":"Global Graph Operations","text":"<p>JanusGraph over Cassandra supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause <code>OutOfMemoryException</code>. Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively.</p>"},{"location":"storage-backend/cassandra/#deploying-on-datastax-astra","title":"Deploying on DataStax Astra","text":"<p>Astra DB simplifies cloud-native Cassandra application development. It reduces deployment time from weeks to minutes, and delivers an unprecedented combination of serverless, pay-as-you-go pricing with the freedom and agility of multi-cloud and open source.</p> <p>\u2014  DataStax Astra</p> <p>Download the secure-connect zipped bundle for your Astra database.</p> <p>While connecting to Astra DB from JanusGraph, it is preferred to make use of the secure bundle connection file as-is without extracting it. There are multiple ways in which a secure bundle connection file can be passed on to the JanusGraph configuration to connect to Astra DB using the DataStax driver.</p>"},{"location":"storage-backend/cassandra/#internal-string-configuration","title":"Internal string configuration","text":"<p>Set the property <code>storage.cql.internal.string-configuration</code> to <code>datastax-java-driver { basic.cloud.secure-connect-bundle=&lt;path-to-secure-bundle-zip-file&gt; }</code> and set the username, password and keyspace details.</p> <p>For example: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=cql\nstorage.cql.keyspace=&lt;keyspace name which was created in AstraDB&gt;\nstorage.username=&lt;clientID&gt;\nstorage.password=&lt;clientSecret&gt;\nstorage.cql.internal.string-configuration=datastax-java-driver { basic.cloud.secure-connect-bundle=&lt;path-to-secure-bundle-zip-file&gt; }\n</code></pre></p> <p>Also, you can set a jvm argument to pass the secure bundle file as shown below and remove that property <code>(storage.cql.internal.string-configuration)</code> from the list above.</p> <p><code>-Ddatastax-java-driver.basic.cloud.secure-connect-bundle=&lt;path-to-secure-bundle-zip-file&gt;</code></p>"},{"location":"storage-backend/cassandra/#internal-file-configuration","title":"Internal file configuration","text":"<p>Set the property <code>storage.cql.internal.file-configuration</code> to an external configuration file if you would like to externalize the astra connection related properties to a separate file and specify the secure bundle and credentials information on that file.</p> <p>For example: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=cql\nstorage.cql.keyspace=&lt;keyspace-name&gt;\n# Link to the external file that DataStax driver understands\nstorage.cql.internal.file-configuration=&lt;path-to-astra.conf&gt;\n</code></pre> astra.conf (external file) <pre><code>datastax-java-driver {\n  basic.cloud {\n    secure-connect-bundle = \"&lt;path-to-secure-bundle-zip-file&gt;\"\n  }\n  advanced.auth-provider {\n    class = PlainTextAuthProvider\n    username = \"&lt;clientID&gt;\"\n    password = \"&lt;clientSecret&gt;\"\n  }\n}\n</code></pre></p> <p>Note: Client id and Client secret need to be generated and copied from your Astra Account as per this doc.</p> <p>To learn more about different configuration options of DataStax driver, please refer to the DataStax driver configuration documentation.</p>"},{"location":"storage-backend/cassandra/#deploying-on-amazon-keyspaces","title":"Deploying on Amazon Keyspaces","text":"<p>Amazon Keyspaces (for Apache Cassandra) is a scalable, highly available, and managed Apache Cassandra\u2013compatible database service. Amazon Keyspaces is serverless, so you pay for only the resources you use and the service can automatically scale tables up and down in response to application traffic.</p> <p>\u2014  Amazon Keyspaces</p> <p>Note</p> <p>The support for Amazon Keyspaces is experimental. We discourage usage in production systems unless you have thoroughly tested it against your use case.</p> <p>Follow these steps to set up a Amazon Keyspaces cluster and deploy JanusGraph over it. Prior to these instructions, make sure you have already followed this guide to sign up for AWS and set up your identity and access management.</p>"},{"location":"storage-backend/cassandra/#creating-credentials","title":"Creating Credentials","text":"<p>You would need to generate service-specific credentials. See this guide for more details.</p> <p>After your service-specific credential is generated, you would get an output similar to the following:</p> <pre><code>{\n    \"ServiceSpecificCredential\": {\n        \"CreateDate\": \"2019-10-09T16:12:04Z\",\n        \"ServiceName\": \"cassandra.amazonaws.com\",\n        \"ServiceUserName\": \"alice-at-111122223333\",\n        \"ServicePassword\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n        \"ServiceSpecificCredentialId\": \"ACCAYFI33SINPGJEBYESF\",\n        \"UserName\": \"alice\",\n        \"Status\": \"Active\"\n    }\n}\n</code></pre> <p>Please save <code>ServiceUserName</code> and <code>ServicePassword</code> in a secure location and you would need them in JanusGraph config later.</p>"},{"location":"storage-backend/cassandra/#setting-up-ssltls","title":"Setting up SSL/TLS","text":"<ol> <li>Download the Starfield digital certificate using the following command</li> </ol> <pre><code>curl https://certs.secureserver.net/repository/sf-class2-root.crt -O\n</code></pre> <ol> <li>Convert the Starfield digital certificate to a trustStore file</li> </ol> <pre><code>openssl x509 -outform der -in sf-class2-root.crt -out temp_file.der\nkeytool -import -alias cassandra -keystore cassandra_truststore.jks -file temp_file.der\n</code></pre> <p>Now you would see a <code>cassandra_truststore.jks</code> file generated locally. You would need this file and your truststore password later.</p> <p>For more details, see this doc. Note that you don't need to follow every step of that doc, since it is written for users who connect to Amazon Keyspaces using a Java client directly. .</p>"},{"location":"storage-backend/cassandra/#configurations","title":"Configurations","text":"<p>Below is a complete sample of configuration. Unlike standard Apache Cassandra or ScyllaDB, Amazon Keyspaces only supports a subset of functionalities. Therefore, some specific configurations are needed.</p> <pre><code># Basic settings for CQL\ngremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=cql\nstorage.hostname=cassandra.&lt;your-datacenter, e.g. ap-east-1&gt;.amazonaws.com\nstorage.port=9142\nstorage.username=&lt;your-service-username&gt;\nstorage.password=&lt;your-service-password&gt;\nstorage.cql.keyspace=janusgraph\nstorage.cql.local-datacenter=&lt;your-datacenter, e.g. ap-east-1&gt;\n\n# Wait for 30 seconds after each table creation, since\n# Amazon Keyspace creates tables asynchronously. Remember to\n# remove this option from config file after the creation of the graph.\nstorage.cql.init-wait-time=30000\n\n# SSL related settings\nstorage.cql.ssl.enabled=true\nstorage.cql.ssl.truststore.location=&lt;your-trust-store-location&gt;\nstorage.cql.ssl.truststore.password=&lt;your-trust-store-password&gt;\n\n# Amazon Keyspaces does not support user-generated timestamps\n# Thus, the below config must be turned off\ngraph.assign-timestamp=false\n\n# We strongly recommend you to turn on this config. It will\n# prohibit all full-scan attempts. This is because Amazon keyspace\n# diverges from Apache Cassandra and might result in incomplete\n# results when a full-scan is executed. See issue #3390 for more\n# details\nquery.force-index=true\n\n# Amazon Keyspaces only supports LOCAL QUORUM consistency\nstorage.cql.only-use-local-consistency-for-system-operations=true\nstorage.cql.read-consistency-level=LOCAL_QUORUM\nstorage.cql.write-consistency-level=LOCAL_QUORUM\nlog.janusgraph.key-consistent=true\nlog.tx.key-consistent=true\n\n# Amazon Keyspaces does not have metadata available to clients\n# Thus, we need to tell JanusGraph that metadata are disabled,\n# and provide a hint of which partitioner AWS is using. Valid\n# partitioner-names are: Murmur3Partitioner, RandomPartitioner,\n# and DefaultPartitioner\nstorage.cql.metadata-schema-enabled=false\nstorage.cql.metadata-token-map-enabled=false\nstorage.cql.partitioner-name=Murmur3Partitioner\n</code></pre> <p>Now you should be able to open the graph via gremlin console or java code, using the above configuration file.</p>"},{"location":"storage-backend/cassandra/#known-problems","title":"Known Problems","text":"<ul> <li>Amazon Keyspaces creates tables on-demand. If you are connecting to it the first time, you would likely see error message like     <pre><code>unconfigured table janusgraph.system_properties\n</code></pre>     At the same time, you should be able to see the same table getting created on Amazon Keyspaces console UI.     The creation process typically takes a few seconds. Once the creation is done, you could open     the graph again, and the error will be gone. Unfortunately, you would have to follow the same process     for all tables. To address this problem, you could set <code>storage.cql.init-wait-time=30000</code> in your config      file to wait for 30 seconds (or any other duration you feel suitable) after creating each table. You should     remove this config after the creation of the graph. Alternatively, you could create the tables on AWS manually,     if you are familiar with JanusGraph. Typically nine tables are needed: <code>edgestore</code>, <code>edgestore_lock_</code>,     <code>graphindex</code>, <code>graphindex_lock_</code>, <code>janusgraph_ids</code>, <code>system_properties</code>, <code>system_properties_lock_</code>, <code>systemlog</code>, and <code>txlog</code>.</li> </ul>"},{"location":"storage-backend/cassandra/#deploying-on-amazon-ec2","title":"Deploying on Amazon EC2","text":"<p>Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.</p> <p>\u2014  Amazon EC2</p> <p>Note</p> <p>The below documentation might be partially out-of-date.</p> <p>Follow these steps to setup a Cassandra cluster on EC2 and deploy JanusGraph over Cassandra. To follow these instructions, you need an Amazon AWS account with established authentication credentials and some basic knowledge of AWS and EC2.</p>"},{"location":"storage-backend/cassandra/#setup-cassandra-cluster","title":"Setup Cassandra Cluster","text":"<p>These instructions for configuring and launching the DataStax Cassandra Community Edition AMI are based on the DataStax AMI Docs and focus on aspects relevant for a JanusGraph deployment.</p>"},{"location":"storage-backend/cassandra/#setting-up-security-group","title":"Setting up Security Group","text":"<ul> <li>Navigate to the EC2 Console Dashboard, then click on \"Security     Groups\" under \"Network &amp; Security\".</li> <li>Create a new security group. Click Inbound. Set the \"Create a new     rule\" dropdown menu to \"Custom TCP rule\". Add a rule for port 22     from source 0.0.0.0/0. Add a rule for ports 1024-65535 from the     security group members. If you don\u2019t want to open all unprivileged     ports among security group members, then at least open 7000, 7199,     and 9160 among security group members. Tip: the \"Source\" dropdown     will autocomplete security group identifiers once \"sg\" is typed in     the box, so you needn\u2019t have the exact value ready beforehand.</li> </ul>"},{"location":"storage-backend/cassandra/#launch-datastax-cassandra-ami","title":"Launch DataStax Cassandra AMI","text":"<ul> <li>\"Launch the DataStax AMI     in your desired zone</li> <li>On the Instance Details page of the Request Instances Wizard, set     \"Number of Instances\" to your desired number of Cassandra nodes. Set     \"Instance Type\" to at least m1.large. We recommend m1.large.</li> <li> <p>On the Advanced Instance Options page of the Request Instances     Wizard, set the \"as text\" radio button under \"User Data\", then fill     this into the text box: <pre><code>--clustername [cassandra-cluster-name]\n--totalnodes [number-of-instances]\n--version community\n--opscenter no\n</code></pre> [number-of-instances] in this configuration must match the number of EC2 instances configured on the previous wizard page. [cassandra-cluster-name] can be any string used for identification. For example: <pre><code>--clustername janusgraph\n--totalnodes 4\n--version community\n--opscenter no\n</code></pre></p> </li> <li> <p>On the Tags page of the Request Instances Wizard you can apply any     desired configurations. These tags exist only at the EC2     administrative level and have no effect on the Cassandra daemons'     configuration or operation.</p> </li> <li>On the Create Key Pair page of the Request Instances Wizard, either     select an existing key pair or create a new one. The PEM file     containing the private half of the selected key pair will be     required to connect to these instances.</li> <li>On the Configure Firewall page of the Request Instances Wizard,     select the security group created earlier.</li> <li>Review and launch instances on the final wizard page.</li> </ul>"},{"location":"storage-backend/cassandra/#verify-successful-instance-launch","title":"Verify Successful Instance Launch","text":"<ul> <li>SSH into any Cassandra instance node:     <code>ssh -i [your-private-key].pem ubuntu@[public-dns-name-of-any-cassandra-instance]</code></li> <li>Run the Cassandra nodetool <code>nodetool -h 127.0.0.1 ring</code> to inspect     the state of the Cassandra token ring. You should see as many nodes     in this command\u2019s output as instances launched in the previous     steps.</li> </ul> <p>Note, that the AMI takes a few minutes to configure each instance. A shell prompt will appear upon successful configuration when you SSH into the instance.</p>"},{"location":"storage-backend/cassandra/#launch-janusgraph-instances","title":"Launch JanusGraph Instances","text":"<p>Launch additional EC2 instances to run JanusGraph which are either configured in Remote Server Mode or Remote Server Mode with Gremlin-Server as described above. You only need to note the IP address of one of the Cassandra cluster instances and configure it as the host name. The particular EC2 instance to run and the particular configuration depends on your use case.</p>"},{"location":"storage-backend/cassandra/#example-janusgraph-instance-on-amazon-linux-ami","title":"Example JanusGraph Instance on Amazon Linux AMI","text":"<ul> <li>Launch the Amazon Linux AMI in the same zone of the     Cassandra cluster. Choose your desired EC2 instance type depending     on the amount of resources you need. Use the default configuration     options and select the same Key Pair and Security Group as for the     Cassandra cluster configured in the previous step.</li> <li>SSH into the newly created instance via     <code>ssh -i [your-private-key].pem ec2-user@[public-dns-name-of-the-instance]</code>.     You may have to wait a little for the instance to launch.</li> <li>Download the     current JanusGraph distribution with <code>wget</code> and unpack the archive     locally to the home directory. Start the Gremlin Console to verify     that JanusGraph runs successfully. For more information on how to     unpack JanusGraph and start the Gremlin Console, please refer to the     Getting Started guide</li> <li>Create a configuration file with <code>vi janusgraph.properties</code> and add     the following lines:: <pre><code>storage.backend = cql\nstorage.hostname = [IP-address-of-one-Cassandra-EC2-instance]\n</code></pre></li> </ul> <p>You may add additional configuration options found on this page or in Configuration Reference.</p> <ul> <li>Start the Gremlin Console again and type the following:: <pre><code>gremlin&gt; graph = JanusGraphFactory.open('janusgraph.properties')\n==&gt;janusgraph[cql:[IP-address-of-one-Cassandra-EC2-instance]]\n</code></pre></li> </ul> <p>Note</p> <p>You have successfully connected this JanusGraph instance to the Cassandra cluster and can start to operate on the graph.</p>"},{"location":"storage-backend/cassandra/#connect-to-cassandra-cluster-in-ec2-from-outside-ec2","title":"Connect to Cassandra cluster in EC2 from outside EC2","text":"<p>Opening the usual Cassandra ports (9042, 7000, 7199) in the security group is not enough, because the Cassandra nodes by default broadcast their ec2-internal IPs, and not their public-facing IPs.</p> <p>The resulting behavior is that you can open a JanusGraph graph on the cluster by connecting to port 9042 on any Cassandra node, but all requests to that graph time out. This is because Cassandra is telling the client to connect to an unreachable IP.</p> <p>To fix this, set the \"broadcast-address\" property for each instance in /etc/cassandra/cassandra.yaml to its public-facing IP, and restart the instance. Do this for all nodes in the cluster. Once the cluster comes back, nodetool reports the correct public-facing IPs to which connections from the local machine are allowed.</p> <p>Changing the \"broadcast-address\" property allows you to connect to the cluster from outside ec2, but it might also mean that traffic originating within ec2 will have to round-trip to the internet and back before it gets to the cluster. So, this approach is only useful for development and testing.</p>"},{"location":"storage-backend/hbase/","title":"Apache HBase","text":"<p>Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google\u2019s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.</p> <p>\u2014  Apache HBase Homepage</p>"},{"location":"storage-backend/hbase/#hbase-setup","title":"HBase Setup","text":"<p>The following sections outline the various ways in which JanusGraph can be used in concert with Apache HBase.</p>"},{"location":"storage-backend/hbase/#local-server-mode","title":"Local Server Mode","text":"<p>HBase can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and HBase communicate with one another via a <code>localhost</code> socket. Running JanusGraph over HBase requires the following setup steps:</p> <ul> <li> <p>Download and extract a stable HBase from     https://www.apache.org/dyn/closer.cgi/hbase/stable/.</p> </li> <li> <p>Start HBase by invoking the <code>start-hbase.sh</code> script in the bin     directory inside the extracted HBase directory. To stop HBase, use     <code>stop-hbase.sh</code>. <pre><code>$ ./bin/start-hbase.sh\nstarting master, logging to ../logs/hbase-master-machine-name.local.out\n</code></pre></p> </li> </ul> <p>Now, you can create an HBase JanusGraph as follows: <pre><code>JanusGraph graph = JanusGraphFactory.build()\n    .set(\"storage.backend\", \"hbase\")\n    .open();\n</code></pre></p> <p>Note, that you do not need to specify a hostname since a localhost connection is attempted by default. Also, in the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/hbase/#remote-server-mode","title":"Remote Server Mode","text":"<p>When the graph needs to scale beyond the confines of a single machine, then HBase and JanusGraph are logically separated into different machines. In this model, the HBase cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the HBase cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph.</p> <p>For example, suppose we have a running HBase cluster with a ZooKeeper quorum composed of three machines at IP address 77.77.77.77, 77.77.77.78, and 77.77.77.79, then connecting JanusGraph with the cluster is accomplished as follows: <pre><code>JanusGraph g = JanusGraphFactory.build()\n    .set(\"storage.backend\", \"hbase\")\n    .set(\"storage.hostname\", \"77.77.77.77, 77.77.77.78, 77.77.77.79\")\n    .open();\n</code></pre></p> <p><code>storage.hostname</code> accepts a comma separated list of IP addresses and hostname for any subset of machines in the HBase cluster JanusGraph should connect to. Also, in the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/hbase/#remote-server-mode-with-gremlin-server","title":"Remote Server Mode with Gremlin Server","text":"<p>Finally, Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph.</p> <pre><code>http://gremlin-server.janusgraph.machine1/mygraph/vertices/1\nhttp://gremlin-server.janusgraph.machine2/mygraph/tp/gremlin?script=g.v(1).out('follows').out('created')\n</code></pre> <p>In this case, each Gremlin Server would be configured to connect to the HBase cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server.</p> <pre><code>...\ngraphs: {\n  g: conf/janusgraph-hbase.properties\n}\nscriptEngines: {\n  gremlin-groovy: {\n    plugins: { org.janusgraph.graphdb.tinkerpop.plugin.JanusGraphGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.server.jsr223.GremlinServerGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.tinkergraph.jsr223.TinkerGraphGremlinPlugin: {},\n               org.apache.tinkerpop.gremlin.jsr223.ImportGremlinPlugin: {classImports: [java.lang.Math], methodImports: [java.lang.Math#*]},\n               org.apache.tinkerpop.gremlin.jsr223.ScriptFileGremlinPlugin: {files: [scripts/empty-sample.groovy]}}}}\n...\n</code></pre>"},{"location":"storage-backend/hbase/#hbase-specific-configuration","title":"HBase Specific Configuration","text":"<p>Refer to Configuration Reference for a complete listing of all HBase specific configuration options in addition to the general JanusGraph configuration options.</p> <p>When configuring HBase it is recommended to consider the following HBase specific configuration options:</p> <ul> <li>storage.hbase.table: Name of the HBase table in which to store     the JanusGraph graph. Allows multiple JanusGraph graphs to co-exist     in the same HBase cluster.</li> </ul> <p>Please refer to the HBase configuration documentation for more HBase configuration options and their description. By prefixing the respective HBase configuration option with <code>storage.hbase.ext</code> in the JanusGraph configuration it will be passed on to HBase at initialization time. For example, to use the znode /hbase-secure for HBase, set the property: <code>storage.hbase.ext.zookeeper.znode.parent=/hbase-secure</code>. The prefix allows arbitrary HBase configuration options to be configured through JanusGraph.</p> <p>Important</p> <p>HBase backend uses millisecond for timestamps. In JanusGraph 0.2.0 and earlier, if the <code>graph.timestamps</code> property is not explicitly set, the default is <code>MICRO</code>. In this case, the <code>graph.timestamps</code> property must be explicitly set to <code>MILLI</code>. Do not set the <code>graph.timestamps</code> property to another value in any cases.</p>"},{"location":"storage-backend/hbase/#global-graph-operations","title":"Global Graph Operations","text":"<p>JanusGraph over HBase supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause <code>OutOfMemoryException</code>. Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively.</p>"},{"location":"storage-backend/hbase/#tips-and-tricks-for-managing-an-hbase-cluster","title":"Tips and Tricks for Managing an HBase Cluster","text":"<p>The HBase shell on the master server can be used to get an overall status check of the cluster. <pre><code>$HBASE_HOME/bin/hbase shell\n</code></pre></p> <p>From the shell, the following commands are generally useful for understanding the status of the cluster.</p> <pre><code>status 'janusgraph'\nstatus 'simple'\nstatus 'detailed'\n</code></pre> <p>The above commands can identify if a region server has gone down. If so, it is possible to <code>ssh</code> into the failed region server machines and do the following:</p> <pre><code>sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh stop regionserver\nsudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh start regionserver\n</code></pre> <p>The use of pssh can make this process easy as there is no need to log into each machine individually to run the commands. Put the IP addresses of the regionservers into a <code>hosts.txt</code> file and then execute the following.</p> <pre><code>pssh -h host.txt sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh stop regionserver\npssh -h host.txt sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh start regionserver\n</code></pre> <p>Next, sometimes you need to restart the master server (e.g. connection refused exceptions). To do so, on the master execute the following:</p> <pre><code>sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh stop master\nsudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh start master\n</code></pre> <p>Finally, if an HBase cluster has already been deployed and more memory is required of the master or region servers, simply edit the <code>$HBASE_HOME/conf/hbase-env.sh</code> files on the respective machines with requisite <code>-Xmx -Xms</code> parameters. Once edited, stop/start the master and/or region servers as described previous.</p>"},{"location":"storage-backend/inmemorybackend/","title":"InMemory Storage Backend","text":"<p>JanusGraph ships with an in-memory storage backend which can be used through the following configuration:</p> <pre><code>storage.backend=inmemory\n</code></pre> <p>Alternatively, an in-memory JanusGraph graph can be opened directly in the Gremlin Console:</p> <pre><code>graph = JanusGraphFactory.build().set('storage.backend', 'inmemory').open()\n</code></pre> <p>There are no additional configuration options for the in-memory storage backend. As the name suggests, this backend holds all data in memory, specifically  in the heap space allocated to the java virtual machine running Janusgraph instance.</p> <p>Shutting down the graph or terminating the process that hosts the JanusGraph graph will irrevocably delete all data from the graph. This backend is local to a particular JanusGraph graph instance and cannot be shared across multiple JanusGraph graphs.</p>"},{"location":"storage-backend/inmemorybackend/#ideal-use-case","title":"Ideal Use Case","text":""},{"location":"storage-backend/inmemorybackend/#rapid-testing","title":"Rapid testing","text":"<p>The in-memory storage backend was initially developed to simplify testing (for those tests that do not require persistence) and graph exploration. Automated testing and ad-hoc prototyping remains its main purpose within Janusgraph project.</p>"},{"location":"storage-backend/inmemorybackend/#production","title":"Production","text":"<p>The initial test-only implementation was further evolved to use a more compact  in-memory representation, which made it suitable for production use in certain scenarios, such as   Janusgraph engine being embedded into an application process, or spawned dynamically on demand, where:</p> <ul> <li>Ease of setup/configuration/maintenance/start up is important (just run Janusgraph from distribution jar,  no management of clusters for backend DBs etc)  </li> <li>Loss of data due to unexpected death of host process is acceptable (the backend provides a  simple mechanism for making fast snapshots to handle expected restarts, and the data can always  be exported on Gremlin/Tinkerpop level to say GraphSON)  </li> <li>Size of the graph data makes it possible to host it in a single JVM process (i.e. a few tens of Gigabytes max,  unless you use a specialized JVM and hardware)  </li> <li>Higher performance is required, but no expertise/resources available to tune more complex backends.  Due to its memory-only nature, in-memory backend typically performs faster than disk-based ones,  in queries using simple indices and in graph modifications. However it is not specifically optimized for performance,  and does not support advanced indexing functionality. </li> </ul>"},{"location":"storage-backend/inmemorybackend/#limitations","title":"Limitations","text":"<ul> <li>Obviously the scalability is limited to the heap size of a single JVM,  and no transparent resilience to failures is offered  </li> <li>The backend offers store-level locking only, whereas a Janusgraph transaction typically changes multiple stores  (e.g. vertex store and index store). This means that in scenarios where data is modified in parallel transactions,  care should be taken on application level to avoid conflicting updates. At a high level, this means that you can  modify unrelated parts of the graph in parallel, but you should avoid changing the same vertex or  its edges in parallel transactions.  </li> <li>The backend does not guarantee a clean rollback once commit has started. Chances of a failure in the middle of commit  to an in-memory data structure are low, however this can happen - e.g. when a large heap nears saturation and the  GC pause exceeds configured backend timeout.  </li> <li>The data layout used by the backend can theoretically be susceptible to fragmentation in certain scenarios  (with a lot of add/delete operations), thus reducing the amount of useful data that can be stored in a heap   of specified size. The backend provides simple mechanisms to report fragmentation and defragment the storage if required.   However scenarios where the level of fragmentation is high enough to be an issue are expected to be rare,    and usually defragmentation is not required at all.</li> </ul>"},{"location":"storage-backend/inmemorybackend/#alternatives","title":"Alternatives","text":"<p>Generally, except for the above class of use cases, there doesn't seem to be much point in specifically 100% in-memory-only backend. Many of the other supported backends, based on mature key-value databases,  can be tuned to provide high performance and resiliency, plus advanced indexing capabilities etc  (provided that you have resources and expertise to host and tune them, or use them as a service). However there are a few in-memory alternatives, such as (not exhaustive list, and in no particular order):</p> <ul> <li>BerkeleyJE backend configured with je.log.memOnly set to true  (care should be taken to avoid using it for scenarios with continuous write operations, even if the effective size of  the data remains the same - continuous write operations can drive uncontrolled log growth, leading to OOM).  The backend has dump/load capability, and supports compression.  </li> <li>Aerospike-based backend (not part of Janugraph but a separate project)  </li> </ul>"},{"location":"storage-backend/scylladb/","title":"ScyllaDB","text":"<p>ScyllaDB is a NoSQL database with a close-to-the-hardware, shared-nothing approach that optimizes raw performance, fully utilizes modern multi-core servers, and minimizes the overhead to DevOps. ScyllaDB is API-compatible with both Cassandra and DynamoDB, yet is much faster, more consistent, and with a lower TCO.</p> <p>\u2014  ScyllaDB Homepage</p>"},{"location":"storage-backend/scylladb/#scylladb-setup-and-connection","title":"ScyllaDB Setup and Connection","text":"<p>ScyllaDB is fully compatible with Cassandra. To use it as the data storage layer:</p> <ol> <li>Spin up a Scylla cluster. You can do this using Scylla Cloud, using Docker, running Scylla in the cloud or on-prem. You can see a step-by-step guide on how to spin up a three node Scylla cluster using Docker in this lesson.</li> <li>Run JanusGraph with \u201ccql\u201d as the storage.backend. </li> <li>Specify the IP address of one of the Scylla nodes in your cluster as the storage.hostname.</li> </ol>"},{"location":"storage-backend/scylladb/#step-by-step-tutorial","title":"Step by Step Tutorial","text":"<p>This Scylla University lesson provides step-by-step instructions for using JanusGraph with ScyllaDB as the data storage layer. The main steps in the lesson are:</p> <ul> <li>Spinning up a virtual machine </li> <li>Installing the prerequisites</li> <li>Running the JanusGraph server (using Docker)</li> <li>Running a Gremlin Console to connect to the new server (also in Docker)</li> <li>Spinning up a three-node Scylla Cluster and setting it as the data storage for the JanusGraph server</li> <li>Performing some basic graph operations</li> </ul>"},{"location":"storage-backend/scylladb/#usage-of-scylla-optimized-driver","title":"Usage of Scylla optimized driver","text":"<p>JanusGraph provides <code>scylla</code> <code>storage.backend</code> options in addition to generic <code>cql</code> option. ScyllaDB can work using any of those storage options, but the dedicated <code>scylla</code> backend is better optimized  for ScyllaDB. Thus, it's recommended to use <code>scylla</code> backend option with ScyllaDB whenever possible. <code>scylla</code> storage option is included in <code>janusgraph-scylla</code> library but this library isn't shipped via <code>janusgraph-all</code>  and this library is not part of the provided JanusGraph distributions. To make <code>scylla</code> option available in JanusGraph it's necessary to replace <code>org.janusgraph:janusgraph-cql</code>  with <code>org.janusgraph:janusgraph-scylla</code> if JanusGraph is used in Embedded Mode.   </p> <p>In case <code>janusgraph-all</code> dependency is used instead of explicit JanusGraph dependencies then the replacement to <code>org.janusgraph:janusgraph-scylla</code> can be done like below.</p> MavenGradle <pre><code>&lt;dependencies&gt;\n    &lt;!-- Exclude `janusgraph-cql` from `janusgraph-all` --&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n        &lt;artifactId&gt;janusgraph-all&lt;/artifactId&gt;\n        &lt;version&gt;1.0.0&lt;/version&gt;\n        &lt;exclusions&gt;\n            &lt;exclusion&gt;\n                &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n                &lt;artifactId&gt;janusgraph-cql&lt;/artifactId&gt;\n            &lt;/exclusion&gt;\n        &lt;/exclusions&gt;\n    &lt;/dependency&gt;\n\n    &lt;!-- Include `janusgraph-scylla` --&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n        &lt;artifactId&gt;janusgraph-scylla&lt;/artifactId&gt;\n        &lt;version&gt;1.0.0&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n</code></pre> <pre><code>/* Exclude `janusgraph-cql` from `janusgraph-all` */\ncompile(\"org.janusgraph:janusgraph-all:1.0.0\") {\n    exclude group: 'org.janusgraph', module: 'janusgraph-cql'\n}\n/* Include `janusgraph-scylla` */\ncompile \"org.janusgraph:janusgraph-scylla:1.0.0\"\n</code></pre>"},{"location":"storage-backend/scylladb/#using-scylla-driver-in-janusgraph-server-provided-via-the-standard-distribution-builds","title":"Using Scylla driver in JanusGraph Server provided via the standard distribution builds","text":"<p>Note</p> <p>The manual process described below is temporary and should be replaced by automated process after the issue janusgraph/janusgraph#3580 is resolved.</p> <p>In case <code>scylla</code> storage option is needed to be used in JanusGraph distribution then it's necessary to replace the next  DataStax provided CQL driver with Scylla provided CQL driver. The default JanusGraph distribution builds contain only DataStax drivers, but not Scylla drivers because they are conflicting.  However, it's possible to build the same distribution build but with Scylla drivers enabled instead.  To do this, you can use the following command inside the JanusGraph repository:</p> Bash <pre><code>mvn clean install -Pjanusgraph-release -Puse-scylla -DskipTests=true --batch-mode --also-make -Dgpg.skip=true\n</code></pre> <p>This command will generate distribution builds (both normal and full distribution build) in the  following directory: <code>janusgraph-dist/target/</code> as well as build local JanusGraph Docker image. If you don't have Docker installed or wish to avoid Docker image build process you can pass <code>-Pskip-docker</code>  (notice, all tests are automatically disabled when this flag is used).  </p> <p>Otherwise, if you can't build distribution on your own, you can use the JanusGraph provided distribution and replace  the following libraries in <code>lib</code> directory (all libraries can be downloaded via Maven Central Repository).  - <code>org.janusgraph:janusgraph-cql</code> with <code>org.janusgraph:janusgraph-scylla</code> - <code>org.janusgraph:cassandra-hadoop-util</code> with <code>org.janusgraph:scylla-hadoop-util</code> - <code>com.datastax.oss:java-driver-core</code> with <code>com.scylladb:java-driver-core</code> - <code>com.datastax.oss:java-driver-query-builder</code> with <code>com.scylladb:java-driver-query-builder</code> - <code>com.datastax.cassandra:cassandra-driver-core</code> with <code>com.scylladb:scylla-driver-core</code></p> <p>The versions of <code>com.scylladb:java-driver-core</code> and <code>com.scylladb:java-driver-query-builder</code> should be equal to the  version found in <code>pom.xml</code> of <code>org.janusgraph:janusgraph-scylla</code>. The version of <code>com.scylladb:scylla-driver-core</code> should  be equal to the version found in <code>org.janusgraph:scylla-hadoop-util</code>.</p> <p>After replacement is done it will be possible to use <code>scylla</code> as <code>storage.backend</code> options which will use the optimized  Scylla driver.</p>"}]}