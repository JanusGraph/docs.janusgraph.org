{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#the-benefits-of-janusgraph","title":"The Benefits of JanusGraph","text":"<p>JanusGraph is designed to support the processing of graphs so large that they require storage and computational capacities beyond what a single machine can provide. Scaling graph data processing for real time traversals and analytical queries is JanusGraph\u2019s foundational benefit. This section will discuss the various specific benefits of JanusGraph and its underlying, supported persistence solutions.</p>"},{"location":"#general-janusgraph-benefits","title":"General JanusGraph Benefits","text":"<ul> <li>Support for very large graphs. JanusGraph graphs scale with the     number of machines in the cluster.</li> <li>Support for very many concurrent transactions and operational graph     processing. JanusGraph\u2019s transactional capacity scales with the     number of machines in the cluster and answers complex traversal     queries on huge graphs in milliseconds.</li> <li>Support for global graph analytics and batch graph processing     through the Hadoop framework.</li> <li>Support for geo, numeric range, and full text search for vertices     and edges on very large graphs.</li> <li>Native support for the popular property graph data model exposed by     Apache TinkerPop.</li> <li>Native support for the graph traversal language     Gremlin.</li> <li>Numerous graph-level configurations provide knobs for tuning     performance.</li> <li>Vertex-centric indices provide vertex-level querying to alleviate     issues with the infamous super node problem.</li> <li>Provides an optimized disk representation to allow for efficient use     of storage and speed of access.</li> <li>Open source under the liberal Apache 2 license.</li> </ul>"},{"location":"#benefits-of-janusgraph-with-apache-cassandra","title":"Benefits of JanusGraph with Apache Cassandra","text":"<ul> <li>Continuously available     with no single point of failure.</li> <li> <p>No read/write bottlenecks to the graph as there is no master/slave     architecture.</p> </li> <li> <p>Elastic scalability allows     for the introduction and removal of machines.</p> </li> <li>Caching layer ensures that continuously accessed data is available     in memory.</li> <li>Increase the size of the cache by adding more machines to the     cluster.</li> <li>Integration with Apache Hadoop.</li> <li>Open source under the liberal Apache 2 license.</li> </ul>"},{"location":"#benefits-of-janusgraph-with-hbase","title":"Benefits of JanusGraph with HBase","text":"<ul> <li>Tight integration with the Apache Hadoop ecosystem.</li> <li>Native support for strong consistency.</li> <li>Linear scalability with the addition of more machines.</li> <li>Strictly consistent reads and writes.</li> <li>Convenient base classes for backing Hadoop     MapReduce jobs with HBase     tables.</li> <li>Support for exporting metrics via     JMX.</li> <li>Open source under the liberal Apache 2 license.</li> </ul>"},{"location":"#janusgraph-and-the-cap-theorem","title":"JanusGraph and the CAP Theorem","text":"<p>Despite your best efforts, your system will experience enough faults that it will have to make a choice between reducing yield (i.e., stop answering requests) and reducing harvest (i.e., giving answers based on incomplete data). This decision should be based on business requirements.</p> <p>\u2014  Coda Hale</p> <p>When using a database, the CAP theorem should be thoroughly considered (C=Consistency, A=Availability, P=Partitionability). JanusGraph is distributed with 3 supporting backends: Apache Cassandra,  Apache HBase, and Oracle Berkeley DB Java Edition. Note that BerkeleyDB JE is a non-distributed database and is typically only used with JanusGraph for testing and exploration purposes.</p> <p>HBase gives preference to consistency at the expense of yield, i.e. the probability of completing a request. Cassandra gives preference to availability at the expense of harvest, i.e. the completeness of the answer to the query (data available/complete data).</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The examples in this section make extensive use of a toy graph distributed with JanusGraph called The Graph of the Gods. This graph is diagrammed below. The abstract data model is known as a  Property Graph Model and this particular instance describes the relationships between the beings and places of the Roman pantheon. Moreover, special text and symbol modifiers in the diagram (e.g. bold, underline, etc.) denote different schematics/typings in the graph.</p> <p></p> visual symbol meaning bold key a graph indexed key bold key with star a graph indexed key that must have a unique value underlined key a vertex-centric indexed key hollow-head edge a functional/unique edge (no duplicates) tail-crossed edge a unidirectional edge (can only traverse in one direction)"},{"location":"#downloading-janusgraph-and-running-the-gremlin-console","title":"Downloading JanusGraph and Running the Gremlin Console","text":"<p>JanusGraph can be downloaded from the Releases section of the project repository. Once retrieved and unpacked, a Gremlin Console can be opened. The Gremlin Console is a REPL (i.e. interactive shell) that is distributed with JanusGraph and only differs from the standard Gremlin Console insofar that JanusGraph is a pre-installed and pre-loaded package. Alternatively, a user may choose to install and activate JanusGraph in an existing Gremlin Console by downloading the JanusGraph package from the central repository. In the example below, <code>janusgraph.zip</code> is used, however, be sure to unzip the zip-file that was downloaded.</p> <p>Important</p> <p>JanusGraph requires Java 8 (Standard Edition). Oracle Java 8 is recommended. JanusGraph\u2019s shell scripts expect that the <code>$JAVA_HOME</code> environment variable points to the directory where JRE or JDK is installed.</p> <pre><code>$ unzip janusgraph-0.2.3-hadoop2.zip\nArchive:  janusgraph-0.2.3-hadoop2.zip\n  creating: janusgraph-0.2.3-hadoop2/\n...\n$ cd janusgraph-0.2.3-hadoop2\n$ bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\n09:12:24 INFO  org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph  - HADOOP_GREMLIN_LIBS is set to: /usr/local/janusgraph/lib\nplugin activated: tinkerpop.hadoop\nplugin activated: janusgraph.imports\ngremlin&gt;\n</code></pre> <p>The Gremlin Console interprets commands using Apache Groovy.  Groovy is a superset of Java that has various shorthand notations that make interactive programming easier. Likewise Gremlin-Groovy is a superset of Groovy with various shorthand notations that make graph traversals easy. The basic examples below demonstrate handling numbers, strings, and maps. The remainder of the tutorial will discuss graph-specific constructs. <pre><code>gremlin&gt; 100-10\n==&gt;90\ngremlin&gt; \"JanusGraph:\" + \" The Rise of Big Graph Data\"\n==&gt;JanusGraph: The Rise of Big Graph Data\ngremlin&gt; [name:'aurelius', vocation:['philosopher', 'emperor']]\n==&gt;name=aurelius\n==&gt;vocation=[philosopher, emperor]\n</code></pre></p> <p>Tip</p> <p>Refer to Apache TinkerPop, SQL2Gremlin, and Gremlin Recipes for more information about using Gremlin.</p>"},{"location":"#loading-the-graph-of-the-gods-into-janusgraph","title":"Loading the Graph of the Gods Into JanusGraph","text":"<p>The example below will open a JanusGraph graph instance and load The Graph of the Gods dataset diagrammed above. <code>JanusGraphFactory</code> provides a set of static <code>open</code> methods, each of which takes a configuration as its argument and returns a graph instance. This tutorial calls one of these <code>open</code> methods on a configuration that uses the BerkeleyDB storage backend and the Elasticsearch index backend, then loads The Graph of the Gods using the helper class <code>GraphOfTheGodsFactory</code>. This section skips over the configuration details, but additional information about storage backends, index backends, and their configuration are available in Storage Backends, Index Backends, and Configuration Reference.</p> <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-berkeleyje-es.properties')\n==&gt;standardjanusgraph[berkeleyje:../db/berkeley]\ngremlin&gt; GraphOfTheGodsFactory.load(graph)\n==&gt;null\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardjanusgraph[berkeleyje:../db/berkeley], standard]\n</code></pre> <p>The <code>JanusGraphFactory.open() and GraphOfTheGodsFactory.load()</code> methods do the following to the newly constructed graph prior to returning it:</p> <ol> <li>Creates a collection of global and vertex-centric indices on the graph.</li> <li>Adds all the vertices to the graph along with their properties.</li> <li>Adds all the edges to the graph along with their properties.</li> </ol> <p>Please see the GraphOfTheGodsFactory source code for details.</p> <p>For those using JanusGraph/Cassandra (or JanusGraph/HBase), be sure to make use of <code>conf/janusgraph-cql-es.properties</code> (or <code>conf/janusgraph-hbase-es.properties</code>) and <code>GraphOfTheGodsFactory.load()</code>.</p> <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; GraphOfTheGodsFactory.load(graph)\n==&gt;null\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard]\n</code></pre> <p>You may also use the <code>conf/janusgraph-cql.properties</code>, <code>conf/janusgraph-berkeleyje.properties</code>, or <code>conf/janusgraph-hbase.properties</code> configuration files to open a graph without an indexing backend configured. In such cases, you will need to use the <code>GraphOfTheGodsFactory.loadWithoutMixedIndex()</code> method to load the Graph of the Gods so that it doesn\u2019t attempt to make use of an indexing backend. <pre><code>gremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql.properties')\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; GraphOfTheGodsFactory.loadWithoutMixedIndex(graph, true)\n==&gt;null\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard]\n</code></pre></p>"},{"location":"#global-graph-indices","title":"Global Graph Indices","text":"<p>The typical pattern for accessing data in a graph database is to first locate the entry point into the graph using a graph index. That entry point is an element (or set of elements)\u2009\u2014\u2009i.e. a vertex or edge. From the entry elements, a Gremlin path description describes how to traverse to other elements in the graph via the explicit graph structure.</p> <p>Given that there is a unique index on <code>name</code> property, the Saturn vertex can be retrieved. The property map (i.e. the key/value pairs of Saturn) can then be examined. As demonstrated, the Saturn vertex has a <code>name</code> of \"saturn, \" an <code>age</code> of 10000, and a <code>type</code> of \"titan.\" The grandchild of Saturn can be retrieved with a traversal that expresses: \"Who is Saturn\u2019s grandchild?\" (the inverse of \"father\" is \"child\"). The result is Hercules.</p> <pre><code>gremlin&gt; saturn = g.V().has('name', 'saturn').next()\n==&gt;v[256]\ngremlin&gt; g.V(saturn).valueMap()\n==&gt;[name:[saturn], age:[10000]]\ngremlin&gt; g.V(saturn).in('father').in('father').values('name')\n==&gt;hercules\n</code></pre> <p>The property <code>place</code> is also in a graph index. The property <code>place</code> is an edge property. Therefore, JanusGraph can index edges in a graph index. It is possible to query The Graph of the Gods for all events that have happened within 50 kilometers of Athens (latitude:37.97 and long:23.72). Then, given that information, which vertices were involved in those events.</p> <p><pre><code>gremlin&gt; g.E().has('place', geoWithin(Geoshape.circle(37.97, 23.72, 50)))\n==&gt;e[a9x-co8-9hx-39s][16424-battled-&gt;4240]\n==&gt;e[9vp-co8-9hx-9ns][16424-battled-&gt;12520]\ngremlin&gt; g.E().has('place', geoWithin(Geoshape.circle(37.97, 23.72, 50))).as('source').inV().as('god2').select('source').outV().as('god1').select('god1', 'god2').by('name')\n==&gt;[god1:hercules, god2:hydra]\n==&gt;[god1:hercules, god2:nemean]\n</code></pre> Graph indices are one type of index structure in JanusGraph. Graph indices are automatically chosen by JanusGraph to answer which ask for all vertices (<code>g.V</code>) or all edges (<code>g.E</code>) that satisfy one or multiple constraints (e.g. <code>has</code> or <code>interval</code>). The second aspect of indexing in JanusGraph is known as vertex-centric indices. Vertex-centric indices are utilized to speed up traversals inside the graph. Vertex-centric indices are described later.</p>"},{"location":"#graph-traversal-examples","title":"Graph Traversal Examples","text":"<p>Hercules, son of Jupiter and Alcmene, bore super human strength. Hercules was a Demigod because his father was a god and his mother was a human. Juno, wife of Jupiter, was furious with Jupiter\u2019s infidelity. In revenge, she blinded Hercules with temporary insanity and caused him to kill his wife and children. To atone for the slaying, Hercules was ordered by the Oracle of Delphi to serve Eurystheus. Eurystheus appointed Hercules to 12 labors.</p> <p>In the previous section, it was demonstrated that Saturn\u2019s grandchild was Hercules. This can be expressed using a <code>loop</code>. In essence, Hercules is the vertex that is 2-steps away from Saturn along the <code>in('father')</code> path. <pre><code>gremlin&gt; hercules = g.V(saturn).repeat(__.in('father')).times(2).next()\n==&gt;v[1536]\n</code></pre></p> <p>Hercules is a demigod. To prove that Hercules is half human and half god, his parent\u2019s origins must be examined. It is possible to traverse from the Hercules vertex to his mother and father. Finally, it is possible to determine the <code>type</code> of each of them\u2009\u2014\u2009yielding \"god\" and \"human.\" <pre><code>gremlin&gt; g.V(hercules).out('father', 'mother')\n==&gt;v[1024]\n==&gt;v[1792]\ngremlin&gt; g.V(hercules).out('father', 'mother').values('name')\n==&gt;jupiter\n==&gt;alcmene\ngremlin&gt; g.V(hercules).out('father', 'mother').label()\n==&gt;god\n==&gt;human\ngremlin&gt; hercules.label()\n==&gt;demigod\n</code></pre></p> <p>The examples thus far have been with respect to the genetic lines of the various actors in the Roman pantheon. The Property Graph Model is expressive enough to represent multiple types of things and relationships. In this way, The Graph of the Gods also identifies Hercules' various heroic exploits --- his famous 12 labors. In the previous section, it was discovered that Hercules was involved in two battles near Athens. It is possible to explore these events by traversing <code>battled</code> edges out of the Hercules vertex. <pre><code>gremlin&gt; g.V(hercules).out('battled')\n==&gt;v[2304]\n==&gt;v[2560]\n==&gt;v[2816]\ngremlin&gt; g.V(hercules).out('battled').valueMap()\n==&gt;[name:[nemean]]\n==&gt;[name:[hydra]]\n==&gt;[name:[cerberus]]\ngremlin&gt; g.V(hercules).outE('battled').has('time', gt(1)).inV().values('name')\n==&gt;cerberus\n==&gt;hydra\n</code></pre></p> <p>The edge property <code>time</code> on <code>battled</code> edges is indexed by the vertex-centric indices of a vertex. Retrieving <code>battled</code> edges incident to Hercules according to a constraint/filter on <code>time</code> is faster than doing a linear scan of all edges and filtering (typically <code>O(log n)</code>, where <code>n</code> is the number incident edges). JanusGraph is intelligent enough to use vertex-centric indices when available. A <code>toString()</code> of a Gremlin expression shows a decomposition into individual steps.</p> <pre><code>gremlin&gt; g.V(hercules).outE('battled').has('time', gt(1)).inV().values('name').toString()\n==&gt;[GraphStep([v[24744]],vertex), VertexStep(OUT,[battled],edge), HasStep([time.gt(1)]), EdgeVertexStep(IN), PropertiesStep([name],value)]\n</code></pre>"},{"location":"#more-complex-graph-traversal-examples","title":"More Complex Graph Traversal Examples","text":"<p>In the depths of Tartarus lives Pluto. His relationship with Hercules was strained by the fact that Hercules battled his pet, Cerberus. However, Hercules is his nephew\u2009\u2014\u2009how should he make Hercules pay for his insolence?</p> <p>The Gremlin traversals below provide more examples over The Graph of the Gods. The explanation of each traversal is provided in the prior line as a <code>//</code> comment.</p>"},{"location":"#cohabiters-of-tartarus","title":"Cohabiters of Tartarus","text":"<pre><code>gremlin&gt; pluto = g.V().has('name', 'pluto').next()\n==&gt;v[2048]\ngremlin&gt; // who are pluto's cohabitants?\ngremlin&gt; g.V(pluto).out('lives').in('lives').values('name')\n==&gt;pluto\n==&gt;cerberus\ngremlin&gt; // pluto can't be his own cohabitant\ngremlin&gt; g.V(pluto).out('lives').in('lives').where(is(neq(pluto))).values('name')\n==&gt;cerberus\ngremlin&gt; g.V(pluto).as('x').out('lives').in('lives').where(neq('x')).values('name')\n==&gt;cerberus\n</code></pre>"},{"location":"#plutos-brothers","title":"Pluto\u2019s Brothers","text":"<pre><code>gremlin&gt; // where do pluto's brothers live?\ngremlin&gt; g.V(pluto).out('brother').out('lives').values('name')\n==&gt;sky\n==&gt;sea\ngremlin&gt; // which brother lives in which place?\ngremlin&gt; g.V(pluto).out('brother').as('god').out('lives').as('place').select('god', 'place')\n==&gt;[god:v[1024], place:v[512]]\n==&gt;[god:v[1280], place:v[768]]\ngremlin&gt; // what is the name of the brother and the name of the place?\ngremlin&gt; g.V(pluto).out('brother').as('god').out('lives').as('place').select('god', 'place').by('name')\n==&gt;[god:jupiter, place:sky]\n==&gt;[god:neptune, place:sea]\n</code></pre> <p>Finally, Pluto lives in Tartarus because he shows no concern for death. His brothers, on the other hand, chose their locations based upon their love for certain qualities of those locations.! <pre><code>gremlin&gt; g.V(pluto).outE('lives').values('reason')\n==&gt;no fear of death\ngremlin&gt; g.E().has('reason', textContains('loves'))\n==&gt;e[6xs-sg-m51-e8][1024-lives-&gt;512]\n==&gt;e[70g-zk-m51-lc][1280-lives-&gt;768]\ngremlin&gt; g.E().has('reason', textContains('loves')).as('source').values('reason').as('reason').select('source').outV().values('name').as('god').select('source').inV().values('name').as('thing').select('god', 'reason', 'thing')\n==&gt;[god:neptune, reason:loves waves, thing:sea]\n==&gt;[god:jupiter, reason:loves fresh breezes, thing:sky]\n</code></pre></p>"},{"location":"#architectural-overview","title":"Architectural Overview","text":"<p>JanusGraph is a graph database engine. JanusGraph itself is focused on compact graph serialization, rich graph data modeling, and efficient query execution. In addition, JanusGraph utilizes Hadoop for graph analytics and batch graph processing. JanusGraph implements robust, modular interfaces for data persistence, data indexing, and client access. JanusGraph\u2019s modular architecture allows it to interoperate with a wide range of storage, index, and client technologies; it also eases the process of extending JanusGraph to support new ones.</p> <p>Between JanusGraph and the disks sits one or more storage and indexing adapters. JanusGraph comes standard with the following adapters, but JanusGraph\u2019s modular architecture supports third-party adapters.</p> <ul> <li>Data storage:<ul> <li>Apache Cassandra</li> <li>Apache HBase</li> <li>Oracle Berkeley DB Java Edition</li> </ul> </li> <li>Indices, which speed up and enable more complex queries:<ul> <li>Elasticsearch</li> <li>Apache Solr</li> <li>Apache Lucene</li> </ul> </li> </ul> <p>Broadly speaking, applications can interact with JanusGraph in two ways:</p> <ul> <li> <p>Embed JanusGraph inside the application executing     Gremlin     queries directly against the graph within the same JVM. Query     execution, JanusGraph\u2019s caches, and transaction handling all happen     in the same JVM as the application while data retrieval from the     storage backend may be local or remote.</p> </li> <li> <p>Interact with a local or remote JanusGraph instance by submitting     Gremlin queries to the server. JanusGraph natively supports the     Gremlin Server component of the Apache TinkerPop stack.</p> </li> </ul> <p></p>"},{"location":"appendices/","title":"Appendices","text":""},{"location":"appendices/#api-documentation-javadoc","title":"API Documentation (JavaDoc)","text":"<p>Refer to the JanusGraph API documentation for a complete documentation of all core APIs exposed by JanusGraph. We strongly encourage all users of JanusGraph to use the Gremlin query language for any queries executed on JanusGraph and to not use JanusGraph\u2019s APIs outside of the management system.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#version-compatibility","title":"Version Compatibility","text":"<p>The JanusGraph project is growing along with the rest of the graph and big data ecosystem and utilized storage and indexing backends. Below are version compatibilities between the various versions of components. For dependent backend systems, different minor versions are typically supported as well. It is strongly encouraged to verify version compatibility prior to deploying JanusGraph.</p> <p>Although JanusGraph may be compatible with older and no longer supported versions of its dependencies, users are warned that there are possible risks and security exposures with running software that is no longer supported or updated. Please check with the software providers to understand their supported versions. Users are strongly encouraged to use the latest versions of the software.</p>"},{"location":"changelog/#version-compatibility-matrix","title":"Version Compatibility Matrix","text":"JanusGraph Storage Version Cassandra HBase Bigtable Elasticsearch Solr TinkerPop Spark Scala 0.1.z 1 1.2.z, 2.0.z, 2.1.z 0.98.z, 1.0.z, 1.1.z, 1.2.z 0.9.z, 1.0.0-preZ, 1.0.0 1.5.z 5.2.z 3.2.z 1.6.z 2.10.z 0.2.z 1 1.2.z, 2.0.z, 2.1.z, 2.2.z, 3.0.z, 3.11.z 0.98.z, 1.0.z, 1.1.z, 1.2.z, 1.3.z 0.9.z, 1.0.0-preZ, 1.0.0 1.5-1.7.z, 2.3-2.4.z, 5.y, 6.y 5.2-5.5.z, 6.2-6.6.z, 7.y 3.2.z 1.6.z 2.10.z"},{"location":"changelog/#release-notes","title":"Release Notes","text":""},{"location":"changelog/#version-023-release-date-may-21-2019","title":"Version 0.2.3 (Release Date: May 21, 2019)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.3/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.3&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.3\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.9</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.3, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/9?closed=1</li> </ul>"},{"location":"changelog/#version-022-release-date-october-9-2018","title":"Version 0.2.2 (Release Date: October 9, 2018)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.2/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.2&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.2\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.9</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.2, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/6?closed=1</li> </ul>"},{"location":"changelog/#version-021-release-date-july-9-2018","title":"Version 0.2.1 (Release Date: July 9, 2018)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.1/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.20, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.5, 6.0.1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.9</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/5?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions","title":"Upgrade Instructions","text":""},{"location":"changelog/#hbase-ttl","title":"HBase TTL","text":"<p>In JanusGraph 0.2.0, time-to-live (TTL) support was added for HBase storage backend. In order to utilize the TTL capability on HBase, the graph timestamps need to be MILLI. If the <code>graph.timestamps</code> property is not explicitly set to MILLI, the default is MICRO in JanusGraph 0.2.0, which does not work for HBase TTL. Since the <code>graph.timestamps</code> property is FIXED, a new graph needs to be created to make any change of the <code>graph.timestamps</code> property effective.</p>"},{"location":"changelog/#version-020-release-date-october-11-2017","title":"Version 0.2.0 (Release Date: October 11, 2017)","text":"<p>Legacy documentation: https://old-docs.janusgraph.org/0.2.0/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.2.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.2.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.18, 2.2.10, 3.0.14, 3.11.0</li> <li>Apache HBase 0.98.24-hadoop2, 1.2.6, 1.3.1</li> <li>Google Bigtable 1.0.0-pre3</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.7.6, 2.4.6, 5.6.2, 6.0.0-rc1</li> <li>Apache Lucene 7.0.0</li> <li>Apache Solr 5.5.4, 6.6.1, 7.0.0</li> <li>Apache TinkerPop 3.2.6</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.2.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/2?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_1","title":"Upgrade Instructions","text":""},{"location":"changelog/#elasticsearch","title":"Elasticsearch","text":"<p>JanusGraph 0.1.z is compatible with Elasticsearch 1.5.z. There were several configuration options available, including transport client, node client, and legacy configuration track. JanusGraph 0.2.0 is compatible with Elasticsearch versions from 1.y through 6.y, however it offers only a single configuration option using the REST client.</p>"},{"location":"changelog/#transport-client","title":"Transport client","text":"<p>The <code>TRANSPORT_CLIENT</code> interface has been replaced with <code>REST_CLIENT</code>. When migrating an existing graph to JanusGraph 0.2.0, the <code>interface</code> property must be set when connecting to the graph: <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.interface=REST_CLIENT\nindex.search.hostname=127.0.0.1\n</code></pre></p> <p>After connecting to the graph, the property update can be made permanent by making the change with <code>JanusGraphManagement</code>: <pre><code>mgmt = graph.openManagement()\nmgmt.set(\"index.search.elasticsearch.interface\", \"REST_CLIENT\")\nmgmt.commit()\n</code></pre></p>"},{"location":"changelog/#node-client","title":"Node client","text":"<p>A node client with JanusGraph can be configured in a few ways. If the node client was configured as a client-only or non-data node, follow the steps from the transport client section to connect to the existing cluster using the <code>REST_CLIENT</code> instead. If the node client was a data node (local-mode), then convert it into a standalone Elasticsearch node, running in a separate JVM from your application process. This can be done by using the node\u2019s configuration from the JanusGraph configuration to start a standalone Elasticsearch 1.5.z node. For example, we start with these JanusGraph 0.1.z properties:</p> <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.interface=NODE\nindex.search.conf-file=es-client.yml\nindex.search.elasticsearch.ext.node.name=alice\n</code></pre> <p>where the configuration file <code>es-client.yml</code> has properties:</p> <pre><code>node.data: true\npath.data: /var/lib/elasticsearch/data\npath.work: /var/lib/elasticsearch/work\npath.logs: /var/log/elasticsearch\n</code></pre> <p>The properties found in the configuration file <code>es-client.yml</code> and the <code>index.search.elasticsearch.ext.*</code> properties can be inserted into <code>$ES_HOME/config/elasticsearch.yml</code> so that a standalone Elasticsearch 1.5.z node can be started with the same properties. Keep in mind that if any <code>path</code> locations have relative paths, those values may need to be updated appropriately. Once the standalone Elasticsearch node is started, follow the directions in the transport client section to complete the migration to the <code>REST_CLIENT</code> interface. Note that the <code>index.search.conf-file</code> and <code>index.search.elasticsearch.ext.*</code> properties are not used by the <code>REST_CLIENT</code> interface, so they can be removed from the configuration properties.</p>"},{"location":"changelog/#legacy-configuration","title":"Legacy configuration","text":"<p>The legacy configuration track was not recommended in JanusGraph 0.1.z and is no longer supported in JanusGraph 0.2.0. Users should refer to the previous sections and migrate to the <code>REST_CLIENT</code>.</p>"},{"location":"changelog/#version-011-release-date-may-11-2017","title":"Version 0.1.1 (Release Date: May 11, 2017)","text":"<p>Documentation: https://old-docs.janusgraph.org/0.1.1/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.1.1&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.1.1\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.9</li> <li>Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4</li> <li>Google Bigtable 0.9.5.1</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.5.1</li> <li>Apache Lucene 4.10.4</li> <li>Apache Solr 5.2.1</li> <li>Apache TinkerPop 3.2.3</li> <li>Java 1.8</li> </ul> <p>For more information on features and bug fixes in 0.1.1, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/3?closed=1</li> </ul>"},{"location":"changelog/#version-010-release-date-april-11-2017","title":"Version 0.1.0 (Release Date: April 11, 2017)","text":"<p>Documentation: https://old-docs.janusgraph.org/0.1.0/index.html</p> MavenGradle <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;org.janusgraph&lt;/groupId&gt;\n    &lt;artifactId&gt;janusgraph-core&lt;/artifactId&gt;\n    &lt;version&gt;0.1.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <pre><code>compile \"org.janusgraph:janusgraph-core:0.1.0\"\n</code></pre> <p>Tested Compatibility:</p> <ul> <li>Apache Cassandra 2.1.9</li> <li>Apache HBase 0.98.8-hadoop2, 1.0.3, 1.1.8, 1.2.4</li> <li>Google Bigtable 0.9.5.1</li> <li>Oracle BerkeleyJE 7.3.7</li> <li>Elasticsearch 1.5.1</li> <li>Apache Lucene 4.10.4</li> <li>Apache Solr 5.2.1</li> <li>Apache TinkerPop 3.2.3</li> <li>Java 1.8</li> </ul> <p>Features added since version Titan 1.0.0:</p> <ul> <li> <p>TinkerPop 3.2.3 compatibility</p> <ul> <li>Includes update to Spark 1.6.1</li> </ul> </li> <li> <p>Query optimizations: JanusGraphStep folds in HasId and HasContainers     can be folded in even mid-traversal</p> </li> <li> <p>Support Google Cloud Bigtable as a backend over the HBase interface</p> </li> <li> <p>Compatibility with newer versions of backend and index stores</p> <ul> <li> <p>HBase 1.2</p> </li> <li> <p>BerkeleyJE 7.3.7</p> </li> </ul> </li> <li> <p>Includes a number of bug fixes and optimizations</p> </li> </ul> <p>For more information on features and bug fixes in 0.1.0, see the GitHub milestone:</p> <ul> <li>https://github.com/JanusGraph/janusgraph/milestone/1?closed=1</li> </ul>"},{"location":"changelog/#upgrade-instructions_2","title":"Upgrade Instructions","text":"<p>JanusGraph is based on the latest commit to the <code>titan11</code> branch of Titan repo.</p> <p>JanusGraph has made the following changes to Titan, so you will need to adjust your code and configuration accordingly:</p> <ol> <li> <p>module names: <code>titan-*</code> are now <code>janusgraph-*</code></p> </li> <li> <p>package names: <code>com.thinkaurelius.titan</code> are now <code>org.janusgraph</code></p> </li> <li> <p>class names: <code>Titan*</code> are now <code>JanusGraph*</code> except in cases where     this would duplicate a word, e.g., <code>TitanGraph</code> is simply     <code>JanusGraph</code> rather than <code>JanusGraphGraph</code></p> </li> </ol> <p>For more information on how to configure JanusGraph to read data which had previously been written by Titan refer to Migration from titan.</p>"},{"location":"development/","title":"Development","text":"<p>The following sections describe the JanusGraph development process.</p>"},{"location":"development/#development-decisions","title":"Development Decisions","text":"<p>Many development decisions will be made during the day-to-day work of JanusGraph contributors without any extra process overhead. However, for larger bodies of work and significant updates and additions, the following process shall be followed. Significant work may include, but is not limited to:</p> <ul> <li>A major new feature or subproject, e.g., a new storage adapter</li> <li>Incrementing the version of a core dependency such as a Apache TinkerPop</li> <li>Addition or deprecation of a public API</li> <li>Internal shared data structure or API change</li> <li>Addition of a new major dependency</li> </ul> <p>For these sorts of changes, contributors will:</p> <ul> <li>Create one or more issues in the GitHub issue tracker</li> <li>Start a DISCUSS thread on the janusgraph-dev  list where the proposed change may be discussed by committers and other community members</li> <li>When the proposer feels it appropriate, a VOTE shall be called</li> <li>Two +1 votes are required for the change to be accepted</li> </ul>"},{"location":"development/#branching","title":"Branching","text":"<p>For features that involve only one developer, developers will work in their own JanusGraph forks, managing their own branches, and submitting pull requests when ready. If multiple developers wish to collaborate on a feature, they may request that a feature branch be created in JanusGraph repository. If the developers are not committers, a JanusGraph committer will be assigned as the shepherd for that branch. The shepherd will be responsible for merging pull requests to that branch.</p>"},{"location":"development/#branch-naming-conventions","title":"Branch Naming Conventions","text":"<p>All branch names hosted in the JanusGraph repository shall be prepended with <code>Issue_#_</code>.</p>"},{"location":"development/#pull-requests","title":"Pull Requests","text":"<p>Users wishing to contribute to JanusGraph should fork the JanusGraph repository and then submit pull requests using the  GitHub pull request process. Every pull request must be associated with an existing issue. Be sure to include the relevant issue number in the title of your pull request, complete the pull request template checklist, and provide a description of what test suites were run to validate the pull request. Pull requests commit messages should clearly describe what work was accomplished. Intermediate work-in-progress commits should be squashed prior to pull request submittal.</p>"},{"location":"development/#review-then-commit-rtc","title":"Review-Then-Commit (RTC)","text":"<p>Pull requests must be reviewed before being merged following a review-then-commit (RTC) process. The JanusGraph project uses the GitHub review process to review pull requests. Non-committers are welcomed and encouraged to review pull requests. Their review will be non-binding, but can be taken into consideration and are still very valuable community input. The following rules apply to the voting process.</p> <ul> <li>Approval flows<ul> <li>2 committer approvals are required to merge a pull request</li> <li>If a committer submits the pull request, it has their implicit     approval so it requires 1 additional committer approval</li> <li>1 committer approval followed a one week review period for     objections at which point a lazy consensus is assumed</li> </ul> </li> <li>One or more -1 votes within a change request will veto the pull     request until the noted issue(s) are addressed and the -1 vote is     withdrawn</li> <li>Change requests will not be considered valid without an explanation</li> </ul>"},{"location":"development/#commit-then-review-ctr","title":"Commit-Then-Review (CTR)","text":"<p>In instances where the committer deems the full RTC process unnecessary, a commit-then-review (CTR) process may be employed. The purpose of invoking CTR is to reduce the burden on the committers and minimize the turnaround time for merging trivial changes. Changes of this sort may include:</p> <ul> <li>Documentation typo or small documentation addition</li> <li>Addition of a new test case</li> <li>Merging approved fixes into an upstream branch</li> </ul> <p>Any commit that is made following CTR shall include the fact that it is a CTR in the commit comments. Community members who wish to review CTRs may subscribe to the JanusGraph commits list so that they will see CTRs as they come through. If another committer responds with a -1, the commit should be rolled back and the formal RTC process should be followed.</p>"},{"location":"development/#merging-of-pull-requests","title":"Merging of Pull Requests","text":"<p>A pull request is ready to be merged when it has been approved (see Review-Then-Commit (RTC)). It can either be merged manually with <code>git</code> commands or through the GitHub UI with the Merge pull request button. If the pull request targets a release branch that is upstream of other release branches (e.g., <code>0.2</code> is upstream of <code>master</code>), then it is important to make sure that the change also lands in the downstream release branches. This is the responsibility of the committer who merges the pull request. Therefore, merge the pull request first into its target branch and then merge that branch (e.g., <code>0.2</code>) into the next downstream branch (e.g., <code>0.3</code>), and so on until you merge the last release branch into <code>master</code>. Afterwards push all release branches, ideally with an atomic commit:</p> <pre><code>git push --atomic origin master 0.3 0.2\n</code></pre> <p>This approach keeps merge conflicts to a minimum when a release branch is merged into a downstream release branch.</p>"},{"location":"development/#release-policy","title":"Release Policy","text":"<p>Any JanusGraph committer may propose a release. To propose a release, simple start a new RELEASE thread on janusgraph-dev proposing the new release and requesting feedback on what should be included in the release. After consensus is reached the release manager will perform the following tasks:</p> <ul> <li>Create a release branch so that work may continue on <code>master</code></li> <li>Prepare the release artifacts</li> <li>Call a vote to approve the release on     janusgraph-dev</li> <li>Committers will be given 72 hours to review and vote on the release     artifacts</li> <li>Three +1 votes are required for a release to be approved</li> <li>One or more -1 votes with explanation will veto the release until     the noted issues are addressed and the -1 votes are withdrawn</li> </ul>"},{"location":"development/#building-janusgraph","title":"Building JanusGraph","text":"<p>To build JanusGraph you need git and Maven.</p> <ol> <li> <p>Clone the JanusGraph repository from     GitHub to a local     directory.</p> </li> <li> <p>In that directory, execute <code>mvn clean install</code>. This will build     JanusGraph and run the internal test suite. The internal test suite     has no external dependencies. Note, that running all test cases     requires a significant amount of time. To skip the tests when     building JanusGraph, execute <code>mvn clean install -DskipTests</code></p> </li> <li> <p>For comprehensive test coverage, execute     <code>mvn clean test -P comprehensive</code>. This will run additional test     covering communication to external storage backends, performance     tests and concurrency tests. The comprehensive test suite uses     Cassandra and HBase as external databases and requires that     Cassandra and HBase are installed. Note, that running the     comprehensive test suite requires a significant amount of of time     (&gt; 1 hour).</p> </li> </ol>"},{"location":"development/#depending-on-janusgraph-snapshots","title":"Depending on JanusGraph Snapshots","text":"<p>For developing against the most current version of JanusGraph, depend on JanusGraph snapshot releases. Note, that these releases are development releases and therefore unstable and likely to change. Unless one is interested in the most recent development status of JanusGraph, we recommend to use the stable JanusGraph release instead.</p> <p>```xml tab='Maven'  org.janusgraph janusgraph-core 0.2.3-SNAPSHOT <pre><code>```groovy tab='Gradle'\ncompile \"org.janusgraph:janusgraph-core:0.2.3-SNAPSHOT\"\n</code></pre></p> <p>Check the master branch for the most current release version. SNAPSHOTs will be available through the Sonatype repository.</p> <p>When adding this dependency, be sure to add the following repository to build file:</p> <p>```xml tab='Maven'  ossrh Sonatype Nexus Snapshots https://oss.sonatype.org/content/repositories/snapshots false true <pre><code>```groovy tab='Gradle'\nmaven {\n  url \"https://oss.sonatype.org/content/repositories/snapshots\"\n  mavenContent {\n    snapshotsOnly()\n  }\n}\n</code></pre></p>"},{"location":"development/#faqs","title":"FAQs","text":"<p>Maven build causes dozens of \"[WARNING] We have a duplicate\u2026\" errors</p> <p>Make sure to use the maven-assembly-plugin when building or depending on JanusGraph.</p>"},{"location":"advanced-topics/advschema/","title":"Advanced Schema","text":"<p>This page describes some of the advanced schema definition options that JanusGraph provides. For general information on JanusGraph\u2019s schema and how to define it, refer to Schema and Data Modeling.</p>"},{"location":"advanced-topics/advschema/#static-vertices","title":"Static Vertices","text":"<p>Vertex labels can be defined as static which means that vertices with that label cannot be modified outside the transaction in which they were created. <pre><code>mgmt = graph.openManagement()\ntweet = mgmt.makeVertexLabel('tweet').setStatic().make()\nmgmt.commit()\n</code></pre></p> <p>Static vertex labels are a method of controlling the data lifecycle and useful when loading data into the graph that should not be modified after its creation.</p>"},{"location":"advanced-topics/advschema/#edge-and-vertex-ttl","title":"Edge and Vertex TTL","text":"<p>Edge and vertex labels can be configured with a time-to-live (TTL). Edges and vertices with such labels will automatically be removed from the graph when the configured TTL has passed after their initial creation. TTL configuration is useful when loading a large amount of data into the graph that is only of temporary use. Defining a TTL removes the need for manual clean up and handles the removal very efficiently. For example, it would make sense to TTL event edges such as user-page visits when those are summarized after a certain period of time or simply no longer needed for analytics or operational query processing.</p> <p>The following storage backends support edge and vertex TTL.</p> <ul> <li> <p>Cassandra</p> </li> <li> <p>HBase</p> </li> </ul>"},{"location":"advanced-topics/advschema/#edge-ttl","title":"Edge TTL","text":"<p>Edge TTL is defined on a per-edge label basis, meaning that all edges of that label have the same time-to-live. Note that the backend must support cell level TTL. Currently only Cassandra and HBase support this. <pre><code>mgmt = graph.openManagement()\nvisits = mgmt.makeEdgeLabel('visits').make()\nmgmt.setTTL(visits, Duration.ofDays(7))\nmgmt.commit()\n</code></pre></p> <p>Note, that modifying an edge resets the TTL for that edge. Also note, that the TTL of an edge label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label.</p>"},{"location":"advanced-topics/advschema/#property-ttl","title":"Property TTL","text":"<p>Property TTL is very similar to edge TTL and defined on a per-property key basis, meaning that all properties of that key have the same time-to-live. Note that the backend must support cell level TTL. Currently only Cassandra and HBase support this. <pre><code>mgmt = graph.openManagement()\nsensor = mgmt.makePropertyKey('sensor').cardinality(Cardinality.LIST).dataType(Double.class).make()\nmgmt.setTTL(sensor, Duration.ofDays(21))\nmgmt.commit()\n</code></pre></p> <p>As with edge TTL, modifying an existing property resets the TTL for that property and modifying the TTL for a property key might not immediately take effect.</p>"},{"location":"advanced-topics/advschema/#vertex-ttl","title":"Vertex TTL","text":"<p>Vertex TTL is defined on a per-vertex label basis, meaning that all vertices of that label have the same time-to-live. The configured TTL applies to the vertex, its properties, and all incident edges to ensure that the entire vertex is removed from the graph. For this reason, a vertex label must be defined as static before a TTL can be set to rule out any modifications that would invalidate the vertex TTL. Vertex TTL only applies to static vertex labels. Note that the backend must support store level TTL. Currently only Cassandra and HBase support this. <pre><code>mgmt = graph.openManagement()\ntweet = mgmt.makeVertexLabel('tweet').setStatic().make()\nmgmt.setTTL(tweet, Duration.ofHours(36))\nmgmt.commit()\n</code></pre></p> <p>Note, that the TTL of a vertex label can be modified but it might take some time for this change to propagate to all running JanusGraph instances which means that two different TTLs can be temporarily in use for the same label.</p>"},{"location":"advanced-topics/advschema/#multi-properties","title":"Multi-Properties","text":"<p>As discussed in Schema and Data Modeling, JanusGraph supports property keys with SET and LIST cardinality. Hence, JanusGraph supports multiple properties with the same key on a single vertex. Furthermore, JanusGraph treats properties similarly to edges in that single-valued property annotations are allowed on properties as shown in the following example. <pre><code>mgmt = graph.openManagement()\nmgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.LIST).make()\nmgmt.commit()\nv = graph.addVertex()\np1 = v.property('name', 'Dan LaRocque')\np1.property('source', 'web')\np2 = v.property('name', 'dalaro')\np2.property('source', 'github')\ngraph.tx().commit()\nv.properties('name')\n==&gt; Iterable over all name properties\n</code></pre> These features are useful in a number of applications such as those where attaching provenance information (e.g. who added a property, when and from where?) to properties is necessary. Support for higher cardinality properties and property annotations on properties is also useful in high-concurrency, scale-out design patterns as described in Eventually-Consistent Storage Backends.</p> <p>Vertex-centric indexes and global graph indexes are supported for properties in the same manner as they are supported for edges. Refer to Indexing for Better Performance for information on defining these indexes for edges and use the corresponding API methods to define the same indexes for properties.</p>"},{"location":"advanced-topics/advschema/#unidirected-edges","title":"Unidirected Edges","text":"<p>Unidirected edges are edges that can only be traversed in the out-going direction. Unidirected edges have a lower storage footprint but are limited in the types of traversals they support. Unidirected edges are conceptually similar to hyperlinks in the world-wide-web in the sense that the out-vertex can traverse through the edge, but the in-vertex is unaware of its existence. <pre><code>mgmt = graph.openManagement()\nmgmt.makeEdgeLabel('author').unidirected().make()\nmgmt.commit()\n</code></pre></p> <p>Note, that unidirected edges do not get automatically deleted when their in-vertices are deleted. The user must ensure that such inconsistencies do not arise or resolve them at query time by explicitly checking vertex existence in a transaction. See the discussion in Ghost Vertices for more information.</p>"},{"location":"advanced-topics/bulk-loading/","title":"Bulk Loading","text":"<p>There are a number of configuration options and tools that make ingesting large amounts of graph data into JanusGraph more efficient. Such ingestion is referred to as bulk loading in contrast to the default transactional loading where small amounts of data are added through individual transactions.</p> <p>There are a number of use cases for bulk loading data into JanusGraph, including:</p> <ul> <li> <p>Introducing JanusGraph into an existing environment with existing     data and migrating or duplicating this data into a new JanusGraph     cluster.</p> </li> <li> <p>Using JanusGraph as an end point of an     ETL     process.</p> </li> <li> <p>Adding an existing or external graph datasets (e.g. publicly     available RDF datasets) to a running     JanusGraph cluster.</p> </li> <li> <p>Updating a JanusGraph graph with results from a graph analytics job.</p> </li> </ul> <p>This page describes configuration options and tools that make bulk loading more efficient in JanusGraph. Please observe the limitations and assumptions for each option carefully before proceeding to avoid data loss or data corruption.</p> <p>This documentation focuses on JanusGraph specific optimization. In addition, consider improving the chosen storage backend and (optional) index backend for high write performance. Please refer to the documentation of the respective backend for more information.</p>"},{"location":"advanced-topics/bulk-loading/#configuration-options","title":"Configuration Options","text":""},{"location":"advanced-topics/bulk-loading/#batch-loading","title":"Batch Loading","text":"<p>Enabling the <code>storage.batch-loading</code> configuration option will have the biggest positive impact on bulk loading times for most applications. Enabling batch loading disables JanusGraph internal consistency checks in a number of places. Most importantly, it disables locking. In other words, JanusGraph assumes that the data to be loaded into JanusGraph is consistent with the graph and hence disables its own checks in the interest of performance.</p> <p>In many bulk loading scenarios it is significantly cheaper to ensure data consistency prior to loading the data then ensuring data consistency while loading it into the database. The <code>storage.batch-loading</code> configuration option exists because of this observation.</p> <p>For example, consider the use case of bulk loading existing user profiles into JanusGraph. Furthermore, assume that the username property key has a unique composite index defined on it, i.e. usernames must be unique across the entire graph. If the user profiles are imported from another database, username uniqueness might already guaranteed. If not, it is simple to sort the profiles by name and filter out duplicates or writing a Hadoop job that does such filtering. Now, we can enable <code>storage.batch-loading</code> which significantly reduces the bulk loading time because JanusGraph does not have to check for every added user whether the name already exists in the database.</p> <p>Important: Enabling <code>storage.batch-loading</code> requires the user to ensure that the loaded data is internally consistent and consistent with any data already in the graph. In particular, concurrent type creation can lead to severe data integrity issues when batch loading is enabled. Hence, we strongly encourage disabling automatic type creation by setting <code>schema.default = none</code> in the graph configuration.</p>"},{"location":"advanced-topics/bulk-loading/#optimizing-id-allocation","title":"Optimizing ID Allocation","text":""},{"location":"advanced-topics/bulk-loading/#id-block-size","title":"ID Block Size","text":"<p>Each newly added vertex or edge is assigned a unique id. JanusGraph\u2019s id pool manager acquires ids in blocks for a particular JanusGraph instance. The id block acquisition process is expensive because it needs to guarantee globally unique assignment of blocks. Increasing <code>ids.block-size</code> reduces the number of acquisitions but potentially leaves many ids unassigned and hence wasted. For transactional workloads the default block size is reasonable, but during bulk loading vertices and edges are added much more frequently and in rapid succession. Hence, it is generally advisable to increase the block size by a factor of 10 or more depending on the number of vertices to be added per machine.</p> <p>Rule of thumb: Set <code>ids.block-size</code> to the number of vertices you expect to add per JanusGraph instance per hour.</p> <p>Important: All JanusGraph instances MUST be configured with the same value for <code>ids.block-size</code> to ensure proper id allocation. Hence, be careful to shut down all JanusGraph instances prior to changing this value.</p>"},{"location":"advanced-topics/bulk-loading/#id-acquisition-process","title":"ID Acquisition Process","text":"<p>When id blocks are frequently allocated by many JanusGraph instances in parallel, allocation conflicts between instances will inevitably arise and slow down the allocation process. In addition, the increased write load due to bulk loading may further slow down the process to the point where JanusGraph considers it failed and throws an exception. There are three configuration options that can be tuned to avoid this.</p> <p>1) <code>ids.authority.wait-time</code> configures the time in milliseconds the id pool manager waits for an id block application to be acknowledged by the storage backend. The shorter this time, the more likely it is that an application will fail on a congested storage cluster.</p> <p>Rule of thumb: Set this to the sum of the 95th percentile read and write times measured on the storage backend cluster under load. Important: This value should be the same across all JanusGraph instances.</p> <p>2) <code>ids.renew-timeout</code> configures the number of milliseconds JanusGraph\u2019s id pool manager will wait in total while attempting to acquire a new id block before failing.</p> <p>Rule of thumb: Set this value to be as large feasible to not have to wait too long for unrecoverable failures. The only downside of increasing it is that JanusGraph will try for a long time on an unavailable storage backend cluster.</p>"},{"location":"advanced-topics/bulk-loading/#optimizing-writes-and-reads","title":"Optimizing Writes and Reads","text":""},{"location":"advanced-topics/bulk-loading/#buffer-size","title":"Buffer Size","text":"<p>JanusGraph buffers writes and executes them in small batches to reduce the number of requests against the storage backend. The size of these batches is controlled by <code>storage.buffer-size</code>. When executing a lot of writes in a short period of time, it is possible that the storage backend can become overloaded with write requests. In that case, increasing <code>storage.buffer-size</code> can avoid failure by increasing the number of writes per request and thereby lowering the number of requests.</p> <p>However, increasing the buffer size increases the latency of the write request and its likelihood of failure. Hence, it is not advisable to increase this setting for transactional loads and one should carefully experiment with this setting during bulk loading.</p>"},{"location":"advanced-topics/bulk-loading/#read-and-write-robustness","title":"Read and Write Robustness","text":"<p>During bulk loading, the load on the cluster typically increases making it more likely for read and write operations to fail (in particular if the buffer size is increased as described above). <code>storage.read-attempts</code> and <code>storage.write-attempts</code> configure how many times JanusGraph will attempt to execute a read or write operation against the storage backend before giving up. If it is expected that there is a high load on the backend during bulk loading, it is generally advisable to increase these configuration options.</p> <p><code>storage.attempt-wait</code> specifies the number of milliseconds that JanusGraph will wait before re-attempting a failed backend operation. A higher value can ensure that operation re-tries do not further increase the load on the backend.</p>"},{"location":"advanced-topics/bulk-loading/#strategies","title":"Strategies","text":""},{"location":"advanced-topics/bulk-loading/#parallelizing-the-load","title":"Parallelizing the Load","text":"<p>By parallelizing the bulk loading across multiple machines, the load time can be greatly reduced if JanusGraph\u2019s storage backend cluster is large enough to serve the additional requests. This is essentially the approach JanusGraph with TinkerPop\u2019s Hadoop-Gremlin takes to bulk loading data into JanusGraph using MapReduce.</p> <p>If Hadoop cannot be used for parallelizing the bulk loading process, here are some high level guidelines for effectively parallelizing the loading process:</p> <ul> <li> <p>In some cases, the graph data can be decomposed into multiple     disconnected subgraphs. Those subgraphs can be loaded independently     in parallel across multiple machines (for instance, using BatchGraph     as described above).</p> </li> <li> <p>If the graph cannot be decomposed, it is often beneficial to load in     multiple steps where the last two steps can be parallelized across     multiple machines:</p> <ol> <li> <p>Make sure the vertex and edge data sets are de-duplicated and     consistent.</p> </li> <li> <p>Set <code>batch-loading=true</code>. Possibly optimize additional     configuration settings described above.</p> </li> <li> <p>Add all the vertices with their properties to the graph (but no     edges). Maintain a (distributed) map from vertex id (as defined     by the loaded data) to JanusGraph\u2019s internal vertex id (i.e.     <code>vertex.getId()</code>) which is a 64 bit long id.</p> </li> <li> <p>Add all the edges using the map to look-up JanusGraph\u2019s vertex     id and retrieving the vertices using that id.</p> </li> </ol> </li> </ul>"},{"location":"advanced-topics/bulk-loading/#qa","title":"Q&amp;A","text":"<ul> <li>What should I do to avoid the following exception during     batch-loading: <code>java.io.IOException: ID renewal thread on partition [X] did not complete in time.</code>?     This exception is mostly likely caused by repeated time-outs during     the id allocation phase due to highly stressed storage backend.     Refer to the section on ID Allocation Optimization above.</li> </ul>"},{"location":"advanced-topics/data-model/","title":"JanusGraph Data Model","text":"<p>JanusGraph stores graphs in adjacency list format which means that a graph is stored as a collection of vertices with their adjacency list. The adjacency list of a vertex contains all of the vertex\u2019s incident edges (and properties).</p> <p>By storing a graph in adjacency list format JanusGraph ensures that all of a vertex\u2019s incident edges and properties are stored compactly in the storage backend which speeds up traversals. The downside is that each edge has to be stored twice - once for each end vertex of the edge.</p> <p>In addition, JanusGraph maintains the adjacency list of each vertex in sort order with the order being defined by the sort key and sort order the edge labels. The sort order enables efficient retrievals of subsets of the adjacency list using vertex centric indices.</p> <p>JanusGraph stores the adjacency list representation of a graph in any storage backend that supports the Bigtable data model.</p>"},{"location":"advanced-topics/data-model/#bigtable-data-model","title":"Bigtable Data Model","text":"<p>Under the Bigtable data model each table is a collection of rows. Each row is uniquely identified by a key. Each row is comprised of an arbitrary (large, but limited) number of cells. A cell is composed of a column and value. A cell is uniquely identified by a column within a given row. Rows in the Bigtable model are called \"wide rows\" because they support a large number of cells and the columns of those cells don\u2019t have to be defined up front as is required in relational databases.</p> <p>JanusGraph has an additional requirement for the Bigtable data model: The cells must be sorted by their columns and a subset of the cells specified by a column range must be efficiently retrievable (e.g. by using index structures, skip lists, or binary search).</p> <p>In addition, a particular Bigtable implementation may keep the rows sorted in the order of their key. JanusGraph can exploit such key-order to effectively partition the graph which provides better loading and traversal performance for very large graphs. However, this is not a requirement.</p>"},{"location":"advanced-topics/data-model/#janusgraph-data-layout","title":"JanusGraph Data Layout","text":"<p>JanusGraph stores each adjacency list as a row in the underlying storage backend. The (64 bit) vertex id (which JanusGraph uniquely assigns to every vertex) is the key which points to the row containing the vertex\u2019s adjacency list. Each edge and property is stored as an individual cell in the row which allows for efficient insertions and deletions. The maximum number of cells allowed per row in a particular storage backend is therefore also the maximum degree of a vertex that JanusGraph can support against this backend.</p> <p>If the storage backend supports key-order, the adjacency lists will be ordered by vertex id, and JanusGraph can assign vertex ids such that the graph is effectively partitioned. Ids are assigned such that vertices which are frequently co-accessed have ids with small absolute difference.</p>"},{"location":"advanced-topics/data-model/#individual-edge-layout","title":"Individual Edge Layout","text":"<p>Each edge and property is stored as one cell in the rows of its adjacent vertices. They are serialized such that the byte order of the column respects the sort key of the edge label. Variable id encoding schemes and compressed object serialization are used to keep the storage footprint of each edge/cell as small as possible.</p> <p>Consider the storage layout of an individual edge as visualized in the top row of the graphic above. The dark blue boxes represent numbers that are encoded with a variable length encoding scheme to reduce the number of bytes they consume. Red boxes represent one or multiple property values (i.e. objects) that are serialized with compressed meta data referenced in the associated property key. Grey boxes represent uncompressed property values (i.e. serialized objects).</p> <p>The serialized representation of an edge starts with the edge label\u2019s unique id (as assigned by JanusGraph). This is typically a small number and compressed well with variable id encoding. The last bit of this id is offset to store whether this is an incoming or outgoing edge. Next, the property value comprising the sort key are stored. The sort key is defined with the edge label and hence the sort key objects meta data can be referenced to the edge label. After that, the id of the adjacent vertex is stored. JanusGraph does not store the actual vertex id but the difference to the id of the vertex that owns this adjacency list. It is likely that the difference is a smaller number than the absolute id and hence compresses better. The vertex id is followed by the id of this edge. Each edge is assigned a unique id by JanusGraph. This concludes the column value of the edge\u2019s cell. The value of the edge\u2019s cell contains the compressed serialization of the signature properties of the edge (as defined by the label\u2019s signature key) and any other properties that have been added to the edge in uncompressed serialization.</p> <p>The serialized representation of a property is simpler and only contains the property\u2019s key id in the column. The property id and the property value are stored in the value. If the property key is defined as <code>list()</code>, however, the property id is stored in the column as well.</p>"},{"location":"advanced-topics/eventual-consistency/","title":"Eventually-Consistent Storage Backends","text":"<p>When running JanusGraph against an eventually consistent storage backend special JanusGraph features must be used to ensure data consistency and special considerations must be made regarding data degradation.</p> <p>This page summarizes some of the aspects to consider when running JanusGraph on top of an eventually consistent storage backend like Apache Cassandra or Apache HBase.</p>"},{"location":"advanced-topics/eventual-consistency/#data-consistency","title":"Data Consistency","text":"<p>On eventually consistent storage backends, JanusGraph must obtain locks in order to ensure consistency because the underlying storage backend does not provide transactional isolation. In the interest of efficiency, JanusGraph does not use locking by default. Hence, the user has to decide for each schema element that defines a consistency constraint whether or not to use locking. Use <code>JanusGraphManagement.setConsistency(element, ConsistencyModifier.LOCK)</code> to explicitly enable locking on a schema element as shown in the following examples. <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('consistentName').dataType(String.class).make()\nindex = mgmt.buildIndex('byConsistentName', Vertex.class).addKey(name).unique().buildCompositeIndex()\nmgmt.setConsistency(name, ConsistencyModifier.LOCK) // Ensures only one name per vertex\nmgmt.setConsistency(index, ConsistencyModifier.LOCK) // Ensures name uniqueness in the graph\nmgmt.commit()\n</code></pre></p> <p>When updating an element that is guarded by a uniqueness constraint, JanusGraph uses the following protocol at the end of a transaction when calling <code>tx.commit()</code>:</p> <ol> <li>Acquire a lock on all elements that have a consistency constraint</li> <li>Re-read those elements from the storage backend and verify that they     match the state of the element in the current transaction prior to     modification. If not, the element was concurrently modified and a     PermanentLocking exception is thrown.</li> <li>Persist the state of the transaction against the storage backend.</li> <li>Release all locks.</li> </ol> <p>This is a brief description of the locking protocol which leaves out optimizations (e.g. local conflict detection) and detection of failure scenarios (e.g. expired locks).</p> <p>The actual lock application mechanism is abstracted such that JanusGraph can use multiple implementations of a locking provider. Currently, two locking providers are supported in the JanusGraph distribution:</p> <ol> <li>A locking implementation based on key-consistent read and write     operations that is agnostic to the underlying storage backend as     long as it supports key-consistent operations (which includes     Cassandra and HBase). This is the default implementation and uses     timestamp based lock applications to determine which transaction     holds the lock.</li> <li>A Cassandra specific locking implementation based on the Astyanax     locking recipe.</li> </ol> <p>Both locking providers require that clocks are synchronized across all machines in the cluster.</p> <p>Warning</p> <p>The locking implementation is not robust against all failure scenarios. For instance, when a Cassandra cluster drops below quorum, consistency is no longer ensured. Hence, it is suggested to use locking-based consistency constraints sparingly with eventually consistent storage backends. For use cases that require strict and or frequent consistency constraint enforcement, it is suggested to use a storage backend that provides transactional isolation.</p>"},{"location":"advanced-topics/eventual-consistency/#data-consistency-without-locks","title":"Data Consistency without Locks","text":"<p>Because of the additional steps required to acquire a lock when committing a modifying transaction, locking is a fairly expensive way to ensure consistency and can lead to deadlock when very many concurrent transactions try to modify the same elements in the graph. Hence, locking should be used in situations where consistency is more important than write latency and the number of conflicting transactions is small.</p> <p>In other situations, it may be better to allow conflicting transactions to proceed and to resolve inconsistencies at read time. This is a design pattern commonly employed in large scale data systems and most effective when the actual likelihood of conflict is small. Hence, write transactions don\u2019t incur additional overhead and any (unlikely) conflict that does occur is detected and resolved at read time and later cleaned up. JanusGraph makes it easy to use this strategy through the following features.</p>"},{"location":"advanced-topics/eventual-consistency/#forking-edges","title":"Forking Edges","text":"<p>Because edge are stored as single records in the underlying storage backend, concurrently modifying a single edge would lead to conflict. Instead of locking, an edge label can be configured to use <code>ConsistencyModifier.FORK</code>. The following example creates a new edge label <code>related</code> and defines its consistency to FORK.</p> <pre><code>mgmt = graph.openManagement()\nrelated = mgmt.makeEdgeLabel('related').make()\nmgmt.setConsistency(related, ConsistencyModifier.FORK)\nmgmt.commit()\n</code></pre> <p>When modifying an edge whose label is configured to FORK the edge is deleted and the modified edge is added as a new one. Hence, if two concurrent transactions modify the same edge, two modified copies of the edge will exist upon commit which can be resolved during querying traversals if needed.</p> <p>Note</p> <p>Edge forking only applies to MULTI edges. Edge labels with a multiplicity constraint cannot use this strategy since a constraint is built into the edge label definition that requires an explicit lock or use the conflict resolution mechanism of the underlying storage backend.</p>"},{"location":"advanced-topics/eventual-consistency/#multi-properties","title":"Multi-Properties","text":"<p>Modifying single valued properties on vertices concurrently can result in a conflict. Similarly to edges, one can allow an arbitrary number of properties on a vertex for a particular property key defined with cardinality LIST and FORK on modification. Hence, instead of conflict one reads multiple properties. Since JanusGraph allows properties on properties, provenance information like <code>author</code> can be added to the properties to facilitate resolution at read time.</p> <p>See multi-properties to learn how to define those.</p>"},{"location":"advanced-topics/eventual-consistency/#data-inconsistency","title":"Data Inconsistency","text":""},{"location":"advanced-topics/eventual-consistency/#temporary-inconsistency","title":"Temporary Inconsistency","text":"<p>On eventually consistent storage backends, writes may not be immediately visible to the entire cluster causing temporary inconsistencies in the graph. This is an inherent property of eventual consistency, in the sense, that accepted updates must be propagated to other instances in the cluster and no guarantees are made with respect to read atomicity in the interest of performance.</p> <p>From JanusGraph\u2019s perspective, eventual consistency might cause the following temporary graph inconsistencies in addition the general inconsistency that some parts of a transaction are visible while others aren\u2019t yet.</p> <p>Stale Index entries Index entries might point to nonexistent vertices or edges. Similarly, a vertex or edge appears in the graph but is not yet indexed and hence ignored by global graph queries.</p> <p>Half-Edges Only one direction of an edge gets persisted or deleted which might lead to the edge not being or incorrectly being retrieved.</p> <p>Note</p> <p>In order to avoid that write failures result in permanent inconsistencies in the graph it is recommended to use storage backends that support batch write atomicity and to ensure that write atomicity is enabled. To get the benefit of write atomicity, the number modifications made in a single transaction must be smaller than the configured <code>buffer-size</code> option documented in Configuration Reference. The buffer size defines the maximum number of modifications that JanusGraph will persist in a single batch. If a transaction has more modifications, the persistence will be split into multiple batches which are persisted individually which is useful for batch loading but invalidates write atomicity.</p>"},{"location":"advanced-topics/eventual-consistency/#ghost-vertices","title":"Ghost Vertices","text":"<p>A permanent inconsistency that can arise when operating JanusGraph on eventually consistent storage backend is the phenomena of ghost vertices. If a vertex gets deleted while it is concurrently being modified, the vertex might re-appear as a ghost.</p> <p>The following strategies can be used to mitigate this issue:</p> <p>Existence checks Configure transactions to (double) check for the existence of vertices prior to returning them. Please see Transaction Configuration for more information and note that this can significantly decrease performance. Note, that this does not fix the inconsistencies but hides some of them from the user.</p> <p>Regular Clean-ups Run regular batch-jobs to repair inconsistencies in the graph using JanusGraph with TinkerPop\u2019s Hadoop-Gremlin.  This is the only strategy that can address all inconsistencies and effectively repair them. We will provide increasing support for such repairs in future versions of Faunus.</p> <p>Soft Deletes Instead of deleting vertices, they are marked as deleted which keeps them in the graph for future analysis but hides them from user-facing transactions.</p>"},{"location":"advanced-topics/hadoop/","title":"JanusGraph with TinkerPop\u2019s Hadoop-Gremlin","text":"<p>This chapter describes how to leverage Apache Hadoop  and Apache Spark to configure JanusGraph for distributed graph processing. These steps will provide an overview on how to get started with those projects, but please refer to those project communities to become more deeply familiar with them.</p> <p>JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP.</p> <p>For the scope of the example below, Apache Spark is the computing framework and Apache Cassandra is the storage backend. The directions can be followed with other packages with minor changes to the configuration properties.</p> <p>Note</p> <p>The examples in this chapter are based on running Spark in local mode or standalone cluster mode. Additional configuration is required when using Spark on YARN or Mesos.</p>"},{"location":"advanced-topics/hadoop/#configuring-hadoop-for-running-olap","title":"Configuring Hadoop for Running OLAP","text":"<p>For running OLAP queries from the Gremlin Console, a few prerequisites need to be fulfilled. You will need to add the Hadoop configuration directory into the <code>CLASSPATH</code>, and the configuration directory needs to point to a live Hadoop cluster.</p> <p>Hadoop provides a distributed access-controlled file system. The Hadoop file system is used by Spark workers running on different machines to have a common source for file based operations. The intermediate computations of various OLAP queries may be persisted on the Hadoop file system.</p> <p>For configuring a single node Hadoop cluster, please refer to official Apache Hadoop Docs</p> <p>Once you have a Hadoop cluster up and running, we will need to specify the Hadoop configuration files in the <code>CLASSPATH</code>. The below document expects that you have those configuration files located under <code>/etc/hadoop/conf</code>.</p> <p>Once verified, follow the below steps to add the Hadoop configuration to the <code>CLASSPATH</code> and start the Gremlin Console, which will play the role of the Spark driver program. <pre><code>export HADOOP_CONF_DIR=/etc/hadoop/conf\nexport CLASSPATH=$HADOOP_CONF_DIR\nbin/gremlin.sh\n</code></pre></p> <p>Once the path to Hadoop configuration has been added to the <code>CLASSPATH</code>, we can verify whether the Gremlin Console can access the Hadoop cluster by following these quick steps: <pre><code>gremlin&gt; hdfs\n==&gt;storage[org.apache.hadoop.fs.LocalFileSystem@65bb9029] // BAD\n\ngremlin&gt; hdfs\n==&gt;storage[DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1229457199_1, ugi=user (auth:SIMPLE)]]] // GOOD\n</code></pre></p>"},{"location":"advanced-topics/hadoop/#olap-traversals","title":"OLAP Traversals","text":"<p>JanusGraph-Hadoop works with TinkerPop\u2019s hadoop-gremlin package for general-purpose OLAP to traverse over the graph, and parallelize queries by leveraging Apache Spark.</p>"},{"location":"advanced-topics/hadoop/#olap-traversals-with-spark-local","title":"OLAP Traversals with Spark Local","text":"<p>The backend demonstrated here is Cassandra for the OLAP example below. Additional configuration will be needed that is specific to that storage backend. The configuration is specified by the <code>gremlin.hadoop.graphReader</code> property which specifies the class to read data from the storage backend.</p> <p>JanusGraph currently supports following graphReader classes:</p> <ul> <li><code>Cassandra3InputFormat</code> for use with Cassandra 3</li> <li><code>CassandraInputFormat</code> for use with Cassandra 2</li> <li><code>HBaseInputFormat</code> for use with HBase</li> </ul> <p>The following properties file can be used to connect a JanusGraph instance in Cassandra such that it can be used with HadoopGraph to run OLAP queries.</p> <pre><code># read-cassandra-3.properties\n#\n# Hadoop Graph Configuration\n#\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.cassandra.Cassandra3InputFormat\ngremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat\n\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\ngremlin.spark.persistContext=true\n\n#\n# JanusGraph Cassandra InputFormat configuration\n#\n# These properties defines the connection properties which were used while write data to JanusGraph.\njanusgraphmr.ioformat.conf.storage.backend=cassandra\n# This specifies the hostname &amp; port for Cassandra data store.\njanusgraphmr.ioformat.conf.storage.hostname=127.0.0.1\njanusgraphmr.ioformat.conf.storage.port=9160\n# This specifies the keyspace where data is stored.\njanusgraphmr.ioformat.conf.storage.cassandra.keyspace=janusgraph\n# This defines the indexing backned configuration used while writing data to JanusGraph.\njanusgraphmr.ioformat.conf.index.search.backend=elasticsearch\njanusgraphmr.ioformat.conf.index.search.hostname=127.0.0.1\n# Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr).\n\n#\n# Apache Cassandra InputFormat configuration\n#\ncassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner\n\n#\n# SparkGraphComputer Configuration\n#\nspark.master=local[*]\nspark.executor.memory=1g\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator\n</code></pre> <p>First create a properties file with above configurations, and load the same on the Gremlin Console to run OLAP queries as follows: <pre><code>bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\ngremlin&gt; :plugin use tinkerpop.hadoop\n==&gt;tinkerpop.hadoop activated\ngremlin&gt; :plugin use tinkerpop.spark\n==&gt;tinkerpop.spark activated\ngremlin&gt; // 1. Open a the graph for OLAP processing reading in from Cassandra 3\ngremlin&gt; graph = GraphFactory.open('conf/hadoop-graph/read-cassandra-3.properties')\n==&gt;hadoopgraph[cassandra3inputformat-&gt;gryooutputformat]\ngremlin&gt; // 2. Configure the traversal to run with Spark\ngremlin&gt; g = graph.traversal().withComputer(SparkGraphComputer)\n==&gt;graphtraversalsource[hadoopgraph[cassandra3inputformat-&gt;gryooutputformat], sparkgraphcomputer]\ngremlin&gt; // 3. Run some OLAP traversals\ngremlin&gt; g.V().count()\n......\n==&gt;808\ngremlin&gt; g.E().count()\n......\n==&gt; 8046\n</code></pre></p>"},{"location":"advanced-topics/hadoop/#olap-traversals-with-spark-standalone-cluster","title":"OLAP Traversals with Spark Standalone Cluster","text":"<p>The steps followed in the previous section can also be used with a Spark standalone cluster with only minor changes:</p> <ul> <li> <p>Update the <code>spark.master</code> property to point to the Spark master URL     instead of local</p> </li> <li> <p>Update the <code>spark.executor.extraClassPath</code> to enable the Spark     executor to find the JanusGraph dependency jars</p> </li> <li> <p>Copy the JanusGraph dependency jars into the location specified in     the previous step on each Spark executor machine</p> </li> </ul> <p>Note</p> <p>We have copied all the jars under janusgraph-distribution/lib into /opt/lib/janusgraph/ and the same directory structure is created across all workers, and jars are manually copied across all workers.</p> <p>The final properties file used for OLAP traversal is as follows: <pre><code># read-cassandra-3.properties\n#\n# Hadoop Graph Configuration\n#\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.cassandra.Cassandra3InputFormat\ngremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat\n\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\ngremlin.spark.persistContext=true\n\n#\n# JanusGraph Cassandra InputFormat configuration\n#\n# These properties defines the connection properties which were used while write data to JanusGraph.\njanusgraphmr.ioformat.conf.storage.backend=cassandra\n# This specifies the hostname &amp; port for Cassandra data store.\njanusgraphmr.ioformat.conf.storage.hostname=127.0.0.1\njanusgraphmr.ioformat.conf.storage.port=9160\n# This specifies the keyspace where data is stored.\njanusgraphmr.ioformat.conf.storage.cassandra.keyspace=janusgraph\n# This defines the indexing backned configuration used while writing data to JanusGraph.\njanusgraphmr.ioformat.conf.index.search.backend=elasticsearch\njanusgraphmr.ioformat.conf.index.search.hostname=127.0.0.1\n# Use the appropriate properties for the backend when using a different storage backend (HBase) or indexing backend (Solr).\n\n#\n# Apache Cassandra InputFormat configuration\n#\ncassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner\n\n#\n# SparkGraphComputer Configuration\n#\nspark.master=spark://127.0.0.1:7077\nspark.executor.memory=1g\nspark.executor.extraClassPath=/opt/lib/janusgraph/*\nspark.serializer=org.apache.spark.serializer.KryoSerializer\nspark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator\n</code></pre></p> <p>Then use the properties file as follows from the Gremlin Console: <pre><code>bin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: janusgraph.imports\ngremlin&gt; :plugin use tinkerpop.hadoop\n==&gt;tinkerpop.hadoop activated\ngremlin&gt; :plugin use tinkerpop.spark\n==&gt;tinkerpop.spark activated\ngremlin&gt; // 1. Open a the graph for OLAP processing reading in from Cassandra 3\ngremlin&gt; graph = GraphFactory.open('conf/hadoop-graph/read-cassandra-3.properties')\n==&gt;hadoopgraph[cassandra3inputformat-&gt;gryooutputformat]\ngremlin&gt; // 2. Configure the traversal to run with Spark\ngremlin&gt; g = graph.traversal().withComputer(SparkGraphComputer)\n==&gt;graphtraversalsource[hadoopgraph[cassandra3inputformat-&gt;gryooutputformat], sparkgraphcomputer]\ngremlin&gt; // 3. Run some OLAP traversals\ngremlin&gt; g.V().count()\n......\n==&gt;808\ngremlin&gt; g.E().count()\n......\n==&gt; 8046\n</code></pre></p>"},{"location":"advanced-topics/hadoop/#other-vertex-programs","title":"Other Vertex Programs","text":"<p>Apache TinkerPop provides various vertex programs. A vertex program runs on each vertex until either a termination criteria is attained or a fixed number of iterations has been reached. Due to the parallel nature of vertex programs, they can leverage parallel computing frameworks like Spark or Giraph to improve their performance.</p> <p>Once you are familiar with how to configure JanusGraph to work with Spark, you can run all the other vertex programs provided by Apache TinkerPop, like Page Rank, Bulk Loading and Peer Pressure. See the TinkerPop VertexProgram docs for more details.</p>"},{"location":"advanced-topics/index-admin/","title":"Index Management","text":""},{"location":"advanced-topics/index-admin/#reindexing","title":"Reindexing","text":"<p>Graph Index and Vertex-centric Indexes describe how to build graph-global and vertex-centric indexes to improve query performance. These indexes are immediately available if the indexed keys or labels have been newly defined in the same management transaction. In this case, there is no need to reindex the graph and this section can be skipped. If the indexed keys and labels already existed prior to index construction it is necessary to reindex the entire graph in order to ensure that the index contains previously added elements. This section describes the reindexing process.</p> <p>Warning</p> <p>Reindexing is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.</p>"},{"location":"advanced-topics/index-admin/#overview","title":"Overview","text":"<p>JanusGraph can begin writing incremental index updates right after an index is defined. However, before the index is complete and usable, JanusGraph must also take a one-time read pass over all existing graph elements associated with the newly indexed schema type(s). Once this reindexing job has completed, the index is fully populated and ready to be used. The index must then be enabled to be used during query processing.</p>"},{"location":"advanced-topics/index-admin/#prior-to-reindex","title":"Prior to Reindex","text":"<p>The starting point of the reindexing process is the construction of an index. Refer to Indexing for Better Performance for a complete discussion of global graph and vertex-centric indexes. Note, that a global graph index is uniquely identified by its name. A vertex-centric index is uniquely identified by the combination of its name and the edge label or property key on which the index is defined - the name of the latter is referred to as the index type in this section and only applies to vertex-centric indexes.</p> <p>After building a new index against existing schema elements it is recommended to wait a few minutes for the index to be announced to the cluster. Note the index name (and the index type in case of a vertex-centric index) since this information is needed when reindexing.</p>"},{"location":"advanced-topics/index-admin/#preparing-to-reindex","title":"Preparing to Reindex","text":"<p>There is a choice between two execution frameworks for reindex jobs:</p> <ul> <li>MapReduce</li> <li>JanusGraphManagement</li> </ul> <p>Reindex on MapReduce supports large, horizontally-distributed databases. Reindex on JanusGraphManagement spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine.</p> <p>Reindexing requires:</p> <ul> <li>The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when     building a new index)</li> <li>The index type (a string\u2009\u2014\u2009the name of the edge label or property     key on which the vertex-centric index is built). This applies only     to vertex-centric indexes - leave blank for global graph indexes.</li> </ul>"},{"location":"advanced-topics/index-admin/#executing-a-reindex-job-on-mapreduce","title":"Executing a Reindex Job on MapReduce","text":"<p>The recommended way to generate and run a reindex job on MapReduce is through the <code>MapReduceIndexManagement</code> class. Here is a rough outline of the steps to run a reindex job using this class:</p> <ul> <li>Open a <code>JanusGraph</code> instance</li> <li>Pass the graph instance into <code>MapReduceIndexManagement</code>'s constructor</li> <li>Call <code>updateIndex(&lt;index&gt;, SchemaAction.REINDEX)</code> on the <code>MapReduceIndexManagement</code> instance</li> <li>If the index has not yet been enabled, enable it through <code>JanusGraphManagement</code></li> </ul> <p>This class implements an <code>updateIndex</code> method that supports only the <code>REINDEX</code> and <code>REMOVE_INDEX</code> actions for its <code>SchemaAction</code> parameter. The class starts a Hadoop MapReduce job using the Hadoop configuration and jars on the classpath. Both Hadoop 1 and 2 are supported. This class gets metadata about the index and storage backend (e.g. the Cassandra partitioner) from the <code>JanusGraph</code> instance given to its constructor. <pre><code>graph = JanusGraphFactory.open(...)\nmgmt = graph.openManagement()\nmr = new MapReduceIndexManagement(graph)\nmr.updateIndex(mgmt.getRelationIndex(mgmt.getRelationType(\"battled\"), \"battlesByTime\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p>"},{"location":"advanced-topics/index-admin/#reindex-example-on-mapreduce","title":"Reindex Example on MapReduce","text":"<p>The following Gremlin snippet outlines all steps of the MapReduce reindex process in one self-contained example using minimal dummy data against the Cassandra storage backend. <pre><code>// Open a graph\ngraph = JanusGraphFactory.open(\"conf/janusgraph-cql-es.properties\")\ng = graph.traversal()\n\n// Define a property\nmgmt = graph.openManagement()\ndesc = mgmt.makePropertyKey(\"desc\").dataType(String.class).make()\nmgmt.commit()\n\n// Insert some data\ngraph.addVertex(\"desc\", \"foo bar\")\ngraph.addVertex(\"desc\", \"foo baz\")\ngraph.tx().commit()\n\n// Run a query -- note the planner warning recommending the use of an index\ng.V().has(\"desc\", containsText(\"baz\"))\n\n// Create an index\nmgmt = graph.openManagement()\n\ndesc = mgmt.getPropertyKey(\"desc\")\nmixedIndex = mgmt.buildIndex(\"mixedExample\", Vertex.class).addKey(desc).buildMixedIndex(\"search\")\nmgmt.commit()\n\n// Rollback or commit transactions on the graph which predate the index definition\ngraph.tx().rollback()\n\n// Block until the SchemaStatus transitions from INSTALLED to REGISTERED\nreport = ManagementSystem.awaitGraphIndexStatus(graph, \"mixedExample\").call()\n\n// Run a JanusGraph-Hadoop job to reindex\nmgmt = graph.openManagement()\nmr = new MapReduceIndexManagement(graph)\nmr.updateIndex(mgmt.getGraphIndex(\"mixedExample\"), SchemaAction.REINDEX).get()\n\n// Enable the index\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"mixedExample\"), SchemaAction.ENABLE_INDEX).get()\nmgmt.commit()\n\n// Block until the SchemaStatus is ENABLED\nmgmt = graph.openManagement()\nreport = ManagementSystem.awaitGraphIndexStatus(graph, \"mixedExample\").status(SchemaStatus.ENABLED).call()\nmgmt.rollback()\n\n// Run a query -- JanusGraph will use the new index, no planner warning\ng.V().has(\"desc\", containsText(\"baz\"))\n\n// Concerned that JanusGraph could have read cache in that last query, instead of relying on the index?\n// Start a new instance to rule out cache hits.  Now we're definitely using the index.\ngraph.close()\ngraph = JanusGraphFactory.open(\"conf/janusgraph-cql-es.properties\")\ng.V().has(\"desc\", containsText(\"baz\"))\n</code></pre></p>"},{"location":"advanced-topics/index-admin/#executing-a-reindex-job-on-janusgraphmanagement","title":"Executing a Reindex job on JanusGraphManagement","text":"<p>To run a reindex job on JanusGraphManagement, invoke <code>JanusGraphManagement.updateIndex</code> with the <code>SchemaAction.REINDEX</code> argument. For example: <pre><code>m = graph.openManagement()\ni = m.getGraphIndex('indexName')\nm.updateIndex(i, SchemaAction.REINDEX).get()\nm.commit()\n</code></pre></p>"},{"location":"advanced-topics/index-admin/#example-for-janusgraphmanagement","title":"Example for JanusGraphManagement","text":"<p>The following loads some sample data into a BerkeleyDB-backed JanusGraph database, defines an index after the fact, reindexes using JanusGraphManagement, and finally enables and uses the index: <pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load some data from a file without any predefined schema\ngraph = JanusGraphFactory.open('conf/janusgraph-berkeleyje.properties')\ng = graph.traversal()\nm = graph.openManagement()\nm.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.LIST).make()\nm.makePropertyKey('lang').dataType(String.class).cardinality(Cardinality.LIST).make()\nm.makePropertyKey('age').dataType(Integer.class).cardinality(Cardinality.LIST).make()\nm.commit()\ngraph.io(IoCore.gryo()).readGraph('data/tinkerpop-modern.gio')\ngraph.tx().commit()\n\n// Run a query -- note the planner warning recommending the use of an index\ng.V().has('name', 'lop')\ngraph.tx().rollback()\n\n// Create an index\nm = graph.openManagement()\nm.buildIndex('names', Vertex.class).addKey(m.getPropertyKey('name')).buildCompositeIndex()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from INSTALLED to REGISTERED\nManagementSystem.awaitGraphIndexStatus(graph, 'names').status(SchemaStatus.REGISTERED).call()\n\n// Reindex using JanusGraphManagement\nm = graph.openManagement()\ni = m.getGraphIndex('names')\nm.updateIndex(i, SchemaAction.REINDEX)\nm.commit()\n\n// Enable the index\nManagementSystem.awaitGraphIndexStatus(graph, 'names').status(SchemaStatus.ENABLED).call()\n\n// Run a query -- JanusGraph will use the new index, no planner warning\ng.V().has('name', 'lop')\ngraph.tx().rollback()\n\n// Concerned that JanusGraph could have read cache in that last query, instead of relying on the index?\n// Start a new instance to rule out cache hits.  Now we're definitely using the index.\ngraph.close()\ngraph = JanusGraphFactory.open(\"conf/janusgraph-berkeleyje.properties\")\ng = graph.traversal()\ng.V().has('name', 'lop')\n</code></pre></p>"},{"location":"advanced-topics/index-admin/#index-removal","title":"Index Removal","text":"<p>Warning</p> <p>Index removal is a manual process comprised of multiple steps. These steps must be carefully followed in the right order to avoid index inconsistencies.</p>"},{"location":"advanced-topics/index-admin/#overview_1","title":"Overview","text":"<p>Index removal is a two-stage process. In the first stage, one JanusGraph signals to all others via the storage backend that the index is slated for deletion. This changes the index\u2019s state to <code>DISABLED</code>. At that point, JanusGraph stops using the index to answer queries and stops incrementally updating the index. Index-related data in the storage backend remains present but ignored.</p> <p>The second stage depends on whether the index is mixed or composite. A composite index can be deleted via JanusGraph. As with reindexing, removal can be done through either MapReduce or JanusGraphManagement. However, a mixed index must be manually dropped in the index backend; JanusGraph does not provide an automated mechanism to delete an index from its index backend.</p> <p>Index removal deletes everything associated with the index except its schema definition and its <code>DISABLED</code> state. This schema stub for the index remains even after deletion, though its storage footprint is negligible and fixed.</p>"},{"location":"advanced-topics/index-admin/#preparing-for-index-removal","title":"Preparing for Index Removal","text":"<p>If the index is currently enabled, it should first be disabled. This is done through the <code>ManagementSystem</code>. <pre><code>mgmt = graph.openManagement()\nrindex = mgmt.getRelationIndex(mgmt.getRelationType(\"battled\"), \"battlesByTime\")\nmgmt.updateIndex(rindex, SchemaAction.DISABLE_INDEX).get()\ngindex = mgmt.getGraphIndex(\"byName\")\nmgmt.updateIndex(gindex, SchemaAction.DISABLE_INDEX).get()\nmgmt.commit()\n</code></pre></p> <p>Once the status of all keys on the index changes to <code>DISABLED</code>, the index is ready to be removed. A utility in ManagementSystem can automate the wait-for-<code>DISABLED</code> step: <pre><code>ManagementSystem.awaitGraphIndexStatus(graph, 'byName').status(SchemaStatus.DISABLED).call()\n</code></pre></p> <p>After a composite index is <code>DISABLED</code>, there is a choice between two execution frameworks for its removal:</p> <ul> <li>MapReduce</li> <li>JanusGraphManagement</li> </ul> <p>Index removal on MapReduce supports large, horizontally-distributed databases. Index removal on JanusGraphManagement spawns a single-machine OLAP job. This is intended for convenience and speed on those databases small enough to be handled by one machine.</p> <p>Index removal requires:</p> <ul> <li>The index name (a string\u2009\u2014\u2009the user provides this to JanusGraph when     building a new index)</li> <li>The index type (a string\u2009\u2014\u2009the name of the edge label or property     key on which the vertex-centric index is built). This applies only     to vertex-centric indexes - leave blank for global graph indexes.</li> </ul> <p>As noted in the overview, a mixed index must be manually dropped from the indexing backend. Neither the MapReduce framework nor the JanusGraphManagement framework will delete a mixed backend from the indexing backend.</p>"},{"location":"advanced-topics/index-admin/#executing-an-index-removal-job-on-mapreduce","title":"Executing an Index Removal Job on MapReduce","text":"<p>As with reindexing, the recommended way to generate and run an index removal job on MapReduce is through the <code>MapReduceIndexManagement</code> class. Here is a rough outline of the steps to run an index removal job using this class:</p> <ul> <li>Open a <code>JanusGraph</code> instance</li> <li>If the index has not yet been disabled, disable it through <code>JanusGraphManagement</code></li> <li>Pass the graph instance into <code>MapReduceIndexManagement</code>'s constructor</li> <li>Call <code>updateIndex(&lt;index&gt;, SchemaAction.REMOVE_INDEX)</code></li> </ul> <p>A commented code example follows in the next subsection.</p>"},{"location":"advanced-topics/index-admin/#example-for-mapreduce","title":"Example for MapReduce","text":"<pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load the \"Graph of the Gods\" sample data\ngraph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\ng = graph.traversal()\nGraphOfTheGodsFactory.load(graph)\n\ng.V().has('name', 'jupiter')\n\n// Disable the \"name\" composite index\nm = graph.openManagement()\nnameIndex = m.getGraphIndex('name')\nm.updateIndex(nameIndex, SchemaAction.DISABLE_INDEX).get()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from INSTALLED to REGISTERED\nManagementSystem.awaitGraphIndexStatus(graph, 'name').status(SchemaStatus.DISABLED).call()\n\n// Delete the index using MapReduceIndexJobs\nm = graph.openManagement()\nmr = new MapReduceIndexManagement(graph)\nfuture = mr.updateIndex(m.getGraphIndex('name'), SchemaAction.REMOVE_INDEX)\nm.commit()\ngraph.tx().commit()\nfuture.get()\n\n// Index still shows up in management interface as DISABLED -- this is normal\nm = graph.openManagement()\nidx = m.getGraphIndex('name')\nidx.getIndexStatus(m.getPropertyKey('name'))\nm.rollback()\n\n// JanusGraph should issue a warning about this query requiring a full scan\ng.V().has('name', 'jupiter')\n</code></pre>"},{"location":"advanced-topics/index-admin/#executing-an-index-removal-job-on-janusgraphmanagement","title":"Executing an Index Removal job on JanusGraphManagement","text":"<p>To run an index removal job on JanusGraphManagement, invoke <code>JanusGraphManagement.updateIndex</code> with the <code>SchemaAction.REMOVE_INDEX</code> argument. For example: <pre><code>m = graph.openManagement()\ni = m.getGraphIndex('indexName')\nm.updateIndex(i, SchemaAction.REMOVE_INDEX).get()\nm.commit()\n</code></pre></p>"},{"location":"advanced-topics/index-admin/#example-for-janusgraphmanagement_1","title":"Example for JanusGraphManagement","text":"<p>The following loads some indexed sample data into a BerkeleyDB-backed JanusGraph database, then disables and removes the index through JanusGraphManagement: <pre><code>import org.janusgraph.graphdb.database.management.ManagementSystem\n\n// Load the \"Graph of the Gods\" sample data\ngraph = JanusGraphFactory.open('conf/janusgraph-cql-es.properties')\ng = graph.traversal()\nGraphOfTheGodsFactory.load(graph)\n\ng.V().has('name', 'jupiter')\n\n// Disable the \"name\" composite index\nm = graph.openManagement()\nnameIndex = m.getGraphIndex('name')\nm.updateIndex(nameIndex, SchemaAction.DISABLE_INDEX).get()\nm.commit()\ngraph.tx().commit()\n\n// Block until the SchemaStatus transitions from INSTALLED to REGISTERED\nManagementSystem.awaitGraphIndexStatus(graph, 'name').status(SchemaStatus.DISABLED).call()\n\n// Delete the index using JanusGraphManagement\nm = graph.openManagement()\nnameIndex = m.getGraphIndex('name')\nfuture = m.updateIndex(nameIndex, SchemaAction.REMOVE_INDEX)\nm.commit()\ngraph.tx().commit()\n\nfuture.get()\n\nm = graph.openManagement()\nnameIndex = m.getGraphIndex('name')\n\ng.V().has('name', 'jupiter')\n</code></pre></p>"},{"location":"advanced-topics/index-admin/#common-problems-with-index-management","title":"Common Problems with Index Management","text":""},{"location":"advanced-topics/index-admin/#illegalargumentexception-when-starting-job","title":"IllegalArgumentException when starting job","text":"<p>When a reindexing job is started shortly after a the index has been built, the job might fail with an exception like one of the following:</p> <pre><code>The index mixedExample is in an invalid state and cannot be indexed.\nThe following index keys have invalid status: desc has status INSTALLED\n(status must be one of [REGISTERED, ENABLED])\n\nThe index mixedExample is in an invalid state and cannot be indexed.\nThe index has status INSTALLED, but one of [REGISTERED, ENABLED] is required\n</code></pre> <p>When an index is built, its existence is broadcast to all other JanusGraph instances in the cluster. Those must acknowledge the existence of the index before the reindexing process can be started. The acknowledgments can take a while to come in depending on the size of the cluster and the connection speed. Hence, one should wait a few minutes after building the index and before starting the reindex process.</p> <p>Note, that the acknowledgment might fail due to JanusGraph instance failure. In other words, the cluster might wait indefinitely on the acknowledgment of a failed instance. In this case, the user must manually remove the failed instance from the cluster registry as described in Failure &amp; Recovery. After the cluster state has been restored, the acknowledgment process must be reinitiated by manually registering the index again in the management system.</p> <pre><code>mgmt = graph.openManagement()\nrindex = mgmt.getRelationIndex(mgmt.getRelationType(\"battled\"),\"battlesByTime\")\nmgmt.updateIndex(rindex, SchemaAction.REGISTER_INDEX).get()\ngindex = mgmt.getGraphIndex(\"byName\")\nmgmt.updateIndex(gindex, SchemaAction.REGISTER_INDEX).get()\nmgmt.commit()\n</code></pre> <p>After waiting a few minutes for the acknowledgment to arrive the reindex job should start successfully.</p>"},{"location":"advanced-topics/index-admin/#could-not-find-index","title":"Could not find index","text":"<p>This exception in the reindexing job indicates that an index with the given name does not exist or that the name has not been specified correctly. When reindexing a global graph index, only the name of the index as defined when building the index should be specified. When reindexing a global graph index, the name of the index must be given in addition to the name of the edge label or property key on which the vertex-centric index is defined.</p>"},{"location":"advanced-topics/index-admin/#cassandra-mappers-fail-with-too-many-open-files","title":"Cassandra Mappers Fail with \"Too many open files\"","text":"<p>The end of the exception stacktrace may look like this: <pre><code>java.net.SocketException: Too many open files\n    at java.net.Socket.createImpl(Socket.java:447)\n    at java.net.Socket.getImpl(Socket.java:510)\n    at java.net.Socket.setSoLinger(Socket.java:988)\n    at org.apache.thrift.transport.TSocket.initSocket(TSocket.java:118)\n    at org.apache.thrift.transport.TSocket.&lt;init&gt;(TSocket.java:109)\n</code></pre></p> <p>When running Cassandra with virtual nodes enabled, the number of virtual nodes seems to set a floor under the number of mappers. Cassandra may generate more mappers than virtual nodes for clusters with lots of data, but it seems to generate at least as many mappers as there are virtual nodes even though the cluster might be empty or close to empty. The default is 256 as of this writing.</p> <p>Each mapper opens and quickly closes several sockets to Cassandra. The kernel on the client side of those closed sockets goes into asynchronous TIME_WAIT, since Thrift uses SO_LINGER. Only a small number of sockets are open at any one time\u2009\u2014\u2009usually low single digits\u2009\u2014\u2009but potentially many lingering sockets can accumulate in TIME_WAIT. This accumulation is most pronounced when running a reindex job locally (not on a distributed MapReduce cluster), since all of those client-side TIME_WAIT sockets are lingering on a single client machine instead of being spread out across many machines in a cluster. Combined with the floor of 256 mappers, a reindex job can open thousands of sockets of the course of its execution. When these sockets all linger in TIME_WAIT on the same client, they have the potential to reach the open-files ulimit, which also controls the number of open sockets. The open-files ulimit is often set to 1024.</p> <p>Here are a few suggestions for dealing with the \"Too many open files\" problem during reindexing on a single machine:</p> <ul> <li> <p>Reduce the maximum size of the Cassandra connection pool. For     example, consider setting the cassandrathrift storage backend\u2019s     <code>max-active</code> and <code>max-idle</code> options to 1 each, and setting     <code>max-total</code> to -1. See Configuration Reference for full listings of     connection pool settings on the Cassandra storage backends.</p> </li> <li> <p>Increase the <code>nofile</code> ulimit. The ideal value depends on the size of     the Cassandra dataset and the throughput of the reindex mappers; if     starting at 1024, try an order of magnitude larger: 10000. This is     just necessary to sustain lingering TIME_WAIT sockets. The reindex     job won\u2019t try to open nearly that many sockets at once.</p> </li> <li> <p>Run the reindex task on a multi-node MapReduce cluster to spread out     the socket load.</p> </li> </ul>"},{"location":"advanced-topics/janusgraph-bus/","title":"JanusGraph Bus","text":"<p>The JanusGraph Bus describes a collection of configurable logs to which JanusGraph writes changes to the graph and its management. The JanusGraph Bus is used for internal (i.e. between multiple JanusGraph instances) and external (i.e. integration with other systems) communication.</p> <p>In particular, JanusGraph maintains three separate logs:</p>"},{"location":"advanced-topics/janusgraph-bus/#trigger-log","title":"Trigger Log","text":"<p>The purpose of the trigger log is to capture the mutations of a transaction so that the resulting changes to the graph can trigger events in other system. Such events may be propagating the change to other data stores, view maintenance, or aggregate computation.</p> <p>The trigger log consists of multiple sub-logs as configured by the user. When opening a transaction, the identifier for the trigger sub-log can be specified: <pre><code>tx = g.buildTransaction().logIdentifier(\"purchase\").start();\n</code></pre></p> <p>In this case, the identifier is \"purchase\" which means that the mutations of this transaction will be written to a log with the name \"trigger_purchase\". This gives the user control over where transactional mutations are logged. If no trigger log is specified, no trigger log entry will be created.</p>"},{"location":"advanced-topics/janusgraph-bus/#transaction-log","title":"Transaction Log","text":"<p>The transaction log is maintained by JanusGraph and contains two entries for each transaction if enabled: 1. Pre-Commit: Before the changes are persisted to the storage and indexing backends, the changes are compiled and written to the log. 2. Post-Commit: The success status of the transaction is written to the log.</p> <p>In this way, the transaction log functions as a Write-Ahead-Log (WAL). This log is not meant for consumption by the user or external systems - use trigger logs for that. It is used internally to store partial transaction persistence against eventually consistent backends.</p> <p>The transaction log can be enabled via the root-level configuration option \"log-tx\".</p>"},{"location":"advanced-topics/janusgraph-bus/#management-log","title":"Management Log","text":"<p>The management log is maintained by JanusGraph internally to communicate and persist all changes to global configuration options or the graph schema.</p>"},{"location":"advanced-topics/migrating/","title":"Migrating from Titan","text":"<p>This page describes some of the Configuration options that JanusGraph provides to allow migration of data from a data store which had previously been created by Titan. </p>"},{"location":"advanced-topics/migrating/#configuration","title":"Configuration","text":"<p>When connecting to an existing Titan data store the <code>graph.titan-version</code> property should already be set in the global configuration to Titan version <code>1.0.0</code>. The ID store name in JanusGraph is configurable via the <code>ids.store-name</code> property whereas in Titan it was a constant. If the <code>graph.titan-version</code> has been set in the existing global configuration, then you do not need to explicitly set the ID store as it will default to <code>titan_ids</code>.</p>"},{"location":"advanced-topics/migrating/#cassandra","title":"Cassandra","text":"<p>The default keyspace used by Titan was <code>titan</code> and in order to reuse that existing keyspace the <code>storage.cassandra.keyspace</code> property needs to be set accordingly. <pre><code>storage.cassandra.keyspace=titan\n</code></pre></p> <p>These configuration options allow JanusGraph to read data from a Cassandra database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in Casssandra before attempting to use it with the JanusGraph release.</p>"},{"location":"advanced-topics/migrating/#hbase","title":"HBase","text":"<p>The name of the table used by Titan was <code>titan</code> and in order to reuse that existing table the <code>storage.hbase.table</code> property needs to be set accordingly. <pre><code>storage.hbase.table=titan\n</code></pre></p> <p>These configuration options allow JanusGraph to read data from an HBase database which had previously been created by Titan. However, once JanusGraph writes back to that database it will register additional serializers which mean that it will no longer be compatible with Titan. Users are therefore encouraged to backup the data in HBase before attempting to use it with the JanusGraph release.</p>"},{"location":"advanced-topics/migrating/#berkeleydb","title":"BerkeleyDB","text":"<p>The BerkeleyDB version has been updated, and it contains changes to the file format stored on disk. This file format change is forward compatible with previous versions of BerkeleyDB, so existing graph data stored with Titan can be read in. However, once the data has been read in with the newer version of BerkeleyDB, those files can no longer be read by the older version. Users are encouraged to backup the BerkeleyDB storage directory before attempting to use it with the JanusGraph release.</p>"},{"location":"advanced-topics/monitoring/","title":"Monitoring JanusGraph","text":""},{"location":"advanced-topics/monitoring/#metrics-in-janusgraph","title":"Metrics in JanusGraph","text":"<p>JanusGraph supports Metrics. JanusGraph can measure the following:</p> <ul> <li> <p>The number of transactions begun, committed, and rolled back</p> </li> <li> <p>The number of attempts and failures of each storage backend     operation type</p> </li> <li> <p>The response time distribution of each storage backend operation     type</p> </li> </ul>"},{"location":"advanced-topics/monitoring/#configuring-metrics-collection","title":"Configuring Metrics Collection","text":"<p>To enable Metrics collection, set the following in JanusGraph\u2019s properties file: <pre><code># Required to enable Metrics in JanusGraph\nmetrics.enabled = true\n</code></pre></p> <p>This setting makes JanusGraph record measurements at runtime using Metrics classes like Timer, Counter, Histogram, etc. To access these measurements, one or more Metrics reporters must be configured as described in the section Configuring Metrics Reporting.</p>"},{"location":"advanced-topics/monitoring/#customizing-the-default-metric-names","title":"Customizing the Default Metric Names","text":"<p>JanusGraph prefixes all metric names with \"org.janusgraph\" by default. This prefix can be set through the <code>metrics.prefix</code> configuration property. For example, to shorten the default \"org.janusgraph\" prefix to just \"janusgraph\": <pre><code># Optional\nmetrics.prefix = janusgraph\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#transaction-specific-metrics-names","title":"Transaction-Specific Metrics Names","text":"<p>Each JanusGraph transaction may optionally specify its own Metrics name prefix, overriding both the default Metrics name prefix and the <code>metrics.prefix</code> configuration property. For example, the prefix could be changed to the name of the frontend application that opened the JanusGraph transaction. Note that Metrics maintains a ConcurrentHashMap of metric names and their associated objects in memory, so it\u2019s probably a good idea to keep the number of distinct metric prefixes small.</p> <p>To do this, call <code>TransactionBuilder.setMetricsPrefix(String)</code>: <pre><code>JanusGraph graph = ...;\nTransactionBuilder tbuilder = graph.buildTransaction();\nJanusGraphTransaction tx = tbuilder.groupName(\"foobar\").start();\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#separating-metrics-by-backend-store","title":"Separating Metrics by Backend Store","text":"<p>JanusGraph combines the Metrics for its various internal storage backend handles by default. All Metrics for storage backend interactions follow the pattern \"&lt;prefix&gt;.stores.&lt;opname&gt;\", regardless of whether they come from the ID store, edge store, etc. When <code>metrics.merge-basic-metrics = false</code> is set in JanusGraph\u2019s properties file, the \"stores\" string in metric names is replaced by \"idStore\", \"edgeStore\", \"vertexIndexStore\", or \"edgeIndexStore\".</p>"},{"location":"advanced-topics/monitoring/#configuring-metrics-reporting","title":"Configuring Metrics Reporting","text":"<p>JanusGraph supports the following Metrics reporters:</p> <ul> <li>Console</li> <li>CSV</li> <li>Ganglia</li> <li>Graphite</li> <li>JMX</li> <li>Slf4j</li> <li>User-provided/Custom</li> </ul> <p>Each reporter type is independent of and can coexist with the others. For example, it\u2019s possible to configure Ganglia, JMX, and Slf4j Metrics reporters to operate simultaneously. Just set all their respective configuration keys in janusgraph.properties (and enable metrics as directed above).</p>"},{"location":"advanced-topics/monitoring/#console-reporter","title":"Console Reporter","text":"Metrics Console Reporter Configuration Options Config Key Required? Value Default metrics.console.interval yes Milliseconds to wait between dumping metrics to the console null <p>Example janusgraph.properties snippet that prints metrics to the console once a minute: <pre><code>metrics.enabled = true\n# Required; specify logging interval in milliseconds\nmetrics.console.interval = 60000\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#csv-file-reporter","title":"CSV File Reporter","text":"Metrics CSV Reporter Configuration Options Config Key Required? Value Default metrics.csv.interval yes Milliseconds to wait between writing CSV lines null metrics.csv.directory yes Directory in which CSV files are written (will be created if it does not exist) null <p>Example janusgraph.properties snippet that writes CSV files once a minute to the directory <code>./foo/bar/</code> (relative to the process\u2019s working directory): <pre><code>metrics.enabled = true\n# Required; specify logging interval in milliseconds\nmetrics.csv.interval = 60000\nmetrics.csv.directory = foo/bar\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#ganglia-reporter","title":"Ganglia Reporter","text":"<p>Note</p> <p>Configuration of Ganglia requires an additional library that is not packaged with JanusGraph due to its LGPL licensing that conflicts with the JanusGraph\u2019s Apache 2.0 License. To run with Ganglia monitoring, download the <code>org.acplt:oncrpc</code> jar from here and copy it to the JanusGraph <code>/lib</code> directory before starting the server.</p> Metrics Ganglia Reporter Configuration Options Config Key Required? Value Default metrics.ganglia.hostname yes Unicast host or multicast group to which our Metrics are sent null metrics.ganglia.interval yes Milliseconds to wait between sending datagrams null metrics.ganglia.port no UDP port to which we send Metrics datagrams 8649 metrics.ganglia.addressing-mode no Must be \"unicast\" or \"multicast\" unicast metrics.ganglia.ttl no Multicast datagram TTL; ignore for unicast 1 metrics.ganglia.protocol-31 no Boolean; true to use Ganglia protocol 3.1, false to use 3.0 true metrics.ganglia.uuid no Host UUID to report instead of IP:hostname null metrics.ganglia.spoof no Override IP:hostname reported to Ganglia null <p>Example janusgraph.properties snippet that sends unicast UDP datagrams to localhost on the default port once every 30 seconds: <pre><code>metrics.enabled = true\n# Required; IP or hostname string\nmetrics.ganglia.hostname = 127.0.0.1\n# Required; specify logging interval in milliseconds\nmetrics.ganglia.interval = 30000\n</code></pre></p> <p>Example janusgraph.properties snippet that sends unicast UDP datagrams to a non-default destination port and which also spoofs the IP and hostname reported to Ganglia: <pre><code>metrics.enabled = true\n# Required; IP or hostname string\nmetrics.ganglia.hostname = 1.2.3.4\n# Required; specify logging interval in milliseconds\nmetrics.ganglia.interval = 60000\n# Optional\nmetrics.ganglia.port = 6789\nmetrics.ganglia.spoof = 10.0.0.1:zombo.com\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#graphite-reporter","title":"Graphite Reporter","text":"Metrics Graphite Reporter Configuration Options Config Key Required? Value Default metrics.graphite.hostname yes IP address or hostname to which Graphite plaintext protocol data are sent null metrics.graphite.interval yes Milliseconds to wait between pushing data to Graphite null metrics.graphite.port no Port to which Graphite plaintext protocol reports are sent 2003 metrics.graphite.prefix no Arbitrary string prepended to all metric names sent to Graphite null <p>Example janusgraph.properties snippet that sends metrics to a Graphite server on 192.168.0.1 every minute: <pre><code>metrics.enabled = true\n# Required; IP or hostname string\nmetrics.graphite.hostname = 192.168.0.1\n# Required; specify logging interval in milliseconds\nmetrics.graphite.interval = 60000\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#jmx-reporter","title":"JMX Reporter","text":"Metrics JMX Reporter Configuration Options Config Key Required? Value Default metrics.jmx.enabled yes Boolean false metrics.jmx.domain no Metrics will appear in this JMX domain Metrics\u2019s own default metrics.jmx.agentid no Metrics will be reported with this JMX agent ID Metrics\u2019s own default <p>Example janusgraph.properties snippet: <pre><code>metrics.enabled = true\n# Required\nmetrics.jmx.enabled = true\n# Optional; if omitted, then Metrics uses its default values\nmetrics.jmx.domain = foo\nmetrics.jmx.agentid = baz\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#slf4j-reporter","title":"Slf4j Reporter","text":"Metrics Slf4j Reporter Configuration Options Config Key Required? Value Default metrics.slf4j.interval yes Milliseconds to wait between dumping metrics to the logger null metrics.slf4j.logger no Slf4j logger name to use \"metrics\" <p>Example janusgraph.properties snippet that logs metrics once a minute to the logger named <code>foo</code>: <pre><code>metrics.enabled = true\n# Required; specify logging interval in milliseconds\nmetrics.slf4j.interval = 60000\n# Optional; uses Metrics default when unset\nmetrics.slf4j.logger = foo\n</code></pre></p>"},{"location":"advanced-topics/monitoring/#user-providedcustom-reporter","title":"User-Provided/Custom Reporter","text":"<p>In case the Metrics reporter configuration options listed above are insufficient, JanusGraph provides a utility method to access the single <code>MetricRegistry</code> instance which holds all of its measurements. <pre><code>com.codahale.metrics.MetricRegistry janusgraphRegistry = \n    org.janusgraph.util.stats.MetricManager.INSTANCE.getRegistry();\n</code></pre></p> <p>Code that accesses <code>janusgraphRegistry</code> this way can then attach non-standard reporter types or standard reporter types with exotic configurations to <code>janusgraphRegistry</code>. This approach is also useful if the surrounding application already has a framework for Metrics reporter configuration, or if the application needs multiple differently-configured instances of one of JanusGraph\u2019s supported reporter types. For instance, one could use this approach to setup multiple unicast Graphite reporters whereas JanusGraph\u2019s properties configuration is limited to just one Graphite reporter.</p>"},{"location":"advanced-topics/partitioning/","title":"Graph Partitioning","text":"<p>When JanusGraph is deployed on a cluster of multiple storage backend instances, the graph is partitioned across those machines. Since JanusGraph stores the graph in an adjacency list representation the assignment of vertices to machines determines the partitioning. By default, JanusGraph uses a random partitioning strategy that randomly assigns vertices to machines. Random partitioning is very efficient, requires no configuration, and results in balanced partitions. However, random partitioning results in less efficient query processing as the JanusGraph cluster grows to accommodate more graph data because of the increasing cross-instance communication required to retrieve the query's result set. Explicit graph partitioning can ensure that strongly connected and frequently traversed subgraphs are stored on the same instance thereby reducing the communication overhead significantly.</p> <p>To enable explicit graph partitioning in JanusGraph, the following configuration options must be set when the JanusGraph cluster is initialized.</p> <pre><code>cluster.partition = true\ncluster.max-partitions = 32\nids.flush = false\n</code></pre> <p>The configuration option <code>max-partitions</code> controls how many virtual partitions JanusGraph creates. This number should be roughly twice the number of storage backend instances. If the cluster of storage backend instances is expected to grow, estimate the size of the cluster in the foreseeable future and take this number as the baseline. Setting this number too large will unnecessarily fragment the cluster which can lead to poor performance.</p> <p>Because explicit graph partitioning controls the assignment of vertices to storage instances it cannot be enabled once a JanusGraph cluster is initialized. Likewise, the number of virtual partitions cannot be changed without reloading the graph.</p> <p>Explicit graph partitioning can only enabled against storage backends that support ordered key storage:</p> <ul> <li>HBase: Always supports explicit graph partitioning</li> <li>Cassandra: Must be configured to use ByteOrderedPartitioner in order to support explicit graph partitioning</li> </ul> <p>There are two aspects to graph partitioning which can be individually controlled: edge cuts and vertex cuts.</p>"},{"location":"advanced-topics/partitioning/#edge-cut","title":"Edge Cut","text":"<p>In assigning vertices to partitions one strives to optimize the assignment such that frequently co-traversed vertices are hosted on the same machine. Assume vertex A is assigned to machine 1 and vertex B is assigned to machine 2. An edge between the vertices is called a cut edge because its end points are hosted on separate machines. Traversing this edge as part of a graph query requires communication between the machines which slows down query processing. Hence, it is desirable to reduce the edge cut for frequently traversed edges. That, in turn, requires placing the adjacent vertices of frequently traversed edges in the same partition.</p> <p>Vertices are placed in a partition by way of the assigned vertex id. A partition is essentially a sequential range of vertex ids. To place a vertex in a particular partition, JanusGraph chooses an id from the partition\u2019s range of vertex ids. JanusGraph controls the vertex-to-partition assignment through the configured placement strategy. By default, vertices created in the same transaction are assigned to the same partition. This strategy is easy to reason about and works well in situations where frequently co-traversed vertices are created in the same transaction - either by optimizing the loading strategy to that effect or because vertices are naturally added to the graph that way. However, the strategy is limited, leads to imbalanced partitions when data is loaded in large transactions and not the optimal strategy for many use cases. The user can provide a use case specific vertex placement strategy by implementing the <code>IDPlacementStrategy</code> interface and registering it in the configuration through the <code>ids.placement</code> option.</p> <p>When implementing <code>IDPlacementStrategy</code>, note that partitions are identified by an integer id in the range from 0 to the number of configured virtual partitions minus 1. For our example configuration, there are partitions 0, 1, 2, 3, ..31. Partition ids are not the same as vertex ids. Edge cuts are more meaningful when the JanusGraph servers are on the same hosts as the storage backend. If you have to make a network call to a different host on each hop of a traversal, the benefit of edge cuts and custom placement strategies can be largely nullified.</p>"},{"location":"advanced-topics/partitioning/#vertex-cut","title":"Vertex Cut","text":"<p>While edge cut optimization aims to reduce the cross communication and thereby improve query execution, vertex cuts address the hotspot issue caused by vertices with a large number of incident edges. While vertex-centric indexes effectively address query performance for large degree vertices, vertex cuts are needed to address the hot spot issue on very large graphs.</p> <p>Cutting a vertex means storing a subset of that vertex\u2019s adjacency list on each partition in the graph. In other words, the vertex and its adjacency list is partitioned thereby effectively distributing the load on that single vertex across all of the instances in the cluster and removing the hot spot.</p> <p>JanusGraph cuts vertices by label. A vertex label can be defined as partitioned which means that all vertices of that label will be partitioned across the cluster in the manner described above. <pre><code>mgmt = graph.openManagement()\nmgmt.makeVertexLabel('user').make()\nmgmt.makeVertexLabel('product').partition().make()\nmgmt.commit()\n</code></pre></p> <p>In the example above, <code>product</code> is defined as a partitioned vertex label whereas <code>user</code> is a normal label. This configuration is beneficial for situations where there are thousands of products but millions of users and one records transactions between users and products. In that case, the product vertices will have a very high degree and the popular products turns into hot spots if they are not partitioned.</p>"},{"location":"advanced-topics/partitioning/#graph-partitioning-faq","title":"Graph Partitioning FAQ","text":""},{"location":"advanced-topics/partitioning/#random-vs-explicit-partitioning","title":"Random vs. Explicit Partitioning","text":"<p>When the graph is small or accommodated by a few storage instances, it is best to use random partitioning for its simplicity. As a rule of thumb, one should strongly consider enabling explicit graph partitioning and configure a suitable partitioning heuristic when the graph grows into the 10s of billions of edges.</p>"},{"location":"advanced-topics/recovery/","title":"Failure &amp; Recovery","text":"<p>JanusGraph is a highly available and robust graph database. In large scale JanusGraph deployments failure is inevitable. This page describes some failure situations and how JanusGraph can handle them.</p>"},{"location":"advanced-topics/recovery/#transaction-failure","title":"Transaction Failure","text":"<p>Transactions can fail for a number of reasons. If the transaction fails before the commit the changes will be discarded and the application can retry the transaction in coherence with the business logic. Likewise, locking or other consistency failures will cause an exception prior to persistence and hence can be retried. The persistence stage of a transaction is when JanusGraph starts persisting data to the various backend systems.</p> <p>JanusGraph first persists all graph mutations to the storage backend. This persistence is executed as one batch mutation to ensure that the mutation is committed atomically for those backends supporting atomicity. If the batch mutation fails due to an exception in the storage backend, the entire transaction is failed.</p> <p>If the primary persistence into the storage backend succeeds but secondary persistence into the indexing backends or the logging system fail, the transaction is still considered to be successful because the storage backend is the authoritative source of the graph.</p> <p>However, this can create inconsistencies with the indexes and logs. To automatically repair such inconsistencies, JanusGraph can maintain a transaction write-ahead log which is enabled through the configuration. <pre><code>tx.log-tx = true\ntx.max-commit-time = 10000\n</code></pre></p> <p>The max-commit-time property is used to determine when a transaction has failed. If the persistence stage of the transaction takes longer than this time, JanusGraph will attempt to recover it if necessary. Hence, this time out should be configured as a generous upper bound on the maximum duration of persistence. Note, that this does not include the time spent before commit.</p> <p>In addition, a separate process must be setup that reads the log to identify partially failed transaction and repair any inconsistencies caused. It is suggested to run the transaction repair process on a separate machine connected to the cluster to isolate failures. Configure a separately controlled process to run the following where the start time specifies the time since epoch where the recovery process should start reading from the write-ahead log. <pre><code>recovery = JanusGraphFactory.startTransactionRecovery(graph, startTime, TimeUnit.MILLISECONDS);\n</code></pre></p> <p>Enabling the transaction write-ahead log causes an additional write operation for mutating transactions which increases the latency. Also note, that additional space is required to store the log. The transaction write-ahead log has a configurable time-to-live of 2 days which means that log entries expire after that time to keep the storage overhead small. Refer to Configuration Reference for a complete list of all log related configuration options to fine tune logging behavior.</p>"},{"location":"advanced-topics/recovery/#janusgraph-instance-failure","title":"JanusGraph Instance Failure","text":"<p>JanusGraph is robust against individual instance failure in that other instances of the JanusGraph cluster are not impacted by such failure and can continue processing transactions without loss of performance while the failed instance is restarted.</p> <p>However, some schema related operations - such as installing indexes - require the coordination of all JanusGraph instances. For this reason, JanusGraph maintains a record of all running instances. If an instance fails, i.e. is not properly shut down, JanusGraph considers it to be active and expects its participation in cluster-wide operations which subsequently fail because this instances did not participate in or did not acknowledge the operation.</p> <p>In this case, the user must manually remove the failed instance record from the cluster and then retry the operation. To remove the failed instance, open a management transaction against any of the running JanusGraph instances, inspect the list of running instances to identify the failed one, and finally remove it. <pre><code>mgmt = graph.openManagement()\nmgmt.getOpenInstances() //all open instances\n==&gt;7f0001016161-dunwich1(current)\n==&gt;7f0001016161-atlantis1\nmgmt.forceCloseInstance('7f0001016161-atlantis1') //remove an instance\nmgmt.commit()\n</code></pre></p> <p>The unique identifier of the current JanusGraph instance is marked with the suffix <code>(current)</code> so that it can be easily identified. This instance cannot be closed via the <code>forceCloseInstance</code> method and instead should be closed via <code>g.close()</code></p> <p>It must be ensured that the manually removed instance is indeed no longer active. Removing an active JanusGraph instance from a cluster can cause data inconsistencies. Hence, use this method with great care in particular when JanusGraph is operated in an environment where instances are automatically restarted.</p>"},{"location":"advanced-topics/serializer/","title":"Datatype and Attribute Serializer Configuration","text":"<p>JanusGraph supports a number of classes for attribute values on properties. JanusGraph efficiently serializes primitives, primitive arrays and <code>Geoshape</code>, <code>UUID</code>, <code>Date</code>, <code>ObjectNode</code> and <code>ArrayNode</code>. JanusGraph supports serializing arbitrary objects as attribute values, but these require custom serializers to be defined.</p> <p>To configure a custom attribute class with a custom serializer, follow these steps:</p> <ol> <li> <p>Implement a custom <code>AttributeSerializer</code> for the custom attribute     class</p> </li> <li> <p>Add the following configuration options where [X] is the custom     attribute id that must be larger than all attribute ids for already     configured custom attributes:</p> <ol> <li> <p><code>attributes.custom.attribute[X].attribute-class = [Full attribute class name]</code></p> </li> <li> <p><code>attributes.custom.attribute[X].serializer-class = [Full serializer class name]</code></p> </li> </ol> </li> </ol> <p>For example, suppose we want to register a special integer attribute class called <code>SpecialInt</code> and have implemented a custom serializer <code>SpecialIntSerializer</code> that implements <code>AttributeSerializer</code>. We already have 9 custom attributes configured in the configuration file, so we would add the following lines  <pre><code>attributes.custom.attribute10.attribute-class = com.example.SpecialInt\nattributes.custom.attribute10.serializer-class = com.example.SpecialIntSerializer\n</code></pre></p>"},{"location":"advanced-topics/serializer/#custom-object-serialization","title":"Custom Object Serialization","text":"<p>JanusGraph supports arbitrary objects as property attributes and can serialize such objects to disk. For this default serializer to work for a custom class, the following conditions must be fulfilled:</p> <ul> <li> <p>The class must implement AttributeSerializer</p> </li> <li> <p>The class must have a no-argument constructor</p> </li> <li> <p>The class must implement the <code>equals(Object)</code> method</p> </li> </ul> <p>The last requirement is needed because JanusGraph will test both serialization and deserialization of a custom class before persisting data to disk.</p>"},{"location":"basics/cache/","title":"JanusGraph Cache","text":""},{"location":"basics/cache/#caching","title":"Caching","text":"<p>JanusGraph employs multiple layers of data caching to facilitate fast graph traversals. The caching layers are listed here in the order they are accessed from within a JanusGraph transaction. The closer the cache is to the transaction, the faster the cache access and the higher the memory footprint and maintenance overhead.</p>"},{"location":"basics/cache/#transaction-level-caching","title":"Transaction-Level Caching","text":"<p>Within an open transaction, JanusGraph maintains two caches:</p> <ul> <li> <p>Vertex Cache: Caches accessed vertices and their adjacency list (or     subsets thereof) so that subsequent access is significantly faster     within the same transaction. Hence, this cache speeds up iterative     traversals.</p> </li> <li> <p>Index Cache: Caches the results for index queries so that subsequent     index calls can be served from memory instead of calling the index     backend and (usually) waiting for one or more network round trips.</p> </li> </ul> <p>The size of both of those is determined by the transaction cache size. The transaction cache size can be configured via <code>cache.tx-cache-size</code> or on a per transaction basis by opening a transaction via the transaction builder <code>graph.buildTransaction()</code> and using the <code>setVertexCacheSize(int)</code> method.</p>"},{"location":"basics/cache/#vertex-cache","title":"Vertex Cache","text":"<p>The vertex cache contains vertices and the subset of their adjacency list that has been retrieved in a particular transaction. The maximum number of vertices maintained in this cache is equal to the transaction cache size. If the transaction workload is an iterative traversal, the vertex cache will significantly speed it up. If the same vertex is not accessed again in the transaction, the transaction level cache will make no difference.</p> <p>Note, that the size of the vertex cache on heap is not only determined by the number of vertices it may hold but also by the size of their adjacency list. In other words, vertices with large adjacency lists (i.e. many incident edges) will consume more space in this cache than those with smaller lists.</p> <p>Furthermore note, that modified vertices are pinned in the cache, which means they cannot be evicted since that would entail loosing their changes. Therefore, transaction which contain a lot of modifications may end up with a larger than configured vertex cache.</p>"},{"location":"basics/cache/#index-cache","title":"Index Cache","text":"<p>The index cache contains the results of index queries executed in the context of this transaction. Subsequent identical index calls will be served from this cache and are therefore significantly cheaper. If the same index call never occurs twice in the same transaction, the index cache makes no difference.</p> <p>Each entry in the index cache is given a weight equal to <code>2 + result set size</code> and the total weight of the cache will not exceed half of the transaction cache size.</p>"},{"location":"basics/cache/#database-level-caching","title":"Database Level Caching","text":"<p>The database level cache retains adjacency lists (or subsets thereof) across multiple transactions and beyond the duration of a single transaction. The database level cache is shared by all transactions across a database. It is more space efficient than the transaction level caches but also slightly slower to access. In contrast to the transaction level caches, the database level caches do not expire immediately after closing a transaction. Hence, the database level cache significantly speeds up graph traversals for read heavy workloads across transactions.</p> <p>Configuration Reference lists all of the configuration options that pertain to JanusGraph\u2019s database level cache. This page attempts to explain their usage.</p> <p>Most importantly, the database level cache is disabled by default in the current release version of JanusGraph. To enable it, set <code>cache.db-cache=true</code>.</p>"},{"location":"basics/cache/#cache-expiration-time","title":"Cache Expiration Time","text":"<p>The most important setting for performance and query behavior is the cache expiration time which is configured via <code>cache.db-cache-time</code>. The cache will hold graph elements for at most that many milliseconds. If an element expires, the data will be re-read from the storage backend on the next access.</p> <p>If there is only one JanusGraph instance accessing the storage backend or if this instance is the only one modifying the graph, the cache expiration can be set to 0 which disables cache expiration. This allows the cache to hold elements indefinitely (unless they are evicted due to space constraints or on update) which provides the best cache performance. Since no other JanusGraph instance is modifying the graph, there is no danger of holding on to stale data.</p> <p>If there are multiple JanusGraph instances accessing the storage backend, the time should be set to the maximum time that can be allowed between another JanusGraph instance modifying the graph and this JanusGraph instance seeing the data. If any change should be immediately visible to all JanusGraph instances, the database level cache should be disabled in a distributed setup. However, for most applications it is acceptable that a particular JanusGraph instance sees remote modifications with some delay. The larger the maximally allowed delay, the better the cache performance. Note, that a given JanusGraph instance will always immediately see its own modifications to the graph irrespective of the configured cache expiration time.</p>"},{"location":"basics/cache/#cache-size","title":"Cache Size","text":"<p>The configuration option <code>cache.db-cache-size</code> controls how much heap space JanusGraph\u2019s database level cache is allowed to consume. The larger the cache, the more effective it will be. However, large cache sizes can lead to excessive GC and poor performance.</p> <p>The cache size can be configured as a percentage (expressed as a decimal between 0 and 1) of the total heap space available to the JVM running JanusGraph or as an absolute number of bytes.</p> <p>Note, that the cache size refers to the amount of heap space that is exclusively occupied by the cache. JanusGraph\u2019s other data structures and each open transaction will occupy additional heap space. If additional software layers are running in the same JVM, those may occupy a significant amount of heap space as well (e.g. Gremlin Server, embedded Cassandra, etc). Be conservative in your heap memory estimation. Configuring a cache that is too large can lead to out-of-memory exceptions and excessive GC.</p>"},{"location":"basics/cache/#clean-up-wait-time","title":"Clean Up Wait Time","text":"<p>When a vertex is locally modified (e.g. an edge is added) all of the vertex\u2019s related database level cache entries are marked as expired and eventually evicted. This will cause JanusGraph to refresh the vertex\u2019s data from the storage backend on the next access and re-populate the cache.</p> <p>However, when the storage backend is eventually consistent, the modifications that triggered the eviction may not yet be visible. By configuring <code>cache.db-cache-clean-wait</code>, the cache will wait for at least this many milliseconds before repopulating the cache with the entry retrieved from the storage backend.</p> <p>If JanusGraph runs locally or against a storage backend that guarantees immediate visibility of modifications, this value can be set to 0.</p>"},{"location":"basics/cache/#storage-backend-caching","title":"Storage Backend Caching","text":"<p>Each storage backend maintains its own data caching layer. These caches benefit from compression, data compactness, coordinated expiration and are often maintained off heap which means that large caches can be used without running into garbage collection issues. While these caches can be significantly larger than the database level cache, they are also slower to access.</p> <p>The exact type of caching and its properties depends on the particular storage backend. Please refer to the respective documentation for more information about the caching infrastructure and how to optimize it.</p>"},{"location":"basics/common-questions/","title":"Common Questions","text":""},{"location":"basics/common-questions/#accidental-type-creation","title":"Accidental type creation","text":"<p>By default, JanusGraph will automatically create property keys and edge labels when a new type is encountered. It is strongly encouraged that users explicitly schemata as documented in Schema and Data Modeling before loading any data and disable automatic type creation by setting the option <code>schema.default = none</code>.</p> <p>Automatic type creation can cause problems in multi-threaded or highly concurrent environments. Since JanusGraph needs to ensure that types are unique, multiple attempts at creating the same type will lead to locking or other exceptions. It is generally recommended to create all needed types up front or in one batch when new property keys and edge labels are needed.</p>"},{"location":"basics/common-questions/#custom-class-datatype","title":"Custom Class Datatype","text":"<p>JanusGraph supports arbitrary objects as attribute values on properties. To use a custom class as data type in JanusGraph, either register a custom serializer or ensure that the class has a no-argument constructor and implements the <code>equals</code> method because JanusGraph will verify that it can successfully de-/serialize objects of that class. Please see Datatype and Attribute Serializer Configuration for more information.</p>"},{"location":"basics/common-questions/#transactional-scope-for-edges","title":"Transactional Scope for Edges","text":"<p>Edges should not be accessed outside the scope in which they were originally created or retrieved.</p>"},{"location":"basics/common-questions/#locking-exceptions","title":"Locking Exceptions","text":"<p>When defining unique types with locking enabled (i.e. requesting that JanusGraph ensures uniqueness) it is likely to encounter locking exceptions of the type <code>PermanentLockingException</code> under concurrent modifications to the graph.</p> <p>Such exceptions are to be expected, since JanusGraph cannot know how to recover from a transactional state where an earlier read value has been modified by another transaction since this may invalidate the state of the transaction. In most cases it is sufficient to simply re-run the transaction. If locking exceptions are very frequent, try to analyze and remove the source of congestion.</p>"},{"location":"basics/common-questions/#ghost-vertices","title":"Ghost Vertices","text":"<p>When the same vertex is concurrently removed in one transaction and modified in another, both transactions will successfully commit on eventually consistent storage backends and the vertex will still exist with only the modified properties or edges. This is referred to as a ghost vertex. It is possible to guard against ghost vertices on eventually consistent backends using key uniqueness but this is prohibitively expensive in most cases. A more scalable approach is to allow ghost vertices temporarily and clearing them out in regular time intervals.</p> <p>Another option is to detect them at read-time using the option <code>checkInternalVertexExistence()</code> documented in Transaction Configuration.</p>"},{"location":"basics/common-questions/#debug-level-logging-slows-execution","title":"Debug-level Logging Slows Execution","text":"<p>When the log level is set to <code>DEBUG</code> JanusGraph produces a lot of logging output which is useful to understand how particular queries get compiled, optimized, and executed. However, the output is so large that it will impact the query performance noticeably. Hence, use <code>INFO</code> severity or higher for production systems or benchmarking.</p>"},{"location":"basics/common-questions/#janusgraph-outofmemoryexception-or-excessive-garbage-collection","title":"JanusGraph OutOfMemoryException or excessive Garbage Collection","text":"<p>If you experience memory issues or excessive garbage collection while running JanusGraph it is likely that the caches are configured incorrectly. If the caches are too large, the heap may fill up with cache entries. Try reducing the size of the transaction level cache before tuning the database level cache, in particular if you have many concurrent transactions. See JanusGraph Cache for more information.</p>"},{"location":"basics/common-questions/#jamm-warning-messages","title":"JAMM Warning Messages","text":"<p>When launching JanusGraph with embedded Cassandra, the following warnings may be displayed:</p> <p><code>958 [MutationStage:25] WARN  org.apache.cassandra.db.Memtable  - MemoryMeter uninitialized (jamm not specified as java agent); assuming liveRatio of 10.0.  Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead</code></p> <p>Cassandra uses a Java agent called <code>MemoryMeter</code> which allows it to measure the actual memory use of an object, including JVM overhead. To use JAMM (Java Agent for Memory Measurements), the path to the JAMM jar must be specific in the Java javaagent parameter when launching the JVM (e.g. <code>-javaagent:path/to/jamm.jar</code>) through either <code>janusgraph.sh</code>, <code>gremlin.sh</code>, or Gremlin Server:</p> <pre><code>export JANUSGRAPH_JAVA_OPTS=-javaagent:$JANUSGRAPH_HOME/lib/jamm-0.3.0.jar\n</code></pre>"},{"location":"basics/common-questions/#cassandra-connection-problem","title":"Cassandra Connection Problem","text":"<p>By default, JanusGraph uses the Astyanax library to connect to Cassandra clusters. On EC2 and Rackspace, it has been reported that Astyanax was unable to establish a connection to the cluster. In those cases, changing the backend to <code>storage.backend=cassandrathrift</code> solved the problem.</p>"},{"location":"basics/common-questions/#elasticsearch-outofmemoryexception","title":"Elasticsearch OutOfMemoryException","text":"<p>When numerous clients are connecting to Elasticsearch, it is likely that an <code>OutOfMemoryException</code> occurs. This is not due to a memory issue, but to the OS not allowing more threads to be spawned by the user (the user running Elasticsearch). To circumvent this issue, increase the number of allowed processes to the user running Elasticsearch. For example, increase the <code>ulimit -u</code> from the default 1024 to 10024.</p>"},{"location":"basics/common-questions/#dropping-a-database","title":"Dropping a Database","text":"<p>To drop a database using the Gremlin Console you can call <code>JanusGraphFactory.drop(graph)</code>. The graph you want to drop needs to be defined prior to running the drop method.</p> <p>With ConfiguredGraphFactory <pre><code>graph = ConfiguredGraphFactory.open('example')\nConfiguredGraphFactory.drop('example');\n</code></pre></p> <p>With JanusGraphFactory <pre><code>graph = JanusGraphFactory.open('path/to/configuration.properties')\nJanusGraphFactory.drop(graph);\n</code></pre></p> <p>Note that on JanusGraph versions prior to 0.3.0 if multiple Gremlin Server instances are connecting to the graph that has been dropped it is reccomended to close the graph on all active nodes by running either <code>JanusGraphFactory.close(graph)</code> or <code>ConfiguredGraphFactory.close(\"example\")</code> depending on which graph manager is in use. Closing and reopening the graph on all active nodes will prevent cached(stale) references to the graph that has been dropped. ConfiguredGraphFactory graphs that are dropped may need to have their configurations recreated using the graph configuration singleton or template configuration.</p>"},{"location":"basics/configuration-reference/","title":"Configuration Reference","text":"<p>This section is the authoritative reference for JanusGraph configuration options. It includes all options for storage and indexing backends that are part of the official JanusGraph distribution.</p> <p>The table is automatically generated by traversing the keys and namespaces in JanusGraph\u2019s internal configuration management API. Hence, the configuration options as listed on this page are synchronized with a particular JanusGraph release. If a reference to a configuration option in other parts of this documentation is in conflict with its representation on this page, assume the version listed here to be correct.</p>"},{"location":"basics/configuration-reference/#mutability-levels","title":"Mutability Levels","text":"<p>Each configuration option has a certain mutability level that governs whether and how it can be modified after the database is opened for the first time. The following listing describes the mutability levels.</p> <ul> <li> <p>FIXED</p> <p>Once the database has been opened, these configuration options cannot be changed for the entire life of the database</p> </li> <li> <p>GLOBAL_OFFLINE</p> <p>These options can only be changed for the entire database cluster at once when all instances are shut down</p> </li> <li> <p>GLOBAL</p> <p>These options can only be changed globally across the entire database cluster</p> </li> <li> <p>MASKABLE</p> <p>These options are global but can be overwritten by a local configuration file</p> </li> <li> <p>LOCAL     These options can only be provided through a local configuration file</p> </li> </ul> <p>Refer to Global Configuration for information on how to change non-local configuration options.</p>"},{"location":"basics/configuration-reference/#umbrella-namespace","title":"Umbrella Namespace","text":"<p>Namespaces marked with an asterisk are umbrella namespaces which means that they can accommodate an arbitrary number of sub-namespaces - each of which uniquely identified by its name. The configuration options listed under an umbrella namespace apply only to those sub-namespaces. Umbrella namespaces are used to configure multiple system components that are of the same type and hence have the same configuration options.</p> <p>For example, the <code>log</code> namespace is an umbrella namespace because JanusGraph can interface with multiple logging backends, such as the <code>user</code> log, each of which has the same core set of configuration options. To configure the send batch size of the <code>user</code> log to 100 transaction changes, one would have to set the following option in the configuration <pre><code>log.user.send-batch-size = 100\n</code></pre></p>"},{"location":"basics/configuration-reference/#configuration-namespaces-and-options","title":"Configuration Namespaces and Options","text":""},{"location":"basics/configuration-reference/#attributescustom","title":"attributes.custom *","text":"<p>Custom attribute serialization and handling</p> Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE"},{"location":"basics/configuration-reference/#cache","title":"cache","text":"<p>Configuration options that modify JanusGraph's caching behavior</p> Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data.  Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them.  This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 GLOBAL_OFFLINE cache.db-cache-size Size of JanusGraph's database level cache.  Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 GLOBAL_OFFLINE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE"},{"location":"basics/configuration-reference/#cluster","title":"cluster","text":"<p>Configuration options for multi-machine deployments</p> Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodesin the JanusGraph graph cluster. Must be bigger than 1 and a power of 2. Integer 32 FIXED"},{"location":"basics/configuration-reference/#computer","title":"computer","text":"<p>GraphComputer related configuration</p> Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE"},{"location":"basics/configuration-reference/#graph","title":"graph","text":"<p>General configuration options</p> Name Description Datatype Default Value Mutability graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL.  These types are managed globally through the storage backend and cannot be overridden by changing the local configuration.  This type of conflict usually indicates misconfiguration.  When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option.  When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagament APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anwyay. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored.  An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance.  This must be unique among all instances concurrently accessing the same stores or indexes.  It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL"},{"location":"basics/configuration-reference/#gremlin","title":"gremlin","text":"<p>Gremlin configuration options</p> Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL"},{"location":"basics/configuration-reference/#ids","title":"ids","text":"<p>General configuration options for graph element IDs</p> Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size.  Setting this too low will make commits frequently block on slow reservation requests.  Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation.  When false, IDs are assigned only when the transaction commits. Must be disabled for graph partitioning to work. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE"},{"location":"basics/configuration-reference/#idsauthority","title":"ids.authority","text":"<p>Configuration options for graph element ID reservation/allocation</p> Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE"},{"location":"basics/configuration-reference/#index","title":"index *","text":"<p>Configuration options for the individual indexing backends</p> Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional.  JanusGraph can use multiple heterogeneous index backends.  Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers.  This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that theindexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maxium number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE"},{"location":"basics/configuration-reference/#indexxelasticsearch","title":"index.[X].elasticsearch","text":"<p>Elasticsearch index configuration</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status.  This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.max-retry-timeout Sets the maximum timeout (in milliseconds) to honour in case of multiple retries of the same request sent using the ElasticSearch Rest Client by JanusGraph. Integer (no default value) MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in secondes) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x  and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-deprecated-multitype-index Whether JanusGraph should group these indices into a single Elasticsearch index (requires Elasticsearch 5.x or earlier). Boolean false GLOBAL_OFFLINE"},{"location":"basics/configuration-reference/#indexxelasticsearchcreate","title":"index.[X].elasticsearch.create","text":"<p>Settings related to index creation</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index.  This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE"},{"location":"basics/configuration-reference/#indexxsolr","title":"index.[X].solr","text":"<p>Solr index configuration</p> Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be resued for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabledthe user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of <code>collection=field</code>. String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP (<code>http</code>) or using SolrCloud (<code>cloud</code>) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE"},{"location":"basics/configuration-reference/#log","title":"log *","text":"<p>Configuration options for JanusGraph's logging system</p> Name Description Datatype Default Value Mutability log.[X].backend Define the log backed to use String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not becomevisible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaningthat all entries added to this log expire after the configured amount of time. Requiresthat the log implementation supports TTL. Duration (no default value) GLOBAL"},{"location":"basics/configuration-reference/#metrics","title":"metrics","text":"<p>Configuration options for metrics reporting</p> Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE"},{"location":"basics/configuration-reference/#metricsconsole","title":"metrics.console","text":"<p>Configuration options for metrics reporting to console</p> Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE"},{"location":"basics/configuration-reference/#metricscsv","title":"metrics.csv","text":"<p>Configuration options for metrics reporting to CSV file</p> Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE"},{"location":"basics/configuration-reference/#metricsganglia","title":"metrics.ganglia","text":"<p>Configuration options for metrics reporting through Ganglia</p> Name Description Datatype Default Value Mutability metrics.ganglia.addressing-mode Whether to communicate to Ganglia via uni- or multicast String unicast MASKABLE metrics.ganglia.hostname The unicast host or multicast group name to which Metrics will send Ganglia data String (no default value) MASKABLE metrics.ganglia.interval The number of milliseconds to wait between sending Metrics data to Ganglia Duration (no default value) MASKABLE metrics.ganglia.port The port to which Ganglia data are sent Integer 8649 MASKABLE metrics.ganglia.protocol-31 Whether to send data to Ganglia in the 3.1 protocol format Boolean true MASKABLE metrics.ganglia.spoof If non-null, it must be a valid Gmetric spoof string formatted as an IP:hostname pair. See https://github.com/ganglia/monitor-core/wiki/Gmetric-Spoofing for information about this setting. String (no default value) MASKABLE metrics.ganglia.ttl The multicast TTL to set on outgoing Ganglia datagrams Integer 1 MASKABLE metrics.ganglia.uuid The host UUID to set on outgoing Ganglia datagrams. See https://github.com/ganglia/monitor-core/wiki/UUIDSources for information about this setting. String (no default value) LOCAL"},{"location":"basics/configuration-reference/#metricsgraphite","title":"metrics.graphite","text":"<p>Configuration options for metrics reporting through Graphite</p> Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE"},{"location":"basics/configuration-reference/#metricsjmx","title":"metrics.jmx","text":"<p>Configuration options for metrics reporting through JMX</p> Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE"},{"location":"basics/configuration-reference/#metricsslf4j","title":"metrics.slf4j","text":"<p>Configuration options for metrics reporting through slf4j</p> Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE"},{"location":"basics/configuration-reference/#query","title":"query","text":"<p>Configuration options for query processing</p> Name Description Datatype Default Value Mutability query.batch Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. Boolean false MASKABLE query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequentproperty access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing solimits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean true MASKABLE"},{"location":"basics/configuration-reference/#schema","title":"schema","text":"<p>Schema related configuration options</p> Name Description Datatype Default Value Mutability schema.default Configures the DefaultSchemaMaker to be used by this graph. If set to 'none', automatic schema creation is disabled. Defaults to a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys String default MASKABLE"},{"location":"basics/configuration-reference/#storage","title":"storage","text":"<p>Configuration options for the storage backend.  Some options are applicable only for certain backends.</p> Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph.  This is required.  It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cassandrathrift, cassandra, astyanax, embeddedcassandra, cql, hbase, inmemory) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers.  This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operationfails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to /. String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operationfails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE"},{"location":"basics/configuration-reference/#storageberkeleyje","title":"storage.berkeleyje","text":"<p>BerkeleyDB JE configuration options</p> Name Description Datatype Default Value Mutability storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE"},{"location":"basics/configuration-reference/#storagecassandra","title":"storage.cassandra","text":"<p>Cassandra storage backend options</p> Name Description Datatype Default Value Mutability storage.cassandra.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean true MASKABLE storage.cassandra.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cassandra.compaction-strategy-options Compaction strategy options.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cassandra.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cassandra.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cassandra.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cassandra.frame-size-mb The thrift frame size in megabytes Integer 15 MASKABLE storage.cassandra.keyspace The name of JanusGraph's keyspace.  It will be created if it does not exist. If it is not supplied, but graph.graphname is, then the the keyspace will be set to that. String janusgraph LOCAL storage.cassandra.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cassandra.replication-factor The number of data replicas (including the original copy) that should be kept. This is only meaningful for storage backends that natively support data replication. Integer 1 GLOBAL_OFFLINE storage.cassandra.replication-strategy-class The replication strategy to use for JanusGraph keyspace String org.apache.cassandra.locator.SimpleStrategy FIXED storage.cassandra.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form.  A replication_factor set here takes precedence over one set with storage.cassandra.replication-factor String[] (no default value) FIXED storage.cassandra.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE"},{"location":"basics/configuration-reference/#storagecassandraastyanax","title":"storage.cassandra.astyanax","text":"<p>Astyanax-specific Cassandra options</p> Name Description Datatype Default Value Mutability storage.cassandra.astyanax.cluster-name Default name for the Cassandra cluster String JanusGraph Cluster MASKABLE storage.cassandra.astyanax.connection-pool-type Astyanax's connection pooler implementation String TOKEN_AWARE MASKABLE storage.cassandra.astyanax.frame-size The thrift frame size in mega bytes Integer 15 MASKABLE storage.cassandra.astyanax.host-supplier Host supplier to use when discovery type is set to DISCOVERY_SERVICE or TOKEN_AWARE String (no default value) MASKABLE storage.cassandra.astyanax.local-datacenter The name of the local or closest Cassandra datacenter.  When set and not whitespace, this value will be passed into ConnectionPoolConfigurationImpl.setLocalDatacenter. When unset or set to whitespace, setLocalDatacenter will not be invoked. String (no default value) MASKABLE storage.cassandra.astyanax.max-cluster-connections-per-host Maximum pooled \"cluster\" connections per host Integer 3 MASKABLE storage.cassandra.astyanax.max-connections Maximum open connections allowed in the pool (counting all hosts) Integer -1 MASKABLE storage.cassandra.astyanax.max-connections-per-host Maximum pooled connections per host Integer 32 MASKABLE storage.cassandra.astyanax.max-operations-per-connection Maximum number of operations allowed per connection before the connection is closed Integer 100000 MASKABLE storage.cassandra.astyanax.node-discovery-type How Astyanax discovers Cassandra cluster nodes String RING_DESCRIBE MASKABLE storage.cassandra.astyanax.read-page-size The page size for Cassandra read operations Integer 4096 MASKABLE storage.cassandra.astyanax.retry-backoff-strategy Astyanax's retry backoff strategy with configuration parameters String com.netflix.astyanax.connectionpool.impl.FixedRetryBackoffStrategy,1000,5000 MASKABLE storage.cassandra.astyanax.retry-delay-slice Astyanax's connection pool \"retryDelaySlice\" parameter Integer 10000 MASKABLE storage.cassandra.astyanax.retry-max-delay-slice Astyanax's connection pool \"retryMaxDelaySlice\" parameter Integer 10 MASKABLE storage.cassandra.astyanax.retry-policy Astyanax's retry policy implementation with configuration parameters String com.netflix.astyanax.retry.BoundedExponentialBackoff,100,25000,8 MASKABLE storage.cassandra.astyanax.retry-suspend-window Astyanax's connection pool \"retryMaxDelaySlice\" parameter Integer 20000 MASKABLE"},{"location":"basics/configuration-reference/#storagecassandrassl","title":"storage.cassandra.ssl","text":"<p>Configuration options for SSL</p> Name Description Datatype Default Value Mutability storage.cassandra.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL"},{"location":"basics/configuration-reference/#storagecassandrassltruststore","title":"storage.cassandra.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability storage.cassandra.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cassandra.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"basics/configuration-reference/#storagecassandrathriftcpool","title":"storage.cassandra.thrift.cpool","text":"<p>Options for the Apache commons-pool connection manager</p> Name Description Datatype Default Value Mutability storage.cassandra.thrift.cpool.evictor-period Approximate number of milliseconds between runs of the idle connection evictor.  Set to -1 to never run the idle connection evictor. Long 30000 MASKABLE storage.cassandra.thrift.cpool.idle-test Whether the idle connection evictor validates idle connections and drops those that fail to validate Boolean false MASKABLE storage.cassandra.thrift.cpool.idle-tests-per-eviction-run When the value is negative, e.g. -n, roughly one nth of the idle connections are tested per run.  When the value is positive, e.g. n, the min(idle-count, n) connections are tested per run. Integer 0 MASKABLE storage.cassandra.thrift.cpool.max-active Maximum number of concurrently in-use connections (-1 to leave undefined) Integer 16 MASKABLE storage.cassandra.thrift.cpool.max-idle Maximum number of concurrently idle connections (-1 to leave undefined) Integer 4 MASKABLE storage.cassandra.thrift.cpool.max-total Max number of allowed Thrift connections, idle or active (-1 to leave undefined) Integer -1 MASKABLE storage.cassandra.thrift.cpool.max-wait Maximum number of milliseconds to block when storage.cassandra.thrift.cpool.when-exhausted is set to BLOCK.  Has no effect when set to actions besides BLOCK.  Set to -1 to wait indefinitely. Long -1 MASKABLE storage.cassandra.thrift.cpool.min-evictable-idle-time Minimum number of milliseconds a connection must be idle before it is eligible for eviction.  See also storage.cassandra.thrift.cpool.evictor-period.  Set to -1 to never evict idle connections. Long 60000 MASKABLE storage.cassandra.thrift.cpool.min-idle Minimum number of idle connections the pool attempts to maintain Integer 0 MASKABLE storage.cassandra.thrift.cpool.when-exhausted What to do when clients concurrently request more active connections than are allowed by the pool.  The value must be one of BLOCK, FAIL, or GROW. String BLOCK MASKABLE"},{"location":"basics/configuration-reference/#storagecql","title":"storage.cql","text":"<p>CQL storage backend options</p> Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.cluster-name Default name for the Cassandra cluster String JanusGraph Cluster MASKABLE storage.cql.compact-storage Whether the storage backend should use compact storage on tables. This option is only available for Cassandra 2 and earlier and defaults to true. Boolean true FIXED storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.keyspace The name of JanusGraph's keyspace.  It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter.  When set and not whitespace, this value will be passed into ConnectionPoolConfigurationImpl.setLocalDatacenter. When unset or set to whitespace, setLocalDatacenter will not be invoked. String (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database.  If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form.  A replication_factor set here takes precedence over one set with storage.cql.replication-factor String[] (no default value) FIXED storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE"},{"location":"basics/configuration-reference/#storagecqlssl","title":"storage.cql.ssl","text":"<p>Configuration options for SSL</p> Name Description Datatype Default Value Mutability storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL"},{"location":"basics/configuration-reference/#storagecqlssltruststore","title":"storage.cql.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"basics/configuration-reference/#storagehbase","title":"storage.hbase","text":"<p>HBase storage options</p> Name Description Datatype Default Value Mutability storage.hbase.compat-class The package and class name of the HBaseCompat implementation. HBaseCompat masks version-specific HBase API differences. When this option is unset, JanusGraph calls HBase's VersionInfo.getVersion() and loads the matching compat class at runtime.  Setting this option forces JanusGraph to instead reflectively load and instantiate the specified class. String (no default value) MASKABLE storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster.  JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances.  This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.table The name of the table JanusGraph will use.  When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL"},{"location":"basics/configuration-reference/#storagelock","title":"storage.lock","text":"<p>Options for locking on eventually-consistent stores</p> Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend.  JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code.  JanusGraph generates an appropriate default value for this option at startup.  Overridding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Duration 100 ms GLOBAL_OFFLINE"},{"location":"basics/configuration-reference/#storagemeta","title":"storage.meta *","text":"<p>Meta data to include in storage backend retrievals</p> Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility Boolean true GLOBAL"},{"location":"basics/configuration-reference/#tx","title":"tx","text":"<p>Configuration options for transaction handling</p> Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL"},{"location":"basics/configuration-reference/#txrecovery","title":"tx.recovery","text":"<p>Configuration options for transaction recovery processes</p> Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE"},{"location":"basics/configuration/","title":"Configuration","text":"<p>A JanusGraph graph database cluster consists of one or multiple JanusGraph instances. To open a JanusGraph instance, a configuration has to be provided which specifies how JanusGraph should be set up.</p> <p>A JanusGraph configuration specifies which components JanusGraph should use, controls all operational aspects of a JanusGraph deployment, and provides a number of tuning options to get maximum performance from a JanusGraph cluster.</p> <p>At a minimum, a JanusGraph configuration must define the persistence engine that JanusGraph should use as a storage backend. Storage Backends lists all supported persistence engines and how to configure them respectively. If advanced graph query support (e.g full-text search, geo search, or range queries) is required an additional indexing backend must be configured. See Index Backends for details. If query performance is a concern, then caching should be enabled. Cache configuration and tuning is described in JanusGraph Cache.</p>"},{"location":"basics/configuration/#example-configurations","title":"Example Configurations","text":"<p>Below are some example configuration files to demonstrate how to configure the most commonly used storage backends, indexing systems, and performance components. This covers only a tiny portion of the available configuration options. Refer to Configuration Reference for the complete list of all options.</p>"},{"location":"basics/configuration/#cassandraelasticsearch","title":"Cassandra+Elasticsearch","text":"<p>Sets up JanusGraph to use the Cassandra persistence engine running locally and a remote Elastic search indexing system:</p> <pre><code>storage.backend=cql\nstorage.hostname=localhost\n\nindex.search.backend=elasticsearch\nindex.search.hostname=100.100.101.1, 100.100.101.2\nindex.search.elasticsearch.client-only=true\n</code></pre>"},{"location":"basics/configuration/#hbasecaching","title":"HBase+Caching","text":"<p>Sets up JanusGraph to use the HBase persistence engine running remotely and uses JanusGraph\u2019s caching component for better performance. <pre><code>storage.backend=hbase\nstorage.hostname=100.100.101.1\nstorage.port=2181\n\ncache.db-cache = true\ncache.db-cache-clean-wait = 20\ncache.db-cache-time = 180000\ncache.db-cache-size = 0.5\n</code></pre></p>"},{"location":"basics/configuration/#berkeleydb","title":"BerkeleyDB","text":"<p>Sets up JanusGraph to use BerkeleyDB as an embedded persistence engine with Elasticsearch as an embedded indexing system. <pre><code>storage.backend=berkeleyje\nstorage.directory=/tmp/graph\n\nindex.search.backend=elasticsearch\nindex.search.directory=/tmp/searchindex\nindex.search.elasticsearch.client-only=false\nindex.search.elasticsearch.local-mode=true\n</code></pre></p> <p>Configuration Reference describes all of these configuration options in detail. The <code>conf</code> directory of the JanusGraph distribution contains additional configuration examples.</p>"},{"location":"basics/configuration/#further-examples","title":"Further Examples","text":"<p>There are several example configuration files in the <code>conf/</code> directory that can be used to get started with JanusGraph quickly. Paths to these files can be passed to <code>JanusGraphFactory.open(...)</code> as shown below:</p> <pre><code>// Connect to Cassandra on localhost using a default configuration\ngraph = JanusGraphFactory.open(\"conf/janusgraph-cql.properties\")\n// Connect to HBase on localhost using a default configuration\ngraph = JanusGraphFactory.open(\"conf/janusgraph-hbase.properties\")\n</code></pre>"},{"location":"basics/configuration/#using-configuration","title":"Using Configuration","text":"<p>How the configuration is provided to JanusGraph depends on the instantiation mode.</p>"},{"location":"basics/configuration/#janusgraphfactory","title":"JanusGraphFactory","text":""},{"location":"basics/configuration/#gremlin-console","title":"Gremlin Console","text":"<p>The JanusGraph distribution contains a command line Gremlin Console which makes it easy to get started and interact with JanusGraph. Invoke <code>bin/gremlin.sh</code> (Unix/Linux) or <code>bin/gremlin.bat</code> (Windows) to start the Console and then open a JanusGraph graph using the factory with the configuration stored in an accessible properties configuration file: <pre><code>graph = JanusGraphFactory.open('path/to/configuration.properties')\n</code></pre></p>"},{"location":"basics/configuration/#janusgraph-embedded","title":"JanusGraph Embedded","text":"<p>JanusGraphFactory can also be used to open an embedded JanusGraph graph instance from within a JVM-based user application. In that case, JanusGraph is part of the user application and the application can call upon JanusGraph directly through its public API.</p>"},{"location":"basics/configuration/#short-codes","title":"Short Codes","text":"<p>If the JanusGraph graph cluster has been previously configured and/or only the storage backend needs to be defined, JanusGraphFactory accepts a colon-separated string representation of the storage backend name and hostname or directory. <pre><code>graph = JanusGraphFactory.open('cql:localhost')\ngraph = JanusGraphFactory.open('berkeleyje:/tmp/graph')\n</code></pre></p>"},{"location":"basics/configuration/#janusgraph-server","title":"JanusGraph Server","text":"<p>JanusGraph, by itself, is simply a set of jar files with no thread of execution. There are two basic patterns for connecting to, and using a JanusGraph database:</p> <ol> <li> <p>JanusGraph can be used by embedding JanusGraph calls in a client     program where the program provides the thread of execution.</p> </li> <li> <p>JanusGraph packages a long running server process that, when     started, allows a remote client or logic running in a separate     program to make JanusGraph calls. This long running server process     is called JanusGraph Server.</p> </li> </ol> <p>For the JanusGraph Server, JanusGraph uses Gremlin Server of the Apache TinkerPop stack to service client requests. JanusGraph provides an out-of-the-box configuration for a quick start with JanusGraph Server, but the configuration can be changed to provide a wide range of server capabilities.</p> <p>Configuring JanusGraph Server is accomplished through a JanusGraph Server yaml configuration file located in the ./conf/gremlin-server directory in the JanusGraph distribution. To configure JanusGraph Server with a graph instance (<code>JanusGraph</code>), the JanusGraph Server configuration file requires the following settings:</p> <pre><code>...\ngraphs: {\n  graph: conf/janusgraph-berkeleyje.properties\n}\nplugins:\n  - janusgraph.imports\n...\n</code></pre> <p>The entry for <code>graphs</code> defines the bindings to specific <code>JanusGraph</code> configurations. In the above case it binds <code>graph</code> to a JanusGraph configuration at <code>conf/janusgraph-berkeleyje.properties</code>. The <code>plugins</code> entry enables the JanusGraph Gremlin Plugin, which enables auto-imports of JanusGraph classes so that they can be referenced in remotely submitted scripts.</p> <p>Learn more about configuring and using JanusGraph Server in JanusGraph Server.</p>"},{"location":"basics/configuration/#server-distribution","title":"Server Distribution","text":"<p>The JanusGraph zip file contains a quick start server component that helps make it easier to get started with Gremlin Server and JanusGraph. Invoke <code>bin/janusgraph.sh start</code> to start Gremlin Server with Cassandra and Elasticsearch.</p> <p>Note</p> <p>For security reasons Elasticsearch and therefore <code>janusgraph.sh</code> must be run under a non-root account</p>"},{"location":"basics/configuration/#global-configuration","title":"Global Configuration","text":"<p>JanusGraph distinguishes between local and global configuration options. Local configuration options apply to an individual JanusGraph instance. Global configuration options apply to all instances in a cluster. More specifically, JanusGraph distinguishes the following five scopes for configuration options:</p> <ul> <li> <p>LOCAL: These options only apply to an individual JanusGraph     instance and are specified in the configuration provided when     initializing the JanusGraph instance.</p> </li> <li> <p>MASKABLE: These configuration options can be overwritten for an     individual JanusGraph instance by the local configuration file. If     the local configuration file does not specify the option, its value     is read from the global JanusGraph cluster configuration.</p> </li> <li> <p>GLOBAL: These options are always read from the cluster     configuration and cannot be overwritten on an instance basis.</p> </li> <li> <p>GLOBAL_OFFLINE: Like GLOBAL, but changing these options     requires a cluster restart to ensure that the value is the same     across the entire cluster.</p> </li> <li> <p>FIXED: Like GLOBAL, but the value cannot be changed once the     JanusGraph cluster is initialized.</p> </li> </ul> <p>When the first JanusGraph instance in a cluster is started, the global configuration options are initialized from the provided local configuration file. Subsequently changing global configuration options is done through JanusGraph\u2019s management API. To access the management API, call <code>g.getManagementSystem()</code> on an open JanusGraph instance handle <code>g</code>. For example, to change the default caching behavior on a JanusGraph cluster: <pre><code>mgmt = graph.openManagement()\nmgmt.get('cache.db-cache')\n// Prints the current config setting\nmgmt.set('cache.db-cache', true)\n// Changes option\nmgmt.get('cache.db-cache')\n// Prints 'true'\nmgmt.commit()\n// Changes take effect\n</code></pre></p>"},{"location":"basics/configuration/#changing-offline-options","title":"Changing Offline Options","text":"<p>Changing configuration options does not affect running instances and only applies to newly started ones. Changing GLOBAL_OFFLINE configuration options requires restarting the cluster so that the changes take effect immediately for all instances. To change GLOBAL_OFFLINE options follow these steps:</p> <ul> <li>Close all but one JanusGraph instance in the cluster</li> <li>Connect to the single instance</li> <li>Ensure all running transactions are closed</li> <li>Ensure no new transactions are started (i.e. the cluster must be offline)</li> <li>Open the management API</li> <li>Change the configuration option(s)</li> <li>Call commit which will automatically shut down the graph instance</li> <li>Restart all instances</li> </ul> <p>Refer to the full list of configuration options in Configuration Reference for more information including the configuration scope of each option.</p>"},{"location":"basics/configured-graph-factory/","title":"ConfiguredGraphFactory","text":"<p>The JanusGraph Server can be configured to use the <code>ConfiguredGraphFactory</code>. The <code>ConfiguredGraphFactory</code> is an access point to your graphs, similar to the <code>JanusGraphFactory</code>. These graph factories provide methods for dynamically managing the graphs hosted on the server.</p>"},{"location":"basics/configured-graph-factory/#overview","title":"Overview","text":"<p><code>JanusGraphFactory</code> is a class that provides an access point to your graphs by providing a Configuration object each time you access the graph.</p> <p><code>ConfiguredGraphFactory</code> provides an access point to your graphs for which you have previously created configurations using the <code>ConfigurationManagementGraph</code>. It also offers an access point to manage graph configurations.</p> <p><code>ConfigurationManagementGraph</code> allows you to manage graph configurations.</p> <p><code>JanusGraphManager</code> is an internal server component that tracks graph references, provided your graphs are configured to use it.</p>"},{"location":"basics/configured-graph-factory/#configuredgraphfactory-versus-janusgraphfactory","title":"ConfiguredGraphFactory versus JanusGraphFactory","text":"<p>However, there is an important distinction between these two graph factories:</p> <ol> <li>The <code>ConfiguredGraphFactory</code> can only be used if you have configured     your server to use the <code>ConfigurationManagementGraph</code> APIs at server     start.</li> </ol> <p>The benefits of using the <code>ConfiguredGraphFactory</code> are that:</p> <ol> <li> <p>You only need to supply a <code>String</code> to access your graphs, as opposed     to the <code>JanusGraphFactory</code>-- which requires you to specify     information about the backend you wish to use when accessing a     graph-- every time you open a graph.</p> </li> <li> <p>If your ConfigurationManagementGraph is configured with a     distributed storage backend then your graph configurations are     available to all JanusGraph nodes in your cluster.</p> </li> </ol>"},{"location":"basics/configured-graph-factory/#how-does-the-configuredgraphfactory-work","title":"How Does the ConfiguredGraphFactory Work?","text":"<p>The <code>ConfiguredGraphFactory</code> provides an access point to graphs under two scenarios:</p> <ol> <li> <p>You have already created a configuration for your specific graph     object using the <code>ConfigurationManagementGraph#createConfiguration</code>.     In this scenario, your graph is opened using the previously created     configuration for this graph.</p> </li> <li> <p>You have already created a template configuration using the     <code>ConfigurationManagementGraph#createTemplateConfiguration</code>. In this     scenario, we create a configuration for the graph you are creating     by copying over all attributes stored in your template configuration     and appending the relevant graphName attribute, and we then open the     graph according to that specific configuration.</p> </li> </ol>"},{"location":"basics/configured-graph-factory/#accessing-the-graphs","title":"Accessing the Graphs","text":"<p>You can either use <code>ConfiguredGraphFactory.create(\"graphName\")</code> or <code>ConfiguredGraphFactory.open(\"graphName\")</code>. Learn more about the difference between these two options by reading the section below about the <code>ConfigurationManagementGraph</code>.</p>"},{"location":"basics/configured-graph-factory/#listing-the-graphs","title":"Listing the Graphs","text":"<p><code>ConfiguredGraphFactory.getGraphNames()</code> will return a set of graph names for which you have created configurations using the <code>ConfigurationManagementGraph</code> APIs.</p> <p><code>JanusGraphFactory.getGraphNames()</code> on the other hand returns a set of graph names for which you have instantiated and the references are stored inside the <code>JanusGraphManager</code>.</p>"},{"location":"basics/configured-graph-factory/#dropping-a-graph","title":"Dropping a Graph","text":"<p><code>ConfiguredGraphFactory.drop(\"graphName\")</code> will drop the graph database, deleting all data in storage and indexing backends. The graph can be open or closed (will be closed as part of the drop operation). Furthermore, this will also remove any existing graph configuration in the <code>ConfigurationManagementGraph</code>.</p> <p>Important</p> <p>This is an irreversible operation that will delete all graph and index data.</p> <p>Important</p> <p>To ensure all graph representations are consistent across all JanusGraph nodes in your cluster, remove the graph from the <code>JanusGraphManager</code>  graph reference tracker on all nodes in your cluster: <code>ConfiguredGraphFactory.close(\"graphName\");</code>.</p>"},{"location":"basics/configured-graph-factory/#configuring-janusgraph-server-for-configuredgraphfactory","title":"Configuring JanusGraph Server for ConfiguredGraphFactory","text":"<p>To be able to use the <code>ConfiguredGraphFactory</code>, you must configure your server to use the <code>ConfigurationManagementGraph</code> APIs. To do this, you have to inject a graph variable named \"ConfigurationManagementGraph\" in your server\u2019s YAML\u2019s <code>graphs</code> map. For example:</p> <pre><code>graphManager: org.janusgraph.graphdb.management.JanusGraphManager\ngraphs: {\n  ConfigurationManagementGraph: conf/JanusGraph-configurationmanagement.properties\n}\n</code></pre> <p>In this example, our <code>ConfigurationManagementGraph</code> graph will be configured using the properties stored inside <code>conf/JanusGraph-configurationmanagement.properties</code>, which for example, look like:</p> <pre><code>gremlin.graph=org.janusgraph.core.ConfiguredGraphFactory\nstorage.backend=cql\ngraph.graphname=ConfigurationManagementGraph\nstorage.hostname=127.0.0.1\n</code></pre> <p>Assuming the GremlinServer started successfully and the <code>ConfigurationManagementGraph</code> was successfully instantiated, then all the APIs available on the <code>ConfigurationManagementGraph</code> <code>Singleton</code> will also act upon said graph. Furthermore, this is the graph that will be used to access the configurations used to create/open graphs using the <code>ConfiguredGraphFactory</code>.</p> <p>Important</p> <p>The <code>pom.xml</code> included in the JanusGraph distribution lists this dependency as optional, but the <code>ConfiguredGraphFactory</code> makes use of the <code>JanusGraphManager</code>, which requires a declared dependency on the <code>org.apache.tinkerpop:gremlin-server</code>. So if you run into <code>NoClassDefFoundError</code> errors, then be sure to update according to this message.</p>"},{"location":"basics/configured-graph-factory/#configurationmanagementgraph","title":"ConfigurationManagementGraph","text":"<p>The <code>ConfigurationManagementGraph</code> is a <code>Singleton</code> that allows you to create/update/remove configurations that you can use to access your graphs using the <code>ConfiguredGraphFactory</code>. See above on configuring your server to enable use of these APIs.</p> <p>Important</p> <p>The ConfiguredGraphFactory offers an access point to manage your graph configurations managed by the <code>ConfigurationManagementGraph</code>, so instead of acting upon the <code>Singleton</code> itself, you may act upon the corresponding <code>ConfiguredGraphFactory</code> static methods. For example, you may use <code>ConfiguredGraphFactory.removeTemplateConfiguration()</code> instead of <code>ConfiguredGraphFactory.getInstance().removeTemplateConfiguration()</code>.</p>"},{"location":"basics/configured-graph-factory/#graph-configurations","title":"Graph Configurations","text":"<p>The <code>ConfigurationManagementGraph</code> singleton allows you to create configurations used to open specific graphs, referenced by the <code>graph.graphname</code> property. For example:</p> <pre><code>map = new HashMap&lt;String, Object&gt;();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nmap.put(\"graph.graphname\", \"graph1\");\nConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));\n</code></pre> <p>Then you could access this graph on any JanusGraph node using: <pre><code>ConfiguredGraphFactory.open(\"graph1\");\n</code></pre></p>"},{"location":"basics/configured-graph-factory/#template-configuration","title":"Template Configuration","text":"<p>The <code>ConfigurationManagementGraph</code> also allows you to create one template configuration, which you can use to create many graphs using the same configuration template. For example: <pre><code>map = new HashMap&lt;String, Object&gt;();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nConfiguredGraphFactory.createTemplateConfiguration(new MapConfiguration(map));\n</code></pre></p> <p>After doing this, you can create graphs using the template configuration: <pre><code>ConfiguredGraphFactory.create(\"graph2\");\n</code></pre></p> <p>This method will first create a new configuration for \"graph2\" by copying over all the properties associated with the template configuration and storing it on a configuration for this specific graph. This means that this graph can be accessed in, on any JanusGraph node, in the future by doing: <pre><code>ConfiguredGraphFactory.open(\"graph2\");\n</code></pre></p>"},{"location":"basics/configured-graph-factory/#updating-configurations","title":"Updating Configurations","text":"<p>All interactions with both the <code>JanusGraphFactory</code> and the <code>ConfiguredGraphFactory</code> that interact with configurations that define the property <code>graph.graphname</code> go through the <code>JanusGraphManager</code> which keeps track of graph references created on the given JVM. Think of it as a graph cache. For this reason:</p> <p>Important</p> <p>IMPORTANT: Any updates to a configuration are not guaranteed to take effect until you remove the graph in question on every JanusGraph node in your cluster.</p> <p>You can do so by calling: <pre><code>ConfiguredGraphFactory.close(\"graph2\");\n</code></pre></p> <p>Since graphs created using the template configuration first create a configuration for that graph in question using a copy and create method, this means that:</p> <p>Important</p> <p>Any updates to a specific graph created using the template configuration are not guaranteed to take effect on the specific graph until: 1. The relevant configuration is removed:     <code>ConfiguredGraphFactory.removeConfiguration(\"graph2\");</code> 2. The graph in question has been closed on every JanusGraph node: <code>ConfiguredGraphFactory.close(\"graph2\");</code> 2. The graph is recreated using the template configuration:     <code>ConfiguredGraphFactory.create(\"graph2\");</code></p>"},{"location":"basics/configured-graph-factory/#update-examples","title":"Update Examples","text":"<p>1) We migrated our Cassandra data to a new server with a new IP address:</p> <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nmap.put(\"graph.graphname\", \"graph1\");\nConfiguredGraphFactory.createConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n\n// Update configuration\nmap = new HashMap();\nmap.put(\"storage.hostname\", \"10.0.0.1\");\nConfiguredGraphFactory.updateConfiguration(\"graph1\",\nmap);\n\n// Close graph\nConfiguredGraphFactory.close(\"graph1\");\n\n// We are now guaranteed to use the updated configuration\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n</code></pre> <p>2) We added an Elasticsearch node to our setup: <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nmap.put(\"graph.graphname\", \"graph1\");\nConfiguredGraphFactory.createConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n\n// Update configuration\nmap = new HashMap();\nmap.put(\"index.search.backend\", \"elasticsearch\");\nmap.put(\"index.search.hostname\", \"127.0.0.1\");\nmap.put(\"index.search.elasticsearch.transport-scheme\", \"http\");\nConfiguredGraphFactory.updateConfiguration(\"graph1\",\nmap);\n\n// We are now guaranteed to use the updated configuration\ng1 = ConfiguredGraphFactory.open(\"graph1\");\n</code></pre></p> <p>3) Update a graph configuration that was created using a template configuration that has been updated: <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nConfiguredGraphFactory.createTemplateConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.create(\"graph1\");\n\n// Update template configuration\nmap = new HashMap();\nmap.put(\"index.search.backend\", \"elasticsearch\");\nmap.put(\"index.search.hostname\", \"127.0.0.1\");\nmap.put(\"index.search.elasticsearch.transport-scheme\", \"http\");\nConfiguredGraphFactory.updateTemplateConfiguration(new\nMapConfiguration(map));\n\n// Remove Configuration\nConfiguredGraphFactory.removeConfiguration(\"graph1\");\n\n// Close graph on all JanusGraph nodes\nConfiguredGraphFactory.close(\"graph1\");\n\n// Recreate\nConfiguredGraphFactory.create(\"graph1\");\n// Now this graph's configuration is guaranteed to be updated\n</code></pre></p>"},{"location":"basics/configured-graph-factory/#janusgraphmanager","title":"JanusGraphManager","text":"<p>The <code>JanusGraphManager</code> is a <code>Singleton</code> adhering to the TinkerPop graphManager specifications.</p> <p>In particular, the <code>JanusGraphManager</code> provides:</p> <ol> <li> <p>a coordinated mechanism by which to instantiate graph references on     a given JanusGraph node</p> </li> <li> <p>a graph reference tracker (or cache)</p> </li> </ol> <p>Any graph you create using the <code>graph.graphname</code> property will go through the <code>JanusGraphManager</code> and thus be instantiated in a coordinated fashion. The graph reference will also be placed in the graph cache on the JVM in question.</p> <p>Thus, any graph you open using the <code>graph.graphname</code> property that has already been instantiated on the JVM in question will be retrieved from the graph cache.</p> <p>This is why updates to your configurations require a few steps to guarantee correctness.</p>"},{"location":"basics/configured-graph-factory/#how-to-use-the-janusgraphmanager","title":"How To Use The JanusGraphManager","text":"<p>This is a new configuration option you can use when defining a property in your configuration that defines how to access a graph. All configurations that include this property will result in the graph instantiation happening through the <code>JanusGraphManager</code> (process explained above).</p> <p>For backwards compatibility, any graphs that do not supply this parameter but supplied at server start in your graphs object in your .yaml file, these graphs will be bound through the JanusGraphManager denoted by their <code>key</code> supplied for that graph. For example, if your .yaml graphs object looks like:</p> <pre><code>graphManager: org.janusgraph.graphdb.management.JanusGraphManager\ngraphs {\n  graph1: conf/graph1.properties,\n  graph2: conf/graph2.properties\n}\n</code></pre> <p>but <code>conf/graph1.properties</code> and <code>conf/graph2.properties</code> do not include the property <code>graph.graphname</code>, then these graphs will be stored in the JanusGraphManager and thus bound in your gremlin script executions as <code>graph1</code> and <code>graph2</code>, respectively.</p>"},{"location":"basics/configured-graph-factory/#important","title":"Important","text":"<p>For convenience, if your configuration used to open a graph specifies <code>graph.graphname</code>, but does not specify the backend\u2019s storage directory, tablename, or keyspacename, then the relevant parameter will automatically be set to the value of <code>graph.graphname</code>. However, if you supply one of those parameters, that value will always take precedence. And if you supply neither, they default to the configuration option\u2019s default value.</p> <p>One special case is <code>storage.root</code> configuration option. This is a new configuration option used to specify the base of the directory that will be used for any backend requiring local storage directory access. If you supply this parameter, you must also supply the <code>graph.graphname</code> property, and the absolute storage directory will be equal to the value of the <code>graph.graphname</code> property appended to the value of the <code>storage.root</code> property.</p> <p>Below are some example use cases:</p> <p>1) Create a template configuration for my Cassandra backend such that each graph created using this configuration gets a unique keyspace equivalent to the <code>String</code> &lt;graphName&gt; provided to the factory: <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"cql\");\nmap.put(\"storage.hostname\", \"127.0.0.1\");\nConfiguredGraphFactory.createTemplateConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.create(\"graph1\"); //keyspace === graph1\ng2 = ConfiguredGraphFactory.create(\"graph2\"); //keyspace === graph2\ng3 = ConfiguredGraphFactory.create(\"graph3\"); //keyspace === graph3\n</code></pre></p> <p>2) Create a template configuration for my BerkeleyJE backend such that each graph created using this configuration gets a unique storage directory equivalent to the \"&lt;storage.root&gt;/&lt;graph.graphname&gt;\": <pre><code>map = new HashMap();\nmap.put(\"storage.backend\", \"berkeleyje\");\nmap.put(\"storage.root\", \"/data/graphs\");\nConfiguredGraphFactory.createTemplateConfiguration(new\nMapConfiguration(map));\n\ng1 = ConfiguredGraphFactory.create(\"graph1\"); //storage directory === /data/graphs/graph1\ng2 = ConfiguredGraphFactory.create(\"graph2\"); //storage directory === /data/graphs/graph2\ng3 = ConfiguredGraphFactory.create(\"graph3\"); //storage directory === /data/graphs/graph3\n</code></pre></p>"},{"location":"basics/configured-graph-factory/#examples","title":"Examples","text":"<p>It is reccomended to use a sessioned connection when creating a Configured Graph Factory template. If a sessioned connection is not used the Configured Graph Factory Template creation must be sent to the server as a single line using semi-colons. See details on sessions can be found in Connecting to Gremlin Server. <pre><code>gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml session\n==&gt;Configured localhost/127.0.0.1:8182\n\ngremlin&gt; :remote console\n==&gt;All scripts will now be sent to Gremlin Server - [localhost:8182]-[5206cdde-b231-41fa-9e6c-69feac0fe2b2] - type ':remote console' to return to local mode\n\ngremlin&gt; ConfiguredGraphFactory.open(\"graph\");\nPlease create configuration for this graph using the\nConfigurationManagementGraph API.\n\ngremlin&gt; ConfiguredGraphFactory.create(\"graph\");\nPlease create a template Configuration using the\nConfigurationManagementGraph API.\n\ngremlin&gt; map = new HashMap();\ngremlin&gt; map.put(\"storage.backend\", \"cql\");\ngremlin&gt; map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; map.put(\"GraphName\", \"graph1\");\ngremlin&gt; ConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));\nPlease include in your configuration the property \"graph.graphname\".\n\ngremlin&gt; map = new HashMap();\ngremlin&gt; map.put(\"storage.backend\", \"cql\");\ngremlin&gt; map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; map.put(\"graph.graphname\", \"graph1\");\ngremlin&gt; ConfiguredGraphFactory.createConfiguration(new MapConfiguration(map));\n==&gt;null\n\ngremlin&gt; ConfiguredGraphFactory.open(\"graph1\").vertices();\n\ngremlin&gt; map = new HashMap(); map.put(\"storage.backend\",\n\"cql\"); map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; map.put(\"graph.graphname\", \"graph1\");\ngremlin&gt; ConfiguredGraphFactory.createTemplateConfiguration(new MapConfiguration(map));\nYour template configuration may not contain the property\n\"graph.graphname\".\n\ngremlin&gt; map = new HashMap();\ngremlin&gt; map.put(\"storage.backend\",\n\"cql\"); map.put(\"storage.hostname\", \"127.0.0.1\");\ngremlin&gt; ConfiguredGraphFactory.createTemplateConfiguration(new MapConfiguration(map));\n==&gt;null\n\n// Each graph is now acting in unique keyspaces equivalent to the\ngraphnames.\ngremlin&gt; g1 = ConfiguredGraphFactory.open(\"graph1\");\ngremlin&gt; g2 = ConfiguredGraphFactory.create(\"graph2\");\ngremlin&gt; g3 = ConfiguredGraphFactory.create(\"graph3\");\ngremlin&gt; g2.addVertex();\ngremlin&gt; l = [];\ngremlin&gt; l &lt;&lt; g1.vertices().size();\n==&gt;0\ngremlin&gt; l &lt;&lt; g2.vertices().size();\n==&gt;1\ngremlin&gt; l &lt;&lt; g3.vertices().size();\n==&gt;0\n\n// After a graph is created, you must access it using .open()\ngremlin&gt; g2 = ConfiguredGraphFactory.create(\"graph2\"); g2.vertices().size();\nConfiguration for graph \"graph2\" already exists.\n\ngremlin&gt; g2 = ConfiguredGraphFactory.open(\"graph2\"); g2.vertices().size();\n==&gt;1\n</code></pre></p>"},{"location":"basics/deployment/","title":"Deployment Scenarios","text":"<p>JanusGraph offers a wide choice of storage and index backends which results in great flexibility of how it can be deployed. This chapter presents a few possible deployment scenarios to help with the complexity that comes with this flexibility.</p> <p>Before discussing the different deployment scenarios, it is important to understand the roles of JanusGraph itself and that of the backends. First of all, applications only communicate directly with JanusGraph, mostly by sending Gremlin traversals for execution. JanusGraph then communicates with the configured backends to execute the received traversal. When JanusGraph is used in the form of JanusGraph Server, then there is nothing like a master JanusGraph Server. Applications can therefore connect to any JanusGraph Server instance. They can also use a load-balancer to schedule requests to the different instances. The JanusGraph Server instances themselves don\u2019t communicate to each other directly which makes it easy to scale them when the need arises to process more traversals.</p> <p>Note</p> <p>The scenarios presented in this chapter are only examples of how JanusGraph can be deployed. Each deployment needs to take into account the concrete use cases and production needs.</p>"},{"location":"basics/deployment/#getting-started-scenario","title":"Getting Started Scenario","text":"<p>This scenario is the scenario most users probably want to choose when they are just getting started with JanusGraph. It offers scalability and fault tolerance with a minimum number of servers required. JanusGraph Server runs together with an instance of the storage backend and optionally also an instance of the index backend on every server.</p> <p></p> <p>A setup like this can be extended by simply adding more servers of the same kind or by moving one of the components onto dedicated servers. The latter describes a growth path to transform the deployment into the Advanced Scenario.</p> <p>Any of the scalable storage backends can be used with this scenario. Note however that for Scylla some configuration is required when it is hosted co-located with other services like in this scenario. When an index backend should be used in this scenario then it also needs to be one that is scalable.</p>"},{"location":"basics/deployment/#advanced-scenario","title":"Advanced Scenario","text":"<p>The advanced scenario is an evolution of the Getting Started Scenario.  Instead of hosting the JanusGraph Server instances together with the storage backend and optionally also the index backend, they are now separated on different servers. The advantage of hosting the different components (JanusGraph Server, storage/index backend) on different servers is that they can be scaled and managed independently of each other. This offers a higher flexibility at the cost of having to maintain more servers.</p> <p></p> <p>Since this scenario offers independent scalability of the different components, it of course makes most sense to also use scalable backends.</p>"},{"location":"basics/deployment/#minimalist-scenario","title":"Minimalist Scenario","text":"<p>It is also possible to host JanusGraph Server together with the backend(s) on just one server. This is especially attractive for testing purposes or for example when JanusGraph just supports a single application which can then also run on the same server.</p> <p></p> <p>Opposed to the previous scenarios, it makes most sense to use backends for this scenario that are not scalable. The in-memory backend can be used for testing purposes or Berkeley DB for production and Lucene as the optional index backend.</p>"},{"location":"basics/deployment/#embedded-janusgraph","title":"Embedded JanusGraph","text":"<p>Instead of connecting to the JanusGraph Server from an application it is also possible to embed JanusGraph as a library inside a JVM based application. While this reduces the administrative overhead, it makes it impossible to scale JanusGraph independently of the application. Embedded JanusGraph can be deployed as a variation of any of the other scenarios. JanusGraph just moves from the server(s) directly into the application as its now just used as a library instead of an independent service.</p>"},{"location":"basics/example-config/","title":"Example Graph Configuration","text":"<p>This page illustrates a number of common graph configurations. Please refer to Configuration Reference and the pages of the respective storage backend, index backend for more information.</p> <p>Also, note that the JanusGraph distribution includes local configuration files in the <code>conf/</code> directory.</p>"},{"location":"basics/example-config/#berkeleydb","title":"BerkeleyDB","text":"<pre><code>storage.backend=berkeleyje\nstorage.directory=/tmp/graph\n\nindex.search.backend=elasticsearch\nindex.search.directory=/tmp/searchindex\nindex.search.elasticsearch.client-only=false\nindex.search.elasticsearch.local-mode=true\n</code></pre> <p>This configuration file configures JanusGraph to use BerkeleyDB as an embedded storage backend, meaning, JanusGraph will start BerkeleyDB internally. The primary data will be stored in the directory <code>/tmp/graph</code>.</p> <p>In addition, this configures an embedded Elasticsearch index backend with the name <code>search</code>. JanusGraph will start Elasticsearch internally and it will not be externally accessible since <code>local-mode</code> is enabled. Elasticsearch stores all data for the <code>search</code> index in <code>/tmp/searchindex</code>. Configuring an index backend is optional.</p>"},{"location":"basics/example-config/#cassandra","title":"Cassandra","text":""},{"location":"basics/example-config/#cassandra-remote","title":"Cassandra Remote","text":"<pre><code>storage.backend=cql\nstorage.hostname=100.100.100.1, 100.100.100.2\n\nindex.search.backend=elasticsearch\nindex.search.hostname=100.100.101.1, 100.100.101.2\nindex.search.elasticsearch.client-only=true\n</code></pre> <p>This configuration file configures JanusGraph to use Cassandra as a remote storage backend. It assumes that a Cassandra cluster is running and accessible at the given IP addresses. If Cassandra is running locally, use the IP address <code>127.0.0.1</code>.</p> <p>In addition, this configures a remote Elasticsearch index backend with the name <code>search</code>. It assumes that an Elasticsearch cluster is running and accessible at the given IP addresses. Enabling <code>client-only</code> ensures that the local instance does not join the existing Elasticsearch cluster as another node but only connects to it. Configuring an index backend is optional.</p>"},{"location":"basics/example-config/#embedded-cassandra","title":"Embedded Cassandra","text":"<pre><code>storage.backend=embeddedcassandra\nstorage.conf-file=config/cassandra.yaml\n\nindex.search.backend=elasticsearch\nindex.search.directory=/tmp/searchindex\nindex.search.elasticsearch.client-only=false\nindex.search.elasticsearch.local-mode=true\n</code></pre> <p>This configuration file configures JanusGraph to start Cassandra internally embedded in JanusGraph and specifies the yaml configuration file for Cassandra. Cassandra is still accessible externally and can connect to other available Cassandra nodes to form a cluster as configured in the yaml file.</p> <p>The optional index backend configuration is identical to embedded index configuration described above.</p>"},{"location":"basics/example-config/#hbase","title":"HBase","text":"<pre><code>storage.backend=hbase\nstorage.hostname=127.0.0.1\nstorage.port=2181\n\nindex.search.backend=elasticsearch\nindex.search.hostname=127.0.0.1\nindex.search.elasticsearch.client-only=true\n</code></pre> <p>This configuration file configures JanusGraph to use HBase as a remote storage backend. It assumes that an HBase cluster is running and accessible at the given IP addresses through the configured port. If HBase is running locally, use the IP address <code>127.0.0.1</code>.</p> <p>The optional index backend configuration is identical to remote index configuration described above.</p>"},{"location":"basics/gremlin/","title":"Gremlin Query Language","text":"<p>Gremlin is JanusGraph\u2019s query language used to retrieve data from and modify data in the graph. Gremlin is a path-oriented language which succinctly expresses complex graph traversals and mutation operations. Gremlin is a functional language whereby traversal operators are chained together to form path-like expressions. For example, \"from Hercules, traverse to his father and then his father\u2019s father and return the grandfather\u2019s name.\"</p> <p>Gremlin is a component of Apache TinkerPop. It is developed independently from JanusGraph and is supported by most graph databases. By building applications on top of JanusGraph through the Gremlin query language, users avoid vendor-lock in because their application can be migrated to other graph databases supporting Gremlin.</p> <p>This section is a brief overview of the Gremlin query language. For more information on Gremlin, refer to the following resources:</p> <ul> <li> <p>Complete Gremlin Manual: Reference manual for all of the Gremlin steps.</p> </li> <li> <p>Gremlin Console Tutorial: Learn how to use the Gremlin Console effectively to traverse and analyze a graph interactively.</p> </li> <li> <p>Gremlin Recipes: A collection of best practices and common traversal patterns for Gremlin.</p> </li> <li> <p>Gremlin Language Drivers:     Connect to a Gremlin Server with different programming languages,     including Go, JavaScript, .NET/C#, PHP, Python, Ruby, Scala, and     TypeScript.</p> </li> <li> <p>Gremlin Language Variants: Learn how to embed Gremlin in a host programming language.</p> </li> <li> <p>Gremlin for SQL developers: Learn Gremlin     using typical patterns found when querying data with SQL.</p> </li> </ul> <p>In addition to these resources, Connecting to JanusGraph explains how Gremlin can be used in different programming languages to query a JanusGraph Server.</p>"},{"location":"basics/gremlin/#introductory-traversals","title":"Introductory Traversals","text":"<p>A Gremlin query is a chain of operations/functions that are evaluated from left to right. A simple grandfather query is provided below over the Graph of the Gods dataset discussed in Getting Started. <pre><code>gremlin&gt; g.V().has('name', 'hercules').out('father').out('father').values('name')\n==&gt;saturn\n</code></pre></p> <p>The query above can be read:</p> <ol> <li><code>g</code>: for the current graph traversal.</li> <li><code>V</code>: for all vertices in the graph</li> <li><code>has('name', 'hercules')</code>: filters the vertices down to those with name property \"hercules\" (there is only one).</li> <li><code>out('father')</code>: traverse outgoing father edge\u2019s from Hercules.</li> <li>\u2018out('father')`: traverse outgoing father edge\u2019s from Hercules\u2019 father\u2019s vertex (i.e. Jupiter).</li> <li><code>name</code>: get the name property of the \"hercules\" vertex\u2019s grandfather.</li> </ol> <p>Taken together, these steps form a path-like traversal query. Each step can be decomposed and its results demonstrated. This style of building up a traversal/query is useful when constructing larger, complex query chains.</p> <pre><code>gremlin&gt; g\n==&gt;graphtraversalsource[janusgraph[cql:127.0.0.1], standard]\ngremlin&gt; g.V().has('name', 'hercules')\n==&gt;v[24]\ngremlin&gt; g.V().has('name', 'hercules').out('father')\n==&gt;v[16]\ngremlin&gt; g.V().has('name', 'hercules').out('father').out('father')\n==&gt;v[20]\ngremlin&gt; g.V().has('name', 'hercules').out('father').out('father').values('name')\n==&gt;saturn\n</code></pre> <p>For a sanity check, it is usually good to look at the properties of each return, not the assigned long id. <pre><code>gremlin&gt; g.V().has('name', 'hercules').values('name')\n==&gt;hercules\ngremlin&gt; g.V().has('name', 'hercules').out('father').values('name')\n==&gt;jupiter\ngremlin&gt; g.V().has('name', 'hercules').out('father').out('father').values('name')\n==&gt;saturn\n</code></pre></p> <p>Note the related traversal that shows the entire father family tree branch of Hercules. This more complicated traversal is provided in order to demonstrate the flexibility and expressivity of the language. A competent grasp of Gremlin provides the JanusGraph user the ability to fluently navigate the underlying graph structure. <pre><code>gremlin&gt; g.V().has('name', 'hercules').repeat(out('father')).emit().values('name')\n==&gt;jupiter\n==&gt;saturn\n</code></pre></p> <p>Some more traversal examples are provided below. <pre><code>gremlin&gt; hercules = g.V().has('name', 'hercules').next()\n==&gt;v[1536]\ngremlin&gt; g.V(hercules).out('father', 'mother').label()\n==&gt;god\n==&gt;human\ngremlin&gt; g.V(hercules).out('battled').label()\n==&gt;monster\n==&gt;monster\n==&gt;monster\ngremlin&gt; g.V(hercules).out('battled').valueMap()\n==&gt;{name=nemean}\n==&gt;{name=hydra}\n==&gt;{name=cerberus}\n</code></pre> Each step (denoted by a separating .) is a function that operates on the objects emitted from the previous step. There are numerous steps in the Gremlin language (see Gremlin Steps). By simply changing a step or order of the steps, different traversal semantics are enacted. The example below returns the name of all the people that have battled the same monsters as Hercules who themselves are not Hercules (i.e. \"co-battlers\" or perhaps, \"allies\").</p> <p>Given that The Graph of the Gods only has one battler (Hercules), another battler (for the sake of example) is added to the graph with Gremlin showcasing how vertices and edges are added to the graph. <pre><code>gremlin&gt; theseus = graph.addVertex('human')\n==&gt;v[3328]\ngremlin&gt; theseus.property('name', 'theseus')\n==&gt;null\ngremlin&gt; cerberus = g.V().has('name', 'cerberus').next()\n==&gt;v[2816]\ngremlin&gt; battle = theseus.addEdge('battled', cerberus, 'time', 22)\n==&gt;e[7eo-2kg-iz9-268][3328-battled-&gt;2816]\ngremlin&gt; battle.values('time')\n==&gt;22\n</code></pre></p> <p>When adding a vertex, an optional vertex label can be provided. An edge label must be specified when adding edges. Properties as key-value pairs can be set on both vertices and edges. When a property key is defined with SET or LIST cardinality, <code>addProperty</code> must be used when adding a respective property to a vertex. <pre><code>gremlin&gt; g.V(hercules).as('h').out('battled').in('battled').where(neq('h')).values('name')\n==&gt;theseus\n</code></pre></p> <p>The example above has 4 chained functions: <code>out</code>, <code>in</code>, <code>except</code>, and <code>values</code> (i.e. <code>name</code> is shorthand for <code>values('name')</code>). The function signatures of each are itemized below, where <code>V</code> is vertex and <code>U</code> is any object, where <code>V</code> is a subset of <code>U</code>.</p> <ol> <li><code>out: V -&gt; V</code></li> <li><code>in: V -&gt; V</code></li> <li><code>except: U -&gt; U</code></li> <li><code>values: V -&gt; U</code></li> </ol> <p>When chaining together functions, the incoming type must match the outgoing type, where <code>U</code> matches anything. Thus, the \"co-battled/ally\" traversal above is correct.</p> <p>Note</p> <p>The Gremlin overview presented in this section focused on the Gremlin-Groovy language implementation used in the Gremlin Console. Refer to Connecting to JanusGraph for information about connecting to JanusGraph with other languages than Groovy and independent of the Gremlin Console.</p>"},{"location":"basics/gremlin/#iterating-the-traversal","title":"Iterating the Traversal","text":"<p>One convenient feature of the Gremlin Console is that it automatically iterates all results from a query executed from the gremlin&gt; prompt. This works well within the REPL environment as it shows you the results as a String. As you transition towards writing a Gremlin application, it is important to understand how to iterate a traversal explicitly because your application\u2019s traversals will not iterate automatically. These are some of the common ways to iterate the Traversal:</p> <ul> <li><code>iterate()</code> - Zero results are expected or can be ignored.</li> <li><code>next()</code> - Get one result. Make sure to check <code>hasNext()</code> first.</li> <li><code>next(int n)</code> - Get the next <code>n</code> results. Make sure to check <code>hasNext()</code> first.</li> <li><code>toList()</code> - Get all results as a list. If there are no results, an empty list is returned.</li> </ul> <p>A Java code example is shown below to demonstrate these concepts: <pre><code>Traversal t = g.V().has(\"name\", \"pluto\"); // Define a traversal\n// Note the traversal is not executed/iterated yet\nVertex pluto = null;\nif (t.hasNext()) { // Check if results are available\n    pluto = g.V().has(\"name\", \"pluto\").next(); // Get one result\n    g.V(pluto).drop().iterate(); // Execute a traversal to drop pluto from graph\n}\n// Note the traversal can be cloned for reuse\nTraversal tt = t.asAdmin().clone();\nif (tt.hasNext()) {\n    System.err.println(\"pluto was not dropped!\");\n}\nList&lt;Vertex&gt; gods = g.V().hasLabel(\"god\").toList(); // Find all the gods\n</code></pre></p>"},{"location":"basics/index-lifecycle/","title":"Index Lifecycle","text":"<p>JanusGraph uses only indexes which have status <code>ENABLED</code>.  When the index is created it will not be used by JanusGraph until it is enabled.  After the index is build you should wait until it is registered (i.e. available) by JanusGraph: <pre><code>//Wait for the index to become available (i.e. wait for status REGISTERED)\nManagementSystem.awaitGraphIndexStatus(graph, \"myIndex\").call();\n</code></pre></p> <p>After the index is registered we should either enable the index (if we are sure that the current data should not be indexed by the newly created index) or we should reindex current data so that it would be available in the newly created index.</p> <p>Reindex the existing data and automatically enable the index example: <pre><code>mgmt = graph.openManagement();\nmgmt.updateIndex(mgmt.getGraphIndex(\"myIndex\"), SchemaAction.REINDEX).get();\nmgmt.commit();\n</code></pre></p> <p>Enable the index without reindexing existing data example: <pre><code>mgmt = graph.openManagement();\nmgmt.updateIndex(mgmt.getGraphIndex(\"myAnotherIndex\"), SchemaAction.ENABLE_INDEX).get();\nmgmt.commit();\n</code></pre></p>"},{"location":"basics/index-lifecycle/#index-states-and-transitions","title":"Index states and transitions","text":""},{"location":"basics/index-lifecycle/#states-schemastatus","title":"States (SchemaStatus)","text":"<p>An index can be in one of the following states:</p> <ul> <li> <p>INSTALLED The index is installed in the system but not yet registered with all instances in the cluster</p> </li> <li> <p>REGISTERED The index is registered with all instances in the cluster but not (yet) enabled</p> </li> <li> <p>ENABLED The index is enabled and in use</p> </li> <li> <p>DISABLED The index is disabled and no longer in use</p> </li> </ul>"},{"location":"basics/index-lifecycle/#actions-schemaaction","title":"Actions (SchemaAction)","text":"<p>The following actions can be performed on an index to change its state via <code>mgmt.updateIndex()</code>:</p> <ul> <li> <p>REGISTER_INDEX Registers the index with all instances in the graph cluster. After an index is installed, it must be registered with all graph instances</p> </li> <li> <p>REINDEX Re-builds the index from the graph</p> </li> <li> <p>ENABLE_INDEX Enables the index so that it can be used by the query processing engine. An index must be registered before it can be enabled.</p> </li> <li> <p>DISABLE_INDEX Disables the index in the graph so that it is no longer used.</p> </li> <li> <p>REMOVE_INDEX Removes the index from the graph (optional operation). Only on composite index.</p> </li> </ul>"},{"location":"basics/index-performance/","title":"Indexing for Better Performance","text":"<p>JanusGraph supports two different kinds of indexing to speed up query processing: graph indexes and vertex-centric indexes. Most graph queries start the traversal from a list of vertices or edges that are identified by their properties. Graph indexes make these global retrieval operations efficient on large graphs. Vertex-centric indexes speed up the actual traversal through the graph, in particular when traversing through vertices with many incident edges.</p>"},{"location":"basics/index-performance/#graph-index","title":"Graph Index","text":"<p>Graph indexes are global index structures over the entire graph which allow efficient retrieval of vertices or edges by their properties for sufficiently selective conditions. For instance, consider the following queries <pre><code>g.V().has('name', 'hercules')\ng.E().has('reason', textContains('loves'))\n</code></pre></p> <p>The first query asks for all vertices with the name <code>hercules</code>. The second asks for all edges where the property reason contains the word <code>loves</code>. Without a graph index answering those queries would require a full scan over all vertices or edges in the graph to find those that match the given condition which is very inefficient and infeasible for huge graphs.</p> <p>JanusGraph distinguishes between two types of graph indexes: composite and mixed indexes. Composite indexes are very fast and efficient but limited to equality lookups for a particular, previously-defined combination of property keys. Mixed indexes can be used for lookups on any combination of indexed keys and support multiple condition predicates in addition to equality depending on the backing index store.</p> <p>Both types of indexes are created through the JanusGraph management system and the index builder returned by <code>JanusGraphManagement.buildIndex(String, Class)</code> where the first argument defines the name of the index and the second argument specifies the type of element to be indexed (e.g. <code>Vertex.class</code>). The name of a graph index must be unique. Graph indexes built against newly defined property keys, i.e. property keys that are defined in the same management transaction as the index, are immediately available. The same applies to graph indexes that are constrained to a label that is created in the same management transaction as the index. Graph indexes built against property keys that are already in use without being constrained to a newly created label require the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the index will not be available. It is encouraged to define graph indexes in the same transaction as the initial schema.</p> <p>Note</p> <p>In the absence of an index, JanusGraph will default to a full graph scan in order to retrieve the desired list of vertices. While this produces the correct result set, the graph scan can be very inefficient and lead to poor overall system performance in a production environment. Enable the <code>force-index</code> configuration option in production deployments of JanusGraph to prohibit graph scans.</p> <p>Info</p> <p>See index lifecycle documentation for more information about index states.</p>"},{"location":"basics/index-performance/#composite-index","title":"Composite Index","text":"<p>Composite indexes retrieve vertices or edges by one or a (fixed) composition of multiple keys. Consider the following composite index definitions. <pre><code>graph.tx().rollback() //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\nage = mgmt.getPropertyKey('age')\nmgmt.buildIndex('byNameComposite', Vertex.class).addKey(name).buildCompositeIndex()\nmgmt.buildIndex('byNameAndAgeComposite', Vertex.class).addKey(name).addKey(age).buildCompositeIndex()\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameComposite').call()\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameAndAgeComposite').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameComposite\"), SchemaAction.REINDEX).get()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameAndAgeComposite\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>First, two property keys <code>name</code> and <code>age</code> are already defined. Next, a simple composite index on just the name property key is built. JanusGraph will use this index to answer the following query. <pre><code>g.V().has('name', 'hercules')\n</code></pre></p> <p>The second composite graph index includes both keys. JanusGraph will use this index to answer the following query. <pre><code>g.V().has('age', 30).has('name', 'hercules')\n</code></pre></p> <p>Note, that all keys of a composite graph index must be found in the query\u2019s equality conditions for this index to be used. For example, the following query cannot be answered with either of the indexes because it only contains a constraint on <code>age</code> but not <code>name</code>. <pre><code>g.V().has('age', 30)\n</code></pre></p> <p>Also note, that composite graph indexes can only be used for equality constraints like those in the queries above. The following query would be answered with just the simple composite index defined on the <code>name</code> key because the age constraint is not an equality constraint. <pre><code>g.V().has('name', 'hercules').has('age', inside(20, 50))\n</code></pre></p> <p>Composite indexes do not require configuration of an external indexing backend and are supported through the primary storage backend. Hence, composite index modifications are persisted through the same transaction as graph modifications which means that those changes are atomic and/or consistent if the underlying storage backend supports atomicity and/or consistency.</p> <p>Note</p> <p>A composite index may comprise just one or multiple keys. A composite index with just one key is sometimes referred to as a key-index.</p>"},{"location":"basics/index-performance/#index-uniqueness","title":"Index Uniqueness","text":"<p>Composite indexes can also be used to enforce property uniqueness in the graph. If a composite graph index is defined as <code>unique()</code> there can be at most one vertex or edge for any given concatenation of property values associated with the keys of that index. For instance, to enforce that names are unique across the entire graph the following composite graph index would be defined. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\nmgmt.buildIndex('byNameUnique', Vertex.class).addKey(name).unique().buildCompositeIndex()\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameUnique').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameUnique\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>Note</p> <p>To enforce uniqueness against an eventually consistent storage backend, the consistency of the index must be explicitly set to enabling locking.</p>"},{"location":"basics/index-performance/#mixed-index","title":"Mixed Index","text":"<p>Mixed indexes retrieve vertices or edges by any combination of previously added property keys. Mixed indexes provide more flexibility than composite indexes and support additional condition predicates beyond equality. On the other hand, mixed indexes are slower for most equality queries than composite indexes.</p> <p>Unlike composite indexes, mixed indexes require the configuration of an indexing backend and use that indexing backend to execute lookup operations. JanusGraph can support multiple indexing backends in a single installation. Each indexing backend must be uniquely identified by name in the JanusGraph configuration which is called the indexing backend name. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\nage = mgmt.getPropertyKey('age')\nmgmt.buildIndex('nameAndAge', Vertex.class).addKey(name).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'nameAndAge').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"nameAndAge\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>The example above defines a mixed index containing the property keys <code>name</code> and <code>age</code>. The definition refers to the indexing backend name <code>search</code> so that JanusGraph knows which configured indexing backend it should use for this particular index. The <code>search</code> parameter specified in the buildMixedIndex call must match the second clause in the JanusGraph configuration definition like this: index.search.backend If the index was named solrsearch then the configuration definition would appear like this: index.solrsearch.backend.</p> <p>The mgmt.buildIndex example specified above uses text search as its default behavior. An index statement that explicitly defines the index as a text index can be written as follows: <pre><code>mgmt.buildIndex('nameAndAge',Vertex.class).addKey(name,Mapping.TEXT.getParameter()).addKey(age,Mapping.TEXT.getParameter()).buildMixedIndex(\"search\")\n</code></pre></p> <p>See Index Parameters and Full-Text Search for more information on text and string search options, and see the documentation section specific to the indexing backend in use for more details on how each backend handles text versus string searches.</p> <p>While the index definition example looks similar to the composite index above, it provides greater query support and can answer any of the following queries. <pre><code>g.V().has('name', textContains('hercules')).has('age', inside(20, 50))\ng.V().has('name', textContains('hercules'))\ng.V().has('age', lt(50))\n</code></pre></p> <p>Mixed indexes support full-text search, range search, geo search and others. Refer to Search Predicates and Data Types for a list of predicates supported by a particular indexing backend.</p> <p>Note</p> <p>Unlike composite indexes, mixed indexes do not support uniqueness.</p>"},{"location":"basics/index-performance/#adding-property-keys","title":"Adding Property Keys","text":"<p>Property keys can be added to an existing mixed index which allows subsequent queries to include this key in the query condition. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nlocation = mgmt.makePropertyKey('location').dataType(Geoshape.class).make()\nnameAndAge = mgmt.getGraphIndex('nameAndAge')\nmgmt.addIndexKey(nameAndAge, location)\nmgmt.commit()\n//Previously created property keys already have the status ENABLED, but\n//our newly created property key \"location\" needs to REGISTER so we wait for both statuses\nManagementSystem.awaitGraphIndexStatus(graph, 'nameAndAge').status(SchemaStatus.REGISTERED, SchemaStatus.ENABLED).call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"nameAndAge\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>To add a newly defined key, we first retrieve the existing index from the management transaction by its name and then invoke the <code>addIndexKey</code> method to add the key to this index.</p> <p>If the added key is defined in the same management transaction, it will be immediately available for querying. If the property key has already been in use, adding the key requires the execution of a reindex procedure to ensure that the index contains all previously added elements. Until the reindex procedure has completed, the key will not be available in the mixed index.</p>"},{"location":"basics/index-performance/#mapping-parameters","title":"Mapping Parameters","text":"<p>When adding a property key to a mixed index - either through the index builder or the <code>addIndexKey</code> method - a list of parameters can be optionally specified to adjust how the property value is mapped into the indexing backend. Refer to the mapping parameters overview for a complete list of parameter types supported by each indexing backend.</p>"},{"location":"basics/index-performance/#ordering","title":"Ordering","text":"<p>The order in which the results of a graph query are returned can be defined using the <code>order().by()</code> directive. The <code>order().by()</code> method expects two parameters:</p> <ul> <li> <p>The name of the property key by which to order the results. The     results will be ordered by the value of the vertices or edges for     this property key.</p> </li> <li> <p>The sort order: either increasing <code>incr</code> or decreasing <code>decr</code></p> </li> </ul> <p>For example, the query <code>g.V().has('name', textContains('hercules')).order().by('age', decr).limit(10)</code> retrieves the ten oldest individuals with hercules in their name.</p> <p>When using <code>order().by()</code> it is important to note that:</p> <ul> <li> <p>Composite graph indexes do not natively support ordering search     results. All results will be retrieved and then sorted in-memory.     For large result sets, this can be very expensive.</p> </li> <li> <p>Mixed indexes support ordering natively and efficiently. However,     the property key used in the order().by() method must have been     previously added to the mixed indexed for native result ordering     support. This is important in cases where the the order().by() key     is different from the query keys. If the property key is not part of     the index, then sorting requires loading all results into memory.</p> </li> </ul>"},{"location":"basics/index-performance/#label-constraint","title":"Label Constraint","text":"<p>In many cases it is desirable to only index vertices or edges with a particular label. For instance, one may want to index only gods by their name and not every single vertex that has a name property. When defining an index it is possible to restrict the index to a particular vertex or edge label using the <code>indexOnly</code> method of the index builder. The following creates a composite index for the property key <code>name</code> that indexes only vertices labeled <code>god</code>. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\nname = mgmt.getPropertyKey('name')\ngod = mgmt.getVertexLabel('god')\nmgmt.buildIndex('byNameAndLabel', Vertex.class).addKey(name).indexOnly(god).buildCompositeIndex()\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitGraphIndexStatus(graph, 'byNameAndLabel').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getGraphIndex(\"byNameAndLabel\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>Label restrictions similarly apply to mixed indexes. When a composite index with label restriction is defined as unique, the uniqueness constraint only applies to properties on vertices or edges for the specified label.</p>"},{"location":"basics/index-performance/#composite-versus-mixed-indexes","title":"Composite versus Mixed Indexes","text":"<ol> <li> <p>Use a composite index for exact match index retrievals. Composite     indexes do not require configuring or operating an external index     system and are often significantly faster than mixed indexes.</p> <ol> <li>As an exception, use a mixed index for exact matches when the     number of distinct values for query constraint is relatively     small or if one value is expected to be associated with many     elements in the graph (i.e. in case of low selectivity).</li> </ol> </li> <li> <p>Use a mixed indexes for numeric range, full-text or geo-spatial     indexing. Also, using a mixed index can speed up the order().by()     queries.</p> </li> </ol>"},{"location":"basics/index-performance/#vertex-centric-indexes","title":"Vertex-centric Indexes","text":"<p>Vertex-centric indexes are local index structures built individually per vertex. In large graphs vertices can have thousands of incident edges. Traversing through those vertices can be very slow because a large subset of the incident edges has to be retrieved and then filtered in memory to match the conditions of the traversal. Vertex-centric indexes can speed up such traversals by using localized index structures to retrieve only those edges that need to be traversed.</p> <p>Suppose that Hercules battled hundreds of monsters in addition to the three captured in the introductory Graph of the Gods. Without a vertex-centric index, a query asking for those monsters battled between time point <code>10</code> and <code>20</code> would require retrieving all <code>battled</code> edges even though there are only a handful of matching edges. <pre><code>h = g.V().has('name', 'hercules').next()\ng.V(h).outE('battled').has('time', inside(10, 20)).inV()\n</code></pre></p> <p>Building a vertex-centric index by time speeds up such traversal queries. Note, this initial index example already exists in the Graph of the Gods as an index named <code>edges</code>. As a result, running the steps below will result in a uniqueness constraint error. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\ntime = mgmt.getPropertyKey('time')\nbattled = mgmt.getEdgeLabel('battled')\nmgmt.buildEdgeIndex(battled, 'battlesByTime', Direction.BOTH, Order.decr, time)\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitRelationIndexStatus(graph, 'battlesByTime').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getRelationIndex(battled, \"battlesByTime\"), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>This example builds a vertex-centric index which indexes <code>battled</code> edges in both direction by time in decreasing order. A vertex-centric index is built against a particular edge label which is the first argument to the index construction method <code>JanusGraphManagement.buildEdgeIndex()</code>. The index only applies to edges of this label - <code>battled</code> in the example above. The second argument is a unique name for the index. The third argument is the edge direction in which the index is built. The index will only apply to traversals along edges in this direction. In this example, the vertex-centric index is built in both direction which means that time restricted traversals along <code>battled</code> edges can be served by this index in both the <code>IN</code> and <code>OUT</code> direction. JanusGraph will maintain a vertex-centric index on both the in- and out-vertex of <code>battled</code> edges. Alternatively, one could define the index to apply to the <code>OUT</code> direction only which would speed up traversals from Hercules to the monsters but not in the reverse direction. This would only require maintaining one index and hence half the index maintenance and storage cost. The last two arguments are the sort order of the index and a list of property keys to index by. The sort order is optional and defaults to ascending order (i.e. <code>Order.ASC</code>). The list of property keys must be non-empty and defines the keys by which to index the edges of the given label. A vertex-centric index can be defined with multiple keys. <pre><code>graph.tx().rollback()  //Never create new indexes while a transaction is active\nmgmt = graph.openManagement()\ntime = mgmt.getPropertyKey('time')\nrating = mgmt.makePropertyKey('rating').dataType(Double.class).make()\nbattled = mgmt.getEdgeLabel('battled')\nmgmt.buildEdgeIndex(battled, 'battlesByRatingAndTime', Direction.OUT, Order.decr, rating, time)\nmgmt.commit()\n//Wait for the index to become available\nManagementSystem.awaitRelationIndexStatus(graph, 'battlesByRatingAndTime', 'battled').call()\n//Reindex the existing data\nmgmt = graph.openManagement()\nmgmt.updateIndex(mgmt.getRelationIndex(battled, 'battlesByRatingAndTime'), SchemaAction.REINDEX).get()\nmgmt.commit()\n</code></pre></p> <p>This example extends the schema by a <code>rating</code> property on <code>battled</code> edges and builds a vertex-centric index which indexes <code>battled</code> edges in the out-going direction by rating and time in decreasing order. Note, that the order in which the property keys are specified is important because vertex-centric indexes are prefix indexes. This means, that <code>battled</code> edges are indexed by <code>rating</code> first and <code>time</code> second. <pre><code>h = g.V().has('name', 'hercules').next()\ng.V(h).outE('battled').property('rating', 5.0) //Add some rating properties\ng.V(h).outE('battled').has('rating', gt(3.0)).inV()\ng.V(h).outE('battled').has('rating', 5.0).has('time', inside(10, 50)).inV()\ng.V(h).outE('battled').has('time', inside(10, 50)).inV()\n</code></pre></p> <p>Hence, the <code>battlesByRatingAndTime</code> index can speed up the first two but not the third query.</p> <p>Multiple vertex-centric indexes can be built for the same edge label in order to support different constraint traversals. JanusGraph\u2019s query optimizer attempts to pick the most efficient index for any given traversal. Vertex-centric indexes only support equality and range/interval constraints.</p> <p>Note</p> <p>The property keys used in a vertex-centric index must have an explicitly defined data type (i.e. not <code>Object.class</code>) which supports a native sort order.</p> <p>If the vertex-centric index is built against an edge label that is defined in the same management transaction, the index will be immediately available for querying. If the edge label has already been in use, building a vertex-centric index against it requires the execution of a reindex procedure to ensure that the index contains all previously added edges. Until the reindex procedure has completed, the index will not be available.</p> <p>Note</p> <p>JanusGraph automatically builds vertex-centric indexes per edge label and property key. That means, even with thousands of incident <code>battled</code> edges, queries like <code>g.V(h).out('mother')</code> or <code>g.V(h).values('age')</code> are efficiently answered by the local index.</p> <p>Vertex-centric indexes cannot speed up unconstrained traversals which require traversing through all incident edges of a particular label. Those traversals will become slower as the number of incident edges increases. Often, such traversals can be rewritten as constrained traversals that can utilize a vertex-centric index to ensure acceptable performance at scale.</p>"},{"location":"basics/index-performance/#ordered-traversals","title":"Ordered Traversals","text":"<p>The following queries specify an order in which the incident edges are to be traversed. Use the <code>localLimit</code> command to retrieve a subset of the edges (in a given order) for EACH vertex that is traversed. <pre><code>h = g..V().has('name', 'hercules').next()\ng.V(h).local(outE('battled').order().by('time', decr).limit(10)).inV().values('name')\ng.V(h).local(outE('battled').has('rating', 5.0).order().by('time', decr).limit(10)).values('place')\n</code></pre></p> <p>The first query asks for the names of the 10 most recently battled monsters by Hercules. The second query asks for the places of the 10 most recent battles of Hercules that are rated 5 stars. In both cases, the query is constrained by an order on a property key with a limit on the number of elements to be returned.</p> <p>Such queries can also be efficiently answered by vertex-centric indexes if the order key matches the key of the index and the requested order (i.e. increasing or decreasing) is the same as the one defined for the index. The <code>battlesByTime</code> index would be used to answer the first query and <code>battlesByRatingAndTime</code> applies to the second. Note, that the <code>battlesByRatingAndTime</code> index cannot be used to answer the first query because an equality constraint on <code>rating</code> must be present for the second key in the index to be effective.</p> <p>Note</p> <p>Ordered vertex queries are a JanusGraph extension to Gremlin which causes the verbose syntax and requires the <code>_()</code> step to convert the JanusGraph result back into a Gremlin pipeline.</p>"},{"location":"basics/janusgraph-cfg/","title":"Janusgraph cfg","text":""},{"location":"basics/janusgraph-cfg/#attributescustom","title":"attributes.custom *","text":"<p>Custom attribute serialization and handling</p> Name Description Datatype Default Value Mutability attributes.custom.[X].attribute-class Class of the custom attribute to be registered String (no default value) GLOBAL_OFFLINE attributes.custom.[X].serializer-class Class of the custom attribute serializer to be registered String (no default value) GLOBAL_OFFLINE"},{"location":"basics/janusgraph-cfg/#cache","title":"cache","text":"<p>Configuration options that modify JanusGraph's caching behavior</p> Name Description Datatype Default Value Mutability cache.db-cache Whether to enable JanusGraph's database-level cache, which is shared across all transactions. Enabling this option speeds up traversals by holding hot graph elements in memory, but also increases the likelihood of reading stale data.  Disabling it forces each transaction to independently fetch graph elements from storage before reading/writing them. Boolean false MASKABLE cache.db-cache-clean-wait How long, in milliseconds, database-level cache will keep entries after flushing them.  This option is only useful on distributed storage backends that are capable of acknowledging writes without necessarily making them immediately visible. Integer 50 GLOBAL_OFFLINE cache.db-cache-size Size of JanusGraph's database level cache.  Values between 0 and 1 are interpreted as a percentage of VM heap, while larger values are interpreted as an absolute size in bytes. Double 0.3 MASKABLE cache.db-cache-time Default expiration time, in milliseconds, for entries in the database-level cache. Entries are evicted when they reach this age even if the cache has room to spare. Set to 0 to disable expiration (cache entries live forever or until memory pressure triggers eviction when set to 0). Long 10000 GLOBAL_OFFLINE cache.tx-cache-size Maximum size of the transaction-level cache of recently-used vertices. Integer 20000 MASKABLE cache.tx-dirty-size Initial size of the transaction-level cache of uncommitted dirty vertices. This is a performance hint for write-heavy, performance-sensitive transactional workloads. If set, it should roughly match the median vertices modified per transaction. Integer (no default value) MASKABLE"},{"location":"basics/janusgraph-cfg/#cluster","title":"cluster","text":"<p>Configuration options for multi-machine deployments</p> Name Description Datatype Default Value Mutability cluster.max-partitions The number of virtual partition blocks created in the partitioned graph. This should be larger than the maximum expected number of nodesin the JanusGraph graph cluster. Must be bigger than 1 and a power of 2. Integer 32 FIXED"},{"location":"basics/janusgraph-cfg/#computer","title":"computer","text":"<p>GraphComputer related configuration</p> Name Description Datatype Default Value Mutability computer.result-mode How the graph computer should return the computed results. 'persist' for writing them into the graph, 'localtx' for writing them into the local transaction, or 'none' (default) String none MASKABLE"},{"location":"basics/janusgraph-cfg/#graph","title":"graph","text":"<p>General configuration options</p> Name Description Datatype Default Value Mutability graph.allow-stale-config Whether to allow the local and storage-backend-hosted copies of the configuration to contain conflicting values for options with any of the following types: FIXED, GLOBAL_OFFLINE, GLOBAL.  These types are managed globally through the storage backend and cannot be overridden by changing the local configuration.  This type of conflict usually indicates misconfiguration.  When this option is true, JanusGraph will log these option conflicts, but continue normal operation using the storage-backend-hosted value for each conflicted option.  When this option is false, JanusGraph will log these option conflicts, but then it will throw an exception, refusing to start. Boolean true MASKABLE graph.graphname This config option is an optional configuration setting that you may supply when opening a graph. The String value you provide will be the name of your graph. If you use the ConfigurationManagament APIs, then you will be able to access your graph by this String representation using the ConfiguredGraphFactory APIs. String (no default value) LOCAL graph.replace-instance-if-exists If a JanusGraph instance with the same instance identifier already exists, the usage of this configuration option results in the opening of this graph anwyay. Boolean false LOCAL graph.set-vertex-id Whether user provided vertex ids should be enabled and JanusGraph's automatic id allocation be disabled. Useful when operating JanusGraph in concert with another storage system that assigns long ids but disables some of JanusGraph's advanced features which can lead to inconsistent data. EXPERT FEATURE - USE WITH GREAT CARE. Boolean false FIXED graph.timestamps The timestamp resolution to use when writing to storage and indices. Sets the time granularity for the entire graph cluster. To avoid potential inaccuracies, the configured time resolution should match those of the backend systems. Some JanusGraph storage backends declare a preferred timestamp resolution that reflects design constraints in the underlying service. When the backend provides a preferred default, and when this setting is not explicitly declared in the config file, the backend default is used and the general default associated with this setting is ignored.  An explicit declaration of this setting overrides both the general and backend-specific defaults. TimestampProviders MICRO FIXED graph.unique-instance-id Unique identifier for this JanusGraph instance.  This must be unique among all instances concurrently accessing the same stores or indexes.  It's automatically generated by concatenating the hostname, process id, and a static (process-wide) counter. Leaving it unset is recommended. String (no default value) LOCAL graph.unique-instance-id-suffix When this is set and unique-instance-id is not, this JanusGraph instance's unique identifier is generated by concatenating the hex encoded hostname to the provided number. Short (no default value) LOCAL graph.use-hostname-for-unique-instance-id When this is set, this JanusGraph's unique instance identifier is set to the hostname. If unique-instance-id-suffix is also set, then the identifier is set to . Boolean false LOCAL"},{"location":"basics/janusgraph-cfg/#gremlin","title":"gremlin","text":"<p>Gremlin configuration options</p> Name Description Datatype Default Value Mutability gremlin.graph The implementation of graph factory that will be used by gremlin server String org.janusgraph.core.JanusGraphFactory LOCAL"},{"location":"basics/janusgraph-cfg/#ids","title":"ids","text":"<p>General configuration options for graph element IDs</p> Name Description Datatype Default Value Mutability ids.block-size Globally reserve graph element IDs in chunks of this size.  Setting this too low will make commits frequently block on slow reservation requests.  Setting it too high will result in IDs wasted when a graph instance shuts down with reserved but mostly-unused blocks. Integer 10000 GLOBAL_OFFLINE ids.flush When true, vertices and edges are assigned IDs immediately upon creation.  When false, IDs are assigned only when the transaction commits. Must be disabled for graph partitioning to work. Boolean true MASKABLE ids.num-partitions Number of partition block to allocate for placement of vertices Integer 10 MASKABLE ids.placement Name of the vertex placement strategy or full class name String simple MASKABLE ids.renew-percentage When the most-recently-reserved ID block has only this percentage of its total IDs remaining (expressed as a value between 0 and 1), JanusGraph asynchronously begins reserving another block. This helps avoid transaction commits waiting on ID reservation even if the block size is relatively small. Double 0.3 MASKABLE ids.renew-timeout The number of milliseconds that the JanusGraph id pool manager will wait before giving up on allocating a new block of ids Duration 120000 ms MASKABLE ids.store-name The name of the ID KCVStore. IDS_STORE_NAME is meant to be used only for backward compatibility with Titan, and should not be used explicitly in normal operations or in new graphs. String janusgraph_ids GLOBAL_OFFLINE"},{"location":"basics/janusgraph-cfg/#idsauthority","title":"ids.authority","text":"<p>Configuration options for graph element ID reservation/allocation</p> Name Description Datatype Default Value Mutability ids.authority.conflict-avoidance-mode This setting helps separate JanusGraph instances sharing a single graph storage backend avoid contention when reserving ID blocks, increasing overall throughput. ConflictAvoidanceMode NONE GLOBAL_OFFLINE ids.authority.conflict-avoidance-tag Conflict avoidance tag to be used by this JanusGraph instance when allocating IDs Integer 0 LOCAL ids.authority.conflict-avoidance-tag-bits Configures the number of bits of JanusGraph-assigned element IDs that are reserved for the conflict avoidance tag Integer 4 FIXED ids.authority.randomized-conflict-avoidance-retries Number of times the system attempts ID block reservations with random conflict avoidance tags before giving up and throwing an exception Integer 5 MASKABLE ids.authority.wait-time The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend Duration 300 ms GLOBAL_OFFLINE"},{"location":"basics/janusgraph-cfg/#index","title":"index *","text":"<p>Configuration options for the individual indexing backends</p> Name Description Datatype Default Value Mutability index.[X].backend The indexing backend used to extend and optimize JanusGraph's query functionality. This setting is optional.  JanusGraph can use multiple heterogeneous index backends.  Hence, this option can appear more than once, so long as the user-defined name between \"index\" and \"backend\" is unique among appearances.Similar to the storage backend, this should be set to one of JanusGraph's built-in shorthand names for its standard index backends (shorthands: lucene, elasticsearch, es, solr) or to the full package and classname of a custom/third-party IndexProvider implementation. String elasticsearch GLOBAL_OFFLINE index.[X].conf-file Path to a configuration file for those indexing backends that require/support a separate config file String (no default value) MASKABLE index.[X].directory Directory to store index data locally String (no default value) MASKABLE index.[X].hostname The hostname or comma-separated list of hostnames of index backend servers.  This is only applicable to some index backends, such as elasticsearch and solr. String[] 127.0.0.1 MASKABLE index.[X].index-name Name of the index if required by the indexing backend String janusgraph GLOBAL_OFFLINE index.[X].map-name Whether to use the name of the property key as the field name in the index. It must be ensured, that theindexed property key names are valid field names. Renaming the property key will NOT rename the field and its the developers responsibility to avoid field collisions. Boolean true GLOBAL index.[X].max-result-set-size Maxium number of results to return if no limit is specified. For index backends that support scrolling, it represents the number of results in each batch Integer 50 MASKABLE index.[X].port The port on which to connect to index backend servers Integer (no default value) MASKABLE"},{"location":"basics/janusgraph-cfg/#indexxelasticsearch","title":"index.[X].elasticsearch","text":"<p>Elasticsearch index configuration</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.bulk-refresh Elasticsearch bulk API refresh setting used to control when changes made by this request are made visible to search String false MASKABLE index.[X].elasticsearch.health-request-timeout When JanusGraph initializes its ES backend, JanusGraph waits up to this duration for the ES cluster health to reach at least yellow status.  This string should be formatted as a natural number followed by the lowercase letter \"s\", e.g. 3s or 60s. String 30s MASKABLE index.[X].elasticsearch.interface Interface for connecting to Elasticsearch. TRANSPORT_CLIENT and NODE were previously supported, but now are required to migrate to REST_CLIENT. See the JanusGraph upgrade instructions for more details. String REST_CLIENT MASKABLE index.[X].elasticsearch.max-retry-timeout Sets the maximum timeout (in milliseconds) to honour in case of multiple retries of the same request sent using the ElasticSearch Rest Client by JanusGraph. Integer (no default value) MASKABLE index.[X].elasticsearch.scroll-keep-alive How long (in secondes) elasticsearch should keep alive the scroll context. Integer 60 GLOBAL_OFFLINE index.[X].elasticsearch.use-all-field Whether JanusGraph should add an \"all\" field mapping. When enabled field mappings will include a \"copy_to\" parameter referencing the \"all\" field. This is supported since Elasticsearch 6.x  and is required when using wildcard fields starting in Elasticsearch 6.x. Boolean true GLOBAL_OFFLINE index.[X].elasticsearch.use-deprecated-multitype-index Whether JanusGraph should group these indices into a single Elasticsearch index (requires Elasticsearch 5.x or earlier). Boolean false GLOBAL_OFFLINE"},{"location":"basics/janusgraph-cfg/#indexxelasticsearchcreate","title":"index.[X].elasticsearch.create","text":"<p>Settings related to index creation</p> Name Description Datatype Default Value Mutability index.[X].elasticsearch.create.allow-mapping-update Whether JanusGraph should allow a mapping update when registering an index. Only applicable when use-external-mappings is true. Boolean false MASKABLE index.[X].elasticsearch.create.sleep How long to sleep, in milliseconds, between the successful completion of a (blocking) index creation request and the first use of that index.  This only applies when creating an index in ES, which typically only happens the first time JanusGraph is started on top of ES. If the index JanusGraph is configured to use already exists, then this setting has no effect. Long 200 MASKABLE index.[X].elasticsearch.create.use-external-mappings Whether JanusGraph should make use of an external mapping when registering an index. Boolean false MASKABLE"},{"location":"basics/janusgraph-cfg/#indexxsolr","title":"index.[X].solr","text":"<p>Solr index configuration</p> Name Description Datatype Default Value Mutability index.[X].solr.configset If specified, the same solr configSet can be resued for each new Collection that is created in SolrCloud. String (no default value) MASKABLE index.[X].solr.dyn-fields Whether to use dynamic fields (which appends the data type to the field name). If dynamic fields is disabledthe user must map field names and define them explicitly in the schema. Boolean true GLOBAL_OFFLINE index.[X].solr.http-compression Enable/disable compression on the HTTP connections made to Solr. Boolean false MASKABLE index.[X].solr.http-connection-timeout Solr HTTP connection timeout. Integer 5000 MASKABLE index.[X].solr.http-max Maximum number of HTTP connections in total to all Solr servers. Integer 100 MASKABLE index.[X].solr.http-max-per-host Maximum number of HTTP connections per Solr host. Integer 20 MASKABLE index.[X].solr.http-urls List of URLs to use to connect to Solr Servers (LBHttpSolrClient is used), don't add core or collection name to the URL. String[] http://localhost:8983/solr MASKABLE index.[X].solr.key-field-names Field name that uniquely identifies each document in Solr. Must be specified as a list of <code>collection=field</code>. String[] (no default value) GLOBAL index.[X].solr.max-shards-per-node Maximum number of shards per node. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.mode The operation mode for Solr which is either via HTTP (<code>http</code>) or using SolrCloud (<code>cloud</code>) String cloud GLOBAL_OFFLINE index.[X].solr.num-shards Number of shards for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.replication-factor Replication factor for a collection. This applies when creating a new collection which is only supported under the SolrCloud operation mode. Integer 1 GLOBAL_OFFLINE index.[X].solr.ttl_field Name of the TTL field for Solr collections. String ttl GLOBAL_OFFLINE index.[X].solr.wait-searcher When mutating - wait for the index to reflect new mutations before returning. This can have a negative impact on performance. Boolean false LOCAL index.[X].solr.zookeeper-url URL of the Zookeeper instance coordinating the SolrCloud cluster String[] localhost:2181 MASKABLE"},{"location":"basics/janusgraph-cfg/#log","title":"log *","text":"<p>Configuration options for JanusGraph's logging system</p> Name Description Datatype Default Value Mutability log.[X].backend Define the log backed to use String default GLOBAL_OFFLINE log.[X].fixed-partition Whether all log entries are written to one fixed partition even if the backend store is partitioned.This can cause imbalanced loads and should only be used on low volume logs Boolean false GLOBAL_OFFLINE log.[X].key-consistent Whether to require consistency for log reading and writing messages to the storage backend Boolean false MASKABLE log.[X].max-partitions The maximum number of partitions to use for logging. Setting up this many actual or virtual partitions. Must be bigger than 0and a power of 2. Integer (no default value) FIXED log.[X].max-read-time Maximum time in ms to try reading log messages from the backend before failing. Duration 4000 ms MASKABLE log.[X].max-write-time Maximum time in ms to try persisting log messages against the backend before failing. Duration 10000 ms MASKABLE log.[X].num-buckets The number of buckets to split log entries into for load balancing Integer 1 GLOBAL_OFFLINE log.[X].read-batch-size Maximum number of log messages to read at a time for logging implementations that read messages in batches Integer 1024 MASKABLE log.[X].read-interval Time in ms between message readings from the backend for this logging implementations that read message in batch Duration 5000 ms MASKABLE log.[X].read-lag-time Maximum time in ms that it may take for reads to appear in the backend. If a write does not becomevisible in the storage backend in this amount of time, a log reader might miss the message. Duration 500 ms MASKABLE log.[X].read-threads Number of threads to be used in reading and processing log messages Integer 1 MASKABLE log.[X].send-batch-size Maximum number of log messages to batch up for sending for logging implementations that support batch sending Integer 256 MASKABLE log.[X].send-delay Maximum time in ms that messages can be buffered locally before sending in batch Duration 1000 ms MASKABLE log.[X].ttl Sets a TTL on all log entries, meaningthat all entries added to this log expire after the configured amount of time. Requiresthat the log implementation supports TTL. Duration (no default value) GLOBAL"},{"location":"basics/janusgraph-cfg/#metrics","title":"metrics","text":"<p>Configuration options for metrics reporting</p> Name Description Datatype Default Value Mutability metrics.enabled Whether to enable basic timing and operation count monitoring on backend Boolean false MASKABLE metrics.merge-stores Whether to aggregate measurements for the edge store, vertex index, edge index, and ID store Boolean true MASKABLE metrics.prefix The default name prefix for Metrics reported by JanusGraph. String org.janusgraph MASKABLE"},{"location":"basics/janusgraph-cfg/#metricsconsole","title":"metrics.console","text":"<p>Configuration options for metrics reporting to console</p> Name Description Datatype Default Value Mutability metrics.console.interval Time between Metrics reports printing to the console, in milliseconds Duration (no default value) MASKABLE"},{"location":"basics/janusgraph-cfg/#metricscsv","title":"metrics.csv","text":"<p>Configuration options for metrics reporting to CSV file</p> Name Description Datatype Default Value Mutability metrics.csv.directory Metrics CSV output directory String (no default value) MASKABLE metrics.csv.interval Time between dumps of CSV files containing Metrics data, in milliseconds Duration (no default value) MASKABLE"},{"location":"basics/janusgraph-cfg/#metricsganglia","title":"metrics.ganglia","text":"<p>Configuration options for metrics reporting through Ganglia</p> Name Description Datatype Default Value Mutability metrics.ganglia.addressing-mode Whether to communicate to Ganglia via uni- or multicast String unicast MASKABLE metrics.ganglia.hostname The unicast host or multicast group name to which Metrics will send Ganglia data String (no default value) MASKABLE metrics.ganglia.interval The number of milliseconds to wait between sending Metrics data to Ganglia Duration (no default value) MASKABLE metrics.ganglia.port The port to which Ganglia data are sent Integer 8649 MASKABLE metrics.ganglia.protocol-31 Whether to send data to Ganglia in the 3.1 protocol format Boolean true MASKABLE metrics.ganglia.spoof If non-null, it must be a valid Gmetric spoof string formatted as an IP:hostname pair. See https://github.com/ganglia/monitor-core/wiki/Gmetric-Spoofing for information about this setting. String (no default value) MASKABLE metrics.ganglia.ttl The multicast TTL to set on outgoing Ganglia datagrams Integer 1 MASKABLE metrics.ganglia.uuid The host UUID to set on outgoing Ganglia datagrams. See https://github.com/ganglia/monitor-core/wiki/UUIDSources for information about this setting. String (no default value) LOCAL"},{"location":"basics/janusgraph-cfg/#metricsgraphite","title":"metrics.graphite","text":"<p>Configuration options for metrics reporting through Graphite</p> Name Description Datatype Default Value Mutability metrics.graphite.hostname The hostname to receive Graphite plaintext protocol metric data String (no default value) MASKABLE metrics.graphite.interval The number of milliseconds to wait between sending Metrics data Duration (no default value) MASKABLE metrics.graphite.port The port to which Graphite data are sent Integer 2003 MASKABLE metrics.graphite.prefix A Graphite-specific prefix for reported metrics String (no default value) MASKABLE"},{"location":"basics/janusgraph-cfg/#metricsjmx","title":"metrics.jmx","text":"<p>Configuration options for metrics reporting through JMX</p> Name Description Datatype Default Value Mutability metrics.jmx.agentid The JMX agentId used by Metrics String (no default value) MASKABLE metrics.jmx.domain The JMX domain in which to report Metrics String (no default value) MASKABLE metrics.jmx.enabled Whether to report Metrics through a JMX MBean Boolean false MASKABLE"},{"location":"basics/janusgraph-cfg/#metricsslf4j","title":"metrics.slf4j","text":"<p>Configuration options for metrics reporting through slf4j</p> Name Description Datatype Default Value Mutability metrics.slf4j.interval Time between slf4j logging reports of Metrics data, in milliseconds Duration (no default value) MASKABLE metrics.slf4j.logger The complete name of the Logger through which Metrics will report via Slf4j String (no default value) MASKABLE"},{"location":"basics/janusgraph-cfg/#query","title":"query","text":"<p>Configuration options for query processing</p> Name Description Datatype Default Value Mutability query.batch Whether traversal queries should be batched when executed against the storage backend. This can lead to significant performance improvement if there is a non-trivial latency to the backend. Boolean false MASKABLE query.fast-property Whether to pre-fetch all properties on first singular vertex property access. This can eliminate backend calls on subsequentproperty access for the same vertex at the expense of retrieving all properties at once. This can be expensive for vertices with many properties Boolean true MASKABLE query.force-index Whether JanusGraph should throw an exception if a graph query cannot be answered using an index. Doing solimits the functionality of JanusGraph's graph queries but ensures that slow graph queries are avoided on large graphs. Recommended for production use of JanusGraph. Boolean false MASKABLE query.ignore-unknown-index-key Whether to ignore undefined types encountered in user-provided index queries Boolean false MASKABLE query.smart-limit Whether the query optimizer should try to guess a smart limit for the query to ensure responsiveness in light of possibly large result sets. Those will be loaded incrementally if this option is enabled. Boolean true MASKABLE"},{"location":"basics/janusgraph-cfg/#schema","title":"schema","text":"<p>Schema related configuration options</p> Name Description Datatype Default Value Mutability schema.default Configures the DefaultSchemaMaker to be used by this graph. If set to 'none', automatic schema creation is disabled. Defaults to a blueprints compatible schema maker with MULTI edge labels and SINGLE property keys String default MASKABLE"},{"location":"basics/janusgraph-cfg/#storage","title":"storage","text":"<p>Configuration options for the storage backend.  Some options are applicable only for certain backends.</p> Name Description Datatype Default Value Mutability storage.backend The primary persistence provider used by JanusGraph.  This is required.  It should be set one of JanusGraph's built-in shorthand names for its standard storage backends (shorthands: berkeleyje, cassandrathrift, cassandra, astyanax, embeddedcassandra, cql, hbase, inmemory) or to the full package and classname of a custom/third-party StoreManager implementation. String (no default value) LOCAL storage.batch-loading Whether to enable batch loading into the storage backend Boolean false LOCAL storage.buffer-size Size of the batch in which mutations are persisted Integer 1024 MASKABLE storage.conf-file Path to a configuration file for those storage backends which require/support a single separate config file. String (no default value) LOCAL storage.connection-timeout Default timeout, in milliseconds, when connecting to a remote database instance Duration 10000 ms MASKABLE storage.directory Storage directory for those storage backends that require local storage. String (no default value) LOCAL storage.drop-on-clear Whether to drop the graph database (true) or delete rows (false) when clearing storage. Note that some backends always drop the graph database when clearing storage. Also note that indices are always dropped when clearing storage. Boolean true MASKABLE storage.hostname The hostname or comma-separated list of hostnames of storage backend servers.  This is only applicable to some storage backends, such as cassandra and hbase. String[] 127.0.0.1 LOCAL storage.page-size JanusGraph break requests that may return many results from distributed storage backends into a series of requests for small chunks/pages of results, where each chunk contains up to this many elements. Integer 100 MASKABLE storage.parallel-backend-ops Whether JanusGraph should attempt to parallelize storage operations Boolean true MASKABLE storage.password Password to authenticate against backend String (no default value) LOCAL storage.port The port on which to connect to storage backend servers. For HBase, it is the Zookeeper port. Integer (no default value) LOCAL storage.read-only Read-only database Boolean false LOCAL storage.read-time Maximum time (in ms) to wait for a backend read operation to complete successfully. If a backend read operationfails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 10000 ms MASKABLE storage.root Storage root directory for those storage backends that require local storage. If you do not supply storage.directory and you do supply graph.graphname, then your data will be stored in the directory equivalent to /. String (no default value) LOCAL storage.setup-wait Time in milliseconds for backend manager to wait for the storage backends to become available when JanusGraph is run in server mode Duration 60000 ms MASKABLE storage.transactions Enables transactions on storage backends that support them Boolean true MASKABLE storage.username Username to authenticate against backend String (no default value) LOCAL storage.write-time Maximum time (in ms) to wait for a backend write operation to complete successfully. If a backend write operationfails temporarily, JanusGraph will backoff exponentially and retry the operation until the wait time has been exhausted. Duration 100000 ms MASKABLE"},{"location":"basics/janusgraph-cfg/#storageberkeleyje","title":"storage.berkeleyje","text":"<p>BerkeleyDB JE configuration options</p> Name Description Datatype Default Value Mutability storage.berkeleyje.cache-percentage Percentage of JVM heap reserved for BerkeleyJE's cache Integer 65 MASKABLE storage.berkeleyje.isolation-level The isolation level used by transactions String REPEATABLE_READ MASKABLE storage.berkeleyje.lock-mode The BDB record lock mode used for read operations String LockMode.DEFAULT MASKABLE"},{"location":"basics/janusgraph-cfg/#storagecassandra","title":"storage.cassandra","text":"<p>Cassandra storage backend options</p> Name Description Datatype Default Value Mutability storage.cassandra.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean true MASKABLE storage.cassandra.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cassandra.compaction-strategy-options Compaction strategy options.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cassandra.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cassandra.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cassandra.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cassandra.frame-size-mb The thrift frame size in megabytes Integer 15 MASKABLE storage.cassandra.keyspace The name of JanusGraph's keyspace.  It will be created if it does not exist. If it is not supplied, but graph.graphname is, then the the keyspace will be set to that. String janusgraph LOCAL storage.cassandra.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cassandra.replication-factor The number of data replicas (including the original copy) that should be kept. This is only meaningful for storage backends that natively support data replication. Integer 1 GLOBAL_OFFLINE storage.cassandra.replication-strategy-class The replication strategy to use for JanusGraph keyspace String org.apache.cassandra.locator.SimpleStrategy FIXED storage.cassandra.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form.  A replication_factor set here takes precedence over one set with storage.cassandra.replication-factor String[] (no default value) FIXED storage.cassandra.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE"},{"location":"basics/janusgraph-cfg/#storagecassandraastyanax","title":"storage.cassandra.astyanax","text":"<p>Astyanax-specific Cassandra options</p> Name Description Datatype Default Value Mutability storage.cassandra.astyanax.cluster-name Default name for the Cassandra cluster String JanusGraph Cluster MASKABLE storage.cassandra.astyanax.connection-pool-type Astyanax's connection pooler implementation String TOKEN_AWARE MASKABLE storage.cassandra.astyanax.frame-size The thrift frame size in mega bytes Integer 15 MASKABLE storage.cassandra.astyanax.host-supplier Host supplier to use when discovery type is set to DISCOVERY_SERVICE or TOKEN_AWARE String (no default value) MASKABLE storage.cassandra.astyanax.local-datacenter The name of the local or closest Cassandra datacenter.  When set and not whitespace, this value will be passed into ConnectionPoolConfigurationImpl.setLocalDatacenter. When unset or set to whitespace, setLocalDatacenter will not be invoked. String (no default value) MASKABLE storage.cassandra.astyanax.max-cluster-connections-per-host Maximum pooled \"cluster\" connections per host Integer 3 MASKABLE storage.cassandra.astyanax.max-connections Maximum open connections allowed in the pool (counting all hosts) Integer -1 MASKABLE storage.cassandra.astyanax.max-connections-per-host Maximum pooled connections per host Integer 32 MASKABLE storage.cassandra.astyanax.max-operations-per-connection Maximum number of operations allowed per connection before the connection is closed Integer 100000 MASKABLE storage.cassandra.astyanax.node-discovery-type How Astyanax discovers Cassandra cluster nodes String RING_DESCRIBE MASKABLE storage.cassandra.astyanax.read-page-size The page size for Cassandra read operations Integer 4096 MASKABLE storage.cassandra.astyanax.retry-backoff-strategy Astyanax's retry backoff strategy with configuration parameters String com.netflix.astyanax.connectionpool.impl.FixedRetryBackoffStrategy,1000,5000 MASKABLE storage.cassandra.astyanax.retry-delay-slice Astyanax's connection pool \"retryDelaySlice\" parameter Integer 10000 MASKABLE storage.cassandra.astyanax.retry-max-delay-slice Astyanax's connection pool \"retryMaxDelaySlice\" parameter Integer 10 MASKABLE storage.cassandra.astyanax.retry-policy Astyanax's retry policy implementation with configuration parameters String com.netflix.astyanax.retry.BoundedExponentialBackoff,100,25000,8 MASKABLE storage.cassandra.astyanax.retry-suspend-window Astyanax's connection pool \"retryMaxDelaySlice\" parameter Integer 20000 MASKABLE"},{"location":"basics/janusgraph-cfg/#storagecassandrassl","title":"storage.cassandra.ssl","text":"<p>Configuration options for SSL</p> Name Description Datatype Default Value Mutability storage.cassandra.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL"},{"location":"basics/janusgraph-cfg/#storagecassandrassltruststore","title":"storage.cassandra.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability storage.cassandra.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cassandra.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"basics/janusgraph-cfg/#storagecassandrathriftcpool","title":"storage.cassandra.thrift.cpool","text":"<p>Options for the Apache commons-pool connection manager</p> Name Description Datatype Default Value Mutability storage.cassandra.thrift.cpool.evictor-period Approximate number of milliseconds between runs of the idle connection evictor.  Set to -1 to never run the idle connection evictor. Long 30000 MASKABLE storage.cassandra.thrift.cpool.idle-test Whether the idle connection evictor validates idle connections and drops those that fail to validate Boolean false MASKABLE storage.cassandra.thrift.cpool.idle-tests-per-eviction-run When the value is negative, e.g. -n, roughly one nth of the idle connections are tested per run.  When the value is positive, e.g. n, the min(idle-count, n) connections are tested per run. Integer 0 MASKABLE storage.cassandra.thrift.cpool.max-active Maximum number of concurrently in-use connections (-1 to leave undefined) Integer 16 MASKABLE storage.cassandra.thrift.cpool.max-idle Maximum number of concurrently idle connections (-1 to leave undefined) Integer 4 MASKABLE storage.cassandra.thrift.cpool.max-total Max number of allowed Thrift connections, idle or active (-1 to leave undefined) Integer -1 MASKABLE storage.cassandra.thrift.cpool.max-wait Maximum number of milliseconds to block when storage.cassandra.thrift.cpool.when-exhausted is set to BLOCK.  Has no effect when set to actions besides BLOCK.  Set to -1 to wait indefinitely. Long -1 MASKABLE storage.cassandra.thrift.cpool.min-evictable-idle-time Minimum number of milliseconds a connection must be idle before it is eligible for eviction.  See also storage.cassandra.thrift.cpool.evictor-period.  Set to -1 to never evict idle connections. Long 60000 MASKABLE storage.cassandra.thrift.cpool.min-idle Minimum number of idle connections the pool attempts to maintain Integer 0 MASKABLE storage.cassandra.thrift.cpool.when-exhausted What to do when clients concurrently request more active connections than are allowed by the pool.  The value must be one of BLOCK, FAIL, or GROW. String BLOCK MASKABLE"},{"location":"basics/janusgraph-cfg/#storagecql","title":"storage.cql","text":"<p>CQL storage backend options</p> Name Description Datatype Default Value Mutability storage.cql.atomic-batch-mutate True to use Cassandra atomic batch mutation, false to use non-atomic batches Boolean false MASKABLE storage.cql.batch-statement-size The number of statements in each batch Integer 20 MASKABLE storage.cql.cluster-name Default name for the Cassandra cluster String JanusGraph Cluster MASKABLE storage.cql.compact-storage Whether the storage backend should use compact storage on tables. This option is only available for Cassandra 2 and earlier and defaults to true. Boolean true FIXED storage.cql.compaction-strategy-class The compaction strategy to use for JanusGraph tables String (no default value) FIXED storage.cql.compaction-strategy-options Compaction strategy options.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form. String[] (no default value) FIXED storage.cql.compression Whether the storage backend should use compression when storing the data Boolean true FIXED storage.cql.compression-block-size The size of the compression blocks in kilobytes Integer 64 FIXED storage.cql.compression-type The sstable_compression value JanusGraph uses when creating column families. This accepts any value allowed by Cassandra's sstable_compression option. Leave this unset to disable sstable_compression on JanusGraph-created CFs. String LZ4Compressor MASKABLE storage.cql.keyspace The name of JanusGraph's keyspace.  It will be created if it does not exist. String janusgraph LOCAL storage.cql.local-datacenter The name of the local or closest Cassandra datacenter.  When set and not whitespace, this value will be passed into ConnectionPoolConfigurationImpl.setLocalDatacenter. When unset or set to whitespace, setLocalDatacenter will not be invoked. String (no default value) MASKABLE storage.cql.only-use-local-consistency-for-system-operations True to prevent any system queries from using QUORUM consistency and always use LOCAL_QUORUM instead Boolean false MASKABLE storage.cql.protocol-version The protocol version used to connect to the Cassandra database.  If no value is supplied then the driver will negotiate with the server. Integer 0 LOCAL storage.cql.read-consistency-level The consistency level of read operations against Cassandra String QUORUM MASKABLE storage.cql.replication-factor The number of data replicas (including the original copy) that should be kept Integer 1 GLOBAL_OFFLINE storage.cql.replication-strategy-class The replication strategy to use for JanusGraph keyspace String SimpleStrategy FIXED storage.cql.replication-strategy-options Replication strategy options, e.g. factor or replicas per datacenter.  This list is interpreted as a map.  It must have an even number of elements in [key,val,key,val,...] form.  A replication_factor set here takes precedence over one set with storage.cql.replication-factor String[] (no default value) FIXED storage.cql.write-consistency-level The consistency level of write operations against Cassandra String QUORUM MASKABLE"},{"location":"basics/janusgraph-cfg/#storagecqlssl","title":"storage.cql.ssl","text":"<p>Configuration options for SSL</p> Name Description Datatype Default Value Mutability storage.cql.ssl.enabled Controls use of the SSL connection to Cassandra Boolean false LOCAL"},{"location":"basics/janusgraph-cfg/#storagecqlssltruststore","title":"storage.cql.ssl.truststore","text":"<p>Configuration options for SSL Truststore.</p> Name Description Datatype Default Value Mutability storage.cql.ssl.truststore.location Marks the location of the SSL Truststore. String LOCAL storage.cql.ssl.truststore.password The password to access SSL Truststore. String LOCAL"},{"location":"basics/janusgraph-cfg/#storagehbase","title":"storage.hbase","text":"<p>HBase storage options</p> Name Description Datatype Default Value Mutability storage.hbase.compat-class The package and class name of the HBaseCompat implementation. HBaseCompat masks version-specific HBase API differences. When this option is unset, JanusGraph calls HBase's VersionInfo.getVersion() and loads the matching compat class at runtime.  Setting this option forces JanusGraph to instead reflectively load and instantiate the specified class. String (no default value) MASKABLE storage.hbase.compression-algorithm An HBase Compression.Algorithm enum string which will be applied to newly created column families. The compression algorithm must be installed and available on the HBase cluster.  JanusGraph cannot install and configure new compression algorithms on the HBase cluster by itself. String GZ MASKABLE storage.hbase.region-count The number of initial regions set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.regions-per-server The number of regions per regionserver to set when creating JanusGraph's HBase table Integer (no default value) MASKABLE storage.hbase.short-cf-names Whether to shorten the names of JanusGraph's column families to one-character mnemonics to conserve storage space Boolean true FIXED storage.hbase.skip-schema-check Assume that JanusGraph's HBase table and column families already exist. When this is true, JanusGraph will not check for the existence of its table/CFs, nor will it attempt to create them under any circumstances.  This is useful when running JanusGraph without HBase admin privileges. Boolean false MASKABLE storage.hbase.table The name of the table JanusGraph will use.  When storage.hbase.skip-schema-check is false, JanusGraph will automatically create this table if it does not already exist. If this configuration option is not provided but graph.graphname is, the table will be set to that value. String janusgraph LOCAL"},{"location":"basics/janusgraph-cfg/#storagelock","title":"storage.lock","text":"<p>Options for locking on eventually-consistent stores</p> Name Description Datatype Default Value Mutability storage.lock.backend Locker type to use String consistentkey GLOBAL_OFFLINE storage.lock.clean-expired Whether to delete expired locks from the storage backend Boolean false MASKABLE storage.lock.expiry-time Number of milliseconds after which a lock is considered to have expired. Lock applications that were not released are considered expired after this time and released. This value should be larger than the maximum time a transaction can take in order to guarantee that no correctly held applications are expired pre-maturely and as small as possible to avoid dead lock. Duration 300000 ms GLOBAL_OFFLINE storage.lock.local-mediator-group This option determines the LocalLockMediator instance used for early detection of lock contention between concurrent JanusGraph graph instances within the same process which are connected to the same storage backend.  JanusGraph instances that have the same value for this variable will attempt to discover lock contention among themselves in memory before proceeding with the general-case distributed locking code.  JanusGraph generates an appropriate default value for this option at startup.  Overridding the default is generally only useful in testing. String (no default value) LOCAL storage.lock.retries Number of times the system attempts to acquire a lock before giving up and throwing an exception Integer 3 MASKABLE storage.lock.wait-time Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend. Also, the time waited at the end of all lock applications before verifying that the applications were successful. This value should be a small multiple of the average consistent write time. Duration 100 ms GLOBAL_OFFLINE"},{"location":"basics/janusgraph-cfg/#storagemeta","title":"storage.meta *","text":"<p>Meta data to include in storage backend retrievals</p> Name Description Datatype Default Value Mutability storage.meta.[X].timestamps Whether to include timestamps in retrieved entries for storage backends that automatically annotated entries with timestamps Boolean false GLOBAL storage.meta.[X].ttl Whether to include ttl in retrieved entries for storage backends that support storage and retrieval of cell level TTL Boolean false GLOBAL storage.meta.[X].visibility Whether to include visibility in retrieved entries for storage backends that support cell level visibility Boolean true GLOBAL"},{"location":"basics/janusgraph-cfg/#tx","title":"tx","text":"<p>Configuration options for transaction handling</p> Name Description Datatype Default Value Mutability tx.log-tx Whether transaction mutations should be logged to JanusGraph's write-ahead transaction log which can be used for recovery of partially failed transactions Boolean false GLOBAL tx.max-commit-time Maximum time (in ms) that a transaction might take to commit against all backends. This is used by the distributed write-ahead log processing to determine when a transaction can be considered failed (i.e. after this time has elapsed).Must be longer than the maximum allowed write time. Duration 10000 ms GLOBAL"},{"location":"basics/janusgraph-cfg/#txrecovery","title":"tx.recovery","text":"<p>Configuration options for transaction recovery processes</p> Name Description Datatype Default Value Mutability tx.recovery.verbose Whether the transaction recovery system should print recovered transactions and other activity to standard output Boolean false MASKABLE"},{"location":"basics/multi-node/","title":"Things to Consider in a Multi-Node JanusGraph Cluster","text":"<p>JanusGraph is a distributed graph database, which means it can be setup in a multi-node cluster. However, when working in such an environment, there are important things to consider. Furthermore, if configured properly, JanusGraph handles some of these special considerations for the user.</p>"},{"location":"basics/multi-node/#dynamic-graphs","title":"Dynamic Graphs","text":"<p>JanusGraph supports dynamically creating graphs. This is deviation from the way in which standard Gremlin Server implementations allow one to access a graph. Traditionally, users create bindings to graphs at server-start, by configuring the gremlin-server.yaml file accordingly. For example, if the <code>graphs</code> section of your yaml file looks like this: <pre><code>graphs {\n  graph1: conf/graph1.properties,\n  graph2: conf/graph2.properties\n}\n</code></pre></p> <p>then you will access your graphs on the Gremlin Server using the fact that the String <code>graph1</code> will be bound to the graph opened on the server as per its supplied properties file, and the same holds true for <code>graph2</code>.</p> <p>However, if we use the <code>ConfiguredGraphFactory</code> to dynamically create graphs, then those graphs are managed by the JanusGraphManager and the graph configurations are managed by the ConfigurationManagementGraph. This is especially useful because it 1. allows you to define graph configurations post-server-start and 2. allows the graph configurations to be managed in a persisted and distributed nature across your JanusGraph cluster.</p> <p>To properly use the <code>ConfiguredGraphFactory</code>, you must configure every Gremlin Server in your cluster to use the <code>JanusGraphManager</code> and the <code>ConfigurationManagementGraph</code>. This procedure is explained in detail here.</p>"},{"location":"basics/multi-node/#graph-reference-consistency","title":"Graph Reference Consistency","text":"<p>If you configure all your JanusGraph servers to use the ConfiguredGraphFactory, JanusGraph will ensure all graph representations are-up-to-date across all JanusGraph nodes in your cluster.</p> <p>For example, if you update or delete the configuration to a graph on one JanusGraph node, then we must evict that graph from the cache on every JanusGraph node in the cluster. Otherwise, we may have inconsistent graph representations across your cluster. JanusGraph automatically handles this eviction using a messaging log queue through the backend system that the graph in question is configured to use.</p> <p>If one of your servers is configured incorrectly, then it may not be able to successfully remove the graph from the cache.</p> <p>Important</p> <p>Any updates to your TemplateConfiguration will not result in the updating of graphs/graph configurations previously created using said template configuration. If you want to update the individual graph configurations, you must do so using the available update APIs. These update APIs will then result in the graphe cache eviction across all JanusGraph nodes in your cluster.</p>"},{"location":"basics/multi-node/#dynamic-graph-and-traversal-bindings","title":"Dynamic Graph and Traversal Bindings","text":"<p>JanusGraph has the ability to bind dynamically created graphs and their traversal references to <code>&lt;graph.graphname&gt;</code> and <code>&lt;graph.graphname&gt;_traversal</code>, respectively, across all JanusGraph nodes in your cluster, with a maximum of a 20s lag for the binding to take effect on any node in the cluster. Read more about this here.</p> <p>JanusGraph accomplishes this by having each node in your cluster poll the <code>ConfigurationManagementGraph</code> for all graphs for which you have created configurations. The <code>JanusGraphManager</code> will then open said graph with its persisted configuration, store it in its graph cache, and bind the <code>&lt;graph.graphname&gt;</code> to the graph reference on the <code>GremlinExecutor</code> as well as bind <code>&lt;graph.graphname&gt;_traversal</code> to the graph\u2019s traversal reference on the <code>GremlinExecutor</code>.</p> <p>This allows you to access a dynamically created graph and its traversal reference by their string bindings, on every node in your JanusGraph cluster. This is particularly important to be able to work with Gremlin Server clients and use TinkerPops\u2019s withRemote functionality.</p>"},{"location":"basics/multi-node/#set-up","title":"Set Up","text":"<p>To set up your cluster to bind dynamically created graphs and their traversal references, you must:</p> <ol> <li> <p>Configure each node to use the     ConfiguredGraphFactory.</p> </li> <li> <p>Configure each node to use a <code>JanusGraphChannelizer</code>, which injects     lower-level Gremlin Server components, like the GremlinExecutor,     into the JanusGraph project, giving us greater control of the     Gremlin Server.</p> </li> </ol> <p>To configure each node to use a <code>JanusGraphChannelizer</code>, we must update the <code>gremlin-server.yaml</code> to do so:</p> <pre><code>channelizer: org.janusgraph.channelizers.JanusGraphWebSocketChannelizer\n</code></pre> <p>There are a few channelizers you can choose from:</p> <ol> <li> <p>org.janusgraph.channelizers.JanusGraphWebSocketChannelizer</p> </li> <li> <p>org.janusgraph.channelizers.JanusGraphHttpChannelizer</p> </li> <li> <p>org.janusgraph.channelizers.JanusGraphNioChannelizer</p> </li> <li> <p>org.janusgraph.channelizers.JanusGraphWsAndHttpChannelizer</p> </li> </ol> <p>All of the channelizers share the exact same functionality as their TinkerPop counterparts.</p>"},{"location":"basics/multi-node/#using-tinkerpops-withremote-functionality","title":"Using TinkerPop\u2019s withRemote Functionality","text":"<p>Since traversal references are bound on the JanusGraph servers, we can make use of TinkerPop\u2019s withRemote functionality. This will allow one to run gremlin queries locally, against a remote graph reference. Traditionally, one runs queries against remote Gremlin Servers by sending String script representations, which are processed on the remote server and the response serialized and sent back. However, TinkerPop also allows for the use of <code>remoteGraph</code>, which could be useful if you are building a TinkerPop compliant graph infrastructure that is easily transferable to multiple implementations.</p> <p>To use this functionality in JanusGraph, we must first ensure we have created a graph on the remote JanusGraph cluster:</p> <p><code>ConfiguredGraphFactory.create(\"graph1\");</code></p> <p>Next, we must wait 20 seconds to ensure the traversal reference is bound on every JanusGraph node in the remote cluster.</p> <p>Finally, we can locally make use of the <code>withRemote</code> method to access a local reference to a remote graph: <pre><code>gremlin&gt; cluster = Cluster.open('conf/remote-objects.yaml')\n==&gt;localhost/127.0.0.1:8182\ngremlin&gt; graph = EmptyGraph.instance()\n==&gt;emptygraph[empty]\ngremlin&gt; g = graph.traversal().withRemote(DriverRemoteConnection.using(cluster, \"graph1_traversal\"))\n==&gt;graphtraversalsource[emptygraph[empty], standard]\n</code></pre></p> <p>For completion, the above <code>conf/remote-objects.yaml</code> should tell the <code>Cluster</code> API how to access the remote JanusGraph servers; for example, it may look like: <pre><code>hosts: [remoteaddress1.com, remoteaddress2.com]\nport: 8182\nusername: admin\npassword: password\nconnectionPool: { enableSsl: true }\nserializer: { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0, config: { ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] }}\n</code></pre></p>"},{"location":"basics/schema/","title":"Schema and Data Modeling","text":"<p>Each JanusGraph graph has a schema comprised of the edge labels, property keys, and vertex labels used therein. A JanusGraph schema can either be explicitly or implicitly defined. Users are encouraged to explicitly define the graph schema during application development. An explicitly defined schema is an important component of a robust graph application and greatly improves collaborative software development. Note, that a JanusGraph schema can be evolved over time without any interruption of normal database operations. Extending the schema does not slow down query answering and does not require database downtime.</p> <p>The schema type - i.e. edge label, property key, or vertex label - is assigned to elements in the graph - i.e. edge, properties or vertices respectively - when they are first created. The assigned schema type cannot be changed for a particular element. This ensures a stable type system that is easy to reason about.</p> <p>Beyond the schema definition options explained in this section, schema types provide performance tuning options that are discussed in Advanced Schema.</p>"},{"location":"basics/schema/#displaying-schema-information","title":"Displaying Schema Information","text":"<p>There are methods to view specific elements of the graph schema within the management API. These methods are <code>mgmt.printIndexes()</code>, <code>mgmt.printPropertyKeys()</code>, <code>mgmt.printVertexLabels()</code>, and <code>mgmt.printEdgeLabels()</code>. There is also a method that displays all the combined output named <code>printSchema()</code>.</p> <pre><code>mgmt = graph.openManagement()\nmgmt.printSchema()\n</code></pre>"},{"location":"basics/schema/#defining-edge-labels","title":"Defining Edge Labels","text":"<p>Each edge connecting two vertices has a label which defines the semantics of the relationship. For instance, an edge labeled <code>friend</code> between vertices A and B encodes a friendship between the two individuals.</p> <p>To define an edge label, call <code>makeEdgeLabel(String)</code> on an open graph or management transaction and provide the name of the edge label as the argument. Edge label names must be unique in the graph. This method returns a builder for edge labels that allows to define its multiplicity. The multiplicity of an edge label defines a multiplicity constraint on all edges of this label, that is, a maximum number of edges between pairs of vertices. JanusGraph recognizes the following multiplicity settings.</p>"},{"location":"basics/schema/#edge-label-multiplicity","title":"Edge Label Multiplicity","text":"<ul> <li>MULTI: Allows multiple edges of the same label between any pair     of vertices. In other words, the graph is a multi graph with     respect to such edge label. There is no constraint on edge     multiplicity.</li> <li>SIMPLE: Allows at most one edge of such label between any pair     of vertices. In other words, the graph is a simple graph with     respect to the label. Ensures that edges are unique for a given     label and pairs of vertices.</li> <li>MANY2ONE: Allows at most one outgoing edge of such label on any     vertex in the graph but places no constraint on incoming edges. The     edge label <code>mother</code> is an example with MANY2ONE multiplicity since     each person has at most one mother but mothers can have multiple     children.</li> <li>ONE2MANY: Allows at most one incoming edge of such label on any     vertex in the graph but places no constraint on outgoing edges. The     edge label <code>winnerOf</code> is an example with ONE2MANY multiplicity since     each contest is won by at most one person but a person can win     multiple contests.</li> <li>ONE2ONE: Allows at most one incoming and one outgoing edge of     such label on any vertex in the graph. The edge label marriedTo is     an example with ONE2ONE multiplicity since a person is married to     exactly one other person.</li> </ul> <p>The default multiplicity is MULTI. The definition of an edge label is completed by calling the <code>make()</code> method on the builder which returns the defined edge label as shown in the following example. <pre><code>mgmt = graph.openManagement()\nfollow = mgmt.makeEdgeLabel('follow').multiplicity(MULTI).make()\nmother = mgmt.makeEdgeLabel('mother').multiplicity(MANY2ONE).make()\nmgmt.commit()\n</code></pre></p>"},{"location":"basics/schema/#defining-property-keys","title":"Defining Property Keys","text":"<p>Properties on vertices and edges are key-value pairs. For instance, the property <code>name='Daniel'</code> has the key <code>name</code> and the value <code>'Daniel'</code>. Property keys are part of the JanusGraph schema and can constrain the allowed data types and cardinality of values.</p> <p>To define a property key, call <code>makePropertyKey(String)</code> on an open graph or management transaction and provide the name of the property key as the argument. Property key names must be unique in the graph, and it is recommended to avoid spaces or special characters in property names. This method returns a builder for the property keys.</p>"},{"location":"basics/schema/#property-key-data-type","title":"Property Key Data Type","text":"<p>Use <code>dataType(Class)</code> to define the data type of a property key. JanusGraph will enforce that all values associated with the key have the configured data type and thereby ensures that data added to the graph is valid. For instance, one can define that the <code>name</code> key has a String data type.</p> <p>Define the data type as <code>Object.class</code> in order to allow any (serializable) value to be associated with a key. However, it is encouraged to use concrete data types whenever possible. Configured data types must be concrete classes and not interfaces or abstract classes. JanusGraph enforces class equality, so adding a sub-class of a configured data type is not allowed.</p> <p>JanusGraph natively supports the following data types.</p> <p>Native JanusGraph Data Types</p> Name Description String Character sequence Character Individual character Boolean true or false Byte byte value Short short value Integer integer value Long long value Float 4 byte floating point number Double 8 byte floating point number Date Specific instant in time (<code>java.util.Date</code>) Geoshape Geographic shape like point, circle or box UUID Universally unique identifier (<code>java.util.UUID</code>)"},{"location":"basics/schema/#property-key-cardinality","title":"Property Key Cardinality","text":"<p>Use <code>cardinality(Cardinality)</code> to define the allowed cardinality of the values associated with the key on any given vertex.</p> <ul> <li>SINGLE: Allows at most one value per element for such key. In     other words, the key\u2192value mapping is unique for all elements in the     graph. The property key <code>birthDate</code> is an example with SINGLE     cardinality since each person has exactly one birth date.</li> <li>LIST: Allows an arbitrary number of values per element for such     key. In other words, the key is associated with a list of values     allowing duplicate values. Assuming we model sensors as vertices in     a graph, the property key <code>sensorReading</code> is an example with LIST     cardinality to allow lots of (potentially duplicate) sensor readings     to be recorded.</li> <li>SET: Allows multiple values but no duplicate values per element     for such key. In other words, the key is associated with a set of     values. The property key <code>name</code> has SET cardinality if we want to     capture all names of an individual (including nick name, maiden     name, etc).</li> </ul> <p>The default cardinality setting is SINGLE. Note, that property keys used on edges and properties have cardinality SINGLE. Attaching multiple values for a single key on an edge or property is not supported.</p> <pre><code>mgmt = graph.openManagement()\nbirthDate = mgmt.makePropertyKey('birthDate').dataType(Long.class).cardinality(Cardinality.SINGLE).make()\nname = mgmt.makePropertyKey('name').dataType(String.class).cardinality(Cardinality.SET).make()\nsensorReading = mgmt.makePropertyKey('sensorReading').dataType(Double.class).cardinality(Cardinality.LIST).make()\nmgmt.commit()\n</code></pre>"},{"location":"basics/schema/#relation-types","title":"Relation Types","text":"<p>Edge labels and property keys are jointly referred to as relation types. Names of relation types must be unique in the graph which means that property keys and edge labels cannot have the same name. There are methods in the JanusGraph API to query for the existence or retrieve relation types which encompasses both property keys and edge labels.</p> <pre><code>mgmt = graph.openManagement()\nif (mgmt.containsRelationType('name'))\n    name = mgmt.getPropertyKey('name')\nmgmt.getRelationTypes(EdgeLabel.class)\nmgmt.commit()\n</code></pre>"},{"location":"basics/schema/#defining-vertex-labels","title":"Defining Vertex Labels","text":"<p>Like edges, vertices have labels. Unlike edge labels, vertex labels are optional. Vertex labels are useful to distinguish different types of vertices, e.g. user vertices and product vertices.</p> <p>Although labels are optional at the conceptual and data model level, JanusGraph assigns all vertices a label as an internal implementation detail. Vertices created by the <code>addVertex</code> methods use JanusGraph\u2019s default label.</p> <p>To create a label, call <code>makeVertexLabel(String).make()</code> on an open graph or management transaction and provide the name of the vertex label as the argument. Vertex label names must be unique in the graph.</p> <pre><code>mgmt = graph.openManagement()\nperson = mgmt.makeVertexLabel('person').make()\nmgmt.commit()\n// Create a labeled vertex\nperson = graph.addVertex(label, 'person')\n// Create an unlabeled vertex\nv = graph.addVertex()\ngraph.tx().commit()\n</code></pre>"},{"location":"basics/schema/#automatic-schema-maker","title":"Automatic Schema Maker","text":"<p>If an edge label, property key, or vertex label has not been defined explicitly, it will be defined implicitly when it is first used during the addition of an edge, vertex or the setting of a property. The <code>DefaultSchemaMaker</code> configured for the JanusGraph graph defines such types.</p> <p>By default, implicitly created edge labels have multiplicity MULTI and implicitly created property keys have cardinality SINGLE and data type <code>Object.class</code>. Users can control automatic schema element creation by implementing and registering their own <code>DefaultSchemaMaker</code>.</p> <p>It is strongly encouraged to explicitly define all schema elements and to disable automatic schema creation by setting <code>schema.default=none</code> in the JanusGraph graph configuration.</p>"},{"location":"basics/schema/#changing-schema-elements","title":"Changing Schema Elements","text":"<p>The definition of an edge label, property key, or vertex label cannot be changed once its committed into the graph. However, the names of schema elements can be changed via <code>JanusGraphManagement.changeName(JanusGraphSchemaElement, String)</code> as shown in the following example where the property key <code>place</code> is renamed to <code>location</code>.</p> <pre><code>mgmt = graph.openManagement()\nplace = mgmt.getPropertyKey('place')\nmgmt.changeName(place, 'location')\nmgmt.commit()\n</code></pre> <p>Note, that schema name changes may not be immediately visible in currently running transactions and other JanusGraph graph instances in the cluster. While schema name changes are announced to all JanusGraph instances through the storage backend, it may take a while for the schema changes to take effect and it may require a instance restart in the event of certain failure conditions - like network partitions - if they coincide with the rename. Hence, the user must ensure that either of the following holds:</p> <ul> <li>The renamed label or key is not currently in active use (i.e.     written or read) and will not be in use until all JanusGraph     instances are aware of the name change.</li> <li>Running transactions actively accommodate the brief intermediate     period where either the old or new name is valid based on the     specific JanusGraph instance and status of the name-change     announcement. For instance, that could mean transactions query for     both names simultaneously.</li> </ul> <p>Should the need arise to re-define an existing schema type, it is recommended to change the name of this type to a name that is not currently (and will never be) in use. After that, a new label or key can be defined with the original name, thereby effectively replacing the old one. However, note that this would not affect vertices, edges, or properties previously written with the existing type. Redefining existing graph elements is not supported online and must be accomplished through a batch graph transformation.</p>"},{"location":"basics/server/","title":"JanusGraph Server","text":"<p>JanusGraph uses the Gremlin Server  engine as the server component to process and answer client queries.  When packaged in JanusGraph, Gremlin Server is called JanusGraph Server.</p> <p>JanusGraph Server must be started manually in order to use it. JanusGraph Server provides a way to remotely execute Gremlin traversals against one or more JanusGraph instances hosted within it. This section will describe how to use the WebSocket configuration, as well as describe how to configure JanusGraph Server to handle HTTP endpoint interactions. For information about how to connect to a JanusGraph Server from different languages refer to Connecting to JanusGraph.</p>"},{"location":"basics/server/#getting-started","title":"Getting Started","text":""},{"location":"basics/server/#using-the-pre-packaged-distribution","title":"Using the Pre-Packaged Distribution","text":"<p>The JanusGraph release comes pre-configured to run JanusGraph Server out of the box leveraging a sample Cassandra and Elasticsearch configuration to allow users to get started quickly with JanusGraph Server. This configuration defaults to client applications that can connect to JanusGraph Server via WebSocket with a custom subprotocol. There are a number of clients developed in different languages to help support the subprotocol. The most familiar client to use the WebSocket interface is the Gremlin Console. The quick-start bundle is not intended to be representative of a production installation, but does provide a way to perform development with JanusGraph Server, run tests and see how the components are wired together. To use this default configuration:</p> <ul> <li> <p>Download a copy of the current <code>janusgraph-$VERSION.zip</code> file from     the Releases     page</p> </li> <li> <p>Unzip it and enter the <code>janusgraph-$VERSION</code> directory</p> </li> <li> <p>Run <code>bin/janusgraph.sh start</code>. This step will start Gremlin Server     with Cassandra/ES forked into a separate process. Note for security     reasons Elasticsearch and therefore <code>janusgraph.sh</code> must be run     under a non-root account.</p> </li> </ul> <pre><code>$ bin/janusgraph.sh start\nForking Cassandra...\nRunning `nodetool statusthrift`.. OK (returned exit status 0 and printed string \"running\").\nForking Elasticsearch...\nConnecting to Elasticsearch (127.0.0.1:9300)... OK (connected to 127.0.0.1:9300).\nForking Gremlin-Server...\nConnecting to Gremlin-Server (127.0.0.1:8182)... OK (connected to 127.0.0.1:8182).\nRun gremlin.sh to connect.\n</code></pre>"},{"location":"basics/server/#connecting-to-gremlin-server","title":"Connecting to Gremlin Server","text":"<p>After running <code>janusgraph.sh</code>, Gremlin Server will be ready to listen for WebSocket connections. The easiest way to test the connection is with Gremlin Console.</p> <p>Start Gremlin Console with bin/gremlin.sh and use the :remote and :&gt; commands to issue Gremlin to Gremlin Server:</p> <pre><code>$  bin/gremlin.sh\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\nplugin activated: tinkerpop.server\nplugin activated: tinkerpop.hadoop\nplugin activated: tinkerpop.utilities\nplugin activated: janusgraph.imports\nplugin activated: tinkerpop.tinkergraph\ngremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Connected - localhost/127.0.0.1:8182\ngremlin&gt; :&gt; graph.addVertex(\"name\", \"stephen\")\n==&gt;v[256]\ngremlin&gt; :&gt; g.V().values('name')\n==&gt;stephen\n</code></pre> <p>The <code>:remote</code> command tells the console to configure a remote connection to Gremlin Server using the <code>conf/remote.yaml</code> file to connect. That file points to a Gremlin Server instance running on <code>localhost</code>. The <code>:&gt;</code> is the \"submit\" command which sends the Gremlin on that line to the currently active remote. Instead of prefixing every single script with <code>:&gt;</code>, the command <code>:remote console</code> can be executed once to implicitly send all subsequent scripts to the current remote connection. By default remote conenctions are sessionless, meaning that each line sent in the console is interpreted as a single request. Multiple statements can be sent on a single line using a semicolon as the delimiter. <pre><code>gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml\n==&gt;Configured localhost/127.0.0.1:8182\ngremlin&gt; :remote console\n==&gt;All scripts will now be sent to Gremlin Server - [localhost/127.0.0.1:8182] - type ':remote console' to return to local mode\ngremlin&gt; graph\n==&gt;standardjanusgraph[cql:[127.0.0.1]]\ngremlin&gt; g\n==&gt;graphtraversalsource[standardjanusgraph[cql:[127.0.0.1]], standard]\ngremlin&gt; g.V()\ngremlin&gt; user = \"Chris\"\n==&gt;Chris\ngremlin&gt; graph.addVertex(\"name\", user)\nNo such property: user for class: Script21\nType ':help' or ':h' for help.\nDisplay stack trace? [yN]\n</code></pre></p> <p>Alternatively, you can establish a console with a session by specifying session when creating the connection. A console session allows you to reuse variables across several lines of input. <pre><code>gremlin&gt; :remote connect tinkerpop.server conf/remote.yaml session\n==&gt;Configured localhost/127.0.0.1:8182-[9acf239e-a3ed-4301-b33f-55c911e04052]\ngremlin&gt; :remote console\n==&gt;All scripts will now be sent to Gremlin Server - [localhost/127.0.0.1:8182]-[9acf239e-a3ed-4301-b33f-55c911e04052] - type ':remote console' to return to local mode\ngremlin&gt; g.V()\ngremlin&gt; user = \"Chris\"\n==&gt;Chris\ngremlin&gt; user\n==&gt;Chris\ngremlin&gt; graph.addVertex(\"name\", user)\n==&gt;v[4344]\ngremlin&gt; g.V().values('name')\n==&gt;Chris\n</code></pre></p>"},{"location":"basics/server/#cleaning-up-after-the-pre-packaged-distribution","title":"Cleaning up after the Pre-Packaged Distribution","text":"<p>If you want to start fresh and remove the database and logs you can use the clean command with <code>janusgraph.sh</code>. The server should be stopped before running the clean operation. <pre><code>$ cd /Path/to/janusgraph/janusgraph-0.2.0-hadoop2/\n$ ./bin/janusgraph.sh stop\nKilling Gremlin-Server (pid 91505)...\nKilling Elasticsearch (pid 91402)...\nKilling Cassandra (pid 91219)...\n$ ./bin/janusgraph.sh clean\nAre you sure you want to delete all stored data and logs? [y/N] y\nDeleted data in /Path/to/janusgraph/janusgraph-0.2.0-hadoop2/db\nDeleted logs in /Path/to/janusgraph/janusgraph-0.2.0-hadoop2/log\n</code></pre></p>"},{"location":"basics/server/#janusgraph-server-as-a-websocket-endpoint","title":"JanusGraph Server as a WebSocket Endpoint","text":"<p>The default configuration described in Getting Started  is already a WebSocket configuration. If you want to alter the default configuration to work with your own Cassandra or HBase environment rather than use the quick start environment, follow these steps:</p> <p>To Configure JanusGraph Server For WebSocket</p> <ol> <li> <p>Test a local connection to a JanusGraph database first. This step     applies whether using the Gremlin Console to test the connection, or     whether connecting from a program. Make appropriate changes in a     properties file in the <code>./conf</code> directory for your environment. For     example, edit <code>./conf/janusgraph-hbase.properties</code> and make sure the     storage.backend, storage.hostname and storage.hbase.table parameters     are specified correctly. For more information on configuring     JanusGraph for various storage backends, see     Storage Backends. Make sure the properties file contains the     following line: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\n</code></pre></p> </li> <li> <p>Once a local configuration is tested and you have a working     properties file, copy the properties file from the <code>./conf</code>     directory to the <code>./conf/gremlin-server</code> directory. <pre><code>cp conf/janusgraph-hbase.properties\nconf/gremlin-server/socket-janusgraph-hbase-server.properties\n</code></pre></p> </li> <li> <p>Copy <code>./conf/gremlin-server/gremlin-server.yaml</code> to a new file     called <code>socket-gremlin-server.yaml</code>. Do this in case you need to     refer to the original version of the file <pre><code>cp conf/gremlin-server/gremlin-server.yaml\nconf/gremlin-server/socket-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>Edit the <code>socket-gremlin-server.yaml</code> file and make the following     updates:</p> <ol> <li> <p>If you are planning to connect to JanusGraph Server from     something other than localhost, update the IP address for host: <pre><code>host: 10.10.10.100\n</code></pre></p> </li> <li> <p>Update the graphs section to point to your new properties file     so the JanusGraph Server can find and connect to your JanusGraph     instance: <pre><code>graphs: { graph:\n    conf/gremlin-server/socket-janusgraph-hbase-server.properties}\n</code></pre></p> </li> </ol> </li> <li> <p>Start the JanusGraph Server, specifying the yaml file you just     configured: <pre><code>bin/gremlin-server.sh ./conf/gremlin-server/socket-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>The JanusGraph Server should now be running in WebSocket mode and     can be tested by following the instructions in Connecting to Gremlin Server</p> </li> </ol> <p>Important</p> <p>Do not use <code>bin/janusgraph.sh</code>. That starts the default configuration, which starts a separate Cassandra/Elasticsearch environment.</p>"},{"location":"basics/server/#janusgraph-server-as-a-http-endpoint","title":"JanusGraph Server as a HTTP Endpoint","text":"<p>The default configuration described in Getting Started is a WebSocket configuration. If you want to alter the default configuration in order to use JanusGraph Server as an HTTP endpoint for your JanusGraph database, follow these steps:</p> <ol> <li> <p>Test a local connection to a JanusGraph database first. This step     applies whether using the Gremlin Console to test the connection, or     whether connecting from a program. Make appropriate changes in a     properties file in the <code>./conf</code> directory for your environment. For     example, edit <code>./conf/janusgraph-hbase.properties</code> and make sure the     storage.backend, storage.hostname and storage.hbase.table parameters     are specified correctly. For more information on configuring     JanusGraph for various storage backends, see     Storage Backends. Make sure the properties file contains the     following line: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\n</code></pre></p> </li> <li> <p>Once a local configuration is tested and you have a working     properties file, copy the properties file from the <code>./conf</code>     directory to the <code>./conf/gremlin-server</code> directory. <pre><code>cp conf/janusgraph-hbase.properties conf/gremlin-server/http-janusgraph-hbase-server.properties\n</code></pre></p> </li> <li> <p>Copy <code>./conf/gremlin-server/gremlin-server.yaml</code> to a new file     called <code>http-gremlin-server.yaml</code>. Do this in case you need to refer     to the original version of the file <pre><code>cp conf/gremlin-server/gremlin-server.yaml conf/gremlin-server/http-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>Edit the <code>http-gremlin-server.yaml</code> file and make the following     updates:</p> <ol> <li> <p>If you are planning to connect to JanusGraph Server from     something other than localhost, update the IP address for host: <pre><code>host: 10.10.10.100\n</code></pre></p> </li> <li> <p>Update the channelizer setting to specify the HttpChannelizer: <pre><code>channelizer: org.apache.tinkerpop.gremlin.server.channel.HttpChannelizer\n</code></pre></p> </li> <li> <p>Update the graphs section to point to your new properties file     so the JanusGraph Server can find and connect to your JanusGraph     instance: <pre><code>graphs: { graph:\n    conf/gremlin-server/http-janusgraph-hbase-server.properties}\n</code></pre></p> </li> </ol> </li> <li> <p>Start the JanusGraph Server, specifying the yaml file you just     configured: <pre><code>bin/gremlin-server.sh ./conf/gremlin-server/http-gremlin-server.yaml\n</code></pre></p> </li> <li> <p>The JanusGraph Server should now be running in HTTP mode and     available for testing. curl can be used to verify the server is     working: <pre><code>curl -XPOST -Hcontent-type:application/json -d *{\"gremlin\":\"g.V().count()\"}* [IP for JanusGraph server host](http://):8182 \n</code></pre></p> </li> </ol>"},{"location":"basics/server/#janusgraph-server-as-both-a-websocket-and-http-endpoint","title":"JanusGraph Server as Both a WebSocket and HTTP Endpoint","text":"<p>As of JanusGraph 0.2.0, you can configure your <code>gremlin-server.yaml</code> to accept both WebSocket and HTTP connections over the same port. This can be achieved by changing the channelizer in any of the previous examples as follows. <pre><code>channelizer: org.apache.tinkerpop.gremlin.server.channel.WsAndHttpChannelizer\n</code></pre></p>"},{"location":"basics/server/#advanced-janusgraph-server-configurations","title":"Advanced JanusGraph Server Configurations","text":""},{"location":"basics/server/#websocket-versus-http","title":"WebSocket versus HTTP","text":"<p>JanusGraph Server must be run with a configuration for either WebSocket (for Gremlin Console or other WebSocket programs) or re-configured for HTTP to work with HTTP clients.</p> <p>Currently, there is no way to configure a single running instance of JanusGraph Server to communicate simultaneously with clients talking WebSocket and HTTP. However, it is possible to create configuration files (following the steps in this chapter) for both WebSocket and HTTP and start two instances of the JanusGraph Server by pointing it to the various configurations when it is started. In this way, the same JanusGraph database instance can be reached through different JanusGraph Servers through both WebSocket and HTTP calls. Make sure the port parameter in each yaml configuration is different if starting more than one JanusGraph Server on the same machine:</p> <pre><code>port: 8182\n</code></pre> <p>In the future, the option to have one running instance of JanusGraph Server supporting multiple protocols may be possible.</p>"},{"location":"basics/server/#using-tinkerpop-gremlin-server-with-janusgraph","title":"Using TinkerPop Gremlin Server with JanusGraph","text":"<p>Since JanusGraph Server is a TinkerPop Gremlin Server packaged  with configuration files for JanusGraph, a version compatible  TinkerPop Gremlin Server can be downloaded separately and used with JanusGraph.  Get started by downloading the appropriate version of Gremlin Server,  which needs to match a version supported by the JanusGraph version in use (3.2.9).</p> <p>Important</p> <p>Any references to file paths in this section refer to paths under a TinkerPop distribution for Gremlin Server and not a JanusGraph distribution with the JanusGraph Server, unless specifically noted.</p> <p>Configuring a standalone Gremlin Server to work with  JanusGraph is similar to configuring the packaged JanusGraph Server.  You should be familiar with graph configuration.  Basically, the Gremlin Server yaml file points to graph-specific configuration files  that are used to instantiate JanusGraph instances that it will then host.  In order to instantiate these Graph instances,  Gremlin Server requires that the appropriate libraries and dependencies  for the JanusGraph be available on its classpath.</p> <p>For purposes of demonstration, these instructions will outline how to configure the BerkeleyDB backend for JanusGraph in Gremlin Server. As stated earlier, Gremlin Server needs JanusGraph dependencies on its classpath. Invoke the following command replacing <code>$VERSION</code> with the version of JanusGraph to use:</p> <pre><code>bin/gremlin-server.sh -i org.janusgraph janusgraph-all $VERSION\n</code></pre> <p>When this process completes, Gremlin Server should now have all the JanusGraph dependencies available to it and will thus be able to instantiate <code>JanusGraph</code> objects.</p> <p>Important</p> <p>The above command uses Groovy Grape and if it is not configured properly  download errors may ensue. Please refer to this section  of the TinkerPop documentation for more information around setting up ~/.groovy/grapeConfig.xml.</p> <p>Create a file called <code>GREMLIN_SERVER_HOME/conf/janusgraph.properties</code> with the following contents: <pre><code>gremlin.graph=org.janusgraph.core.JanusGraphFactory\nstorage.backend=berkeleyje\nstorage.directory=db/berkeley\n</code></pre></p> <p>Configuration of other backends is similar. See Storage Backends. If using Cassandra, then use Cassandra configuration options in the <code>janusgraph.properties</code> file. The only important piece to leave unchanged is the <code>gremlin.graph</code> setting which should always use <code>JanusGraphFactory</code>. This setting tells Gremlin Server how to instantiate a <code>JanusGraph</code> instance.</p> <p>Next create a file called <code>GREMLIN_SERVER_HOME/conf/gremlin-server-janusgraph.yaml</code> that has the following contents: <pre><code>host: localhost\nport: 8182\ngraphs: {\n  graph: conf/janusgraph.properties}\nplugins:\n  - janusgraph.imports\nscriptEngines: {\n  gremlin-groovy: {\n    scripts: [scripts/janusgraph.groovy]}}\nserializers:\n  - { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: { ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] }}\n  - { className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0, config: { serializeResultToString: true }}\nmetrics: {\n  slf4jReporter: {enabled: true, interval: 180000}}\n</code></pre></p> <p>There are several important parts to this configuration file as they relate to JanusGraph.</p> <ol> <li> <p>In the <code>graphs</code> map, there is a key called <code>graph</code> and its value is     <code>conf/janusgraph.properties</code>. This tells Gremlin Server to     instantiate a <code>Graph</code> instance called \"graph\" and use the     <code>conf/janusgraph.properties</code> file to configure it. The \"graph\" key     becomes the unique name for the <code>Graph</code> instance in Gremlin Server     and it can be referenced as such in the scripts submitted to it.</p> </li> <li> <p>In the <code>plugins</code> list, there is a reference to     <code>janusgraph.imports</code>, which tells Gremlin Server to initialize     the \"JanusGraph Plugin\". The \"JanusGraph Plugin\" will auto-import     JanusGraph specific classes for usage in scripts.</p> </li> <li> <p>Note the <code>scripts</code> key and the reference to     <code>scripts/janusgraph.groovy</code>. This Groovy file is an initialization     script for Gremlin Server and that particular ScriptEngine. Create     <code>scripts/janusgraph.groovy</code> with the following contents:</p> </li> </ol> <pre><code>def globals = [:]\nglobals &lt;&lt; [g : graph.traversal()]\n</code></pre> <p>The above script creates a <code>Map</code> called <code>globals</code> and assigns to it a key/value pair. The key is <code>g</code> and its value is a <code>TraversalSource</code> generated from <code>graph</code>, which was configured for Gremlin Server in its configuration file. At this point, there are now two global variables available to scripts provided to Gremlin Server - <code>graph</code> and <code>g</code>.</p> <p>At this point, Gremlin Server is configured and can be used to connect to a new or existing JanusGraph database. To start the server: <pre><code>$ bin/gremlin-server.sh conf/gremlin-server-janusgraph.yaml\n[INFO] GremlinServer -\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\n\n[INFO] GremlinServer - Configuring Gremlin Server from conf/gremlin-server-janusgraph.yaml\n[INFO] MetricManager - Configured Metrics Slf4jReporter configured with interval=180000ms and loggerName=org.apache.tinkerpop.gremlin.server.Settings$Slf4jReporterMetrics\n[INFO] GraphDatabaseConfiguration - Set default timestamp provider MICRO\n[INFO] GraphDatabaseConfiguration - Generated unique-instance-id=7f0000016240-ubuntu1\n[INFO] Backend - Initiated backend operations thread pool of size 8\n[INFO] KCVSLog$MessagePuller - Loaded unidentified ReadMarker start time 2015-10-02T12:28:24.411Z into org.janusgraph.diskstorage.log.kcvs.KCVSLog$MessagePuller@35399441\n[INFO] GraphManager - Graph [graph] was successfully configured via [conf/janusgraph.properties].\n[INFO] ServerGremlinExecutor - Initialized Gremlin thread pool.  Threads in pool named with pattern gremlin-*\n[INFO] ScriptEngines - Loaded gremlin-groovy ScriptEngine\n[INFO] GremlinExecutor - Initialized gremlin-groovy ScriptEngine with scripts/janusgraph.groovy\n[INFO] ServerGremlinExecutor - Initialized GremlinExecutor and configured ScriptEngines.\n[INFO] ServerGremlinExecutor - A GraphTraversalSource is now bound to [g] with graphtraversalsource[standardjanusgraph[berkeleyje:db/berkeley], standard]\n[INFO] AbstractChannelizer - Configured application/vnd.gremlin-v1.0+gryo with org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0\n[INFO] AbstractChannelizer - Configured application/vnd.gremlin-v1.0+gryo-stringd with org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0\n[INFO] GremlinServer$1 - Gremlin Server configured with worker thread pool of 1, gremlin pool of 8 and boss thread pool of 1.\n[INFO] GremlinServer$1 - Channel started at port 8182.\n</code></pre></p> <p>The following section explains how to connect to the running server.</p>"},{"location":"basics/server/#connecting-to-janusgraph-via-gremlin-server","title":"Connecting to JanusGraph via Gremlin Server","text":"<p>Gremlin Server will be ready to listen for WebSocket connections when it is started. The easiest way to test the connection is with Gremlin Console.</p> <p>Follow the instructions here Connecting to Gremlin Server to verify the Gremlin Server is working.</p> <p>Important</p> <p>A difference you should understand is that when working with JanusGraph Server, the Gremlin Console is started from underneath the JanusGraph distribution and when following the test instructions here for a standalone Gremlin Server, the Gremlin Console is started from under the TinkerPop distribution.</p> <pre><code>GryoMapper mapper = GryoMapper.build().addRegistry(JanusGraphIoRegistry.INSTANCE).create();\nCluster cluster = Cluster.build().serializer(new GryoMessageSerializerV3d0(mapper)).create();\nClient client = cluster.connect();\nclient.submit(\"g.V()\").all().get();\n</code></pre> <p>By adding the <code>JanusGraphIoRegistry</code> to the <code>org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV3d0</code>, the driver will know how to properly deserialize custom data types returned by JanusGraph.</p>"},{"location":"basics/server/#extending-janusgraph-server","title":"Extending JanusGraph Server","text":"<p>It is possible to extend Gremlin Server with other means of communication by implementing the interfaces that it provides and leverage this with JanusGraph. See more details in the appropriate TinkerPop documentation.</p>"},{"location":"basics/technical-limitations/","title":"Technical Limitations","text":"<p>There are various limitations and \"gotchas\" that one should be aware of when using JanusGraph. Some of these limitations are necessary design choices and others are issues that will be rectified as JanusGraph development continues. Finally, the last section provides solutions to common issues.</p>"},{"location":"basics/technical-limitations/#design-limitations","title":"Design Limitations","text":"<p>These limitations reflect long-term tradeoffs design tradeoffs which are either difficult or impractical to change. These limitations are unlikely to be removed in the near future.</p>"},{"location":"basics/technical-limitations/#size-limitation","title":"Size Limitation","text":"<p>JanusGraph can store up to a quintillion edges (2^60) and half as many vertices. That limitation is imposed by JanusGraph\u2019s id scheme.</p>"},{"location":"basics/technical-limitations/#datatype-definitions","title":"DataType Definitions","text":"<p>When declaring the data type of a property key using <code>dataType(Class)</code> JanusGraph will enforce that all properties for that key have the declared type, unless that type is <code>Object.class</code>. This is an equality type check, meaning that sub-classes will not be allowed. For instance, one cannot declare the data type to be <code>Number.class</code> and use <code>Integer</code> or <code>Long</code>. For efficiency reasons, the type needs to match exactly. Hence, use <code>Object.class</code> as the data type for type flexibility. In all other cases, declare the actual data type to benefit from increased performance and type safety.</p>"},{"location":"basics/technical-limitations/#edge-retrievals-are-ologk","title":"Edge Retrievals are O(log(k))","text":"<p>Retrieving an edge by id, e.g <code>tx.getEdge(edge.getId())</code>, is not a constant time operation because it requires an index call on one of its adjacent vertices. Hence, the cost of retrieving an individual edge by its id is <code>O(log(k))</code> where <code>k</code> is the number of incident edges on the adjacent vertex. JanusGraph will attempt to pick the adjacent vertex with the smaller degree.</p> <p>This also applies to index retrievals for edges via a standard or external index.</p>"},{"location":"basics/technical-limitations/#type-definitions-cannot-be-changed","title":"Type Definitions cannot be changed","text":"<p>The definition of an edge label, property key, or vertex label cannot be changed once it has been committed to the graph. However, a type can be renamed and new types can be created at runtime to accommodate an evolving schema.</p>"},{"location":"basics/technical-limitations/#reserved-keywords","title":"Reserved Keywords","text":"<p>There are certain keywords that JanusGraph uses internally for types that cannot be used otherwise. These types include vertex labels, edge labels, and property keys. The following are keywords that cannot be used:</p> <ul> <li> <p>vertex</p> </li> <li> <p>element</p> </li> <li> <p>edge</p> </li> <li> <p>property</p> </li> <li> <p>label</p> </li> <li> <p>key</p> </li> </ul> <p>For example, if you attempt to create a vertex with the label of <code>property</code>, you will receive an exception regarding protected system types.</p>"},{"location":"basics/technical-limitations/#temporary-limitations","title":"Temporary Limitations","text":"<p>These are limitations in JanusGraph\u2019s current implementation. These limitations could reasonably be removed in upcoming versions of JanusGraph.</p>"},{"location":"basics/technical-limitations/#limited-mixed-index-support","title":"Limited Mixed Index Support","text":"<p>Mixed indexes only support a subset of the data types that JanusGraph supports. See Mixed Index Data Types for a current listing. Also, mixed indexes do not currently support property keys with SET or LIST cardinality.</p>"},{"location":"basics/technical-limitations/#batch-loading-speed","title":"Batch Loading Speed","text":"<p>JanusGraph provides a batch loading mode that can be enabled through the graph configuration. However, this batch mode only facilitates faster loading into the storage backend, it does not use storage backend specific batch loading techniques that prepare the data in memory for disk storage. As such, batch loading in JanusGraph is currently slower than batch loading modes provided by single machine databases. Bulk Loading contains information on speeding up batch loading in JanusGraph.</p> <p>Another limitation related to batch loading is the failure to load millions of edges into a single vertex at once or in a short time of period. Such supernode loading can fail for some storage backends. This limitation also applies to dense index entries.</p>"},{"location":"basics/transaction-log/","title":"Transaction Log","text":"<p>JanusGraph can automatically log transactional changes for additional processing or as a record of change. To enable logging for a particular transaction, specify the name of the target log during the start of the transaction. <pre><code>tx = graph.buildTransaction().logIdentifier('addedPerson').start()\nu = tx.addVertex(label, 'human')\nu.property('name', 'proteros')\nu.property('age', 36)\ntx.commit()\n</code></pre></p> <p>Upon commit, any changes made during the transaction are logged to the user logging system into a log named <code>addedPerson</code>. The user logging system is a configurable logging backend with a JanusGraph compatible log interface. By default, the log is written to a separate store in the primary storage backend which can be configured as described below. The log identifier specified during the start of the transaction identifies the log in which the changes are recorded thereby allowing different types of changes to be recorded in separate logs for individual processing. <pre><code>tx = graph.buildTransaction().logIdentifier('battle').start()\nh = tx.traversal().V().has('name', 'hercules').next()\nm = tx.addVertex(label, 'monster')\nm.property('name', 'phylatax')\nh.addEdge('battled', m, 'time', 22)\ntx.commit()\n</code></pre></p> <p>JanusGraph provides a user transaction log processor framework to process the recorded transactional changes. The transaction log processor is opened via <code>JanusGraphFactory.openTransactionLog(JanusGraph)</code> against a previously opened JanusGraph graph instance. One can then add processors for a particular log which holds transactional changes. <pre><code>import java.util.concurrent.atomic.*;\nimport org.janusgraph.core.log.*;\nimport java.util.concurrent.*;\nlogProcessor = JanusGraphFactory.openTransactionLog(g);\ntotalHumansAdded = new AtomicInteger(0);\ntotalGodsAdded = new AtomicInteger(0);\nlogProcessor.addLogProcessor(\"addedPerson\").\n    setProcessorIdentifier(\"addedPersonCounter\").\n    setStartTimeNow().\n    addProcessor(new ChangeProcessor() {\n        @Override\n        public void process(JanusGraphTransaction tx, TransactionId txId, ChangeState changeState) {\n            for (v in changeState.getVertices(Change.ADDED)) {\n                if (v.label().equals(\"human\")) totalHumansAdded.incrementAndGet();\n            }\n        }\n    }).\n    addProcessor(new ChangeProcessor() {\n        @Override\n        public void process(JanusGraphTransaction tx, TransactionId txId, ChangeState changeState) {\n            for (v in changeState.getVertices(Change.ADDED)) {\n                if (v.label().equals(\"god\")) totalGodsAdded.incrementAndGet();\n            }\n        }\n    }).\n    build();\n</code></pre></p> <p>In this example, a log processor is built for the user transaction log named <code>addedPerson</code> to process the changes made in transactions which used the <code>addedPerson</code> log identifier. Two change processors are added to this log processor. The first processor counts the number of humans added and the second counts the number of gods added to the graph.</p> <p>When a log processor is built against a particular log, such as the <code>addedPerson</code> log in the example above, it will start reading transactional change records from the log immediately upon successful construction and initialization up to the head of the log. The start time specified in the builder marks the time point in the log where the log processor will start reading records. Optionally, one can specify an identifier for the log processor in the builder. The log processor will use the identifier to regularly persist its state of processing, i.e. it will maintain a marker on the last read log record. If the log processor is later restarted with the same identifier, it will continue reading from the last read record. This is particularly useful when the log processor is supposed to run for long periods of time and is therefore likely to fail. In such failure situations, the log processor can simply be restarted with the same identifier. It must be ensured that log processor identifiers are unique in a JanusGraph cluster in order to avoid conflicts on the persisted read markers.</p> <p>A change processor must implement the <code>ChangeProcessor</code> interface. It\u2019s <code>process()</code> method is invoked for each change record read from the log with a <code>JanusGraphTransaction</code> handle, the id of the transaction that caused the change, and a <code>ChangeState</code> container which holds the transactional changes. The change state container can be queried to retrieve individual elements that were part of the change state. In the example, all added vertices are retrieved. Refer to the API documentation for a description of all the query methods on <code>ChangeState</code>. The provided transaction id can be used to investigate the origin of the transaction which is uniquely identified by the combination of the id of the JanusGraph instance that executed the transaction (<code>txId.getInstanceId()</code>) and the instance specific transaction id (<code>txId.getTransactionId()</code>). In addition, the time of the transaction is available through <code>txId.getTransactionTime()</code>.</p> <p>Change processors are executed individually and in multiple threads. If a change processor accesses global state it must be ensured that such state allows concurrent access. While the log processor reads log records sequentially, the changes are processed in multiple threads so it cannot be guaranteed that the log order is preserved in the change processors.</p> <p>Note, that log processors run each registered change processor at least once for each record in the log which means that a single transactional change record may be processed multiple times under certain failure conditions. One cannot add or remove change processor from a running log processor. In other words, a log processor is immutable after it is built. To change log processing, start a new log processor and shut down an existing one. <pre><code>logProcessor.addLogProcessor(\"battle\").\n    setProcessorIdentifier(\"battleTimer\").\n    setStartTimeNow().\n    addProcessor(new ChangeProcessor() {\n        @Override\n        public void process(JanusGraphTransaction tx, TransactionId txId, ChangeState changeState) {\n            h = tx.V().has(\"name\", \"hercules\").toList().iterator().next();\n            for (edge in changeState.getEdges(h, Change.ADDED, Direction.OUT, \"battled\")) {\n                if (edge.&lt;Integer&gt;value(\"time\")&gt;1000)\n                    h.property(\"oldFighter\", true);\n            }\n        }\n    }).\n    build();\n</code></pre></p> <p>The log processor above processes transactions for the <code>battle</code> log identifier with a single change processor which evaluates <code>battled</code> edges that were added to Hercules. This example demonstrates that the transaction handle passed into the change processor is a normal <code>JanusGraphTransaction</code> which query the JanusGraph graph and make changes to it.</p>"},{"location":"basics/transaction-log/#transaction-log-use-cases","title":"Transaction Log Use Cases","text":""},{"location":"basics/transaction-log/#record-of-change","title":"Record of Change","text":"<p>The user transaction log can be used to keep a record of all changes made against the graph. By using separate log identifiers, changes can be recorded in different logs to distinguish separate transaction types.</p> <p>At any time, a log processor can be built which can processes all recorded changes starting from the desired start time. This can be used for forensic analysis, to replay changes against a different graph, or to compute an aggregate.</p>"},{"location":"basics/transaction-log/#downstream-updates","title":"Downstream Updates","text":"<p>It is often the case that a JanusGraph graph cluster is part of a larger architecture. The user transaction log and the log processor framework provide the tools needed to broadcast changes to other components of the overall system without slowing down the original transactions causing the change. This is particularly useful when transaction latencies need to be low and/or there are a number of other systems that need to be alerted to a change in the graph.</p>"},{"location":"basics/transaction-log/#triggers","title":"Triggers","text":"<p>The user transaction log provides the basic infrastructure to implement triggers that can scale to a large number of concurrent transactions and very large graphs. A trigger is registered with a particular change of data and either triggers an event in an external system or additional changes to the graph. At scale, it is not advisable to implement triggers in the original transaction but rather process triggers with a slight delay through the log processor framework. The second example shows how changes to the graph can be evaluated and trigger additional modifications.</p>"},{"location":"basics/transaction-log/#log-configuration","title":"Log Configuration","text":"<p>There are a number of configuration options to fine tune how the log processor reads from the log. Refer to the complete list of configuration options Configuration Reference for the options under the <code>log</code> namespace. To configure the user transaction log, use the <code>log.user</code> namespace. The options listed there allow the configuration of the number of threads to be used, the number of log records read in each batch, the read interval, and whether the transaction change records should automatically expire and be removed from the log after a configurable amount of time (TTL).</p>"},{"location":"basics/transactions/","title":"Transactions","text":"<p>Almost all interaction with JanusGraph is associated with a transaction. JanusGraph transactions are safe for concurrent use by multiple threads. Methods on a JanusGraph instance like <code>graph.V(...)</code> and <code>graph.tx().commit()</code> perform a <code>ThreadLocal</code> lookup to retrieve or create a transaction associated with the calling thread. Callers can alternatively forego <code>ThreadLocal</code> transaction management in favor of calling <code>graph.tx().createThreadedTx()</code>, which returns a reference to a transaction object with methods to read/write graph data and commit or rollback.</p> <p>JanusGraph transactions are not necessarily ACID. They can be so configured on BerkeleyDB, but they are not generally so on Cassandra or HBase, where the underlying storage system does not provide serializable isolation or multi-row atomic writes and the cost of simulating those properties would be substantial.</p> <p>This section describes JanusGraph\u2019s transactional semantics and API.</p>"},{"location":"basics/transactions/#transaction-handling","title":"Transaction Handling","text":"<p>Every graph operation in JanusGraph occurs within the context of a transaction. According to the TinkerPop\u2019s transactional specification, each thread opens its own transaction against the graph database with the first operation (i.e. retrieval or mutation) on the graph: <pre><code>graph = JanusGraphFactory.open(\"berkeleyje:/tmp/janusgraph\")\njuno = graph.addVertex() //Automatically opens a new transaction\njuno.property(\"name\", \"juno\")\ngraph.tx().commit() //Commits transaction\n</code></pre></p> <p>In this example, a local JanusGraph graph database is opened. Adding the vertex \"juno\" is the first operation (in this thread) which automatically opens a new transaction. All subsequent operations occur in the context of that same transaction until the transaction is explicitly stopped or the graph database is closed. If transactions are still open when <code>close()</code> is called, then the behavior of the outstanding transactions is technically undefined. In practice, any non-thread-bound transactions will usually be effectively rolled back, but the thread-bound transaction belonging to the thread that invoked shutdown will first be committed. Note, that both read and write operations occur within the context of a transaction.</p>"},{"location":"basics/transactions/#transactional-scope","title":"Transactional Scope","text":"<p>All graph elements (vertices, edges, and types) are associated with the transactional scope in which they were retrieved or created. Under TinkerPop\u2019s default transactional semantics, transactions are automatically created with the first operation on the graph and closed explicitly using <code>commit()</code> or <code>rollback()</code>. Once the transaction is closed, all graph elements associated with that transaction become stale and unavailable. However, JanusGraph will automatically transition vertices and types into the new transactional scope as shown in this example: <pre><code>graph = JanusGraphFactory.open(\"berkeleyje:/tmp/janusgraph\")\njuno = graph.addVertex() //Automatically opens a new transaction\ngraph.tx().commit() //Ends transaction\njuno.property(\"name\", \"juno\") //Vertex is automatically transitioned\n</code></pre></p> <p>Edges, on the other hand, are not automatically transitioned and cannot be accessed outside their original transaction. They must be explicitly transitioned: <pre><code>e = juno.addEdge(\"knows\", graph.addVertex())\ngraph.tx().commit() //Ends transaction\ne = g.E(e).next() //Need to refresh edge\ne.property(\"time\", 99)\n</code></pre></p>"},{"location":"basics/transactions/#transaction-failures","title":"Transaction Failures","text":"<p>When committing a transaction, JanusGraph will attempt to persist all changes to the storage backend. This might not always be successful due to IO exceptions, network errors, machine crashes or resource unavailability. Hence, transactions can fail. In fact, transactions will eventually fail in sufficiently large systems. Therefore, we highly recommend that your code expects and accommodates such failures: <pre><code>try {\n    if (g.V().has(\"name\", name).iterator().hasNext())\n        throw new IllegalArgumentException(\"Username already taken: \" + name)\n    user = graph.addVertex()\n    user.property(\"name\", name)\n    graph.tx().commit()\n} catch (Exception e) {\n    //Recover, retry, or return error message\n    println(e.getMessage())\n}\n</code></pre></p> <p>The example above demonstrates a simplified user signup implementation where <code>name</code> is the name of the user who wishes to register. First, it is checked whether a user with that name already exists. If not, a new user vertex is created and the name assigned. Finally, the transaction is committed.</p> <p>If the transaction fails, a <code>JanusGraphException</code> is thrown. There are a variety of reasons why a transaction may fail. JanusGraph differentiates between potentially temporary and permanent failures.</p> <p>Potentially temporary failures are those related to resource unavailability and IO hiccups (e.g. network timeouts). JanusGraph automatically tries to recover from temporary failures by retrying to persist the transactional state after some delay. The number of retry attempts and the retry delay are configurable (see Configuration Reference).</p> <p>Permanent failures can be caused by complete connection loss, hardware failure or lock contention. To understand the cause of lock contention, consider the signup example above and suppose a user tries to signup with username \"juno\". That username may still be available at the beginning of the transaction but by the time the transaction is committed, another user might have concurrently registered with \"juno\" as well and that transaction holds the lock on the username therefore causing the other transaction to fail. Depending on the transaction semantics one can recover from a lock contention failure by re-running the entire transaction.</p> <p>Permanent exceptions that can fail a transaction include:</p> <ul> <li> <p>PermanentLockingException(Local lock contention): Another local     thread has already been granted a conflicting lock.</p> </li> <li> <p>PermanentLockingException(Expected value mismatch for X:     expected=Y vs actual=Z): The verification that the value read in     this transaction is the same as the one in the datastore after     applying for the lock failed. In other words, another transaction     modified the value after it had been read and modified.</p> </li> </ul>"},{"location":"basics/transactions/#multi-threaded-transactions","title":"Multi-Threaded Transactions","text":"<p>JanusGraph supports multi-threaded transactions through TinkerPop\u2019s threaded transactions.  Hence, to speed up transaction processing and utilize multi-core architectures multiple threads can run concurrently in a single transaction.</p> <p>With TinkerPop\u2019s default transaction handling, each thread automatically opens its own transaction against the graph database. To open a thread-independent transaction, use the <code>createThreadedTx()</code> method. <pre><code>threadedGraph = graph.tx().createThreadedTx();\nthreads = new Thread[10];\nfor (int i=0; i&lt;threads.length; i++) {\n    threads[i]=new Thread({\n        println(\"Do something with 'threadedGraph''\");\n    });\n    threads[i].start();\n}\nfor (int i=0; i&lt;threads.length; i++) threads[i].join();\nthreadedGraph.tx().commit();\n</code></pre></p> <p>The <code>createThreadedTx()</code> method returns a new <code>Graph</code> object that represents this newly opened transaction. The graph object <code>tx</code> supports all of the methods that the original graph did, but does so without opening new transactions for each thread. This allows us to start multiple threads which all work concurrently in the same transaction and one of which finally commits the transaction when all threads have completed their work.</p> <p>JanusGraph relies on optimized concurrent data structures to support hundreds of concurrent threads running efficiently in a single transaction.</p>"},{"location":"basics/transactions/#concurrent-algorithms","title":"Concurrent Algorithms","text":"<p>Thread independent transactions started through <code>createThreadedTx()</code> are particularly useful when implementing concurrent graph algorithms. Most traversal or message-passing (ego-centric) like graph algorithms are embarrassingly parallel which means they can be parallelized and executed through multiple threads with little effort. Each of these threads can operate on a single <code>Graph</code> object returned by <code>createThreadedTx()</code> without blocking each other.</p>"},{"location":"basics/transactions/#nested-transactions","title":"Nested Transactions","text":"<p>Another use case for thread independent transactions is nested transactions that ought to be independent from the surrounding transaction.</p> <p>For instance, assume a long running transactional job that has to create a new vertex with a unique name. Since enforcing unique names requires the acquisition of a lock (see Eventually-Consistent Storage Backends for more detail) and since the transaction is running for a long time, lock congestion and expensive transactional failures are likely. <pre><code>v1 = graph.addVertex()\n//Do many other things\nv2 = graph.addVertex()\nv2.property(\"uniqueName\", \"foo\")\nv1.addEdge(\"related\", v2)\n//Do many other things\ngraph.tx().commit() // This long-running tx might fail due to contention on its uniqueName lock\n</code></pre></p> <p>One way around this is to create the vertex in a short, nested thread-independent transaction as demonstrated by the following pseudo code  </p> <pre><code>v1 = graph.addVertex()\n//Do many other things\ntx = graph.tx().createThreadedTx()\nv2 = tx.addVertex()\nv2.property(\"uniqueName\", \"foo\")\ntx.commit() // Any lock contention will be detected here\nv1.addEdge(\"related\", g.V(v2).next()) // Need to load v2 into outer transaction\n//Do many other things\ngraph.tx().commit() // Can't fail due to uniqueName write lock contention involving v2\n</code></pre>"},{"location":"basics/transactions/#common-transaction-handling-problems","title":"Common Transaction Handling Problems","text":"<p>Transactions are started automatically with the first operation executed against the graph. One does NOT have to start a transaction manually. The method <code>newTransaction</code> is used to start multi-threaded transactions only.</p> <p>Transactions are automatically started under the TinkerPop semantics but not automatically terminated. Transactions must be terminated manually with <code>commit()</code> or <code>rollback()</code>. If a <code>commit()</code> transactions fails, it should be terminated manually with <code>rollback()</code> after catching the failure. Manual termination of transactions is necessary because only the user knows the transactional boundary.</p> <p>A transaction will attempt to maintain its state from the beginning of the transaction. This might lead to unexpected behavior in multi-threaded applications as illustrated in the following artificial example  </p> <pre><code>v = g.V(4).next() // Retrieve vertex, first action automatically starts transaction\ng.V(v).bothE()\n&gt;&gt; returns nothing, v has no edges\n//thread is idle for a few seconds, another thread adds edges to v\ng.V(v).bothE()\n&gt;&gt; still returns nothing because the transactional state from the beginning is maintained\n</code></pre> <p>Such unexpected behavior is likely to occur in client-server applications where the server maintains multiple threads to answer client requests. It is therefore important to terminate the transaction after a unit of work (e.g. code snippet, query, etc). So, the example above should be:</p> <pre><code>v = g.V(4).next() // Retrieve vertex, first action automatically starts transaction\ng.V(v).bothE()\ngraph.tx().commit()\n//thread is idle for a few seconds, another thread adds edges to v\ng.V(v).bothE()\n&gt;&gt; returns the newly added edge\ngraph.tx().commit()\n</code></pre> <p>When using multi-threaded transactions via <code>newTransaction</code> all vertices and edges retrieved or created in the scope of that transaction are not available outside the scope of that transaction. Accessing such elements after the transaction has been closed will result in an exception. As demonstrated in the example above, such elements have to be explicitly refreshed in the new transaction using <code>g.V(existingVertex)</code> or <code>g.E(existingEdge)</code>.</p>"},{"location":"basics/transactions/#transaction-configuration","title":"Transaction Configuration","text":"<p>JanusGraph\u2019s <code>JanusGraph.buildTransaction()</code> method gives the user the ability to configure and start a new multi-threaded transaction  against a <code>JanusGraph</code>. Hence, it is identical to <code>JanusGraph.newTransaction()</code> with additional configuration options.</p> <p><code>buildTransaction()</code> returns a <code>TransactionBuilder</code> which allows the following aspects of a transaction to be configured:</p> <ul> <li> <p><code>readOnly()</code> - makes the transaction read-only and any attempt to     modify the graph will result in an exception.</p> </li> <li> <p><code>enableBatchLoading()</code> - enables batch-loading for an individual     transaction. This setting results in similar efficiencies as the     graph-wide setting <code>storage.batch-loading</code> due to the disabling of     consistency checks and other optimizations. Unlike     <code>storage.batch-loading</code> this option will not change the behavior of     the storage backend.</p> </li> <li> <p><code>setTimestamp(long)</code> - Sets the timestamp for this transaction as     communicated to the storage backend for persistence. Depending on     the storage backend, this setting may be ignored. For eventually     consistent backends, this is the timestamp used to resolve write     conflicts. If this setting is not explicitly specified, JanusGraph     uses the current time.</p> </li> <li> <p><code>setVertexCacheSize(long size)</code> - The number of vertices this     transaction caches in memory. The larger this number, the more     memory a transaction can potentially consume. If this number is too     small, a transaction might have to re-fetch data which causes delays     in particular for long running transactions.</p> </li> <li> <p><code>checkExternalVertexExistence(boolean)</code> - Whether this transaction     should verify the existence of vertices for user provided vertex     ids. Such checks requires access to the database which takes time.     The existence check should only be disabled if the user is     absolutely sure that the vertex must exist - otherwise data     corruption can ensue.</p> </li> <li> <p><code>checkInternalVertexExistence(boolean)</code> - Whether this transaction     should double-check the existence of vertices during query     execution. This can be useful to avoid phantom vertices on     eventually consistent storage backends. Disabled by default.     Enabling this setting can slow down query processing.</p> </li> <li> <p><code>consistencyChecks(boolean)</code> - Whether JanusGraph should enforce     schema level consistency constraints (e.g. multiplicity     constraints). Disabling consistency checks leads to better     performance but requires that the user ensures consistency     confirmation at the application level to avoid inconsistencies. USE     WITH GREAT CARE!</p> </li> </ul> <p>Once, the desired configuration options have been specified, the new transaction is started via <code>start()</code> which returns a <code>JanusGraphTransaction</code>.</p>"},{"location":"connecting/","title":"Introduction","text":"<p>JanusGraph can be queried from all languages for which a TinkerPop driver exists. Drivers allow sending of Gremlin traversals to a Gremlin Server like the JanusGraph Server. A list of TinkerPop drivers is available on TinkerPop\u2019s homepage.</p> <p>In addition to drivers, there exist  query languages for TinkerPop that make it easier to use Gremlin in different programming languages like Java, Python, or C#. Some of these languages even construct Gremlin traversals from completely different query languages like Cypher or SPARQL. Since JanusGraph implements TinkerPop, all of these languages can be used together with JanusGraph.</p>"},{"location":"connecting/dotnet/","title":"Connecting from .NET","text":"<p>Gremlin traversals can be constructed with Gremlin.Net just like in Gremlin-Java or Gremiln-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources. The main syntactical difference for Gremlin.Net is that it follows .NET naming conventions, e.g., method names use PascalCase instead of camelCase.</p>"},{"location":"connecting/dotnet/#getting-started-with-janusgraph-and-gremlinnet","title":"Getting Started with JanusGraph and Gremlin.Net","text":"<p>To get started with Gremlin.Net:</p> <ol> <li> <p>Create a console application: <pre><code>dotnet new console -o GremlinExample\n</code></pre></p> </li> <li> <p>Add Gremlin.Net: <pre><code>dotnet add package Gremlin.Net -v 3.2.9\n</code></pre></p> </li> <li> <p>Create a <code>GraphTraversalSource</code> which is the basis for all Gremlin     traversals: <pre><code>var graph = new Graph();\nvar client = new GremlinClient(new GremlinServer(\"localhost\", 8182));\n// The client should be disposed on shut down to release resources\n// and to close open connections with client.Dispose()\nvar g = graph.Traversal().WithRemote(new DriverRemoteConnection(client));\n        // Reuse 'g' across the application\n</code></pre></p> </li> <li> <p>Execute a simple traversal: <pre><code>var herculesAge = g.V().Has(\"name\", \"hercules\").Values&lt;int&gt;(\"age\").Next();\nConsole.WriteLine($\"Hercules is {herculesAge} years old.\");\n</code></pre>     The traversal can also be executed asynchronously by using     <code>Promise()</code> which is the recommended way as the underlying driver in     Gremlin.Net also works asynchronously: <pre><code>var herculesAge = await g.V().Has(\"name\", \"hercules\").Values&lt;int&gt;(\"age\").Promise(t =&gt; t.Next());\n</code></pre></p> </li> </ol>"},{"location":"connecting/dotnet/#janusgraph-specific-types-and-predicates","title":"JanusGraph Specific Types and Predicates","text":"<p>JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin.Net.</p>"},{"location":"connecting/java/","title":"Connecting from Java","text":"<p>While it is possible to embed JanusGraph as a library inside a Java application and then directly connect to the backend, this section assumes that the application connects to JanusGraph Server. For information on how to embed JanusGraph, see the JanusGraph Examples projects.</p> <p>This section only covers how applications can connect to JanusGraph Server. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources.</p>"},{"location":"connecting/java/#getting-started-with-janusgraph-and-gremlin-java","title":"Getting Started with JanusGraph and Gremlin-Java","text":"<p>To get started with JanusGraph in Java:</p> <ol> <li>Create an application with Maven: <pre><code>mvn archetype:generate -DgroupId=com.mycompany.project\n    -DartifactId=gremlin-example\n    -DarchetypeArtifactId=maven-archetype-quickstart\n    -DinteractiveMode=false\n</code></pre></li> <li>Add dependencies on <code>janusgraph-core</code> and <code>gremlin-driver</code> to the dependency manager:</li> </ol> <p>```xml tab='Maven'  org.janusgraph janusgraph-core 0.2.3 org.apache.tinkerpop gremlin-driver 3.2.9 <pre><code>```groovy tab='Gradle'\ncompile \"org.janusgraph:janusgraph-core:0.2.3\"\ncompile \"org.apache.tinkerpop:gremlin-driver:3.2.9\"\n</code></pre></p> <ol> <li>Add two configuration files, <code>conf/remote-graph.properties</code> and     <code>conf/remote-objects.yaml</code>:</li> </ol> <p>```conf tab='conf/remote-graph.properties' gremlin.remote.remoteConnectionClass=org.apache.tinkerpop.gremlin.driver.remote.DriverRemoteConnection gremlin.remote.driver.clusterFile=conf/remote-objects.yaml gremlin.remote.driver.sourceName=g <pre><code>```yaml tab='conf/remote-objects.yaml'\nhosts: [localhost]\nport: 8182\nserializer: { \n    className: org.apache.tinkerpop.gremlin.driver.ser.GryoMessageSerializerV1d0,\n    config: { ioRegistries: [org.janusgraph.graphdb.tinkerpop.JanusGraphIoRegistry] }}\n</code></pre></p> <ol> <li>Create a <code>GraphTraversalSource</code> which is the basis for all Gremlin traversals: <pre><code>    Graph graph = EmptyGraph.instance();\n    GraphTraversalSource g = graph.traversal().withRemote(\"conf/remote-graph.properties\");\n    // Reuse 'g' across the application\n    // and close it on shut-down to close open connections with g.close()\n</code></pre></li> <li>Execute a simple traversal: <pre><code>Object herculesAge = g.V().has(\"name\", \"hercules\").values(\"age\").next();\nSystem.out.println(\"Hercules is \" + herculesAge + \" years old.\");\n</code></pre> <code>next()</code> is a terminal step that submits the traversal to the Gremlin Server and returns a single result.</li> </ol>"},{"location":"connecting/java/#janusgraph-specific-types-and-predicates","title":"JanusGraph Specific Types and Predicates","text":"<p>JanusGraph specific types and predicates can be used directly from a Java application through the dependency <code>janusgraph-core</code>.</p>"},{"location":"connecting/python/","title":"Connecting from Python","text":"<p>Gremlin traversals can be constructed with Gremlin-Python just like in Gremlin-Java or Gremiln-Groovy. Refer to Gremlin Query Language for an introduction to Gremlin and pointers to further resources.</p> <p>Important</p> <p>Some Gremlin step and predicate names are reserved words in Python. Those names are simply postfixed with <code>_</code> in Gremlin-Python, e.g., <code>in()</code> becomes <code>in_()</code>, <code>not()</code> becomes <code>not_()</code>, and so on. The other names affected by this are: <code>all</code>, <code>and</code>, <code>as</code>, <code>from</code>, <code>global</code>, <code>is</code>, <code>list</code>, <code>or</code>, and <code>set</code>.</p>"},{"location":"connecting/python/#getting-started-with-janusgraph-and-gremlin-python","title":"Getting Started with JanusGraph and Gremlin-Python","text":"<p>To get started with Gremlin-Python:</p> <ol> <li>Install Gremlin-Python: <pre><code>pip install gremlinpython==3.2.9\n</code></pre></li> <li> <p>Create a text file <code>gremlinexample.py</code> and add the following imports     to it: <pre><code>from gremlin_python import statics\nfrom gremlin_python.structure.graph import Graph\nfrom gremlin_python.process.graph_traversal import __\nfrom gremlin_python.driver.driver_remote_connection import DriverRemoteConnection\n</code></pre></p> </li> <li> <p>Create a <code>GraphTraversalSource</code> which is the basis for all Gremlin     traversals: <pre><code>graph = Graph()\nconnection = DriverRemoteConnection('ws://localhost:8182/gremlin', 'g')\n# The connection should be closed on shut down to close open connections with connection.close()\ng = graph.traversal().withRemote(connection)\n# Reuse 'g' across the application\n</code></pre></p> </li> <li> <p>Execute a simple traversal: <pre><code>herculesAge = g.V().has('name', 'hercules').values('age').next()\nprint('Hercules is {} years old.'.format(herculesAge))\n</code></pre> <code>next()</code> is a terminal step that submits the traversal to the     Gremlin Server and returns a single result.</p> </li> </ol>"},{"location":"connecting/python/#janusgraph-specific-types-and-predicates","title":"JanusGraph Specific Types and Predicates","text":"<p>JanusGraph contains some types and predicates that are not part of Apache TinkerPop and are therefore also not supported by Gremlin-Python.</p>"},{"location":"index-backend/","title":"Introduction","text":"<p>While JanusGraph's composite graph indexes are natively supported through the primary storage backend, mixed graph indexes require that an indexing backend is configured. Mixed indexes provide support for geo, numeric range, and full-text search.</p> <p>The choice of index backend determines which search features are supported, as well as the performance and scalability of the index. JanusGraph currently supports three index backends: Elasticsearch, Apache Solr and Apache Lucene.</p> <p>Use Elasticsearch or Apache Solr when there is an expectation that JanusGraph will be distributed across multiple machines. Apache Lucene performs better in small scale, single machine applications. It performs better in unit tests, for instance.</p>"},{"location":"index-backend/direct-index-query/","title":"Direct Index Query","text":"<p>JanusGraph\u2019s standard global graph querying mechanism supports boolean queries for vertices or edges. In other words, an element either matches the query or it does not. There are no partial matches or result scoring.</p> <p>Some indexing backends additionally support fuzzy search queries. For those queries, a score is computed for each match to indicate the \"goodness\" of the match and results are returned in the order of their score. Fuzzy search is particularly useful when dealing with full-text search queries where matching more words is considered to be better.</p> <p>Since fuzzy search implementations and scoring algorithms differ significantly between indexing backends, JanusGraph does not support fuzzy search natively. However, JanusGraph provides a direct index query mechanism that allows search queries to be directly send to the indexing backend for evaluation (for those backends that support it).</p> <p>Use <code>Graph.indexQuery()</code> to compose a query that is executed directly against an indexing backend. This query builder expects two parameters:</p> <ol> <li> <p>The name of the indexing backend to query. This must be the name     configured in JanusGraph\u2019s configuration and used in the property     key indexing definitions</p> </li> <li> <p>The query string</p> </li> </ol> <p>The builder allows configuration of the maximum number of elements to be returned via its <code>limit(int)</code> method. The builder\u2019s <code>offset(int)</code> controls number of initial matches in the result set to skip. To retrieve all vertex or edges matching the given query in the specified indexing backend, invoke <code>vertices()</code> or <code>edges()</code>, respectively. It is not possible to query for both vertices and edges at the same time. These methods return an <code>Iterable</code> over <code>Result</code> objects. A result object contains the matched handle, retrievable via <code>getElement()</code>, and the associated score - <code>getScore()</code>.</p> <p>Consider the following example: <pre><code>ManagementSystem mgmt = graph.openManagement();\nPropertyKey text = mgmt.makePropertyKey(\"text\").dataType(String.class).make();\nmgmt.buildIndex(\"vertexByText\", Vertex.class).addKey(text).buildMixedIndex(\"search\");\nmgmt.commit();\n// ... Load vertices ...\nfor (Result&lt;Vertex&gt; result : graph.indexQuery(\"vertexByText\", \"v.text:(farm uncle berry)\").vertices()) {\n   System.out.println(result.getElement() + \": \" + result.getScore());\n}\n</code></pre></p>"},{"location":"index-backend/direct-index-query/#query-string","title":"Query String","text":"<p>The query string is handed directly to the indexing backend for processing and hence the query string syntax depends on what is supported by the indexing backend. For vertex queries, JanusGraph will analyze the query string for property key references starting with \"v.\" and replace those by a handle to the indexing field that corresponds to the property key. Likewise, for edge queries, JanusGraph will replace property key references starting with \"e.\". Hence, to refer to a property of a vertex, use \"v.[KEY_NAME]\" in the query string. Likewise, for edges write \"e.[KEY_NAME]\".</p> <p>Elasticsearch and Lucene support the Lucene query syntax. Refer to the Lucene documentation or the Elasticsearch documentation for more information. The query used in the example above follows the Lucene query syntax.</p> <pre><code>graph.indexQuery(\"vertexByText\", \"v.text:(farm uncle berry)\").vertices()\n</code></pre> <p>This query matches all vertices where the text contains any of the three words (grouped by parentheses) and score matches higher the more words are matched in the text.</p> <p>In addition Elasticsearch supports wildcard queries, use \"v.*\" or \"e.*\" in the query string to query if any of the properties on the element match.</p>"},{"location":"index-backend/direct-index-query/#query-totals","title":"Query Totals","text":"<p>It is sometimes useful to know how many total results were returned from a query without having to retrieve all results. Fortunately, Elasticsearch and Solr provide a shortcut that does not involve retrieving and ranking all documents. This shortcut is exposed through the \".vertexTotals()\", \".edgeTotals()\", and \".propertyTotals()\" methods.</p> <p>The totals can be retrieved using the same query syntax as the indexQuery builder, but size is overwritten to be 0. <pre><code>graph.indexQuery(\"vertexByText\", \"v.text:(farm uncle berry)\").vertexTotals()\n</code></pre></p>"},{"location":"index-backend/direct-index-query/#gotchas","title":"Gotchas","text":""},{"location":"index-backend/direct-index-query/#property-key-names","title":"Property Key Names","text":"<p>Names of property keys that contain non-alphanumeric characters must be placed in quotation marks to ensure that the query is parsed correctly. <pre><code>graph.indexQuery(\"vertexByText\", \"v.\\\"first_name\\\":john\").vertices()\n</code></pre></p> <p>Some property key names may be transformed by the JanusGraph indexing backend implementation. For instance, an indexing backend that does not permit spaces in field names may transform \"My Field Name\" to \"My\u2022Field\u2022Name\", or an indexing backend like Solr may append type information to the name, transforming \"myBooleanField\" to \"myBooleanField_b\". These transformations happen in the index backend\u2019s implementation of IndexProvider, in the \"mapKey2Field\" method. Indexing backends may reserve special characters (such as \u2022) and prohibit indexing of fields that contain them. For this reason it is recommended to avoid spaces and special characters in property names.</p> <p>In general, making direct index queries depends on implementation details of JanusGraph indexing backends that are normally hidden from users, so it\u2019s best to verify a query empirically against the indexing backend in use.</p>"},{"location":"index-backend/direct-index-query/#element-identifier-collision","title":"Element Identifier Collision","text":"<p>The strings \"v.\", \"e.\", and \"p.\" are used to identify a vertex, edge or property element respectively in a query. If the field name or the query value contains the same sequence of characters, this can cause a collision in the query string and parsing errors as in the following example: <pre><code>graph.indexQuery(\"vertexByText\", \"v.name:v.john\").vertices() //DOES NOT WORK!\n</code></pre></p> <p>To avoid such identifier collisions, use the <code>setElementIdentifier</code> method to define a unique element identifier string that does not occur in any other parts of the query: <pre><code>graph.indexQuery(\"vertexByText\", \"$v$name:v.john\").setElementIdentifier(\"$v$\").vertices()\n</code></pre></p>"},{"location":"index-backend/direct-index-query/#mixed-index-availability-delay","title":"Mixed Index Availability Delay","text":"<p>When a query traverses a mixed index immediately after data is inserted the changes may not be visible. In Elasticsearch the configuration option that determines this delay is index refresh interval. In Solr the primary configuration option is max time.</p>"},{"location":"index-backend/elasticsearch/","title":"Elasticsearch","text":"<p>Elasticsearch is a distributed, RESTful search and analytics engine capable of solving a growing number of use cases. As the heart of the Elastic Stack, it centrally stores your data so you can discover the expected and uncover the unexpected.</p> <p>\u2014  Elasticsearch Overview</p> <p>JanusGraph supports Elasticsearch as an index backend. Here are some of the Elasticsearch features supported by JanusGraph:</p> <ul> <li>Full-Text: Supports all <code>Text</code> predicates to search for text     properties that matches a given word, prefix or regular expression.</li> <li>Geo: Supports all <code>Geo</code> predicates to search for geo properties     that are intersecting, within, disjoint to or contained in a given     query geometry. Supports points, circles, boxes, lines and polygons     for indexing. Supports circles, boxes and polygons for querying     point properties and all shapes for querying non-point properties.</li> <li>Numeric Range: Supports all numeric comparisons in <code>Compare</code>.</li> <li>Flexible Configuration: Supports remote operation and open-ended     settings customization.</li> <li>Collections: Supports indexing SET and LIST cardinality     properties.</li> <li>Temporal: Nanosecond granularity temporal indexing.</li> <li>Custom Analyzer: Choose to use a custom analyzer</li> </ul> <p>Please see Version Compatibility for details on what versions of Elasticsearch will work with JanusGraph.</p> <p>Important</p> <p>Beginning with Elasticsearch 5.0 JanusGraph uses sandboxed Painless scripts for inline updates, which are enabled by default in Elasticsearch 5.x.</p> <p>Using JanusGraph with Elasticsearch 2.x requires enabling Groovy inline scripting by setting <code>script.engine.groovy.inline.update</code> to <code>true</code> on the Elasticsearch cluster (see dynamic scripting documentation for more information).</p>"},{"location":"index-backend/elasticsearch/#running-elasticsearch","title":"Running Elasticsearch","text":"<p>JanusGraph supports connections to a running Elasticsearch cluster. JanusGraph provides two options for running local Elasticsearch instances for getting started quickly. JanusGraph server (see Getting started) automatically starts a local Elasticsearch instance. Alternatively JanusGraph releases include a full Elasticsearch distribution to allow users to manually start a local Elasticsearch instance (see this page for more information).</p> <pre><code>$ elasticsearch/bin/elasticsearch\n</code></pre> <p>Note</p> <p>For security reasons Elasticsearch must be run under a non-root account</p>"},{"location":"index-backend/elasticsearch/#elasticsearch-configuration-overview","title":"Elasticsearch Configuration Overview","text":"<p>JanusGraph supports HTTP(S) client connections to a running Elasticsearch cluster. Please see Version Compatibility for details on what versions of Elasticsearch will work with the different client types in JanusGraph.</p> <p>Note</p> <p>JanusGraph\u2019s index options start with the string \"<code>index.[X].</code>\" where \"<code>[X]</code>\" is a user-defined name for the backend. This user-defined name must be passed to JanusGraph\u2019s ManagementSystem interface when building a mixed index, as described in Mixed Index, so that JanusGraph knows which of potentially multiple configured index backends to use. Configuration snippets in this chapter use the name <code>search</code>, whereas prose discussion of options typically write <code>[X]</code> in the same position. The exact index name is not significant as long as it is used consistently in JanusGraph\u2019s configuration and when administering indices.</p> <p>Tip</p> <p>It\u2019s recommended that index names contain only alphanumeric lowercase characters and hyphens, and that they start with a lowercase letter.</p>"},{"location":"index-backend/elasticsearch/#connecting-to-elasticsearch","title":"Connecting to Elasticsearch","text":"<p>The Elasticsearch client is specified as follows: <pre><code>index.search.backend=elasticsearch\n</code></pre></p> <p>When connecting to Elasticsearch a single or list of hostnames for the Elasticsearch instances must be provided. These are supplied via JanusGraph\u2019s <code>index.[X].hostname</code> key. <pre><code>index.search.backend=elasticsearch\nindex.search.hostname=10.0.0.10:9200\n</code></pre></p> <p>Each host or host:port pair specified here will be added to the HTTP client\u2019s round-robin list of request targets. Here\u2019s a minimal configuration that will round-robin over 10.0.0.10 on the default Elasticsearch HTTP port (9200) and 10.0.0.20 on port 7777: <pre><code>index.search.backend=elasticsearch\nindex.search.hostname=10.0.0.10, 10.0.0.20:7777\n</code></pre></p>"},{"location":"index-backend/elasticsearch/#janusgraph-indexx-and-indexxelasticsearch-options","title":"JanusGraph <code>index.[X]</code> and <code>index.[X].elasticsearch</code> options","text":"<p>JanusGraph only uses default values for <code>index-name</code> and <code>health-request-timeout</code>. See Configuration Reference for descriptions of these options and their accepted values.</p> <ul> <li><code>index.[X].elasticsearch.index-name</code></li> <li><code>index.[X].elasticsearch.health-request-timeout</code></li> </ul>"},{"location":"index-backend/elasticsearch/#rest-client-options","title":"REST Client Options","text":"<p>The REST client accepts the <code>index.[X].bulk-refresh</code> option. This option controls when changes are made visible to search. See ?refresh documentation for more information.</p>"},{"location":"index-backend/elasticsearch/#ingest-pipelines","title":"Ingest Pipelines","text":"<p>If using Elasticsearch 5.0 or higher, a different ingest pipelines can be set for each mixed index. Ingest pipeline can be use to pre-process documents before indexing. A pipeline is composed by a series of processors. Each processor transforms the document in some way. For example date processor can extract a date from a text to a date field. So you can query this date with JanusGraph without it being physically in the primary storage.</p> <ul> <li><code>index.[X].elasticsearch.ingest-pipeline.[mixedIndexName] = pipeline_id</code></li> </ul> <p>See ingest documentation for more information about ingest pipelines and processors documentation for more information about ingest processors.</p>"},{"location":"index-backend/elasticsearch/#secure-elasticsearch","title":"Secure Elasticsearch","text":"<p>Elasticsearch does not perform authentication or authorization. A client that can connect to Elasticsearch is trusted by Elasticsearch. When Elasticsearch runs on an unsecured or public network, particularly the Internet, it should be deployed with some type of external security. This is generally done with a combination of firewalling, tunneling of Elasticsearch\u2019s ports or by using Elasticsearch extensions such as X-Pack. Elasticsearch has two client-facing ports to consider:</p> <ul> <li>The HTTP REST API, usually on port 9200</li> <li>The native \"transport\" protocol, usually on port 9300</li> </ul> <p>A client uses either one protocol/port or the other, but not both simultaneously. Securing the HTTP protocol port is generally done with a combination of firewalling and a reverse proxy with SSL encryption and HTTP authentication. There are a couple of ways to approach security on the native \"transport\" protocol port:</p> <p>Tunnel Elasticsearch\u2019s native \"transport\" protocol This approach can be implemented with SSL/TLS tunneling (for instance via stunnel), a VPN, or SSH port forwarding. SSL/TLS tunnels require non-trivial setup and monitoring: one or both ends of the tunnel need a certificate, and the stunnel processes need to be configured and running continuously. The setup for most secure VPNs is likewise non-trivial. Some Elasticsearch service providers handle server-side tunnel management and provide a custom Elasticsearch <code>transport.type</code> to simplify the client setup.</p> <p>Add a firewall rule that allows only trusted clients to connect on Elasticsearch\u2019s native protocol port This is typically done at the host firewall level. Easy to configure, but very weak security by itself.</p>"},{"location":"index-backend/elasticsearch/#index-creation-options","title":"Index Creation Options","text":"<p>JanusGraph supports customization of the index settings it uses when creating its Elasticsearch index. It allows setting arbitrary key-value pairs on the <code>settings</code> object in the Elasticsearch <code>create index</code> request issued by JanusGraph. Here is a non-exhaustive sample of Elasticsearch index settings that can be customized using this mechanism:</p> <ul> <li><code>index.number_of_replicas</code></li> <li><code>index.number_of_shards</code></li> <li><code>index.refresh_interval</code></li> </ul> <p>Settings customized through this mechanism are only applied when JanusGraph attempts to create its index in Elasticsearch. If JanusGraph finds that its index already exists, then it does not attempt to recreate it, and these settings have no effect.</p>"},{"location":"index-backend/elasticsearch/#embedding-elasticsearch-index-creation-settings-with-createext","title":"Embedding Elasticsearch index creation settings with <code>create.ext</code>","text":"<p>JanusGraph iterates over all properties prefixed with <code>index.[X].elasticsearch.create.ext.</code>, where <code>[X]</code> is an index name such as <code>search</code>. It strips the prefix from each property key. The remainder of the stripped key will be interpreted as an Elasticsearch index creation setting. The value associated with the key is not modified. The stripped key and unmodified value are passed as part of the <code>settings</code> object in the Elasticsearch create index request that JanusGraph issues when bootstrapping on Elasticsearch. This allows embedding arbitrary index creation settings settings in JanusGraph\u2019s properties. Here\u2019s an example configuration fragment that customizes three Elasticsearch index settings using the <code>create.ext</code> config mechanism: <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.create.ext.number_of_shards=15\nindex.search.elasticsearch.create.ext.number_of_replicas=3\nindex.search.elasticsearch.create.ext.shard.check_on_startup=true\n</code></pre></p> <p>The configuration fragment listed above takes advantage of Elasticsearch\u2019s assumption, implemented server-side, that unqualified <code>create index</code> setting keys have an <code>index.</code> prefix. It\u2019s also possible to spell out the index prefix explicitly. Here\u2019s a JanusGraph config file functionally equivalent to the one listed above, except that the <code>index.</code> prefix before the index creation settings is explicit: <pre><code>index.search.backend=elasticsearch\nindex.search.elasticsearch.create.ext.index.number_of_shards=15\nindex.search.elasticsearch.create.ext.index.number_of_replicas=3\nindex.search.elasticsearch.create.ext.index.shard.check_on_startup=false\n</code></pre></p> <p>Tip</p> <p>The <code>create.ext</code> mechanism for specifying index creation settings is compatible with JanusGraph\u2019s Elasticsearch configuration.</p>"},{"location":"index-backend/elasticsearch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"index-backend/elasticsearch/#connection-issues-to-remote-elasticsearch-cluster","title":"Connection Issues to remote Elasticsearch cluster","text":"<p>Check that the Elasticsearch cluster nodes are reachable on the HTTP protocol port from the JanusGraph nodes. Check the node listen port by examining the Elasticsearch node configuration logs or using a general diagnostic utility like <code>netstat</code>. Check the JanusGraph configuration.</p>"},{"location":"index-backend/elasticsearch/#optimizing-elasticsearch","title":"Optimizing Elasticsearch","text":""},{"location":"index-backend/elasticsearch/#write-optimization","title":"Write Optimization","text":"<p>For bulk loading or other write-intense applications, consider increasing Elasticsearch\u2019s refresh interval. Refer to this discussion on how to increase the refresh interval and its impact on write performance. Note, that a higher refresh interval means that it takes a longer time for graph mutations to be available in the index.</p> <p>For additional suggestions on how to increase write performance in Elasticsearch with detailed instructions, please read this blog post.</p>"},{"location":"index-backend/elasticsearch/#further-reading","title":"Further Reading","text":"<ul> <li>Please refer to the Elasticsearch homepage     and available documentation for more information on Elasticsearch     and how to setup an Elasticsearch cluster.</li> </ul>"},{"location":"index-backend/field-mapping/","title":"Field Mapping","text":""},{"location":"index-backend/field-mapping/#individual-field-mapping","title":"Individual Field Mapping","text":"<p>By default, JanusGraph will encode property keys to generate a unique field name for the property key in the mixed index. If one wants to query the mixed index directly in the external index backend can be difficult to deal with and are illegible. For this use case, the field name can be explicitly specified through a parameter. <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('bookname').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(name, Parameter.of('mapped-name', 'bookname')).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p> <p>With this field mapping defined as a parameter, JanusGraph will use the same name for the field in the <code>booksBySummary</code> index created in the external index system as for the property key. Note, that it must be ensured that the given field name is unique in the index.</p>"},{"location":"index-backend/field-mapping/#global-field-mapping","title":"Global Field Mapping","text":"<p>Instead of individually adjusting the field mapping for every key added to a mixed index, one can instruct JanusGraph to always set the field name in the external index to be identical to the property key name. This is accomplished by enabling the configuration option <code>map-name</code> which is configured per indexing backend. If this option is enabled for a particular indexing backend, then all mixed indexes defined against said backend will use field names identical to the property key names.</p> <p>However, this approach has two limitations: 1) The user has to ensure that the property key names are valid field names for the indexing backend and 2) renaming the property key will NOT rename the field name in the index which can lead to naming collisions that the user has to be aware of and avoid.</p> <p>Note, that individual field mappings as described above can be used to overwrite the default name for a particular key.</p>"},{"location":"index-backend/field-mapping/#custom-analyzer","title":"Custom Analyzer","text":"<p>By default, JanusGraph will use the default analyzer from the indexing backend for properties with Mapping.TEXT, and no analyzer for properties with Mapping.STRING. If one wants to use another analyzer, it can be explicitly specified through a parameter : ParameterType.TEXT_ANALYZER for Mapping.TEXT and ParameterType.STRING_ANALYZER for Mapping.STRING.</p>"},{"location":"index-backend/field-mapping/#for-elasticsearch","title":"For Elasticsearch","text":"<p>The name of the analyzer must be set as parameter value. <pre><code>mgmt = graph.openManagement()\nstring = mgmt.makePropertyKey('string').dataType(String.class).make()\ntext = mgmt.makePropertyKey('text').dataType(String.class).make()\ntextString = mgmt.makePropertyKey('textString').dataType(String.class).make()\nmgmt.buildIndex('string', Vertex.class).addKey(string, Mapping.STRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), 'standard')).buildMixedIndex(\"search\")\nmgmt.buildIndex('text', Vertex.class).addKey(text, Mapping.TEXT.asParameter(), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), 'english')).buildMixedIndex(\"search\")\nmgmt.buildIndex('textString', Vertex.class).addKey(text, Mapping.TEXTSTRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), 'standard'), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), 'english')).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p> <p>With these settings, JanusGraph will use the standard analyzer for property key string and the english analyzer for property key text.</p>"},{"location":"index-backend/field-mapping/#for-solr","title":"For Solr","text":"<p>The class of the tokenizer must be set as parameter value. <pre><code>mgmt = graph.openManagement()\nstring = mgmt.makePropertyKey('string').dataType(String.class).make()\ntext = mgmt.makePropertyKey('text').dataType(String.class).make()\nmgmt.buildIndex('string', Vertex.class).addKey(string, Mapping.STRING.asParameter(), Parameter.of(ParameterType.STRING_ANALYZER.getName(), 'org.apache.lucene.analysis.standard.StandardTokenizer')).buildMixedIndex(\"search\")\nmgmt.buildIndex('text', Vertex.class).addKey(text, Mapping.TEXT.asParameter(), Parameter.of(ParameterType.TEXT_ANALYZER.getName(), 'org.apache.lucene.analysis.core.WhitespaceTokenizer')).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p> <p>With these settings, JanusGraph will use the standard tokenizer for property key string and the whitespace tokenizer for property key text.</p>"},{"location":"index-backend/lucene/","title":"Apache Lucene","text":"<p>Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java. It is a technology suitable for nearly any application that requires full-text search, especially cross-platform. Apache Lucene is an open source project available for free download.</p> <p>\u2014  Apache Lucene Homepage</p> <p>JanusGraph supports Apache Lucene as a single-machine, embedded index backend. Lucene has a slightly extended feature set and performs better in small-scale applications compared to Elasticsearch, but is limited to single-machine deployments.</p>"},{"location":"index-backend/lucene/#lucene-embedded-configuration","title":"Lucene Embedded Configuration","text":"<p>For single machine deployments, Lucene runs embedded with JanusGraph. JanusGraph starts and interfaces with Lucene internally.</p> <p>To run Lucene embedded, add the following configuration options to the graph configuration file where <code>/data/searchindex</code> specifies the directory where Lucene should store the index data: <pre><code>index.search.backend=lucene\nindex.search.directory=/data/searchindex\n</code></pre></p> <p>In the above configuration, the index backend is named <code>search</code>. Replace <code>search</code> by a different name to change the name of the index.</p>"},{"location":"index-backend/lucene/#feature-support","title":"Feature Support","text":"<ul> <li>Full-Text: Supports all <code>Text</code> predicates to search for text     properties that matches a given word, prefix or regular expression.</li> <li>Geo: Supports <code>Geo</code> predicates to search for geo properties that     are intersecting, within, or contained in a given query geometry.     Supports points, lines and polygons for indexing. Supports circles     and boxes for querying point properties and all shapes for querying     non-point properties.</li> <li>Numeric Range: Supports all numeric comparisons in <code>Compare</code>.</li> <li>Temporal: Nanosecond granularity temporal indexing.</li> </ul>"},{"location":"index-backend/lucene/#configuration-options","title":"Configuration Options","text":"<p>Refer to Configuration Reference for a complete listing of all Lucene specific configuration options in addition to the general JanusGraph configuration options.</p> <p>Note, that each of the index backend options needs to be prefixed with <code>index.[INDEX-NAME].</code> where <code>[INDEX-NAME]</code> stands for the name of the index backend. For instance, if the index backend is named search then these configuration options need to be prefixed with <code>index.search.</code>. To configure an index backend named search to use Lucene as the index system, set the following configuration option: <pre><code>index.search.backend=lucene\n</code></pre></p>"},{"location":"index-backend/lucene/#further-reading","title":"Further Reading","text":"<ul> <li>Please refer to the Apache Lucene     homepage and available documentation for     more information on Lucene.</li> </ul>"},{"location":"index-backend/search-predicates/","title":"Search Predicates and Data Types","text":"<p>This page lists all of the comparison predicates that JanusGraph supports in global graph search and local traversals.</p>"},{"location":"index-backend/search-predicates/#compare-predicate","title":"Compare Predicate","text":"<p>The <code>Compare</code> enum specifies the following comparison predicates used for index query construction and used in the examples above:</p> <ul> <li><code>eq</code> (equal)</li> <li><code>neq</code> (not equal)</li> <li><code>gt</code> (greater than)</li> <li><code>gte</code> (greater than or equal)</li> <li><code>lt</code> (less than)</li> <li><code>lte</code> (less than or equal)</li> </ul>"},{"location":"index-backend/search-predicates/#text-predicate","title":"Text Predicate","text":"<p>The <code>Text</code> enum specifies the Text Search used to query for matching text or string values.  We differentiate between two types of predicates:</p> <ul> <li>Text search predicates which match against the individual words inside a text string after it has been tokenized. These predicates are not case sensitive.<ul> <li><code>textContains</code>: is true if (at least) one word inside the text string matches the query string</li> <li><code>textContainsPrefix</code>: is true if (at least) one word inside the text string begins with the query string</li> <li><code>textContainsRegex</code>: is true if (at least) one word inside the text string matches the given regular expression</li> <li><code>textContainsFuzzy</code>: is true if (at least) one word inside the text string is similar to the query String (based on Levenshtein edit distance)</li> </ul> </li> <li>String search predicates which match against the entire string value<ul> <li><code>textPrefix</code>: if the string value starts with the given query string</li> <li><code>textRegex</code>: if the string value matches the given regular expression in its entirety</li> <li><code>textFuzzy</code>: if the string value is similar to the given query string (based on Levenshtein edit distance)</li> </ul> </li> </ul> <p>See Text Search for more information about full-text and string search.</p>"},{"location":"index-backend/search-predicates/#geo-predicate","title":"Geo Predicate","text":"<p>The <code>Geo</code> enum specifies geo-location predicates.</p> <ul> <li><code>geoIntersect</code> which holds true if the two geometric objects have at least one point in common (opposite of <code>geoDisjoint</code>).</li> <li><code>geoWithin</code> which holds true if one geometric object contains the other.</li> <li><code>geoDisjoint</code> which holds true if the two geometric objects have no points in common (opposite of <code>geoIntersect</code>).</li> <li><code>geoContains</code> which holds true if one geometric object is contained by the other.</li> </ul> <p>See Geo Mapping for more information about geo search.</p>"},{"location":"index-backend/search-predicates/#query-examples","title":"Query Examples","text":"<p>The following query examples demonstrate some of the predicates on the tutorial graph.</p> <pre><code>// 1) Find vertices with the name \"hercules\"\ng.V().has(\"name\", \"hercules\")\n// 2) Find all vertices with an age greater than 50\ng.V().has(\"age\", gt(50))\n// or find all vertices between 1000 (inclusive) and 5000 (exclusive) years of age and order by increasing age\ng.V().has(\"age\", inside(1000, 5000)).order().by(\"age\", incr)\n// which returns the same result set as the following query but in reverse order\ng.V().has(\"age\", inside(1000, 5000)).order().by(\"age\", decr)\n// 3) Find all edges where the place is at most 50 kilometers from the given latitude-longitude pair\ng.E().has(\"place\", geoWithin(Geoshape.circle(37.97, 23.72, 50)))\n// 4) Find all edges where reason contains the word \"loves\"\ng.E().has(\"reason\", textContains(\"loves\"))\n// or all edges which contain two words (need to chunk into individual words)\ng.E().has(\"reason\", textContains(\"loves\")).has(\"reason\", textContains(\"breezes\"))\n// or all edges which contain words that start with \"lov\"\ng.E().has(\"reason\", textContainsPrefix(\"lov\"))\n// or all edges which contain words that match the regular expression \"br[ez]*s\" in their entirety\ng.E().has(\"reason\", textContainsRegex(\"br[ez]*s\"))\n// or all edges which contain words similar to \"love\"\ng.E().has(\"reason\", textContainsFuzzy(\"love\"))\n// 5) Find all vertices older than a thousand years and named \"saturn\"\ng.V().has(\"age\", gt(1000)).has(\"name\", \"saturn\")\n</code></pre>"},{"location":"index-backend/search-predicates/#data-type-support","title":"Data Type Support","text":"<p>While JanusGraph's composite indexes support any data type that can be stored in JanusGraph, the mixed indexes are limited to the following data types.</p> <ul> <li>Byte</li> <li>Short</li> <li>Integer</li> <li>Long</li> <li>Float</li> <li>Double</li> <li>String</li> <li>Geoshape</li> <li>Date</li> <li>Instant</li> <li>UUID</li> </ul> <p>Additional data types will be supported in the future.</p>"},{"location":"index-backend/search-predicates/#geoshape-data-type","title":"Geoshape Data Type","text":"<p>The Geoshape data type supports representing a point, circle, box, line, polygon, multi-point, multi-line and multi-polygon. Index backends currently support indexing points, circles, boxes, lines, polygons, multi-point, multi-line, multi-polygon and geometry collection. Geospatial index lookups are only supported via mixed indexes.</p> <p>To construct a Geoshape use the following methods:</p> <pre><code> //lat, lng\nGeoshape.point(37.97, 23.72)\n//lat, lng, radius in km\nGeoshape.circle(37.97, 23.72, 50)\n//SW lat, SW lng, NE lat, NE lng\nGeoshape.box(37.97, 23.72, 38.97, 24.72)\n//WKT\nGeoshape.fromWkt(\"POLYGON ((35.4 48.9, 35.6 48.9, 35.6 49.1, 35.4 49.1, 35.4 48.9))\")\n//MultiPoint\nGeoshape.geoshape(Geoshape.getShapeFactory().multiPoint().pointXY(60.0, 60.0).pointXY(120.0, 60.0)\n  .build())\n//MultiLine\nGeoshape.geoshape(Geoshape.getShapeFactory().multiLineString()\n  .add(Geoshape.getShapeFactory().lineString().pointXY(59.0, 60.0).pointXY(61.0, 60.0))\n  .add(Geoshape.getShapeFactory().lineString().pointXY(119.0, 60.0).pointXY(121.0, 60.0)).build())\n//MultiPolygon\nGeoshape.geoshape(Geoshape.getShapeFactory().multiPolygon()\n  .add(Geoshape.getShapeFactory().polygon().pointXY(59.0, 59.0).pointXY(61.0, 59.0)\n    .pointXY(61.0, 61.0).pointXY(59.0, 61.0).pointXY(59.0, 59.0))\n  .add(Geoshape.getShapeFactory().polygon().pointXY(119.0, 59.0).pointXY(121.0, 59.0)\n    .pointXY(121.0, 61.0).pointXY(119.0, 61.0).pointXY(119.0, 59.0)).build())\n//GeometryCollection\nGeoshape.geoshape(Geoshape.getGeometryCollectionBuilder()\n  .add(Geoshape.getShapeFactory().pointXY(60.0, 60.0))\n  .add(Geoshape.getShapeFactory().lineString().pointXY(119.0, 60.0).pointXY(121.0, 60.0).build())\n  .add(Geoshape.getShapeFactory().polygon().pointXY(119.0, 59.0).pointXY(121.0, 59.0)\n    .pointXY(121.0, 61.0).pointXY(119.0, 61.0).pointXY(119.0, 59.0)).build())\n</code></pre> <p>In addition, when importing a graph via GraphSON the geometry may be represented by GeoJSON:</p> stringlistGeoJSON featureGeoJSON geometry <pre><code>\"37.97, 23.72\"\n</code></pre> <pre><code>[37.97, 23.72]\n</code></pre> <pre><code>{\n  \"type\": \"Feature\",\n  \"geometry\": {\n    \"type\": \"Point\",\n    \"coordinates\": [125.6, 10.1]\n  },\n  \"properties\": {\n    \"name\": \"Dinagat Islands\"\n  }\n}\n</code></pre> <pre><code>{\n  \"type\": \"Point\",\n  \"coordinates\": [125.6, 10.1]\n}\n</code></pre> <p>GeoJSON may be specified as Point, Circle, LineString or Polygon. Polygons must be closed. Note that unlike the JanusGraph API GeoJSON specifies coordinates as lng lat.</p>"},{"location":"index-backend/search-predicates/#collections","title":"Collections","text":"<p>If you are using Elasticsearch then you can index properties with SET and LIST cardinality. For instance:</p> <pre><code>mgmt = graph.openManagement()\nnameProperty = mgmt.makePropertyKey(\"names\").dataType(String.class).cardinality(Cardinality.SET).make()\nmgmt.buildIndex(\"search\", Vertex.class).addKey(nameProperty, Mapping.STRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n//Insert a vertex\nperson = graph.addVertex()\nperson.property(\"names\", \"Robert\")\nperson.property(\"names\", \"Bob\")\ngraph.tx().commit()\n//Now query it\ng.V().has(\"names\", \"Bob\").count().next() //1\ng.V().has(\"names\", \"Robert\").count().next() //1\n</code></pre>"},{"location":"index-backend/solr/","title":"Apache Solr","text":"<p>Solr is the popular, blazing fast open source enterprise search platform from the Apache Lucene project. Solr is a standalone enterprise search server with a REST-like API. Solr is highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration and more.</p> <p>\u2014  Apache Solr Homepage</p> <p>JanusGraph supports Apache Solr as an index backend. Here are some of the Solr features supported by JanusGraph:</p> <ul> <li>Full-Text: Supports all <code>Text</code> predicates to search for text     properties that matches a given word, prefix or regular expression.</li> <li>Geo: Supports all <code>Geo</code> predicates to search for geo properties     that are intersecting, within, disjoint to or contained in a given     query geometry. Supports points, lines and polygons for indexing.     Supports circles, boxes and polygons for querying point properties     and all shapes for querying non-point properties.</li> <li>Numeric Range: Supports all numeric comparisons in <code>Compare</code>.</li> <li>TTL: Supports automatically expiring indexed elements.</li> <li>Temporal: Millisecond granularity temporal indexing.</li> <li>Custom Analyzer: Choose to use a custom analyzer</li> </ul> <p>Please see Version Compatibility  for details on what versions of Solr will work with JanusGraph.</p>"},{"location":"index-backend/solr/#solr-configuration-overview","title":"Solr Configuration Overview","text":"<p>JanusGraph supports Solr running in either a SolrCloud or Solr Standalone (HTTP) configuration for use with a mixed index  (see Mixed Index).  The desired connection mode is configured via the parameter <code>mode</code> which must be set to either <code>cloud</code> or <code>http</code>, the former being the default value. For example, to explicitly specify that Solr is running in a SolrCloud configuration the following property is specified as a JanusGraph configuration property:</p> <pre><code>index.search.solr.mode=cloud\n</code></pre> <p>These are some key Solr terms:</p> <ul> <li>Core: A single index on a single machine</li> <li>Configuration: solrconfig.xml, schema.xml, and other files     required to define a core.</li> <li>Collection: A single logical index that can span multiple     cores on different machines.</li> <li>Configset: A shared configuration that can be reused by     multiple cores.</li> </ul>"},{"location":"index-backend/solr/#connecting-to-solrcloud","title":"Connecting to SolrCloud","text":"<p>When connecting to a SolrCloud cluster by setting the <code>mode</code> equal to <code>cloud</code>, the Zookeeper URL (and optionally port) must be specified so that JanusGraph can discover and interact with the Solr cluster.</p> <pre><code>index.search.backend=solr\nindex.search.solr.mode=cloud\nindex.search.solr.zookeeper-url=localhost:2181\n</code></pre> <p>A number of additional configuration options pertaining to the creation of new collections (which is only supported in SolrCloud operation mode) can be configured to control sharding behavior among other things. Refer to the Configuration Reference for a complete listing of those options.</p> <p>SolrCloud leverages Zookeeper to coordinate collection and configset information between the Solr servers. The use of Zookeeper with SolrCloud provides the opportunity to significantly reduce the amount of manual configuration required to use Solr as a back end index for JanusGraph.</p>"},{"location":"index-backend/solr/#configset-configuration","title":"Configset Configuration","text":"<p>A configset is required to create a collection. The configset is stored in Zookeeper to enable access to it across the Solr servers.</p> <ul> <li> <p>Each collection can provide its own configset when it is created, so     that each collection may have a different configuration. With this     approach, each collection must be created manually.</p> </li> <li> <p>A shared configset can be uploaded separately to Zookeeper if it     will be reused by multiple collections. With this approach,     JanusGraph can create collections automatically by using the shared     configset. Another benefit is that reusing a configset significantly     reduces the amount of data stored in Zookeeper.</p> </li> </ul>"},{"location":"index-backend/solr/#using-an-individual-configset","title":"Using an Individual Configset","text":"<p>In this example, a collection named <code>verticesByAge</code> is created manually using the default JanusGraph configuration for Solr that is found in the distribution. When the collection is created, the configuration is uploaded into Zookeeper, using the same collection name <code>verticesByAge</code> for the configset name. Refer to the Solr Reference Guide for available parameters.</p> <pre><code># create the collection\n$SOLR_HOME/bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME/conf/solr\n</code></pre> <p>Define a mixed index using <code>JanusGraphManagement</code> and the same collection name.</p> <pre><code>mgmt = graph.openManagement()\nage = mgmt.makePropertyKey(\"age\").dataType(Integer.class).make()\nmgmt.buildIndex(\"verticesByAge\", Vertex.class).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre>"},{"location":"index-backend/solr/#using-a-shared-configset","title":"Using a Shared Configset","text":"<p>When using a shared configset, it is most convenient to upload the configuration first as a one time operation. In this example, a configset named <code>janusgraph-configset</code> is uploaded in to Zookeeper using the default JanusGraph configuration for Solr that is found in the distribution. Refer to the Solr Reference Guide for available parameters.</p> <pre><code># upload the shared configset into Zookeeper\n# Solr 5\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -z localhost:2181 \\\n    -d $JANUSGRAPH_HOME/conf/solr -n janusgraph-configset\n# Solr 6 and higher\n$SOLR_HOME/bin/solr zk upconfig -d $JANUSGRAPH_HOME/conf/solr -n janusgraph-configset \\\n    -z localhost:2181\n</code></pre> <p>When configuring the SolrCloud indexing backend for JanusGraph, make sure to provide the name of the shared configset using the <code>index.search.solr.configset</code> property.</p> <pre><code>index.search.backend=solr\nindex.search.solr.mode=cloud\nindex.search.solr.zookeeper-url=localhost:2181\nindex.search.solr.configset=janusgraph-configset\n</code></pre> <p>Define a mixed index using <code>JanusGraphManagement</code> and the collection name.</p> <pre><code>mgmt = graph.openManagement()\nage = mgmt.makePropertyKey(\"age\").dataType(Integer.class).make()\nmgmt.buildIndex(\"verticesByAge\", Vertex.class).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre>"},{"location":"index-backend/solr/#connecting-to-solr-standalone-http","title":"Connecting to Solr Standalone (HTTP)","text":"<p>When connecting to Solr Standalone via HTTP by setting the <code>mode</code> equal to <code>http</code>, a single or list of URLs for the Solr instances must be provided.</p> <pre><code>index.search.backend=solr\nindex.search.solr.mode=http\nindex.search.solr.http-urls=http://localhost:8983/solr\n</code></pre> <p>Additional configuration options for controlling the maximum number of connections, connection timeout and transmission compression are available for the HTTP mode. Refer to the Configuration Reference for a complete listing of those options.</p>"},{"location":"index-backend/solr/#core-configuration","title":"Core Configuration","text":"<p>Solr Standalone is used for a single instance, and it keeps configuration information on the file system. A core must be created manually for each mixed index.</p> <p>To create a core, a <code>core_name</code> and a <code>configuration</code> directory is required. Refer to the Solr Reference Guide for available parameters. In this example, a core named <code>verticesByAge</code> is created using the default JanusGraph configuration for Solr that is found in the distribution.</p> <pre><code>$SOLR_HOME/bin/solr create -c verticesByAge -d $JANUSGRAPH_HOME/conf/solr\n</code></pre> <p>Define a mixed index using <code>JanusGraphManagement</code> and the same core name. <pre><code>mgmt = graph.openManagement()\nage = mgmt.makePropertyKey(\"age\").dataType(Integer.class).make()\nmgmt.buildIndex(\"verticesByAge\", Vertex.class).addKey(age).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre></p>"},{"location":"index-backend/solr/#solr-schema-design","title":"Solr Schema Design","text":""},{"location":"index-backend/solr/#dynamic-field-definition","title":"Dynamic Field Definition","text":"<p>By default, JanusGraph uses Solr\u2019s Dynamic Fields feature to define the field types for all indexed keys. This requires no extra configuration when adding property keys to a mixed index backed by Solr and provides better performance than schemaless mode.</p> <p>JanusGraph assumes the following dynamic field tags are defined in the backing Solr collection\u2019s schema.xml file. Please note that there is additional xml definition of the following fields required in a solr schema.xml file in order to use them. Reference the example schema.xml file provided in the <code>./conf/solr/schema.xml</code> directory in a JanusGraph installation for more information.</p> <pre><code>&lt;dynamicField name=\"*_i\"    type=\"int\"          indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_s\"    type=\"string\"       indexed=\"true\"  stored=\"true\" /&gt;\n&lt;dynamicField name=\"*_l\"    type=\"long\"         indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_t\"    type=\"text_general\" indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_b\"    type=\"boolean\"      indexed=\"true\" stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_f\"    type=\"float\"        indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_d\"    type=\"double\"       indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_g\"    type=\"geo\"          indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_dt\"   type=\"date\"         indexed=\"true\"  stored=\"true\"/&gt;\n&lt;dynamicField name=\"*_uuid\" type=\"uuid\"         indexed=\"true\"  stored=\"true\"/&gt;\n</code></pre> <p>In JanusGraph\u2019s default configuration, property key names do not have to end with the type-appropriate suffix to take advantage of Solr\u2019s dynamic field feature. JanusGraph generates the Solr field name from the property key name by encoding the property key definition\u2019s numeric identifier and the type-appropriate suffix. This means that JanusGraph uses synthetic field names with type-appropriate suffixes behind the scenes, regardless of the property key names defined and used by application code using JanusGraph. This field name mapping can be overridden through non-default configuration. That\u2019s described in the next section.</p>"},{"location":"index-backend/solr/#manual-field-definition","title":"Manual Field Definition","text":"<p>If the user would rather manually define the field types for each of the indexed fields in a collection, the configuration option <code>dyn-fields</code> needs to be disabled. It is important that the field for each indexed property key is defined in the backing Solr schema before the property key is added to the index.</p> <p>In this scenario, it is advisable to enable explicit property key name to field mapping in order to fix the field names for their explicit definition. This can be achieved in one of two ways:</p> <ol> <li>Configuring the name of the field by providing a <code>mapped-name</code>     parameter when adding the property key to the index. See     Individual Field Mapping for more information.</li> <li>By enabling the <code>map-name</code> configuration option for the Solr index     which will use the property key name as the field name in Solr. See     Global Field Mapping for more information.</li> </ol>"},{"location":"index-backend/solr/#schemaless-mode","title":"Schemaless Mode","text":"<p>JanusGraph can also interact with a SolrCloud cluster that is configured for schemaless mode. In this scenario, the configuration option <code>dyn-fields</code> should be disabled since Solr will infer the field type from the values and not the field name.</p> <p>Note, however, that schemaless mode is recommended only for prototyping and initial application development and NOT recommended for production use.</p>"},{"location":"index-backend/solr/#troubleshooting","title":"Troubleshooting","text":""},{"location":"index-backend/solr/#collection-does-not-exist","title":"Collection Does Not Exist","text":"<p>The collection (and all of the required configuration files) must be initialized before a defined index can use the collection. See Connecting to SolrCloud for more information.</p> <p>When using SolrCloud, the Zookeeper zkCli.sh command line tool can be used to inspect the configurations loaded into Zookeeper. Also verify that the default JanusGraph configuration files are copied to the correct location under solr and that the directory where the files are copied is correct.</p>"},{"location":"index-backend/solr/#cannot-find-the-specified-configset","title":"Cannot Find the Specified Configset","text":"<p>When using SolrCloud, a configset is required to create a mixed index for JanusGraph. See Configset Configuration for more information.</p> <ul> <li>If using an individual configset, the collection must be created     manually first.</li> <li>If using a shared configset, the configset must be uploaded into     Zookeeper first.</li> </ul> <p>You can verify that the configset and its configuration files are in Zookeeper under <code>/configs</code>. Refer to the Solr Reference Guide for other Zookeeper operations.</p> <pre><code># verify the configset in Zookeeper\n# Solr 5\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd list -z localhost:2181\n# Solr 6 and higher\n$SOLR_HOME/bin/solr zk ls -r /configs/configset-name -z localhost:2181\n</code></pre>"},{"location":"index-backend/solr/#http-error-404","title":"HTTP Error 404","text":"<p>This error may be encountered when using Solr Standalone (HTTP) mode. An example of the error:</p> <pre><code>20:01:22 ERROR org.janusgraph.diskstorage.solr.SolrIndex  - Unable to save documents\nto Solr as one of the shape objects stored were not compatible with Solr.\norg.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server\nat http://localhost:8983/solr: Expected mime type application/octet-stream but got text/html.\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/&gt;\n&lt;title&gt;Error 404 Not Found&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;&lt;h2&gt;HTTP ERROR 404&lt;/h2&gt;\n&lt;p&gt;Problem accessing /solr/verticesByAge/update. Reason:\n&lt;pre&gt;    Not Found&lt;/pre&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <p>Make sure to create the core manually before attempting to store data into the index. See Core Configuration for more information.</p>"},{"location":"index-backend/solr/#invalid-core-or-collection-name","title":"Invalid core or collection name","text":"<p>The core or collection name is an identifier. It must consist entirely of periods, underscores, hyphens, and/or alphanumerics, and also it may not start with a hyphen.</p>"},{"location":"index-backend/solr/#connection-problems","title":"Connection Problems","text":"<p>Irrespective of the operation mode, a Solr instance or a cluster of Solr instances must be running and accessible from the JanusGraph instance(s) in order for JanusGraph to use Solr as an indexing backend. Check that the Solr cluster is running correctly and that it is visible and accessible over the network (or locally) from the JanusGraph instances.</p>"},{"location":"index-backend/solr/#jts-classnotfoundexception-with-geo-data","title":"JTS ClassNotFoundException with Geo Data","text":"<p>Solr relies on Spatial4j for geo processing. Spatial4j declares an optional dependency on JTS (\"JTS Topology Suite\"). JTS is required for some geo field definition and query functionality. If the JTS jar is not on the Solr daemon\u2019s classpath and a field in schema.xml uses a geo type, then Solr may throw a ClassNotFoundException on one of the missing JTS classes. The exception can appear when starting Solr using a schema.xml file designed to work with JanusGraph, but can also appear when invoking <code>CREATE</code> in the Solr CoreAdmin API. The exception appears in slightly different formats on the client and server sides, although the root cause is identical.</p> <p>Here\u2019s a representative example from a Solr server log:</p> <pre><code>ERROR [http-8983-exec-5] 2014-10-07 02:54:06, 665 SolrCoreResourceManager.java (line 344) com/vividsolutions/jts/geom/Geometry\njava.lang.NoClassDefFoundError: com/vividsolutions/jts/geom/Geometry\n    at com.spatial4j.core.context.jts.JtsSpatialContextFactory.newSpatialContext(JtsSpatialContextFactory.java:30)\n    at com.spatial4j.core.context.SpatialContextFactory.makeSpatialContext(SpatialContextFactory.java:83)\n    at org.apache.solr.schema.AbstractSpatialFieldType.init(AbstractSpatialFieldType.java:95)\n    at org.apache.solr.schema.AbstractSpatialPrefixTreeFieldType.init(AbstractSpatialPrefixTreeFieldType.java:43)\n    at org.apache.solr.schema.SpatialRecursivePrefixTreeFieldType.init(SpatialRecursivePrefixTreeFieldType.java:37)\n    at org.apache.solr.schema.FieldType.setArgs(FieldType.java:164)\n    at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:141)\n    at org.apache.solr.schema.FieldTypePluginLoader.init(FieldTypePluginLoader.java:43)\n    at org.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:190)\n    at org.apache.solr.schema.IndexSchema.readSchema(IndexSchema.java:470)\n    at com.datastax.bdp.search.solr.CassandraIndexSchema.readSchema(CassandraIndexSchema.java:72)\n    at org.apache.solr.schema.IndexSchema.&lt;init&gt;(IndexSchema.java:168)\n    at com.datastax.bdp.search.solr.CassandraIndexSchema.&lt;init&gt;(CassandraIndexSchema.java:54)\n    at com.datastax.bdp.search.solr.core.CassandraCoreContainer.create(CassandraCoreContainer.java:210)\n    at com.datastax.bdp.search.solr.core.SolrCoreResourceManager.createCore(SolrCoreResourceManager.java:256)\n    at com.datastax.bdp.search.solr.handler.admin.CassandraCoreAdminHandler.handleCreateAction(CassandraCoreAdminHandler.java:117)\n    ...\n</code></pre> <p>Here\u2019s what normally appears in the output of the client that issued the associated <code>CREATE</code> command to the CoreAdmin API:</p> <pre><code>org.apache.solr.common.SolrException: com/vividsolutions/jts/geom/Geometry\n    at com.datastax.bdp.search.solr.core.SolrCoreResourceManager.createCore(SolrCoreResourceManager.java:345)\n    at com.datastax.bdp.search.solr.handler.admin.CassandraCoreAdminHandler.handleCreateAction(CassandraCoreAdminHandler.java:117)\n    at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody(CoreAdminHandler.java:152)\n    ...\n</code></pre> <p>This is resolved by adding the JTS jar to the classpath of JanusGraph and/or the Solr server. JTS is not included in JanusGraph distributions by default due to its LGPL license. Users must download the JTS jar file separately and copy it into the JanusGraph and/or Solr server lib directory. If using Solr\u2019s built in web server, the JTS jar may be copied to the example/solr-webapp/webapp/WEB-INF/lib directory to include it in the classpath. Solr can be restarted, and the exception should be gone. Solr must be started once with the correct schema.xml file in place first, for the example/solr-webapp/webapp/WEB-INF/lib directory to exist.</p> <p>To determine the ideal JTS version for Solr server, first check the version of Spatial4j in use by the Solr cluster, then determine the version of JTS against which that Spatial4j version was compiled. Spatial4j declares its target JTS version in the pom for the <code>com.spatial4j:spatial4j</code> artifact. Copy the JTS jar to the server/solr-webapp/webapp/WEB-INF/lib directory in your solr installation.</p>"},{"location":"index-backend/solr/#advanced-solr-configuration","title":"Advanced Solr Configuration","text":""},{"location":"index-backend/solr/#dse-search","title":"DSE Search","text":"<p>This section covers installation and configuration of JanusGraph with DataStax Enterprise (DSE) Search. There are multiple ways to install DSE, but this section focuses on DSE\u2019s binary tarball install option on Linux. Most of the steps in this section can be generalized to the other install options for DSE.</p> <p>Install DataStax Enterprise as directed by the page Installing DataStax Enterprise using the binary tarball.</p> <p>Export <code>DSE_HOME</code> and append to <code>PATH</code> in your shell environment. Here\u2019s an example using Bash syntax: <pre><code>export DSE_HOME=/path/to/dse-version.number\nexport PATH=\"$DSE_HOME\"/bin:\"$PATH\"\n</code></pre></p> <p>Install JTS for Solr. The appropriate version varies with the Spatial4j version. As of DSE 4.5.2, the appropriate version is 1.13. <pre><code>cd $DSE_HOME/resources/solr/lib\ncurl -O 'http://central.maven.org/maven2/com/vividsolutions/jts/1.13/jts-1.13.jar'\n</code></pre></p> <p>Start DSE Cassandra and Solr in a single background daemon:</p> <pre><code># The \"dse-data\" path below was chosen to match the\n# \"Installing DataStax Enterprise using the binary tarball\"\n# documentation page from DataStax.  The exact path is not\n# significant.\ndse cassandra -s -Ddse.solr.data.dir=\"$DSE_HOME\"/dse-data/solr\n</code></pre> <p>The previous command will write some startup information to the console and to the logfile path <code>log4j.appender.R.File</code> configured in <code>$DSE_HOME/resources/cassandra/conf/log4j-server.properties</code>.</p> <p>Once DSE with Cassandra and Solr has started normally, check the cluster health with <code>nodetool status</code>. A single-instance ring should show one node with flags *U*p and *N*ormal:</p> <pre><code>nodetool status\nNote: Ownership information does not include topology; for complete information, specify a keyspace\n= Datacenter: Solr\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Owns   Host ID                               Token                                    Rack\nUN  127.0.0.1  99.89 KB   100.0%  5484ef7b-ebce-4560-80f0-cbdcd9e9f496  -7317038863489909889                     rack1\n</code></pre> <p>Next, switch to Gremlin Console and open a JanusGraph database against the DSE instance. This will create JanusGraph\u2019s keyspace and column families.</p> <pre><code>cd $JANUSGRAPH_HOME\nbin/gremlin.sh\n\n         \\,,,/\n         (o o)\n-----oOOo-(3)-oOOo-----\ngremlin&gt; graph = JanusGraphFactory.open('conf/janusgraph-cql-solr.properties')\n==&gt;janusgraph[cql:[127.0.0.1]]\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[janusgraph[cql:[127.0.0.1]], standard]\ngremlin&gt;\n</code></pre> <p>Keep this Gremlin Console open. We\u2019ll take a break now to install a Solr core. Then we\u2019ll come back to this console to load some sample data.</p> <p>Next, upload configuration files for JanusGraph\u2019s Solr collection, then create the core in DSE:</p> <pre><code># Change to the directory where JanusGraph was extracted.  Later commands\n# use relative paths to the Solr config files shipped with the JanusGraph\n# distribution.\ncd $JANUSGRAPH_HOME\n\n# The name must be URL safe and should contain one dot/full-stop\n# character. The part of the name after the dot must not conflict with\n# any of JanusGraph's internal CF names.  Starting the part after the dot\n# \"solr\" will avoid a conflict with JanusGraph's internal CF names.\nCORE_NAME=janusgraph.solr1\n# Where to upload collection configuration and send CoreAdmin requests.\nSOLR_HOST=localhost:8983\n\n# The value of index.[X].solr.http-urls in JanusGraph's config file\n# should match $SOLR_HOST and $CORE_NAME.  For example, given the\n# $CORE_NAME and $SOLR_HOST values above, JanusGraph's config file would\n# contain (assuming \"search\" is the desired index alias):\n#\n# index.search.solr.http-urls=http://localhost:8983/solr/janusgraph.solr1\n#\n# The stock JanusGraph config file conf/janusgraph-cql-solr.properties\n# ships with this http-urls value.\n\n# Upload Solr config files to DSE Search daemon\nfor xml in conf/solr/{solrconfig, schema, elevate}.xml ; do\n    curl -v http://\"$SOLR_HOST\"/solr/resource/\"$CORE_NAME/$xml\" \\\n      --data-binary @\"$xml\" -H 'Content-type:text/xml; charset=utf-8'\ndone\nfor txt in conf/solr/{protwords, stopwords, synonyms}.txt ; do\n    curl -v http://\"$SOLR_HOST\"/solr/resource/\"$CORE_NAME/$txt\" \\\n      --data-binary @\"$txt\" -H 'Content-type:text/plain; charset=utf-8'\ndone\nsleep 5\n\n# Create core using the Solr config files just uploaded above\ncurl \"http://\"$SOLR_HOST\"/solr/admin/cores?action=CREATE&amp;name=$CORE_NAME\"\nsleep 5\n\n# Retrieve and print the status of the core we just created\ncurl \"http://localhost:8983/solr/admin/cores?action=STATUS&amp;core=$CORE_NAME\"\n</code></pre> <p>Now the JanusGraph database and backing Solr core are ready for use. We can test it out with the Graph of the Gods dataset. Picking up the Gremlin Console session started above: <pre><code>// Assuming graph = JanusGraphFactory.open('conf/janusgraph-cql-solr.properties')...\ngremlin&gt; GraphOfTheGodsFactory.load(graph)\n==&gt;null\n</code></pre></p> <p>Now we can run any of the queries described in Getting started. Queries involving text and geo predicates will be served by Solr. For more verbose reporting from JanusGraph and the Solr client, run <code>gremlin.sh -l DEBUG</code> and issue some index-backed queries.</p>"},{"location":"index-backend/text-search/","title":"Index Parameters and Full-Text Search","text":"<p>When defining a mixed index, a list of parameters can be optionally specified for each property key added to the index. These parameters control how the particular key is to be indexed. JanusGraph recognizes the following index parameters. Whether these are supported depends on the configured index backend. A particular index backend might also support custom parameters in addition to the ones listed here.</p>"},{"location":"index-backend/text-search/#full-text-search","title":"Full-Text Search","text":"<p>When indexing string values, that is property keys with <code>String.class</code> data type, one has the choice to either index those as text or character strings which is controlled by the <code>mapping</code> parameter type.</p> <p>When the value is indexed as text, the string is tokenized into a bag of words which allows the user to efficiently query for all matches that contain one or multiple words. This is commonly referred to as full-text search. When the value is indexed as a character string, the string is index \"as-is\" without any further analysis or tokenization. This facilitates queries looking for an exact character sequence match. This is commonly referred to as string search.</p>"},{"location":"index-backend/text-search/#full-text-search_1","title":"Full-Text Search","text":"<p>By default, strings are indexed as text. To make this indexing option explicit, one can define a mapping when indexing a property key as text.</p> <pre><code>mgmt = graph.openManagement()\nsummary = mgmt.makePropertyKey('booksummary').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(summary, Mapping.TEXT.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>This is identical to a standard mixed index definition with the only addition of an extra parameter that specifies the mapping in the index - in this case <code>Mapping.TEXT</code>.</p> <p>When a string property is indexed as text, the string value is tokenized into a bag of tokens. The exact tokenization depends on the indexing backend and its configuration. JanusGraph\u2019s default tokenization splits the string on non-alphanumeric characters and removes any tokens with less than 2 characters. The tokenization used by an indexing backend may differ (e.g. stop words are removed) which can lead to minor differences in how full-text search queries are handled for modifications inside a transaction and committed data in the indexing backend.</p> <p>When a string property is indexed as text, only full-text search predicates are supported in graph queries by the indexing backend. Full-text search is case-insensitive.</p> <ul> <li><code>textContains</code>: is true if (at least) one word inside the text     string matches the query string</li> <li><code>textContainsPrefix</code>: is true if (at least) one word inside the text     string begins with the query string</li> <li><code>textContainsRegex</code>: is true if (at least) one word inside the text     string matches the given regular expression</li> <li><code>textContainsFuzzy</code>: is true if (at least) one word inside the text     string is similar to the query String (based on Levenshtein edit     distance)</li> </ul> <pre><code>import static org.janusgraph.core.attribute.Text.*\ng.V().has('booksummary', textContains('unicorns'))\ng.V().has('booksummary', textContainsPrefix('uni'))\ng.V().has('booksummary', textContainsRegex('.*corn.*'))\ng.V().has('booksummary', textContainsFuzzy('unicorn'))\n</code></pre> <p>String search predicates (see below) may be used in queries, but those require filtering in memory which can be very costly.</p>"},{"location":"index-backend/text-search/#string-search","title":"String Search","text":"<p>To index string properties as character sequences without any analysis or tokenization, specify the mapping as <code>Mapping.STRING</code>:</p> <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('bookname').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(name, Mapping.STRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>When a string mapping is configured, the string value is indexed and can be queried \"as-is\" - including stop words and non-letter characters. However, in this case the query must match the entire string value. Hence, the string mapping is useful when indexing short character sequences that are considered to be one token.</p> <p>When a string property is indexed as string, only the following predicates are supported in graph queries by the indexing backend. String search is case-sensitive.</p> <ul> <li><code>eq</code>: if the string is identical to the query string</li> <li><code>neq</code>: if the string is different than the query string</li> <li><code>textPrefix</code>: if the string value starts with the given query string</li> <li><code>textRegex</code>: if the string value matches the given regular     expression in its entirety</li> <li><code>textFuzzy</code>: if the string value is similar to the given query     string (based on Levenshtein edit distance)</li> </ul> <pre><code>import static org.apache.tinkerpop.gremlin.process.traversal.P.*\nimport static org.janusgraph.core.attribute.Text.*\ng.V().has('bookname', eq('unicorns'))\ng.V().has('bookname', neq('unicorns'))\ng.V().has('bookname', textPrefix('uni'))\ng.V().has('bookname', textRegex('.*corn.*'))\ng.V().has('bookname', textFuzzy('unicorn'))\n</code></pre> <p>Full-text search predicates may be used in queries, but those require filtering in memory which can be very costly.</p>"},{"location":"index-backend/text-search/#full-text-and-string-search","title":"Full text and string search","text":"<p>If you are using Elasticsearch it is possible to index properties as both text and string allowing you to use all of the predicates for exact and fuzzy matching.</p> <pre><code>mgmt = graph.openManagement()\nsummary = mgmt.makePropertyKey('booksummary').dataType(String.class).make()\nmgmt.buildIndex('booksBySummary', Vertex.class).addKey(summary, Mapping.TEXTSTRING.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>Note that the data will be stored in the index twice, once for exact matching and once for fuzzy matching.</p>"},{"location":"index-backend/text-search/#geo-mapping","title":"Geo Mapping","text":"<p>By default, JanusGraph supports indexing geo properties with point type and querying geo properties by circle or box. To index a non-point geo property with support for querying by any geoshape type, specify the mapping as <code>Mapping.PREFIX_TREE</code>:</p> <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('border').dataType(Geoshape.class).make()\nmgmt.buildIndex('borderIndex', Vertex.class).addKey(name, Mapping.PREFIX_TREE.asParameter()).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>Additional parameters can be specified to tune the configuration of the underlying prefix tree mapping. These optional parameters include the number of levels used in the prefix tree as well as the associated precision.</p> <pre><code>mgmt = graph.openManagement()\nname = mgmt.makePropertyKey('border').dataType(Geoshape.class).make()\nmgmt.buildIndex('borderIndex', Vertex.class).addKey(name, Mapping.PREFIX_TREE.asParameter(), Parameter.of(\"index-geo-max-levels\", 18), Parameter.of(\"index-geo-dist-error-pct\", 0.0125)).buildMixedIndex(\"search\")\nmgmt.commit()\n</code></pre> <p>Note that some indexing backends (e.g. Solr) may require additional external schema configuration to support and tune indexing non-point properties.</p>"},{"location":"storage-backend/","title":"Introduction","text":"<p>JanusGraph\u2019s data storage layer is pluggable. Implementations of the pluggable storage layer\u2009\u2014\u2009that is, the software component tells JanusGraph how to talk to its data store\u2009\u2014\u2009are called storage backends.</p> <p>This section describes configuration and administration of JanusGraph\u2019s standard storage backends.</p> <p>This section only addresses configuration of the stock storage backends that come with JanusGraph.</p>"},{"location":"storage-backend/bdb/","title":"Oracle Berkeley DB Java Edition","text":"<p>Oracle Berkeley DB Java Edition is an open source, embeddable, transactional storage engine written entirely in Java. It takes full advantage of the Java environment to simplify development and deployment. The architecture of Oracle Berkeley DB Java Edition supports very high performance and concurrency for both read-intensive and write-intensive workloads.</p> <p>\u2014  Oracle Berkeley DB Java Edition Homepage</p> <p>The Oracle Berkeley DB Java Edition storage backend runs in the same JVM as JanusGraph and provides local persistence on a single machine. Hence, the BerkeleyDB storage backend requires that all of the graph data fits on the local disk and all of the frequently accessed graph elements fit into main memory. This imposes a practical limitation of graphs with 10-100s million vertices on commodity hardware. However, for graphs of that size the BerkeleyDB storage backend exhibits high performance because all data can be accessed locally within the same JVM.</p>"},{"location":"storage-backend/bdb/#berkeleydb-je-setup","title":"BerkeleyDB JE Setup","text":"<p>Since BerkeleyDB runs in the same JVM as JanusGraph, connecting the two only requires a simple configuration and no additional setup: <pre><code>JanusGraph g = JanusGraphFactory.build().\n    set(\"storage.backend\", \"berkeleyje\").\n    set(\"storage.directory\", \"/data/graph\").\nopen();\n</code></pre></p> <p>In the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/bdb/#berkeleydb-specific-configuration","title":"BerkeleyDB Specific Configuration","text":"<p>Refer to Configuration Reference for a complete listing of all BerkeleyDB specific configuration options in addition to the general JanusGraph configuration options.</p> <p>When configuring BerkeleyDB it is recommended to consider the following BerkeleyDB specific configuration options:</p> <ul> <li>transactions: Enables transactions and detects conflicting     database operations. CAUTION: While disabling transactions can     lead to better performance it can cause to inconsistencies and even     corrupt the database if multiple JanusGraph instances interact with     the same instance of BerkeleyDB.</li> <li>cache-percentage: The percentage of JVM heap space (configured     via -Xmx) to be allocated to BerkeleyDB for its cache. Try to give     BerkeleyDB as much space as possible without causing memory problems     for JanusGraph. For instance, if JanusGraph only runs short     transactions, use a value of 80 or higher.</li> </ul>"},{"location":"storage-backend/bdb/#ideal-use-case","title":"Ideal Use Case","text":"<p>The BerkeleyDB storage backend is best suited for small to medium size graphs with up to 100 million vertices on commodity hardware. For graphs of that size, it will likely deliver higher performance than the distributed storage backends. Note, that BerkeleyDB is also limited in the number of concurrent requests it can handle efficiently because it runs on a single machine. Hence, it is not well suited for applications with many concurrent users mutating the graph, even if that graph is small to medium size.</p> <p>Since BerkeleyDB runs in the same JVM as JanusGraph, this storage backend is ideally suited for unit testing of application code using JanusGraph.</p>"},{"location":"storage-backend/bdb/#global-graph-operations","title":"Global Graph Operations","text":"<p>JanusGraph backed by BerkeleyDB supports global graph operations such as iterating over all vertices or edges. However, note that such operations need to scan the entire database which can require a significant amount of time for larger graphs.</p> <p>In order to not run out of memory, it is advised to disable transactions (<code>storage.transactions=false</code>) when iterating over large graphs. Having transactions enabled requires BerkeleyDB to acquire read locks on the data it is reading. When iterating over the entire graph, these read locks can easily require more memory than is available.</p>"},{"location":"storage-backend/bigtable/","title":"Google Cloud Bigtable","text":"<p>Cloud Bigtable is Google\u2019s NoSQL Big Data database service. It\u2019s the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail.</p> <p>Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it\u2019s a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis.</p> <p>\u2014  Google Cloud Bigtable Homepage</p>"},{"location":"storage-backend/bigtable/#bigtable-setup","title":"Bigtable Setup","text":"<p>Bigtable implements the HBase interface for all data access operations, and requires a few configuration options to connect.</p>"},{"location":"storage-backend/bigtable/#connecting-to-bigtable","title":"Connecting to Bigtable","text":"<p>Configuring JanusGraph to connect to Bigtable is achieved by using the <code>hbase</code> backend, along with a custom connection implementation, the project id of the Google Cloud Platform project containing the Bigtable instance, and the Cloud Bigtable instance id you are connecting to.</p> <p>Example: <pre><code>storage.backend=hbase\nstorage.hbase.ext.hbase.client.connection.impl=com.google.cloud.bigtable.hbase1_x.BigtableConnection\nstorage.hbase.ext.google.bigtable.project.id=&lt;Google Cloud Platform project id&gt;\nstorage.hbase.ext.google.bigtable.instance.id=&lt;Bigtable instance id&gt;\n</code></pre></p>"},{"location":"storage-backend/cassandra/","title":"Apache Cassandra","text":"<p>The Apache Cassandra database is the right choice when you need scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data. Cassandra\u2019s support for replicating across multiple datacenters is best-in-class, providing lower latency for your users and the peace of mind of knowing that you can survive regional outages. The largest known Cassandra cluster has over 300 TB of data in over 400 machines.</p> <p>\u2014  Apache Cassandra Homepage</p> <p>The following sections outline the various ways in which JanusGraph can be used in concert with Apache Cassandra.</p>"},{"location":"storage-backend/cassandra/#cassandra-storage-backend","title":"Cassandra Storage Backend","text":"<p>JanusGraph provides the following backends for use with Cassandra:</p> <ul> <li><code>cql</code> - CQL based driver. This is the recommended driver.</li> <li><code>cassandrathrift</code> - JanusGraph\u2019s Thrift connection pool driver</li> <li><code>cassandra</code> - Astyanax     driver. The Astyanax project is     retired.</li> <li><code>embeddedcassandra</code> - Embedded driver for running Cassandra and     JanusGraph within the same JVM</li> </ul> <p>Cassandra has two protocols for clients to use: CQL and Thrift. Thrift was the original interface, however it was deprecated starting with Cassandra 2.1. The core of JanusGraph was originally written before the deprecation of Thrift, and it has several classes that support Thrift. With Cassandra 4.0, Thrift support will be removed in Cassandra. JanusGraph users are recommended to use the <code>cql</code> storage backend.</p> <p>Note</p> <p>If you plan to use a Thrift-based driver and you are using Cassandra 2.2 or higher, you need to explicitly enable Thrift so that JanusGraph can connect to the cluster. Do so by running <code>./bin/nodetool enablethrift</code> on every Cassandra node.</p> <p>Note</p> <p>If security is enabled on Cassandra, the user must have <code>CREATE permission on &lt;all keyspaces&gt;</code>, otherwise the keyspace must be created ahead of time by an administrator including the required tables or the user must have <code>CREATE permission on &lt;the configured keyspace&gt;</code>. The create table file containing the required tables is located in <code>conf/cassandra/cassandraTables.cql</code>. Please define your keyspace before executing it.</p>"},{"location":"storage-backend/cassandra/#local-server-mode","title":"Local Server Mode","text":"<p>Cassandra can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and Cassandra communicate with one another via a <code>localhost</code> socket. Running JanusGraph over Cassandra requires the following setup steps:</p> <ol> <li>Download Cassandra, unpack     it, and set filesystem paths in <code>conf/cassandra.yaml</code> and     <code>conf/log4j-server.properties</code></li> <li>Connecting Gremlin Server to Cassandra using the default     configuration files provided in the pre-packaged distribution     requires that Cassandra Thrift is enabled. To enable Cassandra     Thrift open <code>conf/cassandra.yaml</code> and update <code>start_rpc: false</code> to     <code>start_rpc: true</code>. If Cassandra is already running Thrift can be     started manually with <code>bin/nodetool enablethrift</code>. the Thrift status     can be verified with <code>bin/nodetool</code> statusthrift.</li> <li> <p>Start Cassandra by invoking <code>bin/cassandra -f</code> on the command line     in the directory where Cassandra was unpacked. Read output to check     that Cassandra started successfully.</p> <p>Now, you can create a Cassandra JanusGraph as follows <pre><code>JanusGraph g = JanusGraphFactory.build().\nset(\"storage.backend\", \"cql\").\nset(\"storage.hostname\", \"127.0.0.1\").\nopen();\n</code></pre></p> </li> </ol> <p>In the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/cassandra/#local-container-mode","title":"Local Container Mode","text":"<p>Cassandra does not have a native install for Windows or OSX. One of the easiest ways to run Cassandra on OSX, Windows, or Linux is to use a Docker Container. You can download and run Cassandra with a single Docker command. It is important to install a version that is supported by the version of JanusGraph you intend to use. The compatible versions can be found under the Tested Compatibility section of the specific release on the Releases page. The Cassandra Docker Hub page can be referenced for the available versions and useful commands. In the command below an environment variable is being set to enable Cassandra Thrift with <code>-e CASSANDRA_START_RPC=true</code>. A description of the ports can be found here. Port 9160 is used for the Thrift client API. Port 9042 is for CQL native clients. Ports 7000, 7001 and 7099 are for inter-node communication. Version 3.11 of Cassandra was the latest compatible version for JanusGraph 0.2.0 and is specified in the reference command below.</p> <pre><code>docker run --name jg-cassandra -d -e CASSANDRA_START_RPC=true -p 9160:9160 \\\n  -p 9042:9042 -p 7199:7199 -p 7001:7001 -p 7000:7000 cassandra:3.11\n</code></pre>"},{"location":"storage-backend/cassandra/#remote-server-mode","title":"Remote Server Mode","text":"<p>When the graph needs to scale beyond the confines of a single machine, then Cassandra and JanusGraph are logically separated into different machines. In this model, the Cassandra cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the Cassandra cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph.</p> <p>For example, suppose we have a running Cassandra cluster where one of the machines has the IP address 77.77.77.77, then connecting JanusGraph with the cluster is accomplished as follows (comma separate IP addresses to reference more than one machine): <pre><code>JanusGraph graph = JanusGraphFactory.build().\n  set(\"storage.backend\", \"cql\").\n  set(\"storage.hostname\", \"77.77.77.77\").\n  open();\n</code></pre></p> <p>In the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/cassandra/#remote-server-mode-with-gremlin-server","title":"Remote Server Mode with Gremlin Server","text":"<p>Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph.</p> <p>Start Gremlin Server using <code>bin/gremlin-server.sh</code> and then in an external Gremlin Console session using <code>bin/gremlin.sh</code> you can send Gremlin commands over the wire: <pre><code>:plugin use tinkerpop.server\n:remote connect tinkerpop.server conf/remote.yaml\n:&gt; g.addV()\n</code></pre></p> <p>In this case, each Gremlin Server would be configured to connect to the Cassandra cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server.</p> <pre><code>...\ngraphs: {\n  g: conf/janusgraph-cql.properties\n}\nplugins:\n  - janusgraph.imports\n...\n</code></pre> <p>For more information about Gremlin Server see the Apache TinkerPop documentation</p>"},{"location":"storage-backend/cassandra/#janusgraph-embedded-mode","title":"JanusGraph Embedded Mode","text":"<p>Finally, Cassandra can be embedded in JanusGraph, which means, that JanusGraph and Cassandra run in the same JVM and communicate via in process calls rather than over the network. This removes the (de)serialization and network protocol overhead and can therefore lead to considerable performance improvements. In this deployment mode, JanusGraph internally starts a cassandra daemon and JanusGraph no longer connects to an existing cluster but is its own cluster.</p> <p>To use JanusGraph in embedded mode, simply configure <code>embeddedcassandra</code> as the storage backend. The configuration options listed below also apply to embedded Cassandra. In creating a JanusGraph cluster, ensure that the individual nodes can discover each other via the Gossip protocol, so setup a JanusGraph-with-Cassandra-embedded cluster much like you would a stand alone Cassandra cluster. When running JanusGraph in embedded mode, the Cassandra yaml file is configured using the additional configuration option <code>storage.conf-file</code>, which specifies the yaml file as a full url, e.g. <code>storage.conf-file = file:///home/cassandra.yaml</code>.</p> <p>When running a cluster with JanusGraph and Cassandra embedded, it is advisable to expose JanusGraph through the Gremlin Server so that applications can remotely connect to the JanusGraph graph database and execute queries.</p> <p>Note, that running JanusGraph with Cassandra embedded requires GC tuning. While embedded Cassandra can provide lower latency query answering, its GC behavior under load is less predictable.</p>"},{"location":"storage-backend/cassandra/#cassandra-specific-configuration","title":"Cassandra Specific Configuration","text":"<p>Refer to Configuration Reference for a complete listing of all Cassandra specific configuration options in addition to the general JanusGraph configuration options.</p> <p>When configuring Cassandra it is recommended to consider the following Cassandra specific configuration options:</p> <ul> <li>read-consistency-level: Cassandra consistency level for read     operations</li> <li>write-consistency-level: Cassandra consistency level for write     operations</li> <li>replication-factor: The replication factor to use. The higher     the replication factor, the more robust the graph database is to     machine failure at the expense of data duplication. The default     value should be overwritten for production system to ensure     robustness. A value of 3 is recommended. This replication factor     can only be set when the keyspace is initially created. On an     existing keyspace, this value is ignored.</li> <li>thrift.frame_size_mb: The maximum frame size to be used by     thrift for transport. Increase this value when retrieving very large     result sets. Only applicable when     storage.backend=cassandrathrift</li> <li>keyspace: The name of the keyspace to store the JanusGraph graph     in. Allows multiple JanusGraph graphs to co-exist in the same     Cassandra cluster.</li> </ul> <p>For more information on Cassandra consistency levels and acceptable values, please refer to the Cassandra Thrift API. In general, higher levels are more consistent and robust but have higher latency.</p>"},{"location":"storage-backend/cassandra/#global-graph-operations","title":"Global Graph Operations","text":"<p>JanusGraph over Cassandra supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause <code>OutOfMemoryException</code>. Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively.</p>"},{"location":"storage-backend/cassandra/#deploying-on-amazon-ec2","title":"Deploying on Amazon EC2","text":"<p>Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers.</p> <p>\u2014  Amazon EC2</p> <p>Follow these steps to setup a Cassandra cluster on EC2 and deploy JanusGraph over Cassandra. To follow these instructions, you need an Amazon AWS account with established authentication credentials and some basic knowledge of AWS and EC2.</p>"},{"location":"storage-backend/cassandra/#setup-cassandra-cluster","title":"Setup Cassandra Cluster","text":"<p>These instructions for configuring and launching the DataStax Cassandra Community Edition AMI are based on the DataStax AMI Docs and focus on aspects relevant for a JanusGraph deployment.</p>"},{"location":"storage-backend/cassandra/#setting-up-security-group","title":"Setting up Security Group","text":"<ul> <li>Navigate to the EC2 Console Dashboard, then click on \"Security     Groups\" under \"Network &amp; Security\".</li> <li>Create a new security group. Click Inbound. Set the \"Create a new     rule\" dropdown menu to \"Custom TCP rule\". Add a rule for port 22     from source 0.0.0.0/0. Add a rule for ports 1024-65535 from the     security group members. If you don\u2019t want to open all unprivileged     ports among security group members, then at least open 7000, 7199,     and 9160 among security group members. Tip: the \"Source\" dropdown     will autocomplete security group identifiers once \"sg\" is typed in     the box, so you needn\u2019t have the exact value ready beforehand.</li> </ul>"},{"location":"storage-backend/cassandra/#launch-datastax-cassandra-ami","title":"Launch DataStax Cassandra AMI","text":"<ul> <li>\"Launch the DataStax AMI     in your desired zone</li> <li>On the Instance Details page of the Request Instances Wizard, set     \"Number of Instances\" to your desired number of Cassandra nodes. Set     \"Instance Type\" to at least m1.large. We recommend m1.large.</li> <li> <p>On the Advanced Instance Options page of the Request Instances     Wizard, set the \"as text\" radio button under \"User Data\", then fill     this into the text box: <pre><code>--clustername [cassandra-cluster-name]\n--totalnodes [number-of-instances]\n--version community\n--opscenter no\n</code></pre> [number-of-instances] in this configuration must match the number of EC2 instances configured on the previous wizard page. [cassandra-cluster-name] can be any string used for identification. For example: <pre><code>--clustername janusgraph\n--totalnodes 4\n--version community\n--opscenter no\n</code></pre></p> </li> <li> <p>On the Tags page of the Request Instances Wizard you can apply any     desired configurations. These tags exist only at the EC2     administrative level and have no effect on the Cassandra daemons'     configuration or operation.</p> </li> <li>On the Create Key Pair page of the Request Instances Wizard, either     select an existing key pair or create a new one. The PEM file     containing the private half of the selected key pair will be     required to connect to these instances.</li> <li>On the Configure Firewall page of the Request Instances Wizard,     select the security group created earlier.</li> <li>Review and launch instances on the final wizard page.</li> </ul>"},{"location":"storage-backend/cassandra/#verify-successful-instance-launch","title":"Verify Successful Instance Launch","text":"<ul> <li>SSH into any Cassandra instance node:     <code>ssh -i [your-private-key].pem ubuntu@[public-dns-name-of-any-cassandra-instance]</code></li> <li>Run the Cassandra nodetool <code>nodetool -h 127.0.0.1 ring</code> to inspect     the state of the Cassandra token ring. You should see as many nodes     in this command\u2019s output as instances launched in the previous     steps.</li> </ul> <p>Note, that the AMI takes a few minutes to configure each instance. A shell prompt will appear upon successful configuration when you SSH into the instance.</p>"},{"location":"storage-backend/cassandra/#launch-janusgraph-instances","title":"Launch JanusGraph Instances","text":"<p>Launch additional EC2 instances to run JanusGraph which are either configured in Remote Server Mode or Remote Server Mode with Gremlin-Server as described above. You only need to note the IP address of one of the Cassandra cluster instances and configure it as the host name. The particular EC2 instance to run and the particular configuration depends on your use case.</p>"},{"location":"storage-backend/cassandra/#example-janusgraph-instance-on-amazon-linux-ami","title":"Example JanusGraph Instance on Amazon Linux AMI","text":"<ul> <li>Launch the Amazon Linux AMI in the same zone of the     Cassandra cluster. Choose your desired EC2 instance type depending     on the amount of resources you need. Use the default configuration     options and select the same Key Pair and Security Group as for the     Cassandra cluster configured in the previous step.</li> <li>SSH into the newly created instance via     <code>ssh -i [your-private-key].pem ec2-user@[public-dns-name-of-the-instance]</code>.     You may have to wait a little for the instance to launch.</li> <li>Download the     current JanusGraph distribution with <code>wget</code> and unpack the archive     locally to the home directory. Start the Gremlin Console to verify     that JanusGraph runs successfully. For more information on how to     unpack JanusGraph and start the Gremlin Console, please refer to the     Getting Started guide</li> <li>Create a configuration file with <code>vi janusgraph.properties</code> and add     the following lines:: <pre><code>storage.backend = cql\nstorage.hostname = [IP-address-of-one-Cassandra-EC2-instance]\n</code></pre></li> </ul> <p>You may add additional configuration options found on this page or in Configuration Reference.</p> <ul> <li>Start the Gremlin Console again and type the following:: <pre><code>gremlin&gt; graph = JanusGraphFactory.open('janusgraph.properties')\n==&gt;janusgraph[cql:[IP-address-of-one-Cassandra-EC2-instance]]\n</code></pre></li> </ul> <p>Note</p> <p>You have successfully connected this JanusGraph instance to the Cassandra cluster and can start to operate on the graph.</p>"},{"location":"storage-backend/cassandra/#connect-to-cassandra-cluster-in-ec2-from-outside-ec2","title":"Connect to Cassandra cluster in EC2 from outside EC2","text":"<p>Opening the usual Cassandra ports (9160, 7000, 7199) in the security group is not enough, because the Cassandra nodes by default broadcast their ec2-internal IPs, and not their public-facing IPs.</p> <p>The resulting behavior is that you can open a JanusGraph graph on the cluster by connecting to port 9160 on any Cassandra node, but all requests to that graph time out. This is because Cassandra is telling the client to connect to an unreachable IP.</p> <p>To fix this, set the \"broadcast-address\" property for each instance in /etc/cassandra/cassandra.yaml to its public-facing IP, and restart the instance. Do this for all nodes in the cluster. Once the cluster comes back, nodetool reports the correct public-facing IPs to which connections from the local machine are allowed.</p> <p>Changing the \"broadcast-address\" property allows you to connect to the cluster from outside ec2, but it might also mean that traffic originating within ec2 will have to round-trip to the internet and back before it gets to the cluster. So, this approach is only useful for development and testing.</p>"},{"location":"storage-backend/hbase/","title":"Apache HBase","text":"<p>Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google\u2019s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.</p> <p>\u2014  Apache HBase Homepage</p>"},{"location":"storage-backend/hbase/#hbase-setup","title":"HBase Setup","text":"<p>The following sections outline the various ways in which JanusGraph can be used in concert with Apache HBase.</p>"},{"location":"storage-backend/hbase/#local-server-mode","title":"Local Server Mode","text":"<p>HBase can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and HBase communicate with one another via a <code>localhost</code> socket. Running JanusGraph over HBase requires the following setup steps:</p> <ul> <li> <p>Download and extract a stable HBase from     http://www.apache.org/dyn/closer.cgi/hbase/stable/.</p> </li> <li> <p>Start HBase by invoking the <code>start-hbase.sh</code> script in the bin     directory inside the extracted HBase directory. To stop HBase, use     <code>stop-hbase.sh</code>. <pre><code>$ ./bin/start-hbase.sh\nstarting master, logging to ../logs/hbase-master-machine-name.local.out\n</code></pre></p> </li> </ul> <p>Now, you can create an HBase JanusGraph as follows: <pre><code>JanusGraph graph = JanusGraphFactory.build()\n    .set(\"storage.backend\", \"hbase\")\n    .open();\n</code></pre></p> <p>Note, that you do not need to specify a hostname since a localhost connection is attempted by default. Also, in the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/hbase/#remote-server-mode","title":"Remote Server Mode","text":"<p>When the graph needs to scale beyond the confines of a single machine, then HBase and JanusGraph are logically separated into different machines. In this model, the HBase cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the HBase cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph.</p> <p>For example, suppose we have a running HBase cluster with a ZooKeeper quorum composed of three machines at IP address 77.77.77.77, 77.77.77.78, and 77.77.77.79, then connecting JanusGraph with the cluster is accomplished as follows: <pre><code>JanusGraph g = JanusGraphFactory.build()\n    .set(\"storage.backend\", \"hbase\")\n    .set(\"storage.hostname\", \"77.77.77.77, 77.77.77.78, 77.77.77.79\")\n    .open();\n</code></pre></p> <p><code>storage.hostname</code> accepts a comma separated list of IP addresses and hostname for any subset of machines in the HBase cluster JanusGraph should connect to. Also, in the Gremlin Console, you can not define the type of the variables <code>conf</code> and <code>g</code>. Therefore, simply leave off the type declaration.</p>"},{"location":"storage-backend/hbase/#remote-server-mode-with-gremlin-server","title":"Remote Server Mode with Gremlin Server","text":"<p>Finally, Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph.</p> <pre><code>http://gremlin-server.janusgraph.machine1/mygraph/vertices/1\nhttp://gremlin-server.janusgraph.machine2/mygraph/tp/gremlin?script=g.v(1).out('follows').out('created')\n</code></pre> <p>In this case, each Gremlin Server would be configured to connect to the HBase cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to JanusGraph Server for a complete example and more information on how to configure the server.</p> <pre><code>...\ngraphs: {\n  g: conf/janusgraph-hbase.properties\n}\nplugins:\n - janusgraph.imports\n...\n</code></pre>"},{"location":"storage-backend/hbase/#hbase-specific-configuration","title":"HBase Specific Configuration","text":"<p>Refer to Configuration Reference for a complete listing of all HBase specific configuration options in addition to the general JanusGraph configuration options.</p> <p>When configuring HBase it is recommended to consider the following HBase specific configuration options:</p> <ul> <li>storage.hbase.table: Name of the HBase table in which to store     the JanusGraph graph. Allows multiple JanusGraph graphs to co-exist     in the same HBase cluster.</li> </ul> <p>Please refer to the HBase configuration documentation for more HBase configuration options and their description. By prefixing the respective HBase configuration option with <code>storage.hbase.ext</code> in the JanusGraph configuration it will be passed on to HBase at initialization time. For example, to use the znode /hbase-secure for HBase, set the property: <code>storage.hbase.ext.zookeeper.znode.parent=/hbase-secure</code>. The prefix allows arbitrary HBase configuration options to be configured through JanusGraph.</p> <p>Important</p> <p>HBase backend uses millisecond for timestamps. In JanusGraph 0.2.0 and earlier, if the <code>graph.timestamps</code> property is not explicitly set, the default is <code>MICRO</code>. In this case, the <code>graph.timestamps</code> property must be explicitly set to <code>MILLI</code>. Do not set the <code>graph.timestamps</code> property to another value in any cases.</p>"},{"location":"storage-backend/hbase/#global-graph-operations","title":"Global Graph Operations","text":"<p>JanusGraph over HBase supports global vertex and edge iteration. However, note that all these vertices and/or edges will be loaded into memory which can cause <code>OutOfMemoryException</code>. Use JanusGraph with TinkerPop\u2019s Hadoop-Gremlin to iterate over all vertices or edges in large graphs effectively.</p>"},{"location":"storage-backend/hbase/#deploying-on-amazon-ec2","title":"Deploying on Amazon EC2","text":"<p>Amazon EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. - by Amazon EC2</p> <p>Follow these steps to setup an HBase cluster on EC2 and deploy JanusGraph over HBase. To follow these instructions, you need an Amazon AWS account with established authentication credentials and some basic knowledge of AWS and EC2.</p> <p>The following commands first launch a four-node HBase cluster on EC2 via Whirr, then run a basic JanusGraph test case using the cluster.</p> <p>The configuration described below puts one HBase master server in charge of three HBase regionservers.  The master will be the sole member of the Zookeeper quorum by which JanusGraph connects to HBase.</p> <p>Whirr 0.7.1 sometimes fails when run on a machine behind a NAT WHIRR-459.  For this reason, it's recommended to use at least Whirr 0.7.2.  Whirr 0.8.0 was used to test the following commands on a t1.micro instance running Amazon Linux 2012.03.  These commands might need tweaking to produce the intended results on environments besides a t1.micro instance running Amazon Linux 2012.03.</p> <pre><code># These commands were executed on a t1.micro instance running Amazon Linux 2012.03 x86_64.\n# The AMI identifier for Amazon Linux 2012.03 x86_64 is ami-aecd60c7.\n# https://console.aws.amazon.com/ec2/home?region=us-east-1#launchAmi=ami-aecd60c7\nexport AWS_ACCESS_KEY_ID=... # Set your Access Key here\nexport AWS_SECRET_ACCESS_KEY=... # Set your Secret Key here\ncurl -O http://www.apache.org/dist/whirr/whirr-0.8.0/whirr-0.8.0.tar.gz\ntar -xzf whirr-0.8.0.tar.gz &amp;&amp; cd whirr-0.8.0\n# Generate an SSH keypair with which Whirr will deploy and manage instances\nssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa_whirr\n# Download a Whirr recipe for deploying HBase 0.94.1 with hadoop-core 1.0.3\npushd recipes &amp;&amp; wget 'https://raw.github.com/JanusGraph/janusgraph/master/config/whirr-hbase.properties' ; popd\nbin/whirr launch-cluster --config recipes/whirr-hbase.properties --private-key-file ~/.ssh/id_rsa_whirr\n# Run a superficial health check on the hbase-master node (this should print \"imok\")\necho \"ruok\" | nc $(awk '{print $3}' ~/.whirr/hbase-testing/instances | head -1) 2181; echo\n# Login to the HBase master node to run the remaining commands\nssh -i ~/.ssh/id_rsa_whirr -o \"UserKnownHostsFile /dev/null\" \\\n      -o StrictHostKeyChecking=no \\\n      `grep hbase-master ~/.whirr/hbase-testing/instances \\\n      | awk '{print $3}'`\n# Maven 2 is available through the package manager, but an incompatibility\n# with surefire 2.12 makes it a pain to use; here we download Maven 3 without\n# the OS package manager\nwget 'http://archive.apache.org/dist/maven/maven-3/3.0.4/binaries/apache-maven-3.0.4-bin.tar.gz'\ntar -xzf apache-maven-3.0.4-bin.tar.gz\n# Install git\nsudo apt-get install -y git-core\n# Clone JanusGraph\ngit clone 'git://github.com/JanusGraph/janusgraph.git' &amp;&amp; cd janusgraph\n# Run a HBase-backed test of JanusGraph\n#\n# This test should produce pages of output ending in something like this:\n#\n# -------------------------------------------------------\n#  T E S T S\n# -------------------------------------------------------\n# Running org.janusgraph.graphdb.hbase.ExternalHBaseGraphPerformanceTest\n# Starting trial 1/1\n# 10000\n# 20000\n# 30000\n# 40000\n# 50000\n# Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 303.659 sec\n#\n# Results :\n#\n# Tests run: 1, Failures: 0, Errors: 0, Skipped: 0\n#\n# [INFO] ------------------------------------------------------------------------\n# [INFO] BUILD SUCCESS\n# [INFO] ------------------------------------------------------------------------\n~/apache-maven-3.0.4/bin/mvn test -Dtest=ExternalHBaseGraphPerformanceTest#unlabeledEdgeInsertion\n# Check on hadoop\nhadoop version # Should print 1.0.3\n# List the hadoop root; should print something like:\n#\n# Found 4 items\n# drwxr-xr-x   - hadoop supergroup          0 2012-09-20 00:20 /hadoop\n# drwxr-xr-x   - hadoop supergroup          0 2012-09-20 00:42 /hbase\n# drwxrwxrwx   - hadoop supergroup          0 2012-09-20 00:20 /tmp\n# drwxrwxrwx   - hadoop supergroup          0 2012-09-20 00:20 /user\nhadoop fs -ls /\n</code></pre>"},{"location":"storage-backend/hbase/#tips-and-tricks-for-managing-an-hbase-cluster","title":"Tips and Tricks for Managing an HBase Cluster","text":"<p>The HBase shell on the master server can be used to get an overall status check of the cluster. <pre><code>$HBASE_HOME/bin/hbase shell\n</code></pre></p> <p>From the shell, the following commands are generally useful for understanding the status of the cluster.</p> <pre><code>status 'janusgraph'\nstatus 'simple'\nstatus 'detailed'\n</code></pre> <p>The above commands can identify if a region server has gone down. If so, it is possible to <code>ssh</code> into the failed region server machines and do the following:</p> <pre><code>sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh stop regionserver\nsudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh start regionserver\n</code></pre> <p>The use of pssh can make this process easy as there is no need to log into each machine individually to run the commands. Put the IP addresses of the regionservers into a <code>hosts.txt</code> file and then execute the following.</p> <pre><code>pssh -h host.txt sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh stop regionserver\npssh -h host.txt sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh start regionserver\n</code></pre> <p>Next, sometimes you need to restart the master server (e.g. connection refused exceptions). To do so, on the master execute the following:</p> <pre><code>sudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh stop master\nsudo -u hadoop $HBASE_HOME/bin/hbase-daemon.sh start master\n</code></pre> <p>Finally, if an HBase cluster has already been deployed and more memory is required of the master or region servers, simply edit the <code>$HBASE_HOME/conf/hbase-env.sh</code> files on the respective machines with requisite <code>-Xmx -Xms</code> parameters. Once edited, stop/start the master and/or region servers as described previous.</p>"},{"location":"storage-backend/inmemorybackend/","title":"InMemory Storage Backend","text":"<p>JanusGraph ships with an in-memory storage backend which can be used through the following configuration:</p> <pre><code>storage.backend=inmemory\n</code></pre> <p>Alternatively, an in-memory JanusGraph graph can be opened directly in the Gremlin Console:</p> <pre><code>graph = JanusGraphFactory.build().set('storage.backend', 'inmemory').open()\n</code></pre> <p>There are no additional configuration options for the in-memory storage backend. As the name suggests, this backend holds all data in memory. Shutting down the graph or terminating the process that hosts the JanusGraph graph will irrevocably delete all data from the graph. This backend is local to a particular JanusGraph graph instance and cannot be shared across multiple JanusGraph graphs.</p>"},{"location":"storage-backend/inmemorybackend/#ideal-use-case","title":"Ideal Use Case","text":"<p>The in-memory storage backend was primarily developed to simplify testing (for those tests that do not require persistence) and graph exploration. The in-memory storage backend is NOT meant for production use, large graphs, or high performance use cases. The in-memory storage backend is not performance or memory optimized. All data is stored in the heap space allocated to the Java virtual machine.</p>"}]}